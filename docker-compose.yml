services:
  hft-base:
    build: .
    image: ${HFT_IMAGE:-hft-platform:latest}
    restart: always
    # Common Environment for all HFT services
    environment:
      - SHIOAJI_API_KEY=${SHIOAJI_API_KEY}
      - SHIOAJI_SECRET_KEY=${SHIOAJI_SECRET_KEY}
      - SHIOAJI_PERSON_ID=${SHIOAJI_PERSON_ID}
      - SHIOAJI_ACCOUNT=${SHIOAJI_ACCOUNT}
      - HFT_MODE=${HFT_MODE:-sim}
      - HFT_EVENT_MODE=${HFT_EVENT_MODE:-event}
      - HFT_SYMBOLS=${HFT_SYMBOLS:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - SYMBOLS_CONFIG=${SYMBOLS_CONFIG:-config/base/symbols.yaml}
      - HFT_EVENT_MODE=${HFT_EVENT_MODE:-event}
      - HFT_CLICKHOUSE_HOST=clickhouse
      - HFT_CLICKHOUSE_PORT=${HFT_CLICKHOUSE_PORT:-8123}
      - HFT_CLICKHOUSE_ENABLED=1
      - HFT_RECONNECT_DAYS=${HFT_RECONNECT_DAYS:-}
      - HFT_RECONNECT_HOURS=${HFT_RECONNECT_HOURS:-}
      - HFT_RECONNECT_HOURS_2=${HFT_RECONNECT_HOURS_2:-}
      - HFT_RECONNECT_TZ=${HFT_RECONNECT_TZ:-Asia/Taipei}
      - TZ=${TZ:-Asia/Taipei}
      - HFT_PROM_PORT=${HFT_PROM_PORT:-9090}
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      - ./.wal:/app/.wal
      - ./data:/app/data
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./src:/app/src
    ulimits:
      memlock: -1
      nofile: 65535
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import psutil; exit(0)'"]
      interval: 30s
      timeout: 10s
      retries: 3

  hft-engine:
    extends: hft-base
    container_name: hft-engine
    command: ["python", "-m", "hft_platform.main"]
    # Network mode handling
    # Removed default network_mode: ${HFT_NETWORK_MODE:-bridge} to allow proper DNS resolution
    # network_mode: ${HFT_NETWORK_MODE:-}
    ports:
      - "${HFT_PROM_PORT:-9090}:9090"
    volumes:
      # Hugepages mount (only works if mapped on host)
      - /dev/hugepages:/dev/hugepages
    deploy:
      resources:
        limits:
          cpus: '${HFT_CPU_LIMIT:-0.50}'
          memory: ${HFT_MEM_LIMIT:-1500M}

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD:?CLICKHOUSE_PASSWORD required}
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ports:
      - "8123:8123"
      - "9000:9000"
    ulimits:
      nofile: 262144
    volumes:
      # Always inject storage config (it handles default paths gracefully)
      - ./config/clickhouse_storage.xml:/etc/clickhouse-server/config.d/storage.xml
      # Configurable Data Root (Tiered Storage)
      - ${CH_DATA_HOT:-ch_data_hot}:/var/lib/clickhouse/data/hot
      - ${CH_DATA_COLD:-ch_data_cold}:/var/lib/clickhouse/data/cold
      # Metadata volume
      - ch_metadata:/var/lib/clickhouse
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10" 

  redis:
    image: redis:7
    container_name: redis
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD:?REDIS_PASSWORD required}"]
    ports:
      - "6379:6379"
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  wal-loader:
    extends: hft-base
    container_name: wal-loader
    command: ["python", "-m", "hft_platform.recorder.loader"]
    depends_on:
      - clickhouse
 
  hft-monitor:
    extends: hft-base
    container_name: hft-monitor
    command: ["python", "scripts/monitor_runtime_health.py"]
    depends_on:
      - hft-engine
      - clickhouse
    environment:
      - HFT_MONITOR_METRICS_URL=http://hft-engine:9090/metrics
      - HFT_CLICKHOUSE_HOST=clickhouse
      - HFT_CLICKHOUSE_PORT=${HFT_CLICKHOUSE_PORT:-8123}
      - HFT_MONITOR_INTERVAL_S=${HFT_MONITOR_INTERVAL_S:-10}
      - HFT_MONITOR_MAX_INGEST_LAG_S=${HFT_MONITOR_MAX_INGEST_LAG_S:-5}
      - HFT_MONITOR_FUTURE_TOLERANCE_S=${HFT_MONITOR_FUTURE_TOLERANCE_S:-60}
      - HFT_MONITOR_FEED_GAP_WARN_S=${HFT_MONITOR_FEED_GAP_WARN_S:-5}
      - HFT_MONITOR_WAL_RATE_WARN=${HFT_MONITOR_WAL_RATE_WARN:-1}
      - HFT_MONITOR_INGEST_RATE_MIN=${HFT_MONITOR_INGEST_RATE_MIN:-1}

  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./config/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/monitoring/alerts/rules.yaml:/etc/prometheus/alerts/rules.yaml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.path=/prometheus"
    depends_on:
      - hft-engine
      - alertmanager
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/monitoring/alerts/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  grafana:
    image: grafana/grafana:10.4.1
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD required}
    volumes:
      - ./config/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./config/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/monitoring/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

volumes:
  ch_data_hot:
  ch_data_cold:
  ch_metadata:
  grafana_data:
