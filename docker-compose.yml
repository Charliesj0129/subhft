services:
  hft-base:
    build: .
    image: ${HFT_IMAGE:-hft-platform:latest}
    restart: always
    # Common Environment for all HFT services
    environment:
      - SHIOAJI_API_KEY=${SHIOAJI_API_KEY}
      - SHIOAJI_SECRET_KEY=${SHIOAJI_SECRET_KEY}
      - SHIOAJI_PERSON_ID=${SHIOAJI_PERSON_ID}
      - SHIOAJI_PASSWORD=${SHIOAJI_PASSWORD}
      - SHIOAJI_ACCOUNT=${SHIOAJI_ACCOUNT}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - SYMBOLS_CONFIG=${SYMBOLS_CONFIG:-config/base/symbols.yaml}
      - HFT_CLICKHOUSE_HOST=clickhouse
      - HFT_CLICKHOUSE_ENABLED=1
      - HFT_PROM_PORT=${HFT_PROM_PORT:-9090}
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      - ./.wal:/app/.wal
      - ./data:/app/data
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./src:/app/src
    ulimits:
      memlock: -1
      nofile: 65535
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import psutil; exit(0)'"]
      interval: 30s
      timeout: 10s
      retries: 3

  hft-engine:
    extends: hft-base
    container_name: hft-engine
    command: ["python", "-m", "hft_platform.main"]
    # Switch to 'host' mode for low latency (set HFT_NETWORK_MODE=host in .env or via ops.sh)
    network_mode: ${HFT_NETWORK_MODE:-bridge}
    ports:
      - "${HFT_PROM_PORT:-9090}:9090"
    volumes:
      # Hugepages mount (only works if mapped on host)
      - /dev/hugepages:/dev/hugepages
    deploy:
      resources:
        limits:
          cpus: '${HFT_CPU_LIMIT:-0.50}'
          memory: ${HFT_MEM_LIMIT:-1500M}

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ports:
      - "8123:8123"
      - "9000:9000"
    ulimits:
      nofile: 262144
    volumes:
      # Always inject storage config (it handles default paths gracefully)
      - ./config/clickhouse_storage.xml:/etc/clickhouse-server/config.d/storage.xml
      # Configurable Data Root (Tiered Storage)
      - ${CH_DATA_HOT:-ch_data_hot}:/var/lib/clickhouse/data/hot
      - ${CH_DATA_COLD:-ch_data_cold}:/var/lib/clickhouse/data/cold
      # Metadata volume
      - ch_metadata:/var/lib/clickhouse 

  wal-loader:
    extends: hft-base
    container_name: wal-loader
    command: ["python", "-m", "hft_platform.recorder.loader"]
    depends_on:
      - clickhouse

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./config/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/monitoring/alerts/rules.yaml:/etc/prometheus/alerts/rules.yaml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.path=/prometheus"
    depends_on:
      - hft-engine
      - alertmanager

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/monitoring/alerts/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - ./config/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./config/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/monitoring/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana

volumes:
  ch_data_hot:
  ch_data_cold:
  ch_metadata:
  grafana_data:
