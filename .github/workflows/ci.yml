name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      run_recorder_drills:
        description: "Run recorder ClickHouse + WAL stress drills"
        required: false
        default: false
        type: boolean
  schedule:
    - cron: "30 18 * * *" # Daily UTC nightly perf drill

env:
  PYTHONPATH: src
  UV_CACHE_DIR: ~/.cache/uv

jobs:
  # ============================================================================
  # Stage 1: Lint & Format
  # ============================================================================
  lint:
    name: ðŸ” Lint & Format
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --dev

      - name: Check formatting
        run: uv run ruff format --check src/ tests/

      - name: Check linting
        run: uv run ruff check src/ tests/

  # ============================================================================
  # Stage 2a: Rust Build & Lint
  # ============================================================================
  rust:
    name: Rust Build & Lint
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            rust_core/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust_core/Cargo.lock', 'rust_core/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Check formatting
        run: cargo fmt --manifest-path rust_core/Cargo.toml -- --check

      - name: Run clippy
        run: cargo clippy --manifest-path rust_core/Cargo.toml -- -D warnings

      - name: Run tests
        run: cargo test --manifest-path rust_core/Cargo.toml

  # ============================================================================
  # Stage 2b: Type Check
  # ============================================================================
  typecheck:
    name: ðŸ”¬ Type Check
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --dev

      - name: Run mypy
        run: uv run mypy

  # ============================================================================
  # Stage 3: Unit Tests + Coverage Gates
  # ============================================================================
  test:
    name: ðŸ§ª Tests & Coverage
    runs-on: ubuntu-latest
    needs: [typecheck, rust]
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            rust_core/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust_core/Cargo.lock', 'rust_core/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install dependencies
        run: uv sync --dev

      - name: Build Rust extension
        run: uv run maturin develop --manifest-path rust_core/Cargo.toml

      - name: Run unit tests with coverage
        run: |
          uv run pytest tests/unit \
            --cov=src/hft_platform \
            --cov-branch \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-fail-under=70 \
            -v --tb=short

      - name: Check branch coverage
        run: |
          uv run coverage report --fail-under=55 --show-missing

      - name: Upload coverage to GitHub Summary
        if: always()
        run: |
          echo "## ðŸ“Š Coverage Report" >> $GITHUB_STEP_SUMMARY
          uv run coverage report --format=markdown >> $GITHUB_STEP_SUMMARY || true

      - name: Upload coverage artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-report
          path: coverage.xml

  # ============================================================================
  # Stage 4: Benchmark (Darwin Gate)
  # ============================================================================
  benchmark:
    name: âš¡ Benchmark (Darwin Gate)
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            rust_core/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust_core/Cargo.lock', 'rust_core/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install dependencies
        run: uv sync --dev

      - name: Build Rust extension
        run: uv run maturin develop --manifest-path rust_core/Cargo.toml

      - name: Run lightweight perf regression gate (strategy/research/feed/recorder)
        run: uv run python tests/benchmark/perf_regression_gate.py

      - name: Run benchmarks
        run: |
          set -euo pipefail
          uv run pytest tests/benchmark \
            -o addopts= \
            --no-cov \
            --benchmark-only \
            --benchmark-json=benchmark.json \
            --benchmark-save=ci \
            --benchmark-save-data \
            --benchmark-min-rounds=5 \
            --benchmark-disable-gc \
            -v || status=$?
          if [ "${status:-0}" -eq 5 ]; then
            echo "No benchmark tests collected; skipping benchmarks."
            exit 0
          fi
          if [ "${status:-0}" -ne 0 ]; then
            exit "$status"
          fi

      - name: Darwin Gate - Regression Check
        if: always() && hashFiles('benchmark.json') != ''
        run: |
          uv run python scripts/benchmark_gate.py \
            --baseline tests/benchmark/.benchmark_baseline.json \
            --current benchmark.json --threshold 0.10

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: 90
          if-no-files-found: ignore

      - name: Benchmark Summary
        if: always()
        run: |
          echo "## âš¡ Benchmark Results" >> $GITHUB_STEP_SUMMARY
          if [ -f benchmark.json ]; then
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat benchmark.json | head -100 >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No benchmark results found." >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================================================
  # Stage 5: Integration Tests
  # ============================================================================
  integration:
    name: ðŸ”— Integration Tests
    runs-on: ubuntu-latest
    needs: test
    services:
      clickhouse:
        image: clickhouse/clickhouse-server:latest
        ports:
          - 8123:8123
          - 9000:9000
        options: >-
          --health-cmd "clickhouse-client --query 'SELECT 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            rust_core/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust_core/Cargo.lock', 'rust_core/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install dependencies
        run: uv sync --dev

      - name: Build Rust extension
        run: uv run maturin develop --manifest-path rust_core/Cargo.toml

      - name: Run integration tests
        env:
          CLICKHOUSE_HOST: localhost
          CLICKHOUSE_PORT: 8123
          REDIS_HOST: localhost
          REDIS_PORT: 6379
        run: |
          set -euo pipefail
          uv run pytest tests/integration \
            -o addopts= \
            -v --tb=short \
            -m "not slow" || status=$?
          if [ "${status:-0}" -eq 5 ]; then
            echo "No integration tests collected; skipping integration tests."
            exit 0
          fi
          if [ "${status:-0}" -ne 0 ]; then
            exit "$status"
          fi

      - name: Integration Test Summary
        if: always()
        run: |
          echo "## ðŸ”— Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "ClickHouse and Redis services were available for testing." >> $GITHUB_STEP_SUMMARY

      - name: Run ClickHouse writer system smoke
        env:
          HFT_CLICKHOUSE_HOST: localhost
          HFT_CLICKHOUSE_PORT: 8123
          HFT_CLICKHOUSE_ENABLED: "1"
        run: |
          uv run pytest tests/system/test_clickhouse_writer.py \
            -o addopts= \
            --no-cov \
            -q

  recorder-nightly-drills:
    name: ðŸ§ª Recorder CK + WAL Drills
    runs-on: ubuntu-latest
    needs: test
    if: >
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && inputs.run_recorder_drills)
    services:
      clickhouse:
        image: clickhouse/clickhouse-server:latest
        ports:
          - 8123:8123
          - 9000:9000
        options: >-
          --health-cmd "clickhouse-client --query 'SELECT 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            rust_core/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust_core/Cargo.lock', 'rust_core/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install dependencies
        run: uv sync --dev

      - name: Build Rust extension
        run: uv run maturin develop --manifest-path rust_core/Cargo.toml

      - name: Run recorder perf gate (CK + WAL stress drills)
        env:
          HFT_PERF_GATE_RECORDER_IO: "1"
          HFT_CLICKHOUSE_ENABLED: "1"
          HFT_CLICKHOUSE_HOST: localhost
          HFT_CLICKHOUSE_PORT: 8123
        run: uv run python tests/benchmark/perf_regression_gate.py --runs 1 --include-recorder-io

      - name: Recorder Drill Summary
        if: always()
        run: |
          echo "## ðŸ§ª Recorder Nightly Drills" >> $GITHUB_STEP_SUMMARY
          echo "Executed perf_regression_gate with real ClickHouse + WAL stress drills." >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # Stage 6: Security Scanning
  # ============================================================================
  security:
    name: ðŸ”’ Security Scan
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.12

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ${{ env.UV_CACHE_DIR }}
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --dev

      - name: Install pip-audit
        run: uv pip install pip-audit

      - name: Run pip-audit
        run: |
          uv run pip-audit --strict --progress-spinner off 2>&1 | tee audit-output.txt || true
          if grep -q "No known vulnerabilities found" audit-output.txt; then
            echo "âœ… No vulnerabilities found"
          elif grep -q "found [0-9]* known vulnerabilities" audit-output.txt; then
            echo "::warning::Security vulnerabilities detected - see output above"
          fi

      - name: Security Summary
        if: always()
        run: |
          echo "## ðŸ”’ Security Scan Results" >> $GITHUB_STEP_SUMMARY
          if [ -f audit-output.txt ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat audit-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================================================
  # Stage 7: PR Review Gate (PRs only)
  # ============================================================================
  pr-review-gate:
    name: PR Design Review Gate
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Check HFT design review for feat/perf PRs
        env:
          PR_TITLE: ${{ github.event.pull_request.title }}
          PR_BODY: ${{ github.event.pull_request.body }}
        run: |
          # Only enforce for feat: and perf: PRs
          if echo "$PR_TITLE" | grep -qiE '^(feat|perf)(\(.+\))?:'; then
            echo "feat/perf PR detected â€” checking design review sections..."
            MISSING=0
            for section in "Allocation Audit" "Latency Budget" "Threading Model" "Data Layout" "Failure Mode"; do
              if echo "$PR_BODY" | grep -q "### $section"; then
                echo "  [OK] $section"
              else
                echo "  [MISSING] $section"
                MISSING=$((MISSING + 1))
              fi
            done
            if [ "$MISSING" -gt 0 ]; then
              echo ""
              echo "::error::PR is missing $MISSING HFT design review section(s)."
              echo "Please use the PR template and fill in all 5 review sections."
              exit 1
            fi
            echo "All design review sections present."
          else
            echo "Not a feat/perf PR â€” skipping design review check."
          fi
