# Tasks – Async Recorder & ClickHouse Cold Path

| ID | Title | Description & Acceptance | Dependencies |
| --- | --- | --- | --- |
| T1 | Finalize ClickHouse schemas | Add/verify DDLs in `schemas/` for `market_data`, `orders`, `fills`, `positions_intraday`, `account_state`, `risk_decisions`, `guardrail_transitions`, optional `system_events`, including partitioning/TTL/ORDER BY. **Acceptance**: CH migrations apply cleanly; schemas reviewed with analytics/risk. | — |
| T2 | Implement event intake & normalization | Build recorder subscribers for each event class, map hot-path structs to table rows, and apply market data dedup/sampling logic. **Acceptance**: unit tests validate mapping; sampling reduces noise according to config. | T1 |
| T3 | Build batcher/flush pipeline | Create per-table buffers with configurable size/time thresholds; implement ClickHouse insert routines with compression and retry. **Acceptance**: integration test writes batches to test CH and meets latency targets. | T1, T2 |
| T4 | Develop WAL & replay (critical tables) | Add WAL writer for orders/fills/positions/account/risk tables when CH fails or queues near capacity; implement replay worker. **Acceptance**: simulated outage writes to WAL, data replayed on recovery without loss; metrics/logs recorded. | T2, T3 |
| T5 | Sampling/degradation controls | Implement configurable policies for market_data and other best-effort tables (drop oldest, increase sampling) when queues high; add alerts/logs for data drop. **Acceptance**: stress test triggers degradation without blocking hot path; warnings emitted. | T2, T3 |
| T6 | Config & CLI tooling | Provide config files for batch sizes, flush intervals, sampling policies, WAL paths, ClickHouse creds; add CLI commands to show queue depth, WAL size, force flush/replay. **Acceptance**: CLI used in tests to inspect state and trigger replay. | T3, T4 |
| T7 | Observability integration | Export recorder metrics (queue usage, insert latency, WAL size, dropped rows) and structured logs per observability spec; add Grafana panels/alerts for recorder health. **Acceptance**: metrics visible in `/metrics`, dashboards updated, alerts fire in simulated failures. | T3–T6 |
| T8 | Retention & partition maintenance | Implement jobs/scripts to enforce TTLs (drop old `market_data`, etc.) and optional aggregation/downsampling pipelines. **Acceptance**: test environment shows partitions dropped/aggregated per policy without impacting current data. | T1, T3 |
| T9 | End-to-end & failure testing | Run load tests at ≥25k rows/s and outage simulations to validate throughput, WAL durability, and degrade behavior. Document results and update runbooks. **Acceptance**: tests demonstrate meeting throughput/latency targets and safe recovery after outages. | T2–T8 |
