version: "3.9"

x-hft-env: &hft-env
  SHIOAJI_API_KEY: ${SHIOAJI_API_KEY}
  SHIOAJI_SECRET_KEY: ${SHIOAJI_SECRET_KEY}
  SHIOAJI_PERSON_ID: ${SHIOAJI_PERSON_ID}
  SHIOAJI_ACCOUNT: ${SHIOAJI_ACCOUNT}
  HFT_MODE: ${HFT_MODE:-sim}
  HFT_ORDER_MODE: ${HFT_ORDER_MODE:-sim}
  HFT_ORDER_NO_CA: ${HFT_ORDER_NO_CA:-1}
  HFT_QUOTE_VERSION: ${HFT_QUOTE_VERSION:-auto}
  HFT_QUOTE_NO_DATA_S: ${HFT_QUOTE_NO_DATA_S:-30}
  HFT_QUOTE_WATCHDOG_S: ${HFT_QUOTE_WATCHDOG_S:-5}
  HFT_QUOTE_FLAP_WINDOW_S: ${HFT_QUOTE_FLAP_WINDOW_S:-60}
  HFT_QUOTE_FLAP_THRESHOLD: ${HFT_QUOTE_FLAP_THRESHOLD:-5}
  HFT_QUOTE_FLAP_COOLDOWN_S: ${HFT_QUOTE_FLAP_COOLDOWN_S:-300}
  HFT_QUOTE_FORCE_RELOGIN_S: ${HFT_QUOTE_FORCE_RELOGIN_S:-15}
  HFT_EVENT_MODE: ${HFT_EVENT_MODE:-event}
  HFT_SYMBOLS: ${HFT_SYMBOLS:-}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  SYMBOLS_CONFIG: ${SYMBOLS_CONFIG:-config/base/symbols.yaml}
  HFT_CLICKHOUSE_HOST: clickhouse
  HFT_CLICKHOUSE_PORT: ${HFT_CLICKHOUSE_PORT:-8123}
  HFT_CLICKHOUSE_ENABLED: "1"
  CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-changeme}
  HFT_RECONNECT_DAYS: ${HFT_RECONNECT_DAYS:-}
  HFT_RECONNECT_HOURS: ${HFT_RECONNECT_HOURS:-}
  HFT_RECONNECT_HOURS_2: ${HFT_RECONNECT_HOURS_2:-}
  HFT_RECONNECT_TZ: ${HFT_RECONNECT_TZ:-Asia/Taipei}
  TZ: ${TZ:-Asia/Taipei}
  HFT_PROM_PORT: ${HFT_PROM_PORT:-9090}
  PYTHONDONTWRITEBYTECODE: "1"

x-hft-volumes: &hft-volumes
  - ./.wal:/app/.wal
  - ./data:/app/data
  - ./config:/app/config
  - ./scripts:/app/scripts
  - ./src:/app/src

x-hft-common: &hft-common
  image: ${HFT_IMAGE:-hft-platform:latest}
  environment: *hft-env
  volumes: *hft-volumes
  ulimits:
    memlock: -1
    nofile: 65535
  healthcheck:
    test: ["CMD-SHELL", "python -c 'import psutil; exit(0)'"]
    interval: 30s
    timeout: 10s
    retries: 3
  networks:
    - hft-net
  deploy:
    replicas: 1
    restart_policy:
      condition: on-failure
      delay: 5s
      max_attempts: 3
      window: 120s

services:
  hft-engine:
    <<: *hft-common
    command: ["python", "-m", "hft_platform.main"]
    ports:
      - target: 9090
        published: ${HFT_PROM_PORT:-9090}
        protocol: tcp
        mode: ingress
    volumes:
      - ./.wal:/app/.wal
      - ./data:/app/data
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./src:/app/src
      - /dev/hugepages:/dev/hugepages
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: "${HFT_CPU_LIMIT:-0.50}"
          memory: ${HFT_MEM_LIMIT:-1500M}

  clickhouse:
    image: clickhouse/clickhouse-server:25.12.3
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-changeme}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    ports:
      - target: 8123
        published: 8123
        protocol: tcp
        mode: ingress
      - target: 9000
        published: 9000
        protocol: tcp
        mode: ingress
    ulimits:
      nofile: 262144
    volumes:
      - ./config/clickhouse_storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ${CH_DATA_HOT:-ch_data_hot}:/var/lib/clickhouse/data/hot
      - ${CH_DATA_COLD:-ch_data_cold}:/var/lib/clickhouse/data/cold
      - ch_metadata:/var/lib/clickhouse
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: 100m
        max-file: "10"
    networks:
      - hft-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: "2.0"
          memory: 4G

  redis:
    image: redis:7
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD:-changeme}"]
    ports:
      - target: 6379
        published: 6379
        protocol: tcp
        mode: ingress
    logging:
      driver: json-file
      options:
        max-size: 50m
        max-file: "5"
    networks:
      - hft-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: "0.50"
          memory: 512M

  wal-loader:
    <<: *hft-common
    command: ["python", "-m", "hft_platform.recorder.loader"]

  hft-monitor:
    <<: *hft-common
    command: ["python", "scripts/monitor_runtime_health.py"]
    environment:
      <<: *hft-env
      HFT_MONITOR_METRICS_URL: http://hft-engine:9090/metrics
      HFT_CLICKHOUSE_HOST: clickhouse
      HFT_CLICKHOUSE_PORT: ${HFT_CLICKHOUSE_PORT:-8123}
      HFT_CLICKHOUSE_USER: ${HFT_CLICKHOUSE_USER:-default}
      HFT_CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-changeme}
      HFT_MONITOR_INTERVAL_S: ${HFT_MONITOR_INTERVAL_S:-10}
      HFT_MONITOR_MAX_INGEST_LAG_S: ${HFT_MONITOR_MAX_INGEST_LAG_S:-5}
      HFT_MONITOR_FUTURE_TOLERANCE_S: ${HFT_MONITOR_FUTURE_TOLERANCE_S:-60}
      HFT_MONITOR_FEED_GAP_WARN_S: ${HFT_MONITOR_FEED_GAP_WARN_S:-5}
      HFT_MONITOR_WAL_RATE_WARN: ${HFT_MONITOR_WAL_RATE_WARN:-1}
      HFT_MONITOR_INGEST_RATE_MIN: ${HFT_MONITOR_INGEST_RATE_MIN:-1}

  prometheus:
    image: prom/prometheus:v2.51.0
    ports:
      - target: 9090
        published: 9091
        protocol: tcp
        mode: ingress
    volumes:
      - ./config/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/monitoring/alerts/rules.yaml:/etc/prometheus/alerts/rules.yaml:ro
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.retention.time=7d
      - --storage.tsdb.path=/prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: 50m
        max-file: "5"
    networks:
      - hft-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.50"
          memory: 1G

  alertmanager:
    image: prom/alertmanager:v0.27.0
    ports:
      - target: 9093
        published: 9093
        protocol: tcp
        mode: ingress
    volumes:
      - ./config/monitoring/alerts/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: 50m
        max-file: "5"
    networks:
      - hft-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.25"
          memory: 256M

  grafana:
    image: grafana/grafana:10.4.1
    ports:
      - target: 3000
        published: 3000
        protocol: tcp
        mode: ingress
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-changeme}
    volumes:
      - ./config/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./config/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/monitoring/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: 50m
        max-file: "5"
    networks:
      - hft-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.50"
          memory: 512M

volumes:
  ch_data_hot:
  ch_data_cold:
  ch_metadata:
  grafana_data:

networks:
  hft-net:
    driver: overlay
    attachable: true
