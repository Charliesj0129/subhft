1


### ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling

Zhaoqi Su, Shihai Chen, Xinyan Lin, Liqin Huang, _Member, IEEE,_ Zhipeng Su, and Xiaoqiang Lu, _Senior_
_Member, IEEE_



_**Abstract**_ **‚ÄîMulti-modal scene reconstruction integrating RGB**
**and thermal infrared data is essential for robust environmental**
**perception across diverse lighting and weather conditions. How-**
**ever, extending 3D Gaussian Splatting (3DGS) to multi-spectral**
**scenarios remains challenging. Current approaches often struggle**
**to fully leverage the complementary information of multi-modal**
**data, typically relying on mechanisms that either tend to neglect**
**cross-modal correlations or leverage shared representations that**
**fail to adaptively handle the complex structural correlations and**
**physical discrepancies between spectrums. To address these limi-**
**tations, we propose ThermoSplat, a novel framework that enables**
**deep spectral-aware reconstruction through active feature mod-**
**ulation and adaptive geometry decoupling. First, we introduce**
**a Cross-Modal FiLM Modulation mechanism that dynamically**
**conditions shared latent features on thermal structural priors,**
**effectively guiding visible texture synthesis with reliable cross-**
**modal geometric cues. Second, to accommodate modality-specific**
**geometric inconsistencies, we propose a Modality-Adaptive Geo-**
**metric Decoupling scheme that learns independent opacity offsets**
**and executes an independent rasterization pass for the thermal**
**branch. Additionally, a hybrid rendering pipeline is employed**
**to integrate explicit Spherical Harmonics with implicit neural**
**decoding, ensuring both semantic consistency and high-frequency**
**detail preservation. Extensive experiments on the RGBT-Scenes**
**dataset demonstrate that ThermoSplat achieves state-of-the-art**
**rendering quality across both visible and thermal spectrums.**


_**Index Terms**_ **‚Äî3D Gaussian Splatting, RGBT scene reconstruc-**
**tion, multi-modal fusion, neural rendering, feature modulation.**


I. INTRODUCTION

D scene reconstruction has been widely used in the field
of autonomous systems, remote sensing, surveillance, etc.
# **3**
Traditional RGB-based reconstruction methods, while achieving high fidelity in most conditions, often suffer from performance degradation in challenging environments, e.g., lowlight conditions, dense smoke, or darkness. To address these
limitations, multi-modal reconstruction, especially those integrating RGB and thermal, has emerged as a critical research
direction. Unlike RGB sensors, which depend on reflected
light, thermal sensors capture long-wave infrared radiation
emitted by objects, allowing for the acquisition of stable
structural information and heat signatures that are inherently


Zhaoqi Su, Shihai Chen, Xinyan Lin, Liqin Huang, Zhipeng Su, and
Xiaoqiang Lu are with the College of Physics and Information Engineering,
Fuzhou University, Fuzhou 350108, China.
E-mail: _{_ suzhaoqi, 2501120132, 2501127081, hlq, szp01, luxiaoqiang _}_ @fzu.edu.cn
Corresponding author: Zhipeng Su, Xiaoqiang Lu
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.



invariant to illumination changes. This provides a reliable
reference for scene geometry under extreme conditions.
The evolution of neural implicit representations has inspired
research in multi-modal 3D reconstruction. Previous studies [1], [2] extended the Neural Radiance Fields (NeRF) [3]
framework to the infrared spectrum, demonstrating the potential to synthesize thermal views from multi-view observations. However, NeRF-based methods often suffer from high
computational cost and slow inference speeds, limiting their
abilities in real-time applications. Recently, the emergence of
3D Gaussian Splatting (3DGS) [4] has enabled significantly
faster training and rendering, with improved rendering quality.
Building on this, several multi-spectral 3DGS frameworks
have been proposed. Current state-of-the-art methods can be
categorized into two paradigms: either explicitly decomposing
3DGS into modality-specific components to handle property
disparities [5], or integrating multi-spectral information into a
unified latent space for MLP-based decoding [6], [7]. However, achieving an optimal balance between shared geometry
and modality-specific appearance remains non-trivial. The
former paradigm often introduces increased model complexity
and may face challenges in maintaining cross-modal spatial
consistency, while the latter tends to have a limited capacity
to precisely model the inherent physical discrepancies and
structural variations present across different spectrums.
To bridge these gaps, we present ThermoSplat, a novel
cross-modal 3DGS framework that enables deep spectralaware reconstruction through active feature modulation and
adaptive geometry decoupling. Unlike existing methods that
either rely on explicit decomposition or unified latent representation, ThermoSplat introduces a FiLM-based [8] feature
modulation mechanism. This module dynamically conditions
the shared latent representation on thermal structural priors,
enabling the model to actively leverage structural infrared
features to guide visible texture synthesis. Furthermore, to
accommodate the inherent physical discrepancies across different spectral bands, we propose a modality-adaptive geometric
decoupling scheme, which allows for independent geometric
adjustments in the infrared spectrum, effectively resolving
artifacts in regions where transparency or reflectivity varies.
Finally, to overcome the detail-loss inherent in pure featurebased decoding, we employ a hybrid rendering pipeline, which
integrates explicit Spherical Harmonics (SH) with implicit
decoding, achieving high-frequency RGB details while maintaining consistent semantic information across modalities.
Experimental results demonstrate that ThermoSplat achieves
state-of-the-art rendering quality across both visible and thermal spectrums. The main contributions of this work are


summarized as follows:


_‚Ä¢_ **Cross-Modal FiLM Modulation:** We design a Featurewise Linear Modulation (FiLM) framework to establish
deep feature dependencies. By utilizing structural priors
to modulate shared latent features, our method enhances
texture recovery and cross-modal alignment.

_‚Ä¢_ **Modality-Adaptive Geometric Decoupling:** We introduce a learnable thermal opacity offset and execute an
independent rasterization pass that decouples geometric
representations between visible and infrared spectrums.
This mechanism effectively resolves depth and occlusion
misalignments caused by modality-inconsistent physical
properties.

_‚Ä¢_ **Hybrid Explicit-Implicit Rendering:** We propose a
hybrid rendering pipeline that integrates explicit Spherical
Harmonics (SH) with feature-modulated neural decoding.
This architecture preserves high-frequency RGB details
while maintaining consistent low-frequency semantic information across different modalities.


II. RELATED WORK


_A. 3D Neural Scene Representation_


Recent methods in 3D neural scene representation have
shifted from implicit to explicit methods. The implicit NeRFbased methods [3], [9]‚Äì[11] represent the scene as a continuous function in 3D space formulated by a shallow network
like MLP, achieving view-dependent and photo-realistic scene
rendering results. However, these methods suffer from timeconsuming training and rendering, limiting their practical
use in real-time applications. To address this, 3D Gaussian
Splatting (3DGS)-based methods [4], [12]‚Äì[14] propose an
explicit scene representation paradigm, which leverages 3D
Gaussian primitives for explicitly representing the geometric
and texture information of the scene, enabling high-fidelity
and real-time rendering through a differentiable tile-based
rasterization pipeline. Some studies [15]‚Äì[17] leverage the
idea of both feature-based decoding in NeRF and 3DGS
representations to augment the representation capability by
distilling high-dimensional latent features into each Gaussian
primitive. By integrating these latent features with lightweight
decoders, these methods can bypass the limitations of traditional Spherical Harmonics (SH), enabling more complex
attribute modeling and cross-modal information interaction.


_B. Neural Thermal and RGBT Scene Reconstruction_


Compared to visible light, thermal infrared signals possess
distinct physical properties, such as being insensitive to lighting conditions and capable of reflecting the heat distribution
of objects. Early attempts mostly extend NeRF-based representations for representing different modalities in a compact
manner [1], [2], [18]‚Äì[20]. However, due to the volume
rendering process in NeRF [3] and its reliance on dense
sampling, these implicit methods often face challenges in
precisely modeling the high-frequency details. In recent years,
the emergence of 3DGS has shifted the focus toward explicit
Gaussian-based RGBT (RGB + Thermal) scene modeling.
ThermalGaussian [5] pioneered the extension of 3DGS to the



2


RGBT scene, which optimizes the thermal Gaussian by finetuning the pretrained RGB Gaussians and incorporates thermal
priors for better scene modeling. Also, it releases the RGBTScenes dataset to facilitate benchmarking for multi-modal
reconstruction tasks. MS-Splatting [6] formulates the multispectral 3D scene using a unified latent space for decoding
both RGB and other spectral channels, which is also applied
to agricultural NDVI tasks. MS-Splattingv2 [21] uses the
optimized joint strategy with RGB initialization to improve
rendering quality. MMOne [7] introduces a unified framework
that represents multiple modalities, such as RGB, thermal, and
language, within a single scene, which designs a multimodal
decomposition mechanism for better learning properties of
different modalities. Ma et al. [22] decomposes appearance
into reflectance and thermal radiance, leveraging the thermal
modality as a stable geometric prior to rectify distorted surfaces in low-light RGB inputs. Beyond general multimodal
representation, several studies focus on reconstructing thermal
infrared signals to tackle ill-posed problems or extreme environmental constraints. Some studies [23], [24] inject physicsbased temperature or thermodynamics constraints into thermal
3DGS modeling. Veta-GS [25] introduces a view-dependent
deformation field to capture the subtle thermal variations
caused by emissivity and transmission effects, effectively
reducing artifacts in infrared novel-view synthesis. Others
extend the RGBT modeling into more-spectral or hyperspectral
scenarios [26], [27]. Despite these advances, existing RGBT
frameworks either treat different modalities as independent
signals with limited feature interaction, or rely on a shared
representation that tends to overlook modality-specific physical discrepancies. These limitations motivate us to explore a
more flexible modulation and decoupled modeling for RGBT
scene reconstruction.


III. METHOD


_A. Overview_


The overall architecture of ThermoSplat is designed to
achieve high-fidelity multi-modal scene reconstruction by addressing spectral-varying properties. As illustrated in Fig. 1,
we represent the scene using multi-modal feature-enhanced
3DGS [4]. The pipeline first performs active feature interaction
via a **Cross-Modal FiLM Modulation** on the rasterized
latent representations, which utilizes thermal structural priors
to guide visible texture synthesis. To account for geometric
inconsistencies across spectrums, we introduce a **modality-**
**adaptive geometric decoupling** scheme, which uses the
learnable offset ‚àÜ _tŒ±_ and executes an independent rasterization
pass to accommodate modality-specific geometries. Finally, a
**hybrid rendering** strategy is employed to combine explicit
Spherical Harmonics (SH) with implicit feature-decoded outputs for preserving high-frequency details and view-dependent
effects in the visible spectrum.
The remainder of this section provides a detailed formalization of our framework. We first briefly introduce 3D Gaussian
Splatting and feature-based rasterization in Section III-B. In
Section III-C, we describe the cross-modal feature modulation mechanism, which enables active spectral interaction.


3



**Multi-modal Gaussians**

ùùÅ/ùö∫, ùíá, ùú∂, ùö´ùíïùú∂, ùíÑùíîùíâ


ùùÅ/ùö∫ ùíá ùú∂


ùùÅ/ùö∫ ùíÑùíîùíâ ùú∂


ùùÅ/ùö∫ ùíá ùú∂ + ùúüùíïùú∂



Shared
Pixel-wise

MLP



**RGB Branch**


**Cross-Modal FiLM Modulation**



Shared
Pixel-wise

MLP



Film Module

( ùõæ‚ãÖùíá+ ùõΩ )


Thermal

Prior
Head



Spherical Harmonics Gaussian Rasterizer


**Thermal Branch**



RGB
Head


Thermal

Head



Thermal

Prior
Head



**:** _Shared Weights_


Fig. 1. Overview of the proposed ThermoSplat framework. Given multi-spectral inputs, our method optimizes 3D Gaussian primitives with decoupled
properties. (a) Cross-Modal FiLM Modulation dynamically conditions shared latent features on thermal structural priors to guide visible texture synthesis.
(b) Modality-Adaptive Geometric Decoupling resolves geometric inconsistencies between visible and infrared spectrums. (c) The Hybrid Rendering pipeline
integrates explicit Spherical Harmonics (SH) with implicit neural decoding, ensuring high-frequency detail preservation and cross-modal semantic consistency.



Section III-D presents the Multi-spectral Hybrid Rendering
pipeline, where we first detail the modality-adaptive geometric
decoupling for modal-specific geometries, followed by the
hybrid rendering strategy for RGB synthesis to preserve highfrequency details.


_B. Preliminaries_


3D Gaussian Splatting (3DGS) [4] represents a 3D scene
as a collection of _N_ Gaussian primitives. Each Gaussian is
characterized by its center position _¬µ ‚àà_ R [3], an anisotropic
covariance Œ£ = _RSS_ _[T]_ _R_ _[T]_, and an opacity value _Œ±_ . The
influence of a Gaussian at a 3D point _**x**_ _[‚Ä≤]_ is defined as:

_G_ ( _**x**_ _[‚Ä≤]_ ; _**¬µ**_ _i,_ Œ£ _i_ ) = _e_ _[‚àí]_ [1] 2 [(] _**[x]**_ _[‚Ä≤][‚àí]_ _**[¬µ]**_ _[i]_ [)] _[T]_ [ Œ£] _i_ _[‚àí]_ [1] ( _**x**_ _[‚Ä≤]_ _‚àí_ _**¬µ**_ _i_ ) _,_ (1)


where _**x**_ _[‚Ä≤]_ denotes the 3D point in the camera coordinate system. Unlike traditional 3DGS that directly optimizes Spherical
Harmonic (SH) coefficients for color, we follow a featurebased splatting paradigm [6] where each Gaussian carries a
multi-dimensional latent feature _f ‚àà_ R _[d]_ . This feature serves
as a unified latent representation that can be subsequently
decoded into modality-specific signals (e.g., RGB or thermal)
via neural networks.
The rendering process follows the point-based _Œ±_ -blending
model. For a specific pixel, the attributes of projected 2D
Gaussians are sorted by depth and blended to compute the
aggregated pixel value:



where **a** _i_ denotes the generic attribute of the _i_ -th Gaussian
(such as latent feature _fi_ or explicit color attributes) and _Œ±i_ is
the opacity of the Gaussian at that pixel.
In this work, we leverage this differentiable rasterization to
bridge different modalities. By decoupling the feature decoding from the geometric projection, we can perform complex
cross-modal modulations in the latent space before generating
the final visible and thermal images.


_C. Cross-Modal Feature Modulation_


To address the spectral gap between visible and infrared
modalities, we propose a cross-modal modulation mechanism.
Instead of treating visible and thermal signals as independent
entities, our framework leverages structural priors inherent in
the thermal spectrum to guide the synthesis of visible textures.
As shown in Fig. 1, the proposed Cross-Modal FiLM [8]
Modulation integrates feature extraction and conditioning in
a unified neural architecture.
**Shared Latent Encoding.** The process begins with the
rendered feature map _Af ‚àà_ R _[H][√ó][W][ √ó][d]_ by rasterizing the perGaussian feature _fi_ through the 3DGS rendering pipeline [4].
To extract high-level semantic information, we first pass _Af_
through a shared encoder Œ¶ _shared_ consisting of multiple pixelwise linear layers with SiLU activations [28]:


_h_ = Œ¶ _shared_ ( _Af_ ) _,_ (3)


where _h_ represents the intermediate feature representation that
serves as the common basis for both modalities.
**Thermal-Guided FiLM Modulation.** Distinct from directly applying MLP layers for different spectrums, we intro


_A_ = - **a** _iŒ±i_


_i‚ààN_



_i‚àí_ 1
ÔøΩ(1 _‚àí_ _Œ±j_ ) _,_ (2)


_j_ =1


4


Fig. 2. Thermal rendering results with and without geometric decoupling. The thermal rendering results without geometric decoupling may inherit sharp
textures and high-frequency noise from the visible spectrum.



duce a _Thermal Prior Head_ Œ¶ _th_ and a _FiLM-based modula-_
_tion_ [8] layer. Crucially, as _hth_ = Œ¶ _th_ ( _h_ ) is directly supervised
by the thermal rendering task through the subsequent decoding
stage (Eq. 6), it is naturally driven to distill structural priors
that are most representative of the infrared domain. The feature
_hth_ is subsequently mapped to a set of modulation parameters
( _Œ≥, Œ≤_ ) through a linear transformation:


[ _Œ≥, Œ≤_ ] = Linear _film_ ( _hth_ ) _._ (4)


By treating the infrared information as a conditioning signal,
we apply Feature-wise Linear Modulation (FiLM) to the
shared representation _h_ :


_hmod_ = _Œ≥ ‚äô_ _h_ + _Œ≤,_ (5)


where _‚äô_ denotes element-wise multiplication. This operation
dynamically scales and shifts the latent features based on the
thermal structural prior, effectively ‚Äúmasking‚Äù or ‚Äúenhancing‚Äù
regions where visible textures are likely to align with thermal
boundaries.
**Modality-Specific Decoding.** Finally, the modulated feature
_hmod_ and the thermal feature _hth_ are decoded into their
respective spectral domains:


_Cimplicit_ _[rgb]_ [=][ Sigmoid][(Œ¶] _[rgb]_ [(] _[h][mod]_ [))] _[,]_

(6)
_C_ _[thermal]_ = Sigmoid(Œ¶ _th out_ ( _hth_ )) _,_


where _Cimplicit_ _[rgb]_ [provides the base color component for the sub-]
sequent hybrid rendering stage. Notably, the thermal-specific
feature _hth_ serves a dual purpose: it acts as the source
for FiLM parameter generation while simultaneously being
decoded into the infrared signal _C_ _[thermal]_ . This hierarchical
modulation ensures that the synthesis of visible images is
physically constrained by the cross-modal structural consistency.



_D. Multi-spectral Hybrid Rendering_


Based on the modulated cross-modal features, we develop
a dual-branch rendering pipeline to synthesize images in both
visible and thermal spectrums. This pipeline addresses the
geometric inconsistencies and texture fidelity requirements
unique to each modality.
**Modality-Adaptive Geometric Decoupling** Typical multimodal Gaussian representations assume a shared geometry
across all spectrums. However, physical properties such as
transparency and reflectivity vary significantly between visible
and infrared bands. To accommodate these discrepancies, we
introduce a modality-adaptive geometric decoupling scheme.
For the thermal rendering branch, we define a modalityspecific opacity _Œ±t,i_ for each Gaussian _i_ by adding a learnable
offset ‚àÜ _tŒ±i_ to the base opacity _Œ±i_ :


_Œ±t,i_ = Sigmoid(Logit( _Œ±i_ ) + ‚àÜ _tŒ±i_ ) _,_ (7)


where ‚àÜ _tŒ±i_ captures the fine-grained geometric deviations.
Consequently, the thermal representation is generated via an
independent rasterization pass:


_Af_ ( _t_ ) = Rasterize( _¬µ,_ Œ£ _, Œ±t, f_ ) _,_ (8)


where _Af_ ( _t_ ) _‚àà_ R _[H][√ó][W][ √ó][d]_ represents the thermal-specific
feature map generated using the decoupled opacity _Œ±t_ . The
final thermal image _C_ _[thermal]_ is then decoded from _Af_ ( _t_ ) via
the thermal head discussed in Section III-C. This independent
pass ensures that occlusions and structural boundaries in
the thermal image remain physically consistent with infrared
sensors.
As shown in Fig. 2, without the geometric decoupling
mechanism, the thermal branch tends to inherit redundant
high-frequency textures from the visible spectrum that do not
exist in the infrared domain. Our proposed decoupling module
effectively filters out these cross-modal artifacts, ensuring that


**Decoded**


**Rasterized Feature Map**

**trained with** ùìõùíáùíÜùíÇùíïùíìùíÜùíÑ


Fig. 3. Feature level reconstruction loss on the rasterized feature maps. Left:
rendered feature map, right: reconstructed RGB-thermal scene. Note that _Af_
and _Af_ ( _t_ ) are only different in the opacity used in rasterization.


the thermal rendering preserves its natural smoothness while
accurately representing its own structural boundaries.
**Hybrid RGB Synthesis.** While the thermal branch focuses on geometric consistency, the RGB branch requires
high-frequency view-dependent details. We propose a hybrid
strategy that bridges explicit Gaussian Splatting with implicit
neural decoding.
Specifically, the final RGB color **C** _[rgb]_ is formulated as the
summation of two components:


**C** _[rgb]_ = _Rsh_ ( _¬µ,_ Œ£ _, Œ±,_ **c** _sh_ ) _‚äïCimplicit_ _[rgb]_ _[,]_ (9)


where _Rsh_ denotes the explicit color rendered via standard
Spherical Harmonic (SH) coefficients **c** _sh_, capturing viewdependent specular effects. The second term, _Cimplicit_ _[rgb]_ [, is]
the implicit component decoded from the modulated latent
features _hmod_, providing multi-modal consistent textures.
By combining these two components, our hybrid rendering scheme effectively preserves the high-frequency viewdependent properties of explicit rasterization, while simultaneously enriching the visible textures with the structural
intelligence of neural-modulated latent features.


_E. Loss Functions_


The training objective of ThermoSplat is to optimize the
multi-modal Gaussian representation and the neural modulation networks through a composite loss function _L_ . This
objective ensures that the synthesized visible and thermal
images adhere to the ground truth in terms of both pixel
intensity and structural topology.
**Spectral Reconstruction Loss** For both the visible and
infrared modalities, we employ a combination of _‚Ñì_ 1 loss and
Structural Similarity (SSIM) to supervise the final rendered
images against the corresponding ground-truth images **I** _m_ :


_L_ _[m]_ _rec_ [= (1] _[‚àí][Œª][s]_ [)] _[‚à•]_ **[I]** _[m]_ _[‚àí]_ **[C]** _[m][‚à•]_ [1] [+] _[Œª][s]_ [(1] _[‚àí]_ [SSIM][(] **[I]** _[m][,]_ **[ C]** _[m]_ [))] _[,]_ [ (10)]


where _m ‚àà{rgb, thermal}_ denotes the spectral modality, and
**C** _[m]_ are the corresponding output images in our pipeline.
To provide structural guidance during the intermediate
stages, we enforce consistency on the rasterized feature maps



5


_Af_ and _Af_ ( _t_ ) by slicing specific channels corresponding to
physical properties. As shown in Fig. 3, for the visible branch,
we constrain the first three channels of _Af_ to match the
RGB appearance. In parallel, for the thermal branch, we
supervise the subsequent latent channel (index 3) of _Af_ ( _t_ )
using the transformed thermal map derived from the Ironbow
colormap protocol. As this transformed map effectively serves
as a proxy for physical temperature and thermal intensity,
this constraint encourages the model to learn a compact and
structural representation. By applying these latent constraints,
we ensure the latent space captures the fundamental visual
and thermal distribution before it is decoded. The feature-level
reconstruction loss is thus formulated as:


_L_ _[feat]_ _rec_ [=] _[ L]_ [(] _[A][f]_ [[: 3]] _[,]_ **[ I]** _[rgb]_ [) +] _[ Œ∑][ ¬∑ L]_ [(] _[A]_ _f_ ( _t_ ) [[3]] _[,]_ **[ I]** _[trans]_ _th_ ) _,_ (11)


where **I** _[trans]_ _th_ represents the temperature-correlated intensity
map, _L_ denotes the composite _‚Ñì_ 1 and SSIM loss function as
defined in Eq. 10.
**Thermal Spatial Regularization** Due to the high-contrast
and often sparse nature of infrared signals, we introduce a
spatial smoothness constraint on the predicted thermal image:


_Lsmooth_ =       - _|‚àá_ **C** _[thermal]_ ( _p_ ) _|,_ (12)

_p‚àà_ ‚Ñ¶


where _‚àá_ denotes the spatial gradient operator at pixel _p_ .
This term enforces the smooth structural characteristics of the
thermal output.
**Total Objective** The final training objective is a weighted
summation of the aforementioned reconstruction and regularization terms:


_L_ = _Lrec_ + _Œªrf_ _L_ _[feat]_ _rec_ [+] _[ Œª][sm][L][smooth][,]_ (13)


where the image-level reconstruction loss is defined as _Lrec_ =
_L_ _[rgb]_ _rec_ [+] _[L]_ _rec_ _[thermal]_, _Œªrf_ and _Œªsm_ are hyper-parameters balancing
feature terms and smooth terms. By optimizing this joint objective, our framework ensures that the synthesized modalities
satisfy both pixel-level accuracy and the inherent structural
characteristics of thermal radiation.


IV. EXPERIMENTS


_A. Implementation details._


We evaluate our model on the RGBT-Scenes dataset, which
comprises over 1,000 calibrated RGB-thermal pairs across
ten indoor and outdoor scenes under diverse environmental
and lighting conditions. To demonstrate the effectiveness of
our approach, we compare our model with state-of-the-art
methods, including MMOne [7], MS-Splattingv2 [21] and
ThermalGaussian [5]. We also compare our method with the
3DGS [4] baseline trained on both modalities separately. We
use Peak Signal-to-Noise Ratio (PSNR), Structural Similarity
Index (SSIM), and Learned Perceptual Image Patch Similarity
(LPIPS) [29] to evaluate the rendering quality of both visible
and thermal modalities.
Our framework is implemented on PyTorch and trained
with an NVIDIA 3090 GPU. We train our pipeline for
30K iterations, which is the same as the setting used in


6


Fig. 4. Qualitative comparison of novel view synthesis results on the RGBT-Scenes dataset. We compare ThermoSplat against state-of-the-art multi-spectral
reconstruction methods ThermalGaussian [5], MS-Splattingv2 [21], MMOne [7], and the 3DGS baseline [4]. Our method generates more accurate rendering
results and structural details.



3DGS [4], MMOne [7] and ThermalGaussian [5]. For MSSplattingv2 [21], we follow the training strategy proposed
in the paper and train them with 120K iterations. In our
experiments, we set per-Gaussian feature dimension _d_ = 8,
and _Œªs_ = 0 _._ 2, _Œ∑_ = 0 _._ 5, _Œªrf_ = 1, _Œªsm_ = 0 _._ 3 for loss weights.


_B. Results and Comparisons_


We evaluate the novel view synthesis performance on the
test set to validate the rendering quality of our method against
other state-of-the-art methods. As illustrated in Fig. 4, our
method produces results with finer texture details and fewer
visual artifacts compared to existing approaches and the 3DGS
baseline. Specifically, our model performs better in recovering
complex structures that are often blurred or misaligned in the
baseline reconstructions, especially on the RGB branch, which
is attributed to the proposed cross-modality modulation that
effectively leverages structural priors of the scene.
As shown in Tab. I, our method achieves superior performance compared to state-of-the-art baselines across most
scenes. Specifically, our model attains the highest average
scores in all three metrics for both RGB and thermal modalities. Notably, on the average PSNR, our method outperforms
the second-best competitor (MMOne [7]) by 0 _._ 34 dB in the
RGB spectrum and 0 _._ 19 dB in the thermal spectrum. The consistent improvement in SSIM and LPIPS further demonstrates



our model‚Äôs capability to reconstruct fine-grained structural
details and maintain perceptual fidelity.
While MMOne [7] and MS-Splattingv2 [21] show competitive results in specific scenes (e.g., Dim and Trk), our method
demonstrates more robust generalization across diverse environments. These results validate that our modality modulation
and geometric decoupling strategy successfully resolves the
discrepancies between modalities without compromising the
reconstruction quality of the individual branches.


_C. Ablation_

To verify the contribution of each design element, we
conduct ablation experiments as summarized in Tab. II. First,
the comparison between our full model and the ‚ÄúMLP-based‚Äù
variant demonstrates the advantage of our feature-guided modulation over a standard decoding structure, as the former better
leverages spatial-aware features for high-quality appearance
synthesis. Second, removing the geometric decoupling mechanism (‚Äúw.o. geo. decoup.‚Äù) leads to a consistent performance
decline in both modalities, confirming that isolating physical geometry from modality-specific radiation is essential
for robust RGBT scene modeling. Finally, the exclusion of
latent constraints (‚Äúw.o. fea(rgb/th)‚Äù) results in degradation
of perceptual details, which indicates that the feature-level
reconstruction loss _L_ _[feat]_ _rec_ is crucial for encouraging the model
to capture fundamental structural and intensity distributions.


7



TABLE I
QUANTITATIVE EVALUATION OF RGB AND THERMAL (T) RENDERING RESULTS.


M Metric Method Dim DS Ebk RB Trk RK Bldg II Pt LS Avg.


RGB PSNR _‚Üë_ 3DGS 23 _._ 27 21 _._ 18 26 _._ 17 28 _._ 23 22 _._ 45 20 _._ 74 21 _._ 80 24 _._ 40 25 _._ 65 20 _._ 18 23 _._ 41
ThermalGaussian 24 _._ 38 21 _._ 76 26 _._ 85 28 _._ 12 24 _._ 17 23 _._ 14 **24.19** 24 _._ 55 25 _._ 48 21 _._ 71 24 _._ 44
MS-Splattingv2 24 _._ 06 21 _._ 18 26 _._ 87 28 _._ 12 **24.54** 23 _._ 42 23 _._ 90 23 _._ 77 26 _._ 20 22 _._ 05 24 _._ 41
MMOne **24.65** 22 _._ 05 **27.43** **29.03** 23 _._ 96 24 _._ 12 24 _._ 16 25 _._ 65 26 _._ 01 21 _._ 81 24 _._ 89
**Ours** 24 _._ 59 **22.12** 27 _._ 21 28 _._ 96 24 _._ 31 **24.20** 24 _._ 14 **25.98** **26.48** **24.31** **25.23**


SSIM _‚Üë_ 3DGS 0 _._ 842 0 _._ 771 0 _._ 902 0 _._ 917 0 _._ 810 0 _._ 765 0 _._ 827 0 _._ 875 0 _._ 867 0 _._ 688 0 _._ 826
ThermalGaussian 0 _._ 858 0 _._ 797 0 _._ 905 0 _._ 920 0 _._ 840 0 _._ 822 0 _._ 849 0 _._ 884 0 _._ 855 0 _._ 739 0 _._ 847
MS-Splattingv2 0 _._ 859 0 _._ 788 0 _._ 914 0 _._ 922 **0.859** 0 _._ 827 0 _._ 855 0 _._ 877 0 _._ 878 0 _._ 739 0 _._ 852
MMOne 0 _._ 862 0 _._ 810 0 _._ 918 0 _._ 916 0 _._ 845 **0.842** 0 _._ 847 0 _._ 897 0 _._ 876 0 _._ 727 0 _._ 854
**Ours** **0.872** **0.818** **0.934** **0.941** 0 _._ 859 0 _._ 841 **0.858** **0.911** **0.886** **0.788** **0.871**


LPIPS _‚Üì_ 3DGS 0 _._ 199 0 _._ 271 0 _._ 169 0 _._ 197 0 _._ 244 0 _._ 220 0 _._ 183 0 _._ 193 0 _._ 177 0 _._ 289 0 _._ 214
ThermalGaussian 0 _._ 194 0 _._ 253 0 _._ 169 0 _._ 199 0 _._ 211 0 _._ 184 0 _._ 170 0 _._ 186 0 _._ 195 0 _._ 268 0 _._ 203
MS-Splattingv2 **0.150** 0 _._ 224 0 _._ 145 0 _._ 197 0 _._ 170 0 _._ 141 0 _._ 145 0 _._ 161 **0.132** 0 _._ 211 0 _._ 168
MMOne 0 _._ 203 0 _._ 254 0 _._ 160 0 _._ 235 0 _._ 226 0 _._ 178 0 _._ 184 0 _._ 183 0 _._ 178 0 _._ 291 0 _._ 209
**Ours** 0 _._ 155 **0.204** **0.121** **0.164** **0.166** **0.130** **0.131** **0.138** 0 _._ 136 **0.180** **0.153**


T PSNR _‚Üë_ 3DGS 25 _._ 99 18 _._ 71 20 _._ 61 26 _._ 55 25 _._ 30 26 _._ 45 26 _._ 83 29 _._ 69 24 _._ 09 18 _._ 48 24 _._ 27
ThermalGaussian 26 _._ 46 **22.28** 23 _._ 31 27 _._ 17 25 _._ 88 26 _._ 33 26 _._ 72 29 _._ 86 26 _._ 16 22 _._ 27 25 _._ 64
MS-Splattingv2 26 _._ 06 21 _._ 43 23 _._ 32 25 _._ 44 26 _._ 08 27 _._ 24 26 _._ 89 29 _._ 98 **27.01** 22 _._ 64 25 _._ 61
MMOne **26.90** 21 _._ 81 **23.79** **27.39** 25 _._ 44 27 _._ 65 27 _._ 06 **30.27** 26 _._ 05 22 _._ 52 25 _._ 89
**Ours** 25 _._ 99 21 _._ 54 22 _._ 95 26 _._ 83 **26.25** **28.48** **27.45** 29 _._ 78 27 _._ 00 **24.50** **26.08**


SSIM _‚Üë_ 3DGS 0 _._ 889 0 _._ 787 0 _._ 812 0 _._ 914 0 _._ 863 0 _._ 922 0 _._ 896 0 _._ 892 0 _._ 867 0 _._ 768 0 _._ 861
ThermalGaussian 0 _._ 886 0 _._ 835 0 _._ 862 0 _._ 919 0 _._ 874 0 _._ 922 0 _._ 888 0 _._ 896 0 _._ 883 0 _._ 850 0 _._ 882
MS-Splattingv2 0 _._ 876 0 _._ 803 0 _._ 855 0 _._ 900 0 _._ 871 0 _._ 927 0 _._ 888 0 _._ 890 0 _._ 903 0 _._ 853 0 _._ 877
MMOne **0.894** **0.840** **0.874** 0 _._ 926 0 _._ 870 0 _._ 933 0 _._ 902 0 _._ 906 0 _._ 895 0 _._ 861 0 _._ 890
**Ours** 0 _._ 890 0 _._ 839 0 _._ 865 **0.928** **0.889** **0.941** **0.909** **0.910** **0.912** **0.889** **0.897**


LPIPS _‚Üì_ 3DGS 0 _._ 127 0 _._ 259 0 _._ 307 0 _._ 209 0 _._ 142 0 _._ 126 0 _._ 185 0 _._ 091 0 _._ 227 0 _._ 378 0 _._ 205
ThermalGaussian 0 _._ 129 0 _._ 210 0 _._ 203 0 _._ 198 0 _._ 136 0 _._ 124 0 _._ 177 0 _._ 091 0 _._ 181 0 _._ 248 0 _._ 170
MS-Splattingv2 **0.092** 0 _._ 164 **0.148** 0 _._ 133 0 _._ 107 0 _._ 073 0 _._ 103 0 _._ 064 **0.075** 0 _._ 177 0 _._ 114
MMOne 0 _._ 125 0 _._ 194 0 _._ 201 0 _._ 213 0 _._ 142 0 _._ 127 0 _._ 198 0 _._ 083 0 _._ 205 0 _._ 272 0 _._ 176
**Ours** 0 _._ 100 **0.149** 0 _._ 149 **0.096** **0.094** **0.059** **0.085** **0.057** 0 _._ 075 **0.139** **0.101**



TABLE II
ABLATION STUDY. WE CONDUCTED ABLATION EXPERIMENTS ON
DIFFERENT MODULES OF OUR PIPELINE.


RGB Modality Thermal Modality
Method Variant PSNR _‚Üë_ SSIM _‚Üë_ LPIPS _‚Üì_ PSNR _‚Üë_ SSIM _‚Üë_ LPIPS _‚Üì_


MLP-based 25 _._ 14 0 _._ 869 0 _._ 154 25 _._ 88 0 _._ 895 0 _._ 104
w.o. geo. decoup. 25 _._ 07 0 _._ 868 0 _._ 159 25 _._ 82 0 _._ 892 0 _._ 106
w.o. hybrid rgb 25 _._ 07 0 _._ 867 0 _._ 157 25 _._ 86 0 _._ 893 0 _._ 105
w.o. fea(th) 24 _._ 98 0 _._ 868 0 _._ 155 25 _._ 77 0 _._ 894 0 _._ 107
w.o. fea(rgb) 24 _._ 88 0 _._ 858 0 _._ 190 25 _._ 93 **0.897** 0 _._ 104


**Ours** **25.23** **0.871** **0.153** **26.08** 0 _._ 897 **0.101**


Collectively, these results validate that the synergy of the
proposed modulation mechanism, geometric decoupling, and
latent supervision ensures optimal RGBT reconstruction.


V. CONCLUSION


In this paper, we present ThermoSplat, a novel crossmodal 3D Gaussian Splatting framework designed for highfidelity RGBT scene reconstruction. To effectively bridge the
gap between visible and thermal modalities, we introduce
a Cross-Modal FiLM Modulation mechanism that leverages
thermal structural priors to guide visible texture synthesis.
Furthermore, to address the inherent geometric inconsistencies
caused by disparate physical sensing properties, we propose



a Modality-Adaptive Geometric Decoupling scheme, which
enables the model to accurately represent independent spectral
characteristics without compromising spatial alignment. Extensive experiments on the RGBT-Scenes dataset demonstrate
that our approach achieves state-of-the-art performance in
both rendering quality and structural accuracy. By integrating
explicit geometric representations with implicit neural feature modulation, ThermoSplat provides a robust and efficient
solution for multi-spectral scene understanding in visually
degraded environments.


**limitations.** Despite the promising results, ThermoSplat has
certain limitations that offer directions for future research.
First, the current geometric decoupling scheme primarily
focuses on the thermal branch; however, in scenarios with
extreme glass reflections or high-transparency surfaces, more
complex multi-modal interactions might be required to fully
resolve depth ambiguities. Second, the use of latent feature
modulation introduces additional memory overhead during the
neural decoding phase compared to vanilla 3DGS. Future work
will explore more lightweight modulation architectures and
investigate the potential of extending this framework to other
spectral domains, such as near-infrared or hyperspectral data,
to further enhance its versatility and robustness in all-weather
environmental perception.


ACKNOWLEDGMENTS


This paper is supported in part by the National Natural
Science Foundation of China (Grant No. 62402274) and the
Start-up Funding of Fuzhou University (Grant No. XRC25164) to Zhaoqi Su; in part by the Education and Scientific
Research Project for Middle-aged and Young Teachers of
Fujian Province, China (Grant No. JZ250004) to Zhipeng
Su; in part by the Special Fund for Promoting High-Quality
Development of Marine and Fishery Industries in Fujian
Province (Grant No. FJHYF-L-2025-07-005) to Xiaoqiang Lu.
The authors would like to acknowledge the use of Gemini to
improve the language and readability of the manuscript during
the writing process.


REFERENCES


[1] Y. Y. Lin, X.-Y. Pan, S. Fridovich-Keil, and G. Wetzstein, ‚ÄúThermalnerf:
Thermal radiance fields,‚Äù in _2024 IEEE International Conference on_
_Computational Photography (ICCP)_ . IEEE, 2024, pp. 1‚Äì12.

[2] M. Hassan, F. Forest, O. Fink, and M. Mielle, ‚ÄúThermonerf: A
multimodal neural radiance field for joint rgb-thermal novel view
synthesis of building facades,‚Äù _Adv. Eng. Inform._, vol. 65, no. PD, May
2025. [Online]. Available: https://doi.org/10.1016/j.aei.2025.103345

[3] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
and R. Ng, ‚ÄúNerf: Representing scenes as neural radiance fields for view
synthesis,‚Äù _Communications of the ACM_, vol. 65, no. 1, pp. 99‚Äì106,
2021.

[4] B. Kerbl, G. Kopanas, T. Leimk¬®uhler, and G. Drettakis, ‚Äú3d gaussian
splatting for real-time radiance field rendering.‚Äù _ACM Trans. Graph._,
vol. 42, no. 4, pp. 139‚Äì1, 2023.

[5] R. Lu, H. Chen, Z. Zhu, Y. Qin, M. Lu, L. zhang, C. Yan,
and a. xue, ‚ÄúThermalgaussian: Thermal 3d gaussian splatting,‚Äù in
_International Conference on Representation Learning_, Y. Yue, A. Garg,
N. Peng, F. Sha, and R. Yu, Eds., vol. 2025, 2025, pp. 1105‚Äì1117.

[Online]. Available: https://proceedings.iclr.cc/paper files/paper/2025/
file/03bdba50e3741ac5e3eaa0e55423587e-Paper-Conference.pdf

[6] L. Meyer, J. Gr¬®un, M. Weiherer, B. Egger, M. Stamminger, and
L. Franke, ‚ÄúMulti-spectral gaussian splatting with neural color representation,‚Äù _arXiv preprint arXiv:2506.03407_, 2025.

[7] Z. Gu and B. Wang, ‚ÄúMmone: Representing multiple modalities in one
scene,‚Äù in _Proceedings of the IEEE/CVF International Conference on_
_Computer Vision_, 2025, pp. 1088‚Äì1098.

[8] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, ‚ÄúFilm:
Visual reasoning with a general conditioning layer,‚Äù in _Proceedings of_
_the AAAI conference on artificial intelligence_, vol. 32, no. 1, 2018.

[9] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla,
and P. P. Srinivasan, ‚ÄúMip-nerf: A multiscale representation for antialiasing neural radiance fields,‚Äù in _Proceedings of the IEEE/CVF inter-_
_national conference on computer vision_, 2021, pp. 5855‚Äì5864.

[10] T. M¬®uller, A. Evans, C. Schied, and A. Keller, ‚ÄúInstant neural graphics
primitives with a multiresolution hash encoding,‚Äù _ACM transactions on_
_graphics (TOG)_, vol. 41, no. 4, pp. 1‚Äì15, 2022.

[11] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, T. Wang, A. Kristoffersen,
J. Austin, K. Salahi, A. Ahuja _et al._, ‚ÄúNerfstudio: A modular framework
for neural radiance field development,‚Äù in _ACM SIGGRAPH 2023_
_conference proceedings_, 2023, pp. 1‚Äì12.

[12] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, ‚Äú3d gaussian
splatting as new era: A survey,‚Äù _IEEE Transactions on Visualization and_
_Computer Graphics_, 2024.

[13] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, ‚ÄúMip-splatting:
Alias-free 3d gaussian splatting,‚Äù in _Proceedings of the IEEE/CVF_
_conference on computer vision and pattern recognition_, 2024, pp.
19 447‚Äì19 456.

[14] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai,
‚ÄúScaffold-gs: Structured 3d gaussians for view-adaptive rendering,‚Äù in
_Proceedings of the IEEE/CVF Conference on Computer Vision and_
_Pattern Recognition_, 2024, pp. 20 654‚Äì20 664.

[15] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari,
S. You, Z. Wang, and A. Kadambi, ‚ÄúFeature 3dgs: Supercharging 3d
gaussian splatting to enable distilled feature fields,‚Äù in _Proceedings of_
_the IEEE/CVF Conference on Computer Vision and Pattern Recognition_,
2024, pp. 21 676‚Äì21 685.



8


[16] Z. Dai, T. Liu, and Y. Zhang, ‚ÄúEfficient decoupled feature 3d gaussian
splatting via hierarchical compression,‚Äù in _Proceedings of the Computer_
_Vision and Pattern Recognition Conference_, 2025, pp. 11 156‚Äì11 166.

[17] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, ‚ÄúLanguage-driven physicsbased scene synthesis and editing via feature splatting,‚Äù in _European_
_Conference on Computer Vision_ . Springer, 2024, pp. 368‚Äì383.

[18] J. Xu, M. Liao, R. P. Kathirvel, and V. M. Patel, ‚ÄúLeveraging thermal
modality to enhance reconstruction in low-light conditions,‚Äù in _European_
_Conference on Computer Vision_ . Springer, 2024, pp. 321‚Äì339.

[19] T. Ye, Q. Wu, J. Deng, G. Liu, L. Liu, S. Xia, L. Pang, W. Yu, and
L. Pei, ‚ÄúThermal-nerf: Neural radiance fields from an infrared camera,‚Äù
in _2024 IEEE/RSJ International Conference on Intelligent Robots and_
_Systems (IROS)_ . IEEE, 2024, pp. 1046‚Äì1053.

[20] M. Ozer, M. Weiherer, M. Hundhausen, and B. Egger, ‚ÄúExploring [¬®]
multi-modal neural scene representations with applications on thermal
imaging,‚Äù in _European Conference on Computer Vision_ . Springer, 2024,
pp. 82‚Äì98.

[21] J. Gr¬®un, L. Meyer, M. Weiherer, B. Egger, M. Stamminger, and
L. Franke, ‚ÄúTowards Integrating Multi-Spectral Imaging with Gaussian Splatting,‚Äù in _Vision, Modeling, and Visualization_, B. Egger and
T. G¬®unther, Eds. The Eurographics Association, 2025.

[22] Q. Ma, C. Zou, D. Wang, J. Wang, L. Xiang, and Z. He, ‚ÄúBeyond
darkness: Thermal-supervised 3d gaussian splatting for low-light novel
view synthesis,‚Äù _arXiv preprint arXiv:2511.13011_, 2025.

[23] Q. Chen, S. Shu, and X. Bai, ‚ÄúThermal3d-gs: Physics-induced 3d
gaussians for thermal infrared novel-view synthesis,‚Äù in _European Con-_
_ference on Computer Vision_ . Springer, 2024, pp. 253‚Äì269.

[24] K. Yang, Y. Liu, Z. Cui, Y. Liu, M. Zhang, S. Yan, and Q. Wang, ‚ÄúNtrgaussian: Nighttime dynamic thermal reconstruction with 4d gaussian
splatting based on thermodynamics,‚Äù in _Proceedings of the Computer_
_Vision and Pattern Recognition Conference_, 2025, pp. 691‚Äì700.

[25] M. Nam, W. Park, M. Kim, H. Hur, and S. Lee, ‚ÄúVeta-gs: Viewdependent deformable 3d gaussian splatting for thermal infrared novelview synthesis,‚Äù in _2025 IEEE International Conference on Image_
_Processing (ICIP)_ . IEEE, 2025, pp. 965‚Äì970.

[26] S. N. Sinha, H. Graf, and M. Weinmann, ‚ÄúSpectralgaussians: Semantic,
spectral 3d gaussian splatting for multi-spectral scene representation,
visualization and analysis,‚Äù _ISPRS Journal of Photogrammetry and_
_Remote Sensing_, vol. 227, pp. 789‚Äì803, 2025.

[27] C. Thirgood, O. Mendez, E. Ling, J. Storey, and S. Hadfield, ‚ÄúHypergs:
Hyperspectral 3d gaussian splatting,‚Äù in _Proceedings of the Computer_
_Vision and Pattern Recognition Conference_, 2025, pp. 5970‚Äì5979.

[28] S. Elfwing, E. Uchibe, and K. Doya, ‚ÄúSigmoid-weighted linear units
for neural network function approximation in reinforcement learning,‚Äù
_Neural networks_, vol. 107, pp. 3‚Äì11, 2018.

[29] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ‚ÄúThe
unreasonable effectiveness of deep features as a perceptual metric,‚Äù in
_Proceedings of the IEEE conference on computer vision and pattern_
_recognition_, 2018, pp. 586‚Äì595.


