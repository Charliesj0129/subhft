## **Dualformer: Time-Frequency Dual Domain Learning for** **Long-term Time Series Forecasting**

**Jingjing Bai** **Yoshinobu Kawahara***
The University of Osaka & RIKEN AIP The University of Osaka & RIKEN AIP
jingjing.bai@ist.osaka-u.ac.jp kawahara@ist.osaka-u.ac.jp



**Abstract**


Transformer-based models, despite their
promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass
filtering effect that limits their effectiveness.
This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of highfrequency information crucial for capturing
fine-grained temporal variations. To address
this limitation, we propose Dualformer, a
principled dual-domain framework that rethinks frequency modeling from a layer-wise
perspective. Dualformer introduces three
key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and
frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in
deeper layers; and (3) a periodicity-aware
weighting mechanism that dynamically balances contributions from the dual branches
based on the harmonic energy ratio of inputs,
supported theoretically by a derived lower
bound. This design enables structured frequency modeling and adaptive integration of
time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments
conducted on eight widely used benchmarks
demonstrate Dualformer’s robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is
publicly available at `https://github.com/`
`Akira-221/Dualformer` .



**1** **Introduction**


Long-term time series forecasting (LTSF) has attracted increasing attention due to its wide applicability in domains such as finance (Ariyo et al.,
2014), healthcare (Alaa and van der Schaar, 2019),energy (Bilal et al., 2022), and climate modeling (Duchon
and Hale, 2012). Recent deep learning models, particularly Transformer-based architectures (Liu et al.,
2023; Wu et al., 2021; Zhou et al., 2021; Liu et al.,
2022), have shown strong ability to model longrange dependencies. To further enhance global pattern learning, several state-of-the-art models, including FEDformer (Zhou et al., 2022b), TimesNet (Wu
et al., 2022), and PDF (Dai et al., 2024), introduce
frequency-enhanced components and have achieved
impressive performance on various benchmarks.


Despite these advances, the effectiveness of these models in capturing informative temporal variations remains limited. A fundamental reason lies in the
inherent frequency bias of the self-attention mechanism. Theoretical analyses (Dong et al., 2021; Zhang
et al., 2021) have shown that deep self-attention networks suffer from rank collapse and diminished representation diversity due to exponentially decaying attention distributions. Recent studies (Wang et al.,
2022; Zhang et al., 2024) further revealed that selfattention inherently acts as a low-pass filter, progressively suppressing high-frequency components. These
high-frequency components, often representing rapid
changes or short-term fluctuations, are critical for accurate forecasting in real-world scenarios like weather,
finance, or energy. Mitigating this frequency bias is
therefore key to leveraging both local and global temporal patterns.


We argue that this low-pass bias of Transformers arises
from their uniform propagation of all frequency components across layers, allowing higher-energy, lowfrequency patterns (e.g., trends and seasonality) to
naturally dominate the attention scores. As the network deepens, this imbalance intensifies, leading to the


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**



gradual fading of lower-energy, high-frequency signals.
To address this, we propose to explicitly restructure
the frequency propagation path within the model. Inspired by the hierarchical nature of neural representation learning (Zeiler and Fergus, 2013; Tenney et al.,
2019), we introduce a layer-wise frequency decomposition strategy that allocates high-frequency components to lower layers and low-frequency components
to deeper layers. This design reflects the fact that
shallow layers are more suited for capturing localized,
fast-varying patterns, while deeper layers progressively
aggregate broader context to model long-term trends.
By explicitly controlling frequency flow across layers,
our model prevents low-frequency dominance and preserves fine-grained signals that would otherwise vanish
in deeper stages.


Based on these insights, we propose Dualformer, a
dual-domain Transformer framework that models time
and frequency features in parallel and mitigates frequency degradation through structural design. It consists of three key components: (1) a dual-branch architecture that jointly learns temporal and spectral features, capturing complementary information for more
expressive and adaptive representations, thereby overcoming the limitations of single-domain approaches;
(2) a hierarchical frequency sampling module that
structurally addresses high-frequency degradation by
assigning distinct frequency bands to different layers,
allocating high-frequency components to shallow layers and low-frequency trends to deeper ones, to enable
effective multi-scale and multi-resolution feature extraction; and (3) a periodicity-aware weighting mechanism that adaptively fuses the two branches based
on input’s periodic level measured by its harmonic energy ratio, allowing the model to generalize well to
time series with diverse spectral characteristics. Together, these designs allow Dualformer to dynamically
integrate information across domains and frequencies,
enhancing both accuracy and generalization.


We validated Dualformer through extensive experiments on eight benchmark datasets. The results
show that Dualformer consistently outperforms existing models, especially on heterogeneous or weakly periodic data.


**2** **Related Work**


Transformer-based models have achieved notable success in long-term time series forecasting by leveraging self-attention to model long-range dependencies.
Early advances such as Autoformer (Wu et al., 2021)
introduced seasonal-trend decomposition and autocorrelation mechanisms to enhance temporal structure
modeling, while PatchTST (Nie et al., 2022) improved



efficiency through patch-level tokenization. Interestingly, even simple linear models like DLinear (Zeng
et al., 2023) have demonstrated strong performance,
suggesting that complex attention mechanisms can
sometimes overfit. However, most of these methods
operate purely in the time domain and often ignore
the underlying frequency characteristics, leading to the
well-documented low-pass filtering effect where highfrequency signals are progressively attenuated.


To mitigate this issue, several frequency-domain methods have been proposed. FEDformer (Zhou et al.,
2022b) replaced self-attention with a Fourier-based attention to capture global frequency structure, while
FiLM (Zhou et al., 2022a) combined Fourier and Legendre projections to improve robustness. Others, like
FreTS (Yi et al., 2024), simplify spectral modeling
with frequency-wise MLPs. Although these methods
effectively extract global structure, they typically apply frequency modeling uniformly across layers, lacking a progressive, resolution-aware processing.


Hybrid approaches like Ye et al. (2024); Wu et al.
(2022); Yue et al. (2025); Li et al. (2023); Kui et al.
(2025); Luo et al. (2025); Dai et al. (2024) have
emerged to combine time and frequency information.
TFKAN (Kui et al., 2025) and TFDNet (Luo et al.,
2025) introduced joint time and frequency modeling
architectures, while TimesNet (Wu et al., 2022) and
PDF (Dai et al., 2024) reshaped the 1D series into a 2D
tensor using FFT to capture both long and short-term
variations. Although these models incorporated richer
structures, they often rely on static or fixed domain
integration schemes. Without adapting to the specific
spectral characteristics of each input, their generalization ability can be limited on heterogeneous or nonstationary series.


Motivated by these challenges, our work proposes
a structured and input-adaptive approach to timefrequency modeling. In contrast to prior methods with
uniform processing or static fusion, our model explicitly aligns frequency components with network depth
and dynamically adjusts its domain reliance based on
the input’s spectral properties.


**3** **Proposed Model**


The overall architecture of Dualformer is illustrated
in Figure 1. Given an input time series _X ∈_ R _[L][×][C]_,
where _C_ is the number of variables, and _L_ is the input sequence length, it is first processed by an embedding layer and reversible instance normalization
(RevIN) (Kim et al., 2021). The resulting representation is then transformed into the frequency domain using Fast Fourier Transform (FFT), yielding a complexvalued frequency spectrum. Subsequently, to address


**Jingjing Bai and Yoshinobu Kawahara**


Figure 1: The overall architecture of our proposed Dualformer model.



the imbalance in spectral modeling, a hierarchical frequency sampling (HFS) module dynamically selects
subsets of frequency components across layers, ensuring that lower layers focus on high-frequency features
while higher layers progressively shift attention to lowfrequency features.


The sampled frequency features are then processed in
parallel by a time branch and a frequency branch.
In the time branch, the selected frequency components are first converted back to the time domain using inverse FFT (IFFT), followed by a vanilla selfattention that models temporal dependencies and generates time-domain representations. In contrast, the
frequency branch applies an autocorrelation-based attention directly in the frequency domain to measure
periodic dependencies and global patterns. The resulting correlations are padded, inverse transformed,
and subjected to a top- _k_ selection to retain dominant
periodic components via time delay aggregation.


The outputs of the two branches are adaptively
fused through a periodicity-aware weighting algorithm, which computes a soft weight based on the input’s periodicity level. This allows the model to dynamically balance the contributions of time-domain
and frequency-domain features. The fused features
are finally passed through a linear projection and renormalized to produce the output time series.


Detailed formulations and design choices for each module are presented in the following sections.



**3.1** **Hierarchical frequency sampling**


To structurally mitigate the low-frequency bias inherent in Transformer-based models, we propose a hierarchical frequency sampling (HFS) strategy that explicitly decomposes the spectrum across model depth. Instead of propagating all frequencies through each layer,
HFS assigns high-frequency components to lower layers and low-frequency components to deeper layers,
thereby counteracting the implicit low-pass filtering effect of self-attention.


Formally, for the _n_ -th layer input _X_ _[n]_ _∈_ R _[L][×][C]_, we
apply FFT along the temporal axis of each channel to
obtain its spectral representation:


_F_ ( _X_ _[n]_ ) _→_ _W_ _[n]_ _∈_ C _[M]_ _[×][C]_ _,_


where _M_ = _⌈L/_ 2 _⌉_ + 1 due to the conjugate symmetry
property of FFT (Brigham, 1988). We then apply the
following layer-specific sampling function Sampling _[n]_ _α_
to select frequency ranges for the given layer:


_W_ ˜ _[n]_ = Sampling _[n]_ _α_ [(] _[W][ n]_ [) =] _[ W]_ [[] _[p][n]_ [ :] _[ q][n][,]_ [ :]] _[,]_


where _W_ [˜] _[n]_ _∈_ C _[F][ ×][C]_ represents the sampled frequency
components for layer _n_, and _F_ denotes the range of
the frequency features. The sampling ratio _α_ = _M_ _[F]_ [is a]

tunable hyperparameter, which controls the frequency
bandwidth allocated to each layer.


The sampling intervals _p_ _[n]_ and _q_ _[n]_ are dynamically adjusted based on the layer index _n_, defined as:


_p_ _[n]_ = _M_ (1 _−_ _α_ )(1 _−_ _[n][ −]_ [1] _q_ _[n]_ = _p_ _[n]_ + _αM,_

_N −_ 1 [)] _[,]_


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


padded and reconstructed using inverse FFT:


_Q_ ˜ _[n]_ _t_ _[,]_ [ ˜] _[K]_ _t_ _[n][,]_ [ ˜] _[V][ n]_ _t_ [=] _[ F]_ _[−]_ [1][(Padding( ˜] _[Q][n][,]_ [ ˜] _[K]_ _[n][,]_ [ ˜] _[V][ n]_ [))] _[.]_


A detailed explanation and pseudocode of the padding
operation are provided in the supplementary material.
Vanilla self-attention is then performed on these reconstructed features to capture contextual interactions
between time steps:


_Q_ ˜ _[n]_ _t_ _[·]_ [ ( ] _[K]_ [˜] _t_ _[n]_ [)] _[⊤]_
Attention( _Q_ [˜] _[n]_ _t_ _[,]_ [ ˜] _[K]_ _t_ _[n][,]_ [ ˜] _[V][ n]_ _t_ [) = Softmax(] ~~_√_~~ ) _V_ [˜] _t_ _[n][.]_
_dk_


We adopt the standard multi-head attention, where
the _query_, _key_, and _value_ of the _i_ -th head are:



Figure 2: Hierarchical frequency sampling. When _α ≤_
1 _/N_, case 1 is adopted; otherwise, case 2 is adopted.


where _N_ is the number of encoder layers. This design
ensures that as _n_ increases, the sampling window shifts
from high to low frequencies. Additionally, the overlap
between adjacent layers _n_ and _n_ + 1 is:


_M_
_q_ _[n]_ [+1] _−_ _p_ _[n]_ = ( _αN −_ 1)
_N −_ 1 _[.]_


Therefore, when _α >_ 1 _/N_, the frequency ranges between layers overlap, as shown in case 2 of Figure 2.
While _α <_ 1 _/N_, frequency loss occurs between layers.
To prevent such information gaps, we enforce uniform
partitioning, as shown in case 1 of Figure 2. Correspondingly, the updated formulas of _p_ _[n]_ and _q_ _[n]_ are:



_p_ _[n]_ = _M_ (1 _−_ _[n]_



_N_ _[.]_




_[n]_ _q_ _[n]_ = _p_ _[n]_ + _[M]_

_N_ [)] _[,]_ _N_



This hierarchical frequency sampling allows each layer
to focus on a distinct frequency range, enabling the
model to learn multi-resolution representations while
avoiding frequency redundancy or loss. It provides a
principled and structurally grounded solution to highfrequency degradation.


**3.2** **Time branch**


As shown in Figure 1, the time branch is designed
to capture local temporal dependencies by combining
frequency-guided feature selection with time-domain
self-attention. Given the embedded input _X_ _[n]_, we apply linear projections to obtain _Q_ _[n]_ _, K_ _[n]_ _, V_ _[n]_, which are
first transformed into the frequency domain via FFT.
Then, our hierarchical frequency sampling (HFS) module extracts a layer-specific spectral subset _Q_ [˜] _[n]_ _,_ _K_ [˜] _[n]_ _,_ _V_ [˜] _[n]_

to emphasize high-frequency components in shallow
layers and low-frequency trends in deeper layers. To
enable time-domain modeling, the sampled spectra are



_Qi_ = _Wi_ _[Q][Q, K][i]_ [ =] _[ W]_ _i_ _[ K][K, V][i]_ [=] _[ W]_ _i_ _[ V]_ _[V, Q][i][, K][i][, V][i]_ _[∈]_ [R] _[L][×]_ _[D]_ _h_ _,_


where _D_ is the hidden variable dimension and _h_ is the
number of heads. The outputs of multiple heads are
concatenated and projected as:


Multi-head( _Q, K, V_ ) = Concat(head1 _, . . .,_ head _h_ ) _Wt_ _[o][,]_


where head _i_ = Attention( _Qi, Ki, Vi_ ).


In essence, the time branch is not a generic temporal
feature extractor but a scale-specific dynamics modeler. Our model design breaks down the complex task
of modeling all dependencies at once into more manageable, scale-focused sub-problems. This layered decomposition and parallel modeling enhance its ability
to capture localized variations, complementing the frequency branch’s focus on global periodicity, leading to
a more comprehensive and robust representation.


**3.3** **Frequency branch**


The frequency branch aims to capture global periodic patterns and long-range dependencies by leveraging an autocorrelation mechanism, inspired by Autoformer (Wu et al., 2021). Unlike time-domain attention, it directly models frequency interactions to extract repeatable structures and phase-aligned dependencies at the sub-series level.


Given the hierarchically sampled frequency components _Q_ [˜] _[n]_ _,_ _K_ [˜] _[n]_ _,_ _V_ [˜] _[n]_, a sub-series similarity is calculated
in the frequency domain using the well-known WienerKhinchin theorem (Wiener, 1930), which links timedomain similarity to frequency-domain operations:



= _F_ _[−]_ [1] (Padding( _Q_ [˜] _[n]_ _⊙_ ( _K_ [˜] _[n]_ ) _[∗]_ )) _,_


where _RXX_ ( _τ_ ) denotes the time-delay similarity at lag
_τ_ . Here, _⊙_ represents the element-wise product, and _[∗]_



1
_RXX_ ( _τ_ ) = lim
_L→∞_ _L_



_L−_ 1

- _XtXt−τ_


_t_ =1


**Jingjing Bai and Yoshinobu Kawahara**



denotes complex conjugation. Note that zero-padding
is also required before the IFFT operation, like the
time branch, to ensure dimension alignment.


After computing the autocorrelation scores _RQ_ ˜ _[n]_ [ ˜] _K_ _[n]_ [,]
which correspond to the attention scores in the standard self-attention mechanism, the top- _k_ most informative lags are selected as follows:


_τ_ 1 _, . . ., τk_ = argTop _k_ lags ( _RQ_ ˜ _[n]_ [ ˜] _K_ _[n]_ [(] _[τ]_ [))] _[.]_
_τ_ _∈{_ 1 _,...,L}_


Here, _k_ lags is the autocorrelation factor and follows the
setting in Autoformer (Wu et al., 2021). The top- _k_ lags
autocorrelation values, _RQ_ ˜ _[n]_ [ ˜] _K_ _[n]_ [(] _[τ]_ [), are then normal-]
ized into probabilities using the softmax function for
subsequent computations:


_R_ ˆ _Q_ ˜ _[n]_ [ ˜] _K_ _[n]_ [(] _[τ]_ [1][)] _[, . . .,]_ [ ˆ] _[R]_ _Q_ [ ˜] _[n]_ [ ˜] _K_ _[n]_ [(] _[τ][k]_ lags [)]

= Softmax( _RQ_ ˜ _[n]_ [ ˜] _K_ _[n]_ [(] _[τ]_ [1][)] _[, . . .,][ R]_ _Q_ [ ˜] _[n]_ [ ˜] _K_ _[n]_ [(] _[τ][k]_ lags [))] _[.]_


To align the lagged information directly with the current _query_ ’s time step, the _values_ _V_ [˜] _[n]_ are then cyclically shifted using a _rolling_ operator and aggregated
based on their respective weights:



Although directly computing periodicity is nontrivial,
this ratio serves as an interpretable and tractable surrogate that reflects the spectral regularity of the input signal. The following lower bound theorem further
convinces of its validity as a periodicity measure.


**Theorem 1.** _Let f_ ( _t_ ) _be a continuous time series_
_defined on_ [0 _, L_ ] _, with its discrete form sampled as_

[ _f_ (0) _, · · ·, f_ ( _L −_ 1)] _._ _Assume that f_ ( _t_ ) _can be de-_
_composed intof_ ( _t_ ) = _fp_ ( _t_ ) + _fr_ ( _t_ ) _, where fp_ ( _t_ ) _is the_
_strictly periodic component with period τ_ _, satisfying_
_L_ = _mτ, m ∈_ N _[∗]_ _, i.e. fp_ ( _t_ + _τ_ ) = _fp_ ( _t_ ) _, ∀t ∈_ [0 _, L_ _−_ _τ_ ] _._
_And fr_ ( _t_ ) _is the residual component, representing the_
_non-periodic part. Define the following energy metrics:_



_L−_ 1

- _fr_ ( _t_ ) [2] _,_


_t_ =0



_Ep_ =



_L−_ 1

- _fp_ ( _t_ ) [2] _, Er_ =


_t_ =0




- _|F_ [ _n_ ] _|_ [2] _, Fh_ = _{k, . . ., nk},_

_n∈Fh_



_Ef_ =



_L−_ 1





- _|F_ [ _n_ ] _|_ [2] _, Eh_ = 
_n_ =0 _n∈F_



_where Ep is energy of periodic part, Er is energy of_
_residual part, Ef is total spectral energy, and Eh is_
_harmonic energy with a basis frequency k._

_Let λ_ = _EEpr_ _be the energy ratio of the periodic_
_part to the residual part._ _According to the defini-_
_tion/decomposability of the given time series, we as-_
_sume that this ratio can be relatively high. Then the_
_lower bound for the ratio of harmonic energy to total_
_energy is given by:_



AutoCorr( _Q_ [˜] _[n]_ _,_ _K_ [˜] _[n]_ _,_ _V_ [˜] _[n]_ ) =



_k_

- _Roll_ ( _V_ [˜] _[n]_ _, τi_ ) _R_ [ˆ] _Q_ ˜ _[n]_ [ ˜] _K_ _[n]_ [(] _[τ][i]_ [)] _[.]_

_i_ =1



This process preserves long-term temporal alignment
while allowing efficient modeling of global periodicity.
The multi-head variant is defined analogously:


head _i_ = AutoCorr( _Qi, Ki, Vi_ ) _,_


Multi-head( _Q, K, V_ ) = Concat(head1 _, . . .,_ head _h_ ) _Wf_ _[o][.]_


**3.4** **Periodicity-aware weighting**


Periodicity is a fundamental characteristic of many
real-world time series, typically manifesting as
concentrated energy in specific harmonic components (Pukhova et al., 2018). Specifically, periodic
time series normally exhibit concentrated energy at
a particular frequency and its harmonics, with sharp
spectral peaks in the spectrum indicating stronger periodicity (Oppenheim and Verghese, 2017; Pukhova
et al., 2018). Conversely, non-periodic time series generally lack such distinct characteristics, displaying flatter, dispersed energy distributions.


This observation motivates the use of frequencydomain energy concentration as a quantitative proxy
for measuring periodicity. Inspired by these insights
in the previous work (Wiener, 1930; Ye et al., 2024),
we propose using the harmonic energy ratio as a periodicity measure, which captures the proportion of total spectral energy concentrated in certain harmonics.



_√_
_Eh_ _≥_ _λ −_ ~~_√_~~ 2
_Ef_ _λ −_ 2 _λ_



_λ −_ 2 _λ_

~~_√_~~
_λ −_ 2 _λ_



_._
_λ_ + 1



A detailed proof and further discussion are provided
in the supplementary material. The result provides a
theoretical lower bound for the ratio of harmonic energy to total energy, under the assumption that the
time series exhibits sufficiently strong periodicity (i.e.,
_λ >_ 4). In practice, most of the long-term forecasting
datasets satisfy this condition, validating the use of the
harmonic energy ratio as a proxy for periodicity. The
lower bound ensures that as _λ_ increases, the periodicity of the time series strengthens, with greater energy
concentration in its harmonic components. Building
on this insight, we employ the ratio _Eh/Ef_ to guide
the adaptive weighting of two branches, ensuring a balanced contribution from temporal and frequency features. The specific implementation is detailed in Algorithm 1.


Specifically, we compute the harmonic energy ratio
from the spectrum of each input sequence, with the
basis frequency _k_ automatically identified via spectral peak detection. This method identifies the most
prominent low-frequency component with the highest


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


**Algorithm 1** Periodicity-aware weighting
**Input** : Time series _x_, length _L_, number of harmonics
_n_ (set to 3 in experiments)
**Output** : The weight _wf_ _, wt_ of the frequency branch
and time branch


1: _F_ = FFT( _x_ )
2: _Fh_ = _{k,_ 2 _k, . . ., nk}_, where basis frequency _k_ =
PeakDetection( _F_ )



_i∈Fh_ _[|][F]_ [[] _[i]_ []] _[|]_ [2] _[, E][f]_ [ =][ �] _i_ _[⌊]_ =0 _[L/]_ [2] _[⌋]_ [+1] _|F_ [ _i_ ] _|_ [2]



3: _Eh_ = [�]



4: **return** _wf_ = _Eh/Ef_ _, wt_ = 1 _−_ _wf_



energy, thereby determining the signal’s primary periodicity (Palshikar et al., 2009). The implementation
details are provided in the supplementary material. A
higher ratio indicates stronger periodicity and thus assigns greater weight to the frequency branch, while
a lower ratio favors the time branch. This adaptive
weighting mechanism ensures that Dualformer dynamically allocates attention to local or global features depending on the nature of the input, enhancing its flexibility and robustness across diverse temporal patterns.


**4** **Experiments**


In this section, we evaluated the performance of Dualformer through a series of extensive experiments.


**Datasets** We conducted both multivariate and univariate forecasting experiments on eight real-world
datasets, including Electricity, Solar energy, Traffic, Weather, and 4 ETT datasets (ETTh1, ETTh2,
ETTm1, and ETTm2). Detailed descriptions of the
datasets are available in the supplementary material.
Following previous work (Wu et al., 2021), the datasets
were split into training, validation, and test sets with
a ratio of 6:2:2 for the ETT datasets and 7:1:2 for the
rest.


**Baselines and experimental setup** We selected
several state-of-the-art models as baselines, including TimeMixer (Wang et al., 2024a), PDF (Dai
et al., 2024), TimesNet (Wu et al., 2022), FEDformer (Zhou et al., 2022b), FiLM (Zhou et al., 2022a),
PatchTST (Nie et al., 2022), iTransformer(Liu et al.,
2023), FreTS (Yi et al., 2024), and DLinear (Zeng
et al., 2023). For all models and datasets, the prediction length was set as _T ∈{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_, and
the lookback window size _L_ was fixed at 96. Both
multivariate and univariate predictions were evaluated
using widely adopted metrics: Mean Squared Error
(MSE) and Mean Absolute Error (MAE). All experiments were repeated three times for robustness. Additional details about the baseline models and experimental setup are provided in the supplementary.



Figure 3: Ablation study results on ETTh1 dataset.
Average results of all prediction lengths _T_ _∈_
_{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_ on multivariate forecasting.


**4.1** **Main Results**


The average results across four prediction lengths for
multivariate and univariate forecasting are shown in
Table 1 and 2, respectively, where **bold** /underline values denote the best/second performances. Full results
are detailed in the supplementary material.


Dualformer consistently outperforms baseline models
across most datasets, demonstrating its strong capability to handle long-term forecasting. In the multivariate setting, it achieves the top rank in 13 of 16
average cases and 44 of 64 total outcomes across two
metrics over eight benchmarks. For the univariate setting, Dualformer also shows competitive performance,
consistently ranking among the top methods. However, performance on the Traffic dataset is relatively
weaker, likely due to its strong periodicity, which particularly benefits models like PDF that explicitly extract dominant periods. In contrast, Dualformer captures a broader frequency spectrum without isolating
specific periodic components, which may limit its advantage on highly seasonal data.


**4.2** **Ablation Study**


We further conducted ablation studies to evaluate
the contributions of each component in Dualformer.
As shown in Figure 3, we compare the full model
against four variants: **w/o T** ~~**b**~~ **ranch** : removing the
time branch; **w/o F** **branch** : removing the frequency
branch; **w/o weighting** : replacing the periodicityaware weighting with uniform averaging; **w/o RevIN** :
removing the RevIN (Kim et al., 2021) block.


Removing the frequency branch led to the largest performance drop (+0.638 MSE / +0.378 MAE), highlighting its critical role in modeling global periodicity.
The time branch also proved important for capturing local variations. Replacing the periodicity-aware


**Jingjing Bai and Yoshinobu Kawahara**


Table 1: Multivariate long-term time series forecasting results (average) with various prediction length _T ∈_
_{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_ and fixed lookback window size _L_ = 96. **Bold** indicates the best and underline indicates the
second best result, respectively.


**Models** **Dualformer TimeMixer PatchTST iTransformer** **PDF** **FEDformer TimesNet** **FiLM** **DLinear** **FreTS**
(Ours) (2024) (2023) (2024) (2024) (2022) (2023) (2022) (2023) (2023)


Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


_ETTh1_ **0.407 0.420** 0.411 0.423 0.418 0.437 0.445 0.434 **0.407** 0.424 0.433 0.455 0.458 0.450 0.440 0.452 0.439 0.449 0.454 0.447


_ETTh2_ 0.335 **0.377 0.316** 0.384 0.343 0.378 0.374 0.398 0.351 0.392 0.431 0.447 0.414 0.427 0.359 0.401 0.458 0.459 0.383 0.407


_ETTm1_ **0.345 0.367** 0.348 0.376 0.349 0.381 0.407 0.410 0.347 0.374 0.417 0.440 0.400 0.406 0.361 0.379 0.355 0.380 0.414 0.407


_ETTm2_ **0.245 0.304** 0.256 0.316 0.250 0.314 0.288 0.332 0.254 0.315 0.300 0.348 0.291 0.333 0.254 0.317 0.281 0.343 0.286 0.327


_Electricity_ **0.158 0.249** 0.165 0.253 0.161 0.255 0.161 0.257 0.178 0.270 0.213 0.326 0.193 0.295 0.190 0.283 0.167 0.264 0.163 0.259


_Solar_ **0.191 0.239** 0.192 0.244 0.256 0.298 0.233 0.262 0.203 0.248 0.243 0.350 0.244 0.334 0.213 0.266 0.329 0.390 0.315 0.364


_Traffic_ 0.406 0.268 0.393 0.271 0.407 0.282 0.428 0.282 **0.389 0.264** 0.608 0.375 0.620 0.336 0.443 0.313 0.435 0.298 0.626 0.378


_Weather_ **0.220 0.253** 0.222 0.262 0.221 0.263 0.258 0.278 0.222 0.261 0.337 0.377 0.259 0.287 0.253 0.284 0.245 0.297 0.272 0.291


Table 2: Univariate long-term time series forecasting results (average) on ETT datasets with various prediction
length _T ∈{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_ and fixed lookback window size _L_ = 96. **Bold** indicates the best and underline
indicates the second best result, respectively.


Models Dualformer Autoformer FEDformer FiLM PatchTST iTransformer FreTS DLinear


Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


_ETTh1_ 0.073 **0.209** 0.105 0.257 0.115 0.263 0.076 0.214 0.074 0.211 **0.072 0.209** 0.102 0.248 0.098 0.240


_ETTh2_ **0.173 0.329** 0.209 0.357 0.206 0.350 0.185 0.340 0.178 0.337 0.179 0.335 0.203 0.347 0.202 0.354


_ETTm1_ **0.047 0.161** 0.091 0.236 0.070 0.204 0.049 0.165 0.048 0.163 0.048 0.163 0.066 0.196 0.053 0.168


_ETTm2_ **0.110 0.247** 0.157 0.305 0.130 0.273 0.115 0.255 0.113 0.251 0.111 0.246 0.118 0.259 0.113 0.249


weighting with uniform averaging and removing RevIN
caused moderate degradations, confirming the value of
input-adaptive fusion and normalization. Full results
across all prediction lengths are in the supplementary
material.


**4.2.1** **Hierarchical frequency sampling**
**evaluation**



To investigate the effectiveness of hierarchical frequency sampling (HFS), we compared it with four
fixed or non-adaptive baselines: low-frequency only,
high-frequency only, full FFT, and random band selection. Experiments were conducted on two representative datasets with contrasting spectral characteristics: Traffic, dominated by strong periodic patterns
with concentrated low-frequency energy; and Weather,
which exhibits weak periodicity and contains substantial high-frequency fluctuations and abrupt changes.


As shown in Figure 4, HFS consistently achieved the
lowest MSE on both datasets, outperforming all baseline strategies. On Traffic, HFS slightly outperforms
low-frequency-only and full FFT baselines, indicating its ability to capture dominant periodic patterns
while integrating complementary multi-scale information. On Weather, where high-frequency variations



Figure 4: MSE across different frequency selection
strategies in two heterogeneous datasets: weather and
traffic.


are more critical, HFS notably outperforms the highfrequency-only and random strategies, suggesting that
static selection is insufficient for datasets with complex or unstable spectral structures. These results
empirically validate that HFS provides a principled
way of organizing spectral inputs across model layers. By assigning different frequency bands to different layers, HFS allows the model to gradually process
coarse-to-fine structures, thereby improving its capacity to capture both long-term trends and short-term
fluctuations. This hierarchical treatment of frequency


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**













Figure 5: Weight distribution for various time series
across four distinct datasets. The examples represent
different segments from the ETTh1 and Weather series, and different variables from the Electricity and
Traffic datasets.


enhances the model’s representation power and contributes to more accurate long-term forecasting.


**4.2.2** **Periodicity-aware weighting evaluation**


To validate the effectiveness of the periodicity-aware
weighting mechanism, we analyzed its behavior across
four benchmark datasets: ETTh1, Weather, Electricity, and Traffic.


Fig. 5 visualizes the frequency branch weights _wf_ assigned by Dualformer across eight time series, including different segments or variables from each dataset.
For strongly periodic data such as Electricity, the
model consistently assigned larger weights to the frequency branch. In contrast, for data with weak or
shifting periodicity (e.g., Weather, Traffic, ETTh1),
the model adaptively modulated _wf_ in response to the
input, indicating its ability to track temporal variation
in spectral structure.


Figure 6 (left) shows the global _wf_ distribution across
a 96-sized sliding window. Datasets with stable periodicity (Electricity, Traffic) exhibited distributions
skewed toward higher _wf_, while non-periodic data
(Weather) concentrated around lower values. ETTh1
shows a wider spread, reflecting varying degrees of periodicity over time.


Figure 6 (right) further illustrates the temporal evolution of _wf_ . For example, Electricity maintains a
high and stable frequency preference, while Weather
exhibits low and fluctuating values, consistent with its
non-stationary nature.



Figure 6: (Left) Distribution of _wf_ across datasets.
(Right) Temporal evolution of _wf_ under sliding windows.


These analyses confirm that Dualformer not only captures strong periodic structures by assigning higher
weights to the frequency branch but also adapts effectively to weak or unstable periodic patterns by modulating the branch contributions accordingly. This
flexibility demonstrates the practical value of the
periodicity-aware weighting mechanism, contributing
to the model’s consistently strong forecasting performance across diverse time series scenarios.


In addition to the above main results, we conducted
several supplementary experiments to comprehensively evaluate Dualformer. These include model efficiency (speed and memory usage) test, evaluation on
model robustness, an ablation study on the look-back
window size, a sensitivity check on key hyperparameters, and performance evaluation on re-normalized
data. The results are presented in the supplementary
material.


**5** **Conclusion**


In this work, we introduced Dualformer, a novel timefrequency dual-domain framework for long-term time
series forecasting, which explicitly addresses the lowpass filtering bias of Transformers. Our approach
introduced a structured method to model frequency
components across network layers and incorporated
a dynamic weighting mechanism to balance contributions of time and frequency features based on the input
periodicity, enabling adaptive feature prioritization.
Extensive experiments on eight benchmarks demonstrate its superior performance over both time- and
frequency-domain baselines, highlighting its robustness and adaptability to diverse time series. Future
work will explore extensions to high-dimensional multivariate, irregularly sampled, and event-driven time
series, as well as broader applications in time series
classification and anomaly detection.


**Jingjing Bai and Yoshinobu Kawahara**



**References**


Alaa, A. M. and van der Schaar, M. (2019). Attentive state-space modeling of disease progression. _Ad-_
_vances in neural information processing systems_, 32.


Ariyo, A. A., Adewumi, A. O., and Ayo, C. K. (2014).
Stock price prediction using the arima model. In
_2014 UKSim-AMSS 16th int’l conf. on computer_
_modelling and simulation_, pages 106–112. IEEE.


Bilal, M., Kim, H., Fayaz, M., and Pawar, P. (2022).
Comparative analysis of time series forecasting approaches for household electricity consumption prediction. _arXiv preprint arXiv:2207.01019_ .


Brigham, E. O. (1988). _The fast Fourier transform_
_and its applications_ . Prentice-Hall, Inc.


Dai, T., Wu, B., Liu, P., Li, N., Bao, J., Jiang, Y.,
and Xia, S.-T. (2024). Periodicity decoupling framework for long-term series forecasting. In _The Twelfth_
_International Conference on Learning Representa-_
_tions_ .


Dong, Y., Cordonnier, J.-B., and Loukas, A. (2021).
Attention is not all you need: Pure attention loses
rank doubly exponentially with depth. In _Interna-_
_tional conference on machine learning_, pages 2793–
2803. PMLR.


Duchon, C. and Hale, R. (2012). _Time series analy-_
_sis in meteorology and climatology: an introduction_ .
John Wiley & Sons.


Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and
Choo, J. (2021). Reversible instance normalization
for accurate time-series forecasting against distribution shift. In _Int’l Conf. on Learning Representa-_
_tions_ .


Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. (2022).
Reducing activation recomputation in large transformer models.


Kui, X., Liu, C., Li, Q., Hu, Z., Shi, Y., Si, W., and
Zou, B. (2025). Tfkan: Time-frequency kan for longterm time series forecasting.


Lai, G., Chang, W.-C., Yang, Y., and Liu, H. (2018).
Modeling long- and short-term temporal patterns
with deep neural networks.


Li, C., Shi, Z., Zhou, L., Zhang, Z., Wu, C., Ren, X.,
Hei, X., Zhao, M., Zhang, Y., Liu, H., You, Z., and
He, L. (2023). Tfformer: A time–frequency information fusion-based cnn-transformer model for osa
detection with single-lead ecg. _IEEE Transactions_
_on Instrumentation and Measurement_, 72:1–17.


Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu,
A. X., and Dustdar, S. (2022). Pyraformer: Lowcomplexity pyramidal attention for long-range time



series modeling and forecasting. In _International_
_Conference on Learning Representations_ .


Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma,
L., and Long, M. (2023). itransformer: Inverted
transformers are effective for time series forecasting.
_arXiv preprint arXiv:2310.06625_ .


Luo, Y., Zhang, S., Lyu, Z., and Hu, Y. (2025). Tfdnet:
Time–frequency enhanced decomposed network for
long-term time series forecasting. _Pattern Recogni-_
_tion_, 162:111412.


Nie, Y., Nguyen, N. H., Sinthong, P., and
Kalagnanam, J. (2022). A time series is worth
64 words: Long-term forecasting with transformers.
_arXiv preprint arXiv:2211.14730_ .


Oppenheim, A. V. and Verghese, G. C. (2017). _Signals,_
_systems & inference_ . Pearson London.


Palshikar, G. et al. (2009). Simple algorithms for peak
detection in time-series. In _Proc. 1st Int. Conf. ad-_
_vanced data analysis, business analytics and intelli-_
_gence_, volume 122.


Pukhova, V. M., Kustov, T. V., and Ferrini, G. (2018).
Time-frequency analysis of non-stationary signals.
In _2018 ieee conference of russian young researchers_
_in electrical and electronic engineering (eiconrus)_,
pages 1141–1145. IEEE.


Tenney, I., Das, D., and Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline.


Wang, P., Zheng, W., Chen, T., and Wang, Z. (2022).
Anti-oversmoothing in deep vision transformers via
the fourier domain analysis: From theory to practice. _arXiv preprint arXiv:2203.05962_ .


Wang, S., Wu, H., Shi, X., Hu, T., Luo, H., Ma, L.,
Zhang, J. Y., and Zhou, J. (2024a). Timemixer:
Decomposable multiscale mixing for time series forecasting.


Wang, Y., Wu, H., Dong, J., Liu, Y., Long, M., and
Wang, J. (2024b). Deep time series models: A comprehensive survey and benchmark.


Wiener, N. (1930). Generalized harmonic analysis.
_Acta mathematica_, 55(1):117–258.


Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long,
M. (2022). Timesnet: Temporal 2d-variation modeling for general time series analysis. _arXiv preprint_
_arXiv:2210.02186_ .


Wu, H., Xu, J., Wang, J., and Long, M. (2021). Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting. _Ad-_
_vances in neural information processing systems_,
34:22419–22430.


Ye, H., Chen, J., Gong, S., Jiang, F., Zhang, T., Chen,
J., and Gao, X. (2024). Atfnet: Adaptive time

**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


frequency ensembled network for long-term time series forecasting. _arXiv preprint arXiv:2404.05192_ .


Yi, K., Zhang, Q., Fan, W., Wang, S., Wang, P., He,
H., An, N., Lian, D., Cao, L., and Niu, Z. (2024).
Frequency-domain mlps are more effective learners
in time series forecasting. _Advances in Neural Infor-_
_mation Processing Systems_, 36.


Yue, W., Liu, Y., Ying, X., Xing, B., Guo, R., and Shi,
J. (2025). Freeformer: Frequency enhanced transformer for multivariate time series forecasting.


Zeiler, M. D. and Fergus, R. (2013). Visualizing and
understanding convolutional networks.


Zeng, A., Chen, M., Zhang, L., and Xu, Q. (2023).
Are transformers effective for time series forecasting? In _Proceedings of the AAAI conf. on artificial_
_intelligence_, volume 37, pages 11121–11128.


Zhang, A., Chan, A., Tay, Y., Fu, J., Wang, S., Zhang,
S., Shao, H., Yao, S., and Lee, R. K.-W. (2021). On
orthogonality constraints for transformers. In Zong,
C., Xia, F., Li, W., and Navigli, R., editors, _Proceed-_
_ings of the 59th Annual Meeting of the Association_
_for Computational Linguistics and the 11th Interna-_
_tional Joint Conference on Natural Language Pro-_
_cessing (Volume 2: Short Papers)_, pages 375–382,
Online. Association for Computational Linguistics.


Zhang, X., Zhao, S., Song, Z., Guo, H., Zhang, J.,
Zheng, C., and Qiang, W. (2024). Not all frequencies are created equal: towards a dynamic fusion of
frequencies in time-series forecasting. In _Proceedings_
_of the 32nd ACM Int’l Conf. on Multimedia_, pages
4729–4737.


Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong,
H., and Zhang, W. (2021). Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conf. on artifi-_
_cial intelligence_, volume 35, pages 11106–11115.


Zhou, T., Ma, Z., Wen, Q., Sun, L., Yao, T., Yin,
W., Jin, R., et al. (2022a). Film: Frequency improved legendre memory model for long-term time
series forecasting. _Advances in neural information_
_processing systems_, 35:12677–12690.


Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and
Jin, R. (2022b). Fedformer: Frequency enhanced
decomposed transformer for long-term series forecasting. In _Int’l conf. on machine learning_, pages
27268–27286. PMLR.


**Proof of Theorem 1**



**Jingjing Bai and Yoshinobu Kawahara**


Similarly, for the harmonics _nk_ ( _n_ = 2 _, . . ., τ −_ 1), we
have



_Proof._ Considering the bandlimited property of most
time series, here we assume that the energy of those
frequency components above the Nyquist frequency( _π_ )
is small enough to be neglected, which is also helpful
for the following computation. Therefore, for the periodic part _fp_ ( _t_ ), the energy _Ep_ is computed by only
its first _q_ = _⌈π/_ 2 _⌉−_ 1 Fourier components as follows:



_F_ [ _nk_ ] =


=


=



_L_ _[t]_



_L−_ 1




�( _fp_ ( _t_ ) + _fr_ ( _t_ )) _e_ _[−]_ [2] _[πi]_ _[nk]_ _L_


_t_ =0



_L−_ 1



_t_ =0


_L−_ 1



_t_ =0




- _cme_ [2] _[πi]_ _[m]_ _τ_ _[−][n]_


_m_ = _−q_




_[m]_ _τ_ _[t]_ + _fr_ ( _t_ )




- _q_

 



- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_







_e_ _[−]_ [2] _[πi]_ _[n]_ _τ_ _[t]_



�2

_[m]_ _τ_ _[t]_



_L−_ 1

- _fp_ ( _t_ ) [2] =


_t_ =0



_L−_ 1




_L−_ 1



_t_ =0




- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_




- _q_

 


_q_




_Ep_ =



_τ_ _[−][n]_ _t_ +



_τ_ _[t]_



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _[n]_ _τ_


_t_ =0



�2

_[m]_ _τ_ _[t]_



= _k_


= _k_


= _k_



_τ_ _−_ 1



_t_ =0


_τ_ _−_ 1



_t_ =0



_q_



_m_ = _−q_




- _q_

 



- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



_q_

- _cmcn_


_n_ = _−q_



_τ_ _t_



_q_



_m_ = _−q_




- _cmcne_ [2] _[πi]_ _[m]_ _τ_ [+] _[n]_


_n_ = _−q_



_q_




_τ_ _t_



_q_

 - _cm_


_m_ = _−q,m_ = _n_



_L−_ 1





- _e_ [2] _[πi]_ _[m]_ _τ_ _[−][n]_


_t_ =0



= _cnL_ +



_τ_ _[t]_



_τ_ _−_ 1





- _e_ [2] _[πi]_ _[m]_ _τ_ [+] _[n]_


_t_ =0



_q_




_τ_ _t_ (1)



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _[n]_ _τ_


_t_ =0



+



_τ_ _[t]_ (4)



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _[n]_ _τ_


_t_ =0



According to the summation of geometric series, we
have



= _cnL_ +



_τ_ _−_ 1




_τ_ [+] _[n]_ _t_ = [1] _[ −]_ _[e]_ [2] _[πi]_ [(] _[m]_ [+] _[n]_ [)]




[+] _[n]_ =

_τ_




- _e_ [2] _[πi]_ _[m]_ _τ_ [+] _[n]_


_t_ =0




0 if _m_ + _n ̸_ = 0

_τ_ if _m_ + _n_ = 0



1 _−_ _e_ [2] _[πi]_ _[m]_ _τ_ [+] _[n]_



Then the energy of the periodic part is as follows:



_q_

- _cmcn_


_n_ = _−q_



_q_




_τ_ _−_ 1





- _e_ [2] _[πi]_ _[m]_ _τ_ [+] _[n]_


_t_ =0



_τ_ _t_



Then the energy of this harmonic series is computed
by



_Ep_ = _k_


= _k_


= _L_



_q_



_m_ = _−q_



_q_

- _τ_ _|cm|_ [2]


_m_ = _−q_


_q_

- _|cm|_ [2] (2)


_m_ = _−q_



_Eh_ =


=



_q_

- _|F_ [ _nk_ ] _|_ [2]


_n_ = _−q_



_τ_ _−_ 1

- _|F_ [ _nk_ ] _|_ [2] =


_n_ =0



_q_

- _|cnL_ +


_n_ = _−q_




- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _[n]_ _τ_


_t_ =0



_τ_ _[t]_ _|_ [2]



For the harmonic energy with the basis frequency _k_ =
_L/τ_, we first compute the following Fourier component



_L−_ 1




_L−_ 1



_t_ =0




- _cne_ [2] _[πi]_ _[n]_ _τ_


_n_ = _−q_



_τ_ _[t]_ _fr_ ( _t_ )



_q_




_L_ _[t]_



_q_
= _L_ [2] - _|cn|_ [2] + 2 _L_


_n_ = _−q_



_F_ [ _k_ ] =


=


=



_L−_ 1




�( _fp_ ( _t_ ) + _fr_ ( _t_ )) _e_ _[−]_ [2] _[πi]_ _L_ _[k]_


_t_ =0



_L−_ 1



_t_ =0


_L−_ 1



_t_ =0




- _cme_ [2] _[πi]_ _[m]_ _τ_ _[−]_ [1]


_m_ = _−q_



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _[n]_ _τ_


_t_ =0



_τ_ _[t]_ _|_ [2]




- _q_

 



_[m]_ _τ_ _[t]_ + _fr_ ( _t_ )



+







_q_

- _|_


_n_ = _−q_




- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



_e_ _[−]_ [2] _[πi]_ _τ_ _[t]_



_q_




_τ_ _[t]_ _fr_ ( _t_ ) _|_ (5)



_q_




_q_




_τ_ _[−]_ [1] _t_ +



_τ_



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _τ_ _[t]_


_t_ =0



_L−_ 1



_t_ =0




- _cne_ [2] _[πi]_ _[n]_ _τ_


_n_ = _−q_



_≥_ _L|Ep_ + 2



_L−_ 1





- _e_ [2] _[πi]_ _[m]_ _τ_ _[−]_ [1]


_t_ =0



_τ_ _[−]_ [1] _t_ +



_τ_



_L−_ 1





- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _τ_ _[t]_


_t_ =0



= _c_ 1 _L_ +


= _c_ 1 _L_ +



_q_

 - _cm_


_m_ = _−q,m_ =1




- _fr_ ( _t_ ) _e_ _[−]_ [2] _[πi]_ _τ_ _[t]_


_t_ =0



_q_




_L−_ 1




_τ_ (3)



As for the total energy _Ef_ = [�] _n_ _[L]_ =0 _[−]_ [1] _[|][F]_ [[] _[n]_ []] _[|]_ [2][, according]


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**



to Parseval’s theorem, we have



_L−_ 1

- _|f_ ( _t_ ) _|_ [2]


_t_ =0



_Ef_ =



_L−_ 1

- _|F_ [ _n_ ] _|_ [2] = _L_


_n_ =0




_[m]_ _τ_ _[t]_ + _fr_ ( _t_ )



�2



_L−_ 1



_t_ =0



= _L_




 - _q_

 



- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



+



_L−_ 1

- _fr_ [2][(] _[t]_ [)]

_t_ =0




- _q_

 


�2

_[m]_ _τ_ _[t]_



_L−_ 1

= _L_ (�



_L−_ 1




they may encode global features that significantly influence the signal’s periodicity or overall trend.


Given this, it is essential to consider scenarios where
high-frequency components are retained and refine the
theorem’s assumptions accordingly.


To accommodate high-frequency contributions, we can
extend the Fourier expansion as follows:




- _|cm|_ [2] + 
_m_ = _−q_ _|m|_



_L−_ 1





- _|cm|_ [2] ]


_|m|>q_



_Ep_ _[′]_ [=] _[ L]_ [[]



_q_




_t_ =0




- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



_τ_ _[t]_ _fr_ ( _t_ ))



_L−_ 1

+ 2 

_t_ =0


  


_q_





- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



_τ_ _[t]_ _fr_ ( _t_ )



(6)



_q_








Note that the same changes occur in the remaining
energy computation. Consequently, the energy ratio
of the periodic component to the residual component
would be redefined as

_λ_ _[′]_ = _[E][p]_ [ +] _[ E]_ [high freq]

_Er_


Leading to a revised inequality as



= _L_



_Ep_ + _Er_ + 2



_L−_ 1



_t_ =0




- _cme_ [2] _[πi]_ _[m]_ _τ_


_m_ = _−q_



So it is easy to compute that



_Eh_ _Ep_ + 2 [�] _t_ _[L]_ =0 _[−]_ [1] - _qn_ = _−q_ _[c][n][e]_ [2] _[πi]_ _[n]_ _τ_
_≥_
_Ef_ _E_ _E_ ~~[�]~~ _[L][−]_ [1] ~~�~~ _q_ _[c]_ _[e]_ [2]



_τ_ _[t]_ _fr_ ( _t_ )



_τ_ _[t]_ _fr_ ( _t_ )



_Ep_ + _Er_ + 2 ~~[�]~~ _t_ _[L]_ =0 _[−]_ [1] ~~�~~ _qn_ = _−q_ _[c][n][e]_ [2] _[πi]_ _[m]_ _τ_



_√_
_Eh_ _≥_ _λ_ _[′]_ _−_ ~~_√_~~ 2
_Ef_ _λ_ _[′]_ _−_ 2 _λ_ _[′]_



~~_√_~~
_λ_ _[′]_ _−_ 2



_λ_ _[′]_ + 1



_λ_ _[′]_



The last part satisfies the Cauchy-Schwarz inequality
as follows:

    - _L−_ 1 _q_ �2

  -  - [2] _[πi]_ _[m]_ _[t]_



_q_




_τ_ _[t]_ _fr_ ( _t_ )



2





- _cne_ [2] _[πi]_ _[m]_ _τ_


_n_ = _−q_



_t_ =0




_·_



�2

_[m]_ _τ_ _[t]_



_L−_ 1

- _fr_ [2][(] _[t]_ [)]

_t_ =0



_≤_



_L−_ 1



_t_ =0




- _q_

 



- _cne_ [2] _[πi]_ _[m]_ _τ_


_n_ = _−q_



_≤_ _Ep · Er_ (7)


which means that



_τ_ _[t]_ _fr_ ( _t_ ) _≤_ 


_Ep · Er_



_q_





_−_ 


_Ep · Er ≤_



_L−_ 1



_t_ =0




- _cne_ [2] _[πi]_ _[m]_ _τ_


_n_ = _−q_



Finally, we obtain that



_Eh_ _≥_ _Ep_ _−_ 2�
_Ef_ _Ep_ + _Er −_ 2



_√_

_· Er_ _λ −_ 2

= ~~_√_~~
_Ep · Er_ _λ −_ 2 _λ_



_λ −_ 2 _λ_

~~_√_~~
_λ −_ 2 _λ_



_λ_ + 1



_Ep_ _−_ 2� _Ep_ _· Er_

_Ep_ + _Er −_ 2 ~~�~~ _Ep ·_



**Discussion 1**


Although our proof disregards frequency components
above the Nyquist frequency, this simplification is both
computationally convenient and theoretically justified.
According to the Nyquist-Shannon sampling theorem,
frequency components beyond the Nyquist limit cannot be fully reconstructed from the sampling. However, in certain types of data, such as complex physical
systems, financial market trends, or physiological signals, high-frequency components are not merely noise;



**Discussion 2**


The theorem ensures that when a time series contains a
dominant periodic structure (i.e., when _λ_ is sufficiently
large), the harmonic energy ratio _Eh/Ef_ is bounded
away from zero, offering a degree of theoretical validation. While the bound is not guaranteed for all _λ_,
especially in weakly periodic settings, it still provides
insight into the asymptotic behavior of highly structured signals. In practice, we empirically observe that
even mildly periodic sequences exhibit non-negligible
harmonic energy concentration (e.g., _Eh/Ef >_ 0 _._ 1),
supporting the utility of this ratio as a continuous,
data-driven measure of periodicity.


**Experimental Details**


**Datasets**


Details about the datasets used in this work are given
below. Further descriptions are in Table 3.


**ETT (Electricity Transformer Temperature)** :
consists of two hourly datasets (ETTh) and two 15minute datasets (ETTm), containing loads and oil
temperature information of the electricity transformers collected from July 2016 to July 2018.


**Electricity** : electricity consumption of 321 customers
recorded hourly from 2012 to 2014.


**Traffic** : road occupancy rates recorded hourly from
highway sensors in San Francisco between 2015 and
2016.


**Jingjing Bai and Yoshinobu Kawahara**



**Solar Energy** : solar power production records sampled every 10 minutes from 137 PV plants in Alabama
State in 2006.


**Weather** : records of 21 meteorological indicators, including air temperature and humidity, collected at 10minute intervals during 2020.


**Baselines**


**FEDformer** (Zhou et al., 2022b)(ICML 2022): FEDformer employs seasonal-trend decomposition and a
novel attention mechanism applied in the frequency
domain to mitigate distribution shift between input
and output time series, improving robustness against
noise.
**FiLM** (Zhou et al., 2022a)(NeurIPS 2022): FiLM
(Frequency-Improved Legendre Memory Model) leverages polynomial projections to approximate historical
data, applies Fourier projections to eliminate noise,
and employs low-rank approximations to enhance
computational efficiency.
**PatchTST** (Nie et al., 2022)(ICLR 2023): PatchTST
segments time series into patches with predefined window sizes and strides, treating them as input tokens to
capture local temporal information. Furthermore, it
enforces channel independence, treating each variable
in a multivariate time series as a separate univariate
sequence while sharing the same embedding and
weight structures.
**iTransformer** (Liu et al., 2023)(ICLR 2024): iTransformer reverses the data dimensions, treating each
variable as a sequence and each timestep as a feature
dimension, enhancing the model’s ability to capture
inter-variable relationships and temporal trends more
effectively.
**FreTS** (Yi et al., 2024)(NeurIPS 2023): FreTS transforms time-series data from the time domain to the
frequency domain, where an MLP-based architecture
is designed to model global dependencies and key
patterns.
**DLinear** (Zeng et al., 2023)(AAAI 2023): DLinear challenges the effectiveness of Transformers in
time-series forecasting by introducing a lightweight
architecture that directly enables multi-step prediction. It decomposes time series into a trend series and
a remainder series, modeling them separately using
two single-layer linear networks.
**TimeMixer** (Wang et al., 2024a)(ICLR 2024):
TimeMixer employs a multiscale mixing architecture
to address the complex temporal variations in time
series forecasting. By utilizing Past-DecomposableMixing and Future-Multipredictor-Mixing blocks,
TimeMixer leveraged disentangled variations and
complementary forecasting capabilities.
**PDF** (Dai et al., 2024)(ICLR 2024): PDF decouples



variations of different scales in multi-variate long-term
time series based on their periodicity. It then leverages the respective modeling strengths of CNNs and
Transformer models to represent these scale-distinct
variations.
**TimesNet** (Wu et al., 2022)(ICLR 2023): TimesNet
focuses on temporal variation modeling and proposes
a representation method intended for incorporating
multiple intraperiod- and interperiod-variations. For
each selected frequency, the proposed method generates a two-dimensional representation of the original
time series and applies a convolutional network to the
representation.


**Implementation details**


All baseline models were implemented based on the
Time-Series-Library repository (Wang et al., 2024b).
The experiments were conducted using PyTorch and
trained on an NVIDIA A100 40GB GPU.
**Optimizer** : We employed ADAM with an initial
learning rate of 1 _e_ _[−]_ [4], which was adjusted using a cosine decay schedule.
**Batchsize** : For the Traffic and Electricity datasets,
batchsize was set to 32, while for all other datasets,
batchsize was set to 256 to ensure efficient training
while maintaining stable gradient updates.
**Early Stopping** : A three-epoch early stopping criterion was used to prevent overfitting.
**Model Configuration** : The model consists of three
encoder layers, designed to effectively capture hierarchical temporal dependencies. The loss function used
is L2 loss(Mean Squared Error, MSE), ensuring stable
optimization and accurate error minimization.


**Related Pseudocodes**


Here, we provide pseudocodes for several core operations in our model. To ensure dimensional alignment
after hierarchical frequency sampling, a zero-padding
operation is applied in both the time and frequency
branches before IFFT transformation, as shown in
Algorithm 2. Additionally, for the periodicity-aware
weighting mechanism, we employ a simple yet effective peak detection strategy to estimate the basis frequency. This method identifies the frequency component with the maximum amplitude (excluding the DC
component) as the dominant periodicity, which is then
used to compute harmonic energy. The corresponding
procedure is detailed in Algorithm 3.


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


Table 3: Dataset description.


Dataset Source Frequency Variate Length Time Range


ETTh1 Informer(Zhou et al., 2021) 1h 7 17420 2016-2017


ETTh2 Informer 1h 7 17420 2017-2018


ETTm1 Informer 15min 7 69680 2016-2017


ETTm2 Informer 15min 7 69680 2017-2018


Electricity UCI ML Repository 1h 321 26304 2012-2014


Solar energy (Lai et al., 2018) 10min 137 52560 2006


Traffic Informer 1h 862 17544 2015-2016


Weather MPI for Biogeochemistry 10min 21 52696 2020



**Algorithm 2** Zero Padding Operation after HFS

**Input:** Frequency slice _W_ ˜ _∈_ R _[B][×][F][ ×][C]_, Original
frequency length _M_, Start index _p_ _[n]_ for _n_ -th layer
**Output:** _W_ padded _∈_ R _[B][×][M]_ _[×][C]_ padded spectrum


1: Initialize _W_ padded _←_ **0** _[B][×][M]_ _[×][C]_

2: Compute end index _q_ _[n]_ _←_ _p_ _[n]_ + _F_
3: **for** _b_ = 1 to _B_ **do**
4: **for** _l_ = 1 to _C_ **do**
5: _W_ padded[ _b, p_ _[n]_ : _q_ _[n]_ _, l_ ] _←_ _W_ [˜] [ _b,_ : _, l_ ]
6: **end for**
7: **end for**
8: **return** _W_ padded


**Algorithm 3** Peak Detection for Periodicity-aware
Weighting
**Input:** Magnitude spectrum _|F_ _| ∈_ R _[B][×][M]_ _[×][C]_

**Output:** Basis frequency index _k ∈_ N for each channel


1: **for** _b_ = 1 to _B_ **do**
2: **for** _c_ = 1 to _C_ **do**
3: Remove DC component: _|F_ _|b,_ 0 _,c ←_ 0

_|F_ _|b,_ : _,c_
4: Normalize: _|F_ _|b,_ : _,c ←_ ~~�~~ _|F_ _|b,_ : _,c|_
5: _kb,c ←_ arg max _i≥_ 1 _|F_ _|b,i,c_ // Find dominant
frequency
6: **end for**
7: **end for**
8: **return** _kb,c_ for each ( _b, c_ )


**Model Efficiency**


We evaluated the efficiency of Dualformer under varying input lengths _L ∈{_ 192 _,_ 336 _,_ 720 _}_ on the Electricity
dataset with a fixed prediction length of 720. As shown
in Figure 7, Dualformer consistently demonstrates the



lowest memory usage (712MB to 2046MB) and fastest
running time per epoch (177s to 222s) among all baselines, demonstrating enhanced scalability for longer input sequences.


This efficiency stems from its unique dual-branch architecture and scale-aware processing strategy. Although the model incorporates both time-domain attention ( _O_ ( _L_ [2] )) and frequency-domain autocorrelation
( _LlogL_ ), its per-layer theoretical computational complexity remains _O_ ( _L_ [2] + _αLlogL_ ), where _α <_ 1 denotes the frequency sampling ratio. Crucially, the
_O_ ( _L_ [2] ) complexity of the time branch is only nominal. This is because each time-domain layer receives
a scale-specific, frequency-filtered input rather than
the full, raw signal. Through hierarchical frequency
sampling, the model only processes a small subset of
spectral components ( _αL_ frequency bins), which are
then zero-padded and IFFT-transformed back to the
time domain. This provides the time-domain branch
with a more refined, less noisy input at a specific scale.
This mechanism effectively decomposes global modeling into localized, scale-aware sub-tasks, which significantly reduces the effective computation required by
the time-domain attention in practice. By minimizing
redundant information and attention interference, it
enables efficient learning while effectively lowering the
constant factor associated with the _L_ [2] term, despite
retaining the nominal quadratic complexity.


Furthermore, the time and frequency branches are designed to execute in parallel. This parallelization leads
to improved GPU utilization, thereby reducing overall runtime. While parallel execution can often increase instantaneous memory peaks, Dualformer effectively manages memory overhead through optimized
memory management and activation re-computation
strategies (e.g., recomputing certain intermediate activations during gradient calculation instead of storing
them throughout(Korthikanti et al., 2022)), resulting


**Jingjing Bai and Yoshinobu Kawahara**



Figure 7: Memory usage and running time per epoch
of PatchTST, TimesNet, TimeMixer, and our Dualformer under varying input lengths and fixed prediction length _T_ = 720 on the Electricity dataset.


Figure 8: MSE comparison w.r.t different sampling ratios on four datasets. The orange lines indicate baselines with second-best performance.


in overall memory efficiency that remains significantly
superior to other baselines.


**Hyperparameter Sensitivity**


**Sampling ratio** _α_


We further conducted a sensitivity analysis on the
sampling ratio _α_, which controls the range of frequency
components selected at each layer. This analysis was
under an input-96-predict-96 setting on the ETTh1,
Electricity, ETTm1, and Weather datasets. As shown
in Fig. 8, the optimal value of _α_ varies across datasets.
For instance, lower sampling ratios (e.g., _α_ = 0 _._ 4) yield
better performance on ETT and Weather datasets,
likely due to the importance of high-frequency variations. In contrast, the Electricity dataset shows less
sensitivity to _α_, possibly due to its dominant lowfrequency patterns. Notably, across all datasets, our
model with a well-tuned _α_ surpasses the performance



Figure 9: Performance comparison w.r.t different top_k_ time delay aggregation on fourt datasets.


of strong baseline models, highlighting the importance
of tuning this hyperparameter to the specific characteristics of the time-series data to achieve optimal results.


**Autocorrelation factor** _k_ **lags**


We also conducted a sensitivity analysis on the autocorrelation factor _k_ lags to investigate its impact on
prediction performance. As shown in Figure 9, we
compared the baseline model (PatchTST) with different values of _k_ lags ranging from 1 to 5 under the
input-96-predict-96 setting. The results indicate that
moderate values of _k_ lags generally lead to improved
accuracy. For instance, _k_ lags = 3 yields the best performance on ETTh1, while _k_ lags = 4 achieves the lowest error on both Electricity and Traffic datasets. In
contrast, the Weather dataset benefits most from a
smaller _k_ lags = 1, suggesting that overly aggregating delayed dependencies may introduce noise for data
with weak temporal patterns. These findings highlight
the importance of carefully tuning _k_ lags to balance dependency modeling and noise suppression.


**Number of harmonics** _n_


We similarly conducted a sensitivity analysis on the
number of harmonics _n_ utilized in the periodicityaware weighting algorithm. We varied _n_ from 1 to
5 under the input-96-predict-96 setting across the
ETTh1, ETTm1, Electricity, and Weather datasets.
As shown in Figure 10, while the exact optimal value
exhibits slight variations across datasets (e.g., _n_ = 4
for Electricity and _n_ = 2 for Weather), setting _n_ = 3
generally achieves the best or near-optimal forecasting accuracy. Specifically, we observe that relying on
a single harmonic ( _n_ = 1) often yields suboptimal results, whereas increasing _n_ beyond 3 offers marginal
gains and, in some cases (such as ETTh1), leads to
performance degradation likely caused by noise from
higher-order harmonics.


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


Figure 10: Performance comparison w.r.t different number of harmonics _n_ on four datasets.


**Dataset** **Metrics** DLinear iTransformer Dualformer PathTST


MAE 1.76 1.71 1.60 **1.58**
_ETTh1_ RMSE 3.12 3.15 3.08 **3.06**
WAPE 37.61% 36.89% 34.49% **34.06%**


MAE 1.54 1.53 **1.37** 1.39
_ETTm1_ RMSE 2.92 2.89 3.01 **2.78**
WAPE 33.36% 33.03% **29.60%** 30.02%


MAE 281.63 295.98 **253.57** 256.60
_Electricity_ RMSE 2936.83 **2761.75** 2896.04 2883.63
WAPE 10.56% 11.91% 9.51% **9.38%**



MAE 12.29 15.61 **10.85** 12.08
_Weather_ RMSE 44.57 **40.74** 41.70 43.22
WAPE 7.29% 6.75% **6.49%** 7.32%


Table 4: Average forecasting results on re-normalized
data. **Bold** indicates the best performance, while
underline indicates the second best.


**Supplementary of Main Results**


**Ablation study**


We conducted several ablation studies to demonstrate
the contribution of each model component on the
ETTh1 dataset in section 4.2. The full results are
available in Table 5.


**Model robustness**


All experiments were repeated three times using three
different random seeds to ensure statistical robustness. We report the mean and standard deviation
(Mean ± STD) for Dualformer and three other models
(TimeMixer, PatchTST, and iTransformer). As shown
in Table 6, our Dualformer model consistently achieves
strong and stable performance across various datasets
and prediction lengths, highlighting both its effectiveness and reliability.


**Normalized vs. Re-normalized evaluation**


Previous work in long-term time series forecasting
(LTSF) typically reports MAE and MSE on z-score
normalized data (Zhou et al., 2021; Wu et al., 2021;



Figure 11: The forecasting performance on ETTh1
and Weather datasets, across 4 different look-back
window sizes. The prediction length was fixed as
_T_ = 336.


Zhou et al., 2022b). However, these absolute metrics
are affected by the original data scale, making the results difficult to interpret. Normalized evaluation can
produce deceptively low error values, which may not
reflect real-world performance. To address this issue,
we further assessed model predictions on re-normalized
data using MAE, RMSE, and WAPE.


Table 4 presents results on re-normalized data across
four datasets. Compared to Table 1, it shows that
errors appear larger when evaluated on the original
scale. Despite this, our model demonstrates stable performance across all datasets. Notably, the consistently
low WAPE values indicate that Dualformer generalizes
well and remains robust to varying data distributions.


**Look-back window analysis**


To evaluate the impact of the input window size
on forecasting performance, we conducted experiments by varying the look-back window size _L ∈_
_{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_, while keeping the prediction length
_T_ = 336. As shown in Fig. 11, longer input consistently improved the forecasting accuracy on both
ETTh1 and Weather datasets. This highlights the
benefit of using extended temporal context in longterm forecasting tasks.


**Jingjing Bai and Yoshinobu Kawahara**


Original w/o T ~~b~~ ranch w/o F branch w/o weighting w/o RevIN


MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


96 0.359 0.386 0.449 0.455 0.916 0.741 0.395 0.419 0.377 0.396
192 0.398 0.412 0.468 0.465 0.975 0.765 0.424 0.445 0.409 0.437
336 0.432 0.431 0.506 0.494 1.099 0.819 0.446 0.466 0.440 0.449
720 0.438 0.453 0.516 0.513 1.191 0.865 0.491 0.468 0.446 0.466


_Avg._ 0.407 0.420 0.485(+0.078) 0.482(+0.062) 1.045(+0.638) 0.798(+0.378) 0.440(+0.033) 0.449(+0.028) 0.439(+0.032) 0.452(+0.031)


Table 5: Ablation study results on ETTh1 dataset.


Models **Dualformer** **TimeMixer** **PatchTST** **iTransformer**


Metric MSE MAE MSE MAE MSE MAE MSE MAE


96 **0.359** _±_ **0.0014 0.386** _±_ **0.0003** 0.361 _±_ 0.0003 0.390 _±_ 0.0002 0.373 _±_ 0.0009 0.402 _±_ 0.0008 0.381 _±_ 0.0021 0.395 _±_ 0.0024
192 **0.398** _±_ **0.0015 0.412** _±_ **0.0010** 0.409 _±_ 0.0005 0.414 _±_ 0.0008 0.411 _±_ 0.0014 0.428 _±_ 0.0015 0.437 _±_ 0.0020 0.424 _±_ 0.0022
336 0.432 _±_ 0.0011 0.431 _±_ 0.0007 **0.430** _±_ **0.0011 0.429** _±_ **0.0013** 0.432 _±_ 0.0059 0.445 _±_ 0.0048 0.479 _±_ 0.0040 0.446 _±_ 0.0037
720 **0.438** _±_ **0.0016 0.453** _±_ **0.0016** 0.445 _±_ 0.0034 0.460 _±_ 0.0033 0.455 _±_ 0.0049 0.473 _±_ 0.0038 0.481 _±_ 0.0055 0.470 _±_ 0.0039


96 **0.283** _±_ **0.0010 0.330** _±_ **0.0006** 0.291 _±_ 0.0005 0.340 _±_ 0.0004 0.288 _±_ 0.0006 0.341 _±_ 0.0010 0.334 _±_ 0.0004 0.368 _±_ 0.0004
192 **0.326** _±_ **0.0007 0.355** _±_ **0.0005** 0.327 _±_ 0.0001 0.365 _±_ 0.0004 0.332 _±_ 0.0009 0.370 _±_ 0.0007 0.377 _±_ 0.0014 0.391 _±_ 0.0006
336 **0.359** _±_ **0.0009 0.377** _±_ **0.0005** 0.360 _±_ 0.0022 0.381 _±_ 0.0018 0.362 _±_ 0.0018 0.392 _±_ 0.0002 0.426 _±_ 0.0008 0.420 _±_ 0.0004
720 **0.412** _±_ **0.0013 0.408** _±_ **0.0005** 0.415 _±_ 0.0001 0.417 _±_ 0.0003 0.416 _±_ 0.0002 0.419 _±_ 0.0003 0.491 _±_ 0.0009 0.459 _±_ 0.0007


96 **0.128** _±_ **0.0001 0.221** _±_ **0.0001** 0.135 _±_ 0.0003 0.222 _±_ 0.0004 0.129 _±_ 0.0005 0.223 _±_ 0.0005 0.131 _±_ 0.0000 0.227 _±_ 0.0002
192 **0.145** _±_ **0.0002** 0.237 _±_ 0.0001 0.147 _±_ 0.0006 **0.235** _±_ **0.0007** 0.149 _±_ 0.0006 0.243 _±_ 0.0014 0.148 _±_ 0.0002 0.244 _±_ 0.0003
336 **0.160** _±_ **0.0004** 0.253 _±_ 0.0004 0.164 _±_ 0.0004 **0.245** _±_ **0.0003** 0.165 _±_ 0.0010 0.260 _±_ 0.0012 0.164 _±_ 0.0001 0.262 _±_ 0.0003
720 **0.197** _±_ **0.0010 0.285** _±_ **0.0006** 0.212 _±_ 0.0005 0.310 _±_ 0.0004 0.200 _±_ 0.0027 0.292 _±_ 0.0020 0.200 _±_ 0.0003 0.295 _±_ 0.0002


96 0.377 _±_ 0.0015 **0.256** _±_ **0.0004 0.366** _±_ **0.0007** 0.259 _±_ 0.0008 0.383 _±_ 0.0182 0.272 _±_ 0.0174 0.395 _±_ 0.0012 0.268 _±_ 0.0001
192 0.392 _±_ 0.0001 0.263 _±_ 0.0001 0.381 _±_ 0.0005 0.265 _±_ 0.0003 **0.380** _±_ **0.0011 0.259** _±_ **0.0007** 0.417 _±_ 0.0001 0.276 _±_ 0.0002
336 0.406 _±_ 0.0008 **0.266** _±_ **0.0007 0.397** _±_ **0.0008** 0.269 _±_ 0.0008 0.410 _±_ 0.0129 0.285 _±_ 0.0147 0.433 _±_ 0.0006 0.283 _±_ 0.0008
720 0.447 _±_ 0.0020 **0.287** _±_ **0.0008 0.429** _±_ **0.0016** 0.292 _±_ 0.0019 0.454 _±_ 0.0018 0.313 _±_ 0.0014 0.467 _±_ 0.0009 0.302 _±_ 0.0002


Table 6: Multivariate long-term forecasting results with error bars (Mean ± STD). All experiments were repeated
3 times. **Bold** indicates the best.


**Full Results**


Here we show the full results for multivariate forecasting (Table 7) and univariate forecasting (Table 8). The
**best** and second-best results are highlighted. Our
dualformer model consistently outperforms selected
baselines on most datasets across different prediction
lengths.


**Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting**


Models **Dualformer TimeMixer** **PatchTST** **iTransformer** **PDF** **FEDformer TimesNet** **FiLM** **DLinear** **FreTS**
(Ours) (2024) (2023) (2024) (2024) (2022) (2023) (2022) (2023) (2023)


Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


96 **0.359 0.386** 0.361 0.390 0.373 0.402 0.381 0.395 0.369 0.397 0.376 0.416 0.384 0.402 0.377 0.401 0.375 0.396 0.386 0.405
192 **0.398 0.412** 0.409 0.414 0.411 0.428 0.437 0.424 0.401 0.416 0.422 0.445 0.436 0.429 0.419 0.428 0.428 0.437 0.441 0.436
336 0.432 0.431 0.430 0.429 0.432 0.445 0.479 0.446 **0.418 0.427** 0.452 0.463 0.491 0.469 0.466 0.466 0.448 0.449 0.487 0.458
720 **0.438 0.453** 0.445 0.460 0.455 0.473 0.481 0.470 0.439 0.454 0.483 0.496 0.521 0.500 0.499 0.512 0.505 0.514 0.503 0.491


96 **0.268 0.329** 0.271 0.330 0.275 0.338 0.288 0.338 0.274 0.337 0.343 0.385 0.340 0.374 0.280 0.343 0.296 0.360 0.297 0.349
192 0.332 **0.370 0.317** 0.402 0.338 0.380 0.374 0.390 0.351 0.383 0.429 0.438 0.402 0.414 0.345 0.390 0.391 0.423 0.380 0.400
336 0.359 **0.393 0.332** 0.396 0.366 0.403 0.415 0.426 0.375 0.411 0.489 0.485 0.452 0.452 0.372 0.415 0.445 0.460 0.428 0.432
720 0.381 0.417 **0.342 0.408** 0.391 0.431 0.420 0.440 0.402 0.438 0.463 0.481 0.462 0.468 0.438 0.455 0.700 0.592 0.427 0.445


96 **0.283 0.330** 0.291 0.340 0.288 0.341 0.334 0.368 0.284 0.339 0.356 0.406 0.338 0.375 0.303 0.345 0.301 0.344 0.355 0.376
192 0.326 **0.355** 0.327 0.365 0.332 0.370 0.377 0.391 **0.322** 0.362 0.391 0.424 0.327 0.387 0.342 0.369 0.336 0.366 0.391 0.392
336 **0.359 0.377** 0.360 0.381 0.362 0.392 0.426 0.420 0.360 0.380 0.441 0.453 0.410 0.411 0.371 0.387 0.371 0.387 0.424 0.415
720 **0.412 0.408** 0.415 0.417 0.416 0.419 0.491 0.459 0.421 0.416 0.482 0.476 0.478 0.450 0.430 0.416 0.426 0.422 0.487 0.450


96 **0.157 0.244** 0.164 0.254 0.162 0.254 0.180 0.264 0.167 0.258 0.189 0.281 0.187 0.267 0.167 0.257 0.170 0.264 0.182 0.265
192 **0.213 0.282** 0.223 0.295 0.217 0.293 0.250 0.309 0.221 0.294 0.257 0.324 0.249 0.309 0.219 0.293 0.233 0.311 0.246 0.304
336 **0.264 0.318** 0.279 0.330 0.267 0.326 0.311 0.348 0.270 0.327 0.325 0.364 0.321 0.351 0.273 0.331 0.300 0.358 0.307 0.342
720 **0.345 0.371** 0.359 0.383 0.353 0.382 0.412 0.407 0.356 0.382 0.429 0.424 0.408 0.403 0.356 0.387 0.422 0.439 0.407 0.398


96 **0.128 0.221** 0.135 0.222 0.129 0.223 0.131 0.227 0.148 0.240 0.189 0.305 0.168 0.272 0.154 0.248 0.140 0.237 0.133 0.229
192 **0.145** 0.237 0.147 **0.235** 0.149 0.243 0.148 0.244 0.162 0.253 0.205 0.320 0.184 0.289 0.167 0.260 0.154 0.250 0.152 0.248
336 **0.160** 0.253 0.164 **0.245** 0.165 0.260 0.164 0.262 0.178 0.269 0.212 0.327 0.198 0.300 0.189 0.285 0.169 0.268 0.167 0.263
720 **0.197 0.285** 0.212 0.310 0.200 0.292 0.200 0.295 0.225 0.317 0.245 0.352 0.220 0.320 0.250 0.341 0.204 0.300 0.201 0.295


96 0.175 0.228 **0.167 0.220** 0.224 0.278 0.203 0.237 0.181 0.240 0.201 0.304 0.219 0.314 0.188 0.252 0.289 0.337 0.284 0.325
192 **0.186 0.235** 0.187 0.249 0.253 0.298 0.233 0.261 0.196 0.252 0.237 0.337 0.231 0.322 0.215 0.280 0.319 0.397 0.307 0.362
336 **0.200** 0.246 **0.200** 0.258 0.273 0.306 0.248 0.273 0.216 **0.243** 0.254 0.362 0.246 0.337 0.222 0.267 0.352 0.415 0.333 0.384
720 **0.203 0.249** 0.215 0.250 0.272 0.308 0.249 0.275 0.220 0.256 0.280 0.397 0.280 0.363 0.226 0.264 0.356 0.412 0.335 0.383


96 0.377 0.256 0.366 0.259 0.383 0.272 0.395 0.268 **0.360 0.249** 0.577 0.360 0.593 0.321 0.413 0.290 0.412 0.286 0.649 0.389
192 0.392 0.263 0.381 0.265 0.380 0.259 0.417 0.276 **0.379 0.256** 0.607 0.374 0.617 0.336 0.409 0.289 0.424 0.291 0.601 0.366
336 0.406 **0.266** 0.397 0.269 0.410 0.285 0.433 0.283 **0.385** 0.270 0.624 0.384 0.629 0.336 0.425 0.299 0.438 0.299 0.609 0.369
720 0.447 0.287 **0.429** 0.292 0.454 0.313 0.467 0.302 0.430 **0.281** 0.625 0.381 0.640 0.350 0.525 0.373 0.467 0.317 0.647 0.387


96 **0.143 0.188** 0.147 0.197 0.148 0.200 0.174 0.214 0.145 0.197 0.221 0.304 0.172 0.220 0.194 0.234 0.175 0.235 0.192 0.232
192 **0.186 0.230** 0.189 0.239 0.191 0.241 0.221 0.254 0.187 0.238 0.325 0.372 0.219 0.261 0.229 0.266 0.216 0.274 0.240 0.271
336 **0.236 0.269** 0.241 0.280 0.240 0.281 0.278 0.296 0.240 0.280 0.386 0.408 0.280 0.306 0.266 0.295 0.262 0.314 0.292 0.307
720 0.314 **0.326** 0.310 0.330 **0.307** 0.329 0.358 0.347 0.315 0.330 0.415 0.423 0.365 0.359 0.323 0.340 0.327 0.367 0.364 0.353


Table 7: Full result of multivariate long-term time series forecasting on eight datasets with various prediction
lengths _T ∈{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_ and fixed look-back window size _L_ = 96.


Models Dualformer Autoformer FEDformer FiLM PatchTST iTransformer FreTS DLinear


Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


96 **0.054 0.176** 0.088 0.238 0.084 0.220 0.057 0.182 **0.054** 0.177 0.055 0.181 0.071 0.206 0.056 0.180
192 **0.069 0.202** 0.097 0.239 0.108 0.249 0.072 0.207 0.071 0.205 0.071 0.204 0.104 0.245 0.075 0.209
336 **0.077 0.221** 0.104 0.260 0.123 0.278 0.084 0.230 0.083 0.228 0.081 0.226 0.107 0.258 0.092 0.238
720 0.090 0.237 0.132 0.290 0.146 0.303 0.091 0.239 0.086 0.234 **0.080 0.226** 0.126 0.283 0.168 0.334


96 **0.126** 0.274 0.152 0.303 0.129 **0.271** 0.128 0.273 0.130 0.283 0.129 0.278 0.128 **0.271** 0.132 0.280
192 **0.163 0.319** 0.191 0.340 0.187 0.330 0.189 0.343 0.169 0.328 0.169 0.324 0.185 0.330 0.176 0.329
336 **0.188 0.351** 0.233 0.380 0.231 0.378 0.201 0.364 0.193 0.358 0.194 0.355 0.231 0.378 0.210 0.369
720 **0.216 0.373** 0.260 0.404 0.278 0.422 0.224 0.380 0.221 0.379 0.225 0.381 0.268 0.409 0.290 0.438


96 **0.026 0.121** 0.050 0.174 0.034 0.143 0.029 0.128 **0.026 0.121 0.026** 0.122 0.033 0.140 0.028 0.125
192 **0.039** 0.150 0.110 0.250 0.066 0.203 0.041 0.154 **0.039** 0.150 **0.039** 0.149 0.058 0.186 0.043 0.154
336 **0.052** 0.173 0.085 0.236 0.071 0.210 0.053 0.174 0.053 0.173 **0.052 0.172** 0.071 0.209 0.062 0.183
720 **0.070 0.200** 0.120 0.283 0.109 0.259 0.071 0.205 0.073 0.206 0.073 0.207 0.102 0.248 0.080 0.211


96 **0.062 0.182** 0.098 0.241 0.066 0.197 0.066 0.191 0.065 0.186 0.063 **0.182** 0.063 0.189 0.064 0.184
192 0.091 0.224 0.164 0.315 0.104 0.250 0.096 0.235 0.094 0.231 **0.090 0.223** 0.102 0.245 0.092 0.227
336 **0.117** 0.260 0.178 0.325 0.155 0.303 0.123 0.269 0.120 0.265 **0.117 0.259** 0.130 0.279 0.122 0.265
720 **0.170** 0.321 0.189 0.340 0.193 0.343 0.173 0.323 0.172 0.322 0.175 **0.320** 0.178 0.325 0.174 **0.320**


Table 8: Full result of univariate long-term time series forecasting on ETT datasets with various prediction
lengths _T ∈{_ 96 _,_ 192 _,_ 336 _,_ 720 _}_ and fixed look-back window size _L_ = 96.


