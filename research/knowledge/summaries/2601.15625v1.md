## **Robust Tool Use via FISSION-GRPO: Learning to Recover from Execution** **Errors**

**Zhiwei Zhang** **[1]** **, Fei Zhao** **[2]** **, Rui Wang** **[1]** **, Zezhong Wang** **[1]** **,**
**Bin Liang** **[1]**, **Jiakang Wang** **[2]**, **Yao Hu** **[2]**, **Shaosheng Cao** **[2]**, **Kam-Fai Wong** **[1]**


1The Chinese University of Hong Kong,
2Xiaohongshu Inc.
zhangzhiwei1019@link.cuhk.edu.hk, caoshaosheng@xiaohongshu.com



**Abstract**


Large language models (LLMs) can call tools
effectively, yet they remain brittle in multi-turn
execution: following a tool call error, smaller
models often degenerate into repetitive invalid
re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders
reliable real-world deployment, where the execution errors are inherently inevitable during
tool interaction procedures. We identify a key
limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse
negative rewards, providing no guidance on
_how_ to recover, while pre-collected synthetic
error-correction datasets suffer from distribution mismatch with the model’s on-policy error modes. To bridge this gap, we propose
FISSION-GRPO, a framework that converts
execution errors into corrective supervision
within the RL training loop. Our core mechanism _fissions_ each failed trajectory into a new
training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy.
This enables the model to learn from the precise errors it makes during exploration, rather
than from static, pre-collected error cases. On
the BFCL v4 Multi-Turn, FISSION-GRPO improves the error recovery rate of Qwen3-8B by
5.7% absolute, crucially, yielding a 4% overall
accuracy gain (42.75% _→_ 46.75%) over GRPO
and outperforming specialized tool-use agents.


**1** **Introduction**


Agentic AI is moving from prototypes to production, driving demand for tool-using agents that are
not only capable but also efficient enough for lowlatency and on-device deployment (Belcak et al.,
2025). Smaller language models (SLMs) are increasingly recognized as the practical foundation
for such systems (Sharma and Mehta, 2025). However, for SLMs to fulfill this role, they must exhibit _robustness_, the ability to handle the inevitable



**TURN 1: SUCCESS**


**USER: Check status of flight JL-123.**


**ASSISTANT:** get_flight_status (flight_id='JL-123').


TOOL_OUTPUT: {'status': 'CHECKED_IN'}


**The flight JL-123 is checked in and ready to go.**

**TURN 2: ERROR TRIGGER**


**USER: Cancel this flight immediately.**


**ASSISTANT:** cancel_flight(flight_id='JL-123').


TOOL OUTPUT: Error 409: StateConflict.
Status is CHECKED_IN. Must be OPEN.


**TURN 3: HALLUCINATED FIX**

**ASSISTANT:** cancel_flight (flight_id='JL-123', force=True)


TOOL OUTPUT: Error: cancel_flight got
unexpected argument 'force'.


**TURN 4: REPETITIVE HALLUCINATION (COLLAPSE)**

**ASSISTANT:** cancel_flight (flight_id='JL-123', force=True)


TOOL OUTPUT: Error: cancel_flight got
unexpected argument 'force'.

**…**

**FAILURE**


**(a)** Typical failure: an API error triggers a hallucinated
retry loop.


**(b)** Error recovery rates on BFCL v4 Multi-Turn across
model scales and evaluation subsets.


**Figure 1:** Error recovery is a key bottleneck for smaller toolusing models in multi-turn execution. (a) shows a representative hallucinated retry loop after an API error, while (b) reports
recovery rates on BFCL v4 across model scales.


execution errors that arise in dynamic, multi-turn
tool-use environments (Patil et al., 2025).
This robustness requirement exposes a critical
gap. In practice, APIs return errors, parameters
become invalid, and system states change unexpectedly; a robust agent must interpret such feed


1


back, diagnose the fault, and self-correct (Yao et al.,
2022; Shinn et al., 2023). Yet as shown in Figure 1b, smaller models exhibit a pronounced deficiency in this error recovery capability (defined as
success rate conditioned on at least one prior execution error). On the BFCL v4 Multi-Turn benchmark (Patil et al., 2025), Claude Sonnet 4 achieves
recovery rates exceeding 50%, while Qwen3-8B
averages only around 20% across the four evaluation subsets. Figure 1a illustrates a representative failure mode: upon receiving an error (e.g.,
StateConflict), the model fails to diagnose the
root cause and instead hallucinates invalid parameters (e.g., a non-existent force argument), entering
a repetitive loop until the conversation collapses.
Bridging this robustness gap is essential for enabling smaller models to serve as reliable foundations for agentic systems.
Current approaches fall short of addressing this
challenge. Methods based on **static synthetic**
**datasets** (Liu et al., 2024; Zhang et al., 2025a,b)
construct error-correction pairs offline, but the error
distribution shifts as the policy improves, making
offline error corpora quickly stale and leading to
distribution mismatch. Meanwhile, **reinforcement**
**learning (RL)** approaches such as GRPO (Shao
et al., 2024) treat errors merely as sparse negative
rewards. This signals that something went wrong,
but offers no guidance on _how_ to recover: the gradient discourages the failed action without teaching
a corrective alternative. When all sampled rollouts fail, the advantage variance collapses, yielding
vanishing gradients that stall learning entirely (Yu
et al., 2025; Nan et al., 2025). In essence, existing methods treat errors as outcomes to be _avoided_
rather than opportunities to be _learned from_ .
To bridge this gap, we propose FISSION-GRPO,
a framework that transforms execution errors into
dense, on-policy-aligned corrective supervision
(Figure 2). The framework operates in three stages.
In **Stage 1**, we perform standard GRPO exploration,
sampling multiple rollouts per query and updating
the policy with group-relative advantages under the
GRPO objective. In **Stage 2**, failed rollouts are intercepted and augmented with diagnostic feedback
from a learned **Error Simulator**, constructing corrective contexts of the form [ _dialogue_ ; _failed call_ ;
_feedback_ ]. In **Stage 3**, these contexts trigger a _fis-_
_sion_ update: each error is expanded into _G_ _[′]_ parallel
recovery attempts by resampling new rollouts conditioned on the augmented context, analogous to
nuclear fission where one event induces a multi


plicative chain of subsequent reactions and thus
generates many new training signals.
The Error Simulator is trained via supervised
fine-tuning to produce realistic, context-aware diagnostics that resemble runtime error traces. To
avoid trivial target leakage, its outputs are restricted
to non-revealing error descriptions (e.g., “parameter status expects value OPEN”) rather than the
full target call. This closed-loop process continuously focuses learning on the model’s current error modes, mitigating the distribution mismatch of
static error-correction datasets.
We evaluate FISSION-GRPO on the BFCL v4
Multi-Turn benchmark and demonstrate substantial
improvements. Our main contributions are:

- **Fission-GRPO Framework.** We propose a RL
framework that dynamically converts execution
errors into corrective training instances. By
resampling from augmented error contexts onpolicy, our approach maintains alignment with
the model’s evolving error distribution.

- **Learned Error Simulator.** We develop a supervised fine-tuned error simulator to generate
realistic diagnostic feedback resembling runtime
error traces, enabling effective recovery training
without live API interactions or target leakage.

- **Empirical Validation.** On the BFCL v4 MultiTurn benchmark, FISSION-GRPO achieves stateof-the-art performance across the Qwen3 model
family (1.7B, 4B, and 8B), consistently outperforming GRPO, DAPO, and Dr.GRPO baselines.
For Qwen3-8B, our method improves the error
recovery rate by 5.7% absolute, yielding a 4%
overall accuracy gain (42.75% _→_ 46.75%) and
surpassing specialized 8B-scale tool agents.


**2** **Related Work**


**2.1** **RL for Tool Use**


RL has become the standard for aligning
LLMs (Schulman et al., 2017; Ouyang et al., 2022).
Among recent algorithms, GRPO (Shao et al.,
2024) reduces memory overhead by estimating
baselines from group averages, making it particularly suitable for tool-calling tasks characterized
by binary or scalar rewards (Guo et al., 2025).
Despite its efficiency, GRPO relies on intragroup variance, creating vulnerabilities when a
sampled group is homogeneously incorrect. In
such cases, the reward variance drops to zero, yielding null gradients and wasting training signals—a
limitation targeted by DAPO (Yu et al., 2025) and



2


**Stage1:Standard GRPO**

**Sampling & Update**


Rollout


**Sample G**
**rollouts m**


Rollout



**GRPO**
**Update**



**Stage2:Error Identification &**
**Corrective Sample Construction**


**Recursive Fission Loop**
**(Optional, if new errors Rʹ< )**



Input : **Corrective**

**Sample**



**Standard**



Identify

Errors



**Stage3:Corrective Batch Training**

**(Fission Occurs !)**


Resample
G rollouts

using m


**Batch Trigger**
**(Size=2 Reached!)**


Resample
G rollouts

using m



**Corrective**
**Sample Pool**


Accumulated: 2


**Corrective**

**Sample**


**Recursive Fission Loop**
**(Optional, if new errors Rʹ< )**



Query



**Policy**
**Model** **Error**
**Simulator**



Rollout



Identify

Errors Input :



Rollout



**Core Fission Mechanism: 2 Errors (Stage 2) →2 Corrective Samples→ 2 x 4= 8 New Rollouts (Stage 3)**


**Figure 2: Overview of the FISSION-GRPO Framework.** The framework operates in three stages: (1) **Standard Exploration**,
utilizing GRPO to optimize policy _πθ_ on the query distribution _D_ ; (2) **Error Identification & Synthesis**, where a simulator _Sϕ_
generates diagnostic feedback for filtered error trajectories; and (3) **Fission-based Update**, where corrective samples trigger a
multiplicative resampling process (factor _G_ _[′]_ ) to align the policy with recovery paths.



NGRPO (Nan et al., 2025). Furthermore, even
when gradients exist, blindly applying negative
feedback can trigger _Lazy Likelihood Displacement_
_(LLD)_ (Deng et al., 2025), where valid reasoning
steps are suppressed simply because they appear in
failed trajectories.
While strategies exist to mitigate these issues, such as filtering homogeneous batches
(DAPO), calibrating advantages (NGRPO), or
down-weighting negative gradients (NTHR (Deng
et al., 2025)), they primarily optimize the _loss land-_
_scape_ of negative signals. They do not fundamentally address the scarcity of positive guidance during exploration. Our approach bridges this gap by
actively constructing recovery trajectories via fission, transforming zero-reward errors into dense,
supervised learning signals.


**2.2** **Robust Tool Use and Error-Driven**
**Synthesis**


Research in tool utilization has evolved from ensuring syntactic correctness in single-turn interactions (Schick et al., 2023; Patil et al., 2024) to
maintaining reliability across complex, multi-turn
workflows (Qin et al., 2023; Yao et al., 2022). As
tasks grow in complexity, the capacity to recover
from inevitable environment errors (e.g., timeouts,
invalid parameters) becomes a defining metric of
robustness. This requirement is codified in benchmarks like BFCL (Patil et al., 2025) and Stable


ToolBench (Guo et al., 2024), which specifically
evaluate persistence under error conditions.
To address these challenges, recent approaches
have formalized “diagnosis-and-repair” mechanisms (Su et al., 2025; Huang et al., 2025) or
trained models on diverse error scenarios (Vuddanti
et al., 2025). In parallel, synthetic correction methods, originally proven in reasoning domains (Pan
et al., 2025; Xu et al., 2025), have been adapted to
tool-use by frameworks like ToolACE (Liu et al.,
2024) and LoopTool (Zhang et al., 2025b) to expand training coverage through model-based synthesis.
However, a critical limitation persists: these
methods predominantly rely on _offline_ data construction. This creates a temporal mismatch where
static training data fails to reflect the model’s evolving on-policy error distribution (Kumar et al., 2024;
Zhang et al., 2025b). Unlike prior offline synthesis approaches (Pan et al., 2025; Su et al., 2025),
our work integrates error simulation directly into
the training loop, ensuring alignment with current
policy limitations.


**3** **Method**


We propose FISSION-GRPO, a framework designed to imbue small language models with robust
error recovery capabilities. As illustrated in Figure 2, our approach operates in a dual-stream manner: standard exploration to maintain general tool


3


use competence, and a conditional fission stream
that intercepts errors to enable active remedial
learning.


**3.1** **Preliminaries**


We formulate tool use as a language generation
task. Given a query _x_ and a tool library, a policy
_πθ_ generates a trajectory _τ_ consisting of reasoning
thoughts and tool calls. We adopt **GRPO** (Shao
et al., 2024) as our optimization backbone. Unlike PPO, GRPO eliminates the need for a value
network by estimating the baseline from the group
average. For each query _x_, we sample a group of
outputs _{τi}_ _[G]_ _i_ =1 [and optimize:]



from 2 to 3 to prioritize parameter precision in later
stages.


**Efficiency Regularization (** _R_ **len).** To prevent
verbose or degenerate reasoning, we impose a
length penalty _R_ len _∈_ [0 _,_ 1] via a piecewise Gaussian function with time-annealing tolerance.


**3.3** **The FISSION-GRPO Framework**


As illustrated in Figure 2, FISSION-GRPO operates in a three-stage closed loop. Stage 1 focuses
on optimizing **fundamental tool-use capabilities**,
while Stages 2 and 3 are dedicated to developing
**error recovery skills** through targeted error correction.


**3.3.1** **Stage 1: Standard Exploration and**
**Update**


This stage aims to establish and maintain the
model’s base performance on tool-calling tasks.


**Sampling and Evaluation.** Given a query _x_, we
sample a group of trajectories _{τi}_ _[G]_ _i_ =1 [from the cur-]
rent policy _πθ_ . We evaluate these rollouts using the
composite reward function defined in §3.2, computing format compliance _R_ fmt, functional correctness
_R_ corr, and efficiency regularization _R_ len, which are
then aggregated into the total reward.


**Optimization.** We apply the standard GRPO update (Eq. 1) using these trajectories to improve the
model’s fundamental tool-use capabilities. This
step ensures continuous optimization on the core
skill of accurate tool invocation and parameter
grounding. Subsequently, all sampled trajectories
from this stage are forwarded to Stage 2 for diagnostic error analysis and corrective training.


**3.3.2** **Stage 2: Error Identification and**
**Corrective Sample Construction**


Stage 2 converts error traces produced in Stage 1
into actionable corrective instances. Concretely,
we apply a two-level filter to isolate erroneous trajectories and then synthesize feedback that can be
appended to the original context for subsequent
corrective updates.


**Error Identification.** We decompose error detection into _format validity_ and _functional correctness_ .
Let _R_ fmt denote whether the tool-call format is
valid. If _R_ fmt( _τ_ ) = 0, the trajectory is immediately
treated as an error without consulting correctness.
Otherwise, we further evaluate correctness with a



_G_ 
- _R_ ˆ( _τi_ ) _· π_ ratio( _τi_ ) _−_ _β_ DKL


_i_ =1



_J_ ( _θ_ ) = E _x∼D_




1

_G_



(1)
where _R_ [ˆ] ( _τi_ ) = _[R]_ [(] _σ_ _[τ]_ _R_ _[i]_ [)] + _[−]_ _ϵ_ _[µ][R]_ is the normalized reward,

with _µR_ and _σR_ being the mean and standard deviation of rewards within the group, and _π_ ratio is the
clipped probability ratio.


**3.2** **Reward Design**


To guide the policy from syntactic compliance to
semantic precision, we design a time-dependent
composite reward function _R_ ( _τ, t_ ), where _t_ denotes
the training step. The total reward is a weighted
sum of three components:


**Format Compliance (** _R_ **fmt).** This binary term
_R_ fmt( _τ_ ) _∈{_ 0 _,_ 1 _}_ enforces structural constraints,
ensuring outputs adhere to the required XML/JSON schema. We apply a decaying weight _w_ fmt( _t_ )
that reduces its maximum contribution from 2 to 1,
shifting focus from syntax to semantics as training
progresses.


**Functional Correctness (** _R_ **corr).** This term evaluates alignment between invoked tools and user
intent. To accommodate partial matching in complex parameters, we define _R_ corr _∈_ [0 _,_ 2] as:


_R_ corr( _τ, y_ _[∗]_ ) = _α ·_ I( _N_ = _N_ _[∗]_ )+



1
(1 _−_ _α_ ) _·_
_|M|_




 
F1( _a, a_ _[∗]_ )

( _a,a_ _[∗]_ ) _∈M_



(2)
where I( _N_ = _N_ _[∗]_ ) indicates correct function selection, _M_ denotes matched argument pairs between
prediction _τ_ and ground truth _y_ _[∗]_, and F1 measures
token-level overlap. The weight _w_ corr( _t_ ) increases
monotonically, scaling its maximum contribution



4


scalar score _R_ corr and flag the trajectory when it
falls below a tunable threshold _δ_ corr:


_E_ = _{τi | R_ corr( _τi_ ) _< δ_ corr _∨_ _R_ fmt( _τi_ ) = 0 _}_ (3)


In Fig. 2, we use a simplified illustration (e.g., _R <_
_δ_ ) to emphasize the gating effect; this does not
contradict Eq. (3).


**Hybrid Feedback Synthesis.** For effective correction, a scalar penalty is insufficient; we require
an explicit diagnostic message _f_ that resembles the
runtime system feedback. We adopt a hybrid strategy: (i) for _format errors_ ( _R_ fmt = 0), we use deterministic error messages (e.g., parser/compiler-style
feedback) to explicitly state the violated schema/serialization constraints; (ii) for _semantic errors_
( _R_ corr _< δ_ corr), we query a learned **Error Sim-**
**ulator** _Sϕ_ to produce a concise, actionable runtime
error string.
The simulator is implemented as a Qwen332B model fine-tuned via SFT to emulate runtime environment responses. We construct a
training set of approximately 2K instances from
error logs, where each instance comprises: (i)
the original system prompt and tool specification
along with the dialogue state, (ii) the model’s
failed tool call ( _τ_ err), (iii) the ground-truth tool
call ( _τ_ gt), and (iv) a teacher-written diagnostic error message (e.g., generated via ClaudeSonnet-4), followed by quality filtering. During
both training and inference, the simulator consumes (system + tools _,_ dialogue history _, τ_ gt _, τ_ err)
and produces a concise feedback string _f_ _←_
_Sϕ_ ( _x, τ_ err _, τ_ gt), where _f_ is constrained to be a
realistic runtime response. We provide the exact prompting template used to query _Sϕ_ in Appendix A.


**Corrective Sample Construction and LIFO**
**Buffering.** Given a flagged trajectory _τ_ err with
feedback _f_, we construct a corrective context by
appending the failed attempt and the diagnostic
message to the original multi-turn input:


_x_ corr =      - _x_ ; _τ_ err; _f_      - _._ (4)


We optionally deduplicate corrective instances by
hashing ( _x, τ_ err) to avoid repeatedly training on
near-identical errors. All corrective samples are
stored in a **LIFO** buffer _B_ corr, so that the most recent errors are consumed first during corrective
updates. This design keeps the corrective batch distribution closer to the current policy _πθ_, improving



the on-policy approximation in multi-turn tool-use
training.


**3.3.3** **Stage 3: Corrective Batch Training**

Once the LIFO buffer accumulates sufficient _re-_
_cent_ errors (Batch Trigger), we activate **Fission** to
perform targeted remedial updates for recovery.


**Multiplicative Resampling.** We pop the freshest
corrective contexts _x_ corr and, for each of them, sample a “fission group” of _G_ _[′]_ trajectories conditioned
on the same context:


_{τj_ _[′][}]_ _j_ _[G]_ =1 _[′]_ _[∼]_ _[π][θ]_ [(] _[· |][ x]_ [corr][)] _[.]_ (5)


This turns a single error case into multiple parallel recovery attempts, densifying training signals
around the observed error.


**More Informative Advantages.** Hard queries
can yield near-homogeneous outcomes in standard
exploration, weakening within-group relative advantages. Conditioning on explicit feedback _f_ typically increases outcome diversity within the fission
group, improving the usefulness of advantage estimates for recovery updates. We optimize the same
GRPO-style objective as Eq. (1), but over the corrective distribution:




- _A_ ˆ( _τj_ _[′]_ [)] _[ ∇]_ [log] _[ π][θ]_ [(] _[τ][ ′]_ _j_ _[|][ x]_ [corr][)]

_j_ =1







 _._



_J_ corr( _θ_ ) = E _x_ corr





 [1]

_G_ _[′]_



_G_ _[′]_




(6)


**Summary.** These three stages form a continuous
loop; detailed pseudocode and hyperparameters are
provided in Algorithm 1 (Appendix B).


**4** **Experiments**


**4.1** **Experimental Setup**


**Data Construction** Diverging from prevalent
tool-learning paradigms that emphasize extensive
scaling of synthetic corpora (e.g., ToolACE (Liu
et al., 2024), XLAM (Zhang et al., 2025a)), we
prioritize _data quality_ and _trajectory correctness_ .
We implement a three-stage pipeline to construct a
compact yet rigorous training set:
(1) **Domain Schema Curation** : We curated a
diverse schema library spanning 11 domains (e.g.,
_Healthcare, Smart Home, Vehicle Control_ ), prompting Claude-4-Sonnet to generate realistic API definitions grounded in BFCL characteristics.
(2) **Trajectory Synthesis** : Utilizing Claude-4Sonnet, we first synthesized multi-turn user queries



5


Model Method Overall Acc Base Miss Func Miss Param Long Context



Qwen3-1.7B


Qwen3-4B



Base 7.80% 10.00% 11.00% 8.00% 2.50%
GRPO 17.12% 22.00% **18.50%** 15.50% 12.50%
DAPO 16.00% 22.00% 17.00% 14.00% 11.00%
Dr.GRPO 16.12% 19.50% 17.50% 14.50% 13.00%
**FISSION-GRPO (Ours)** **20.38%** **29.00%** **18.50%** **16.00%** **18.00%**


Base 19.37% 24.50% 19.00% 14.50% 19.50%
GRPO 36.38% 46.50% 34.50% 27.50% 37.00%
DAPO 38.25% 48.50% 36.50% 28.00% **40.00%**
Dr.GRPO 34.75% 43.00% 34.50% 27.50% 34.00%
**FISSION-GRPO (Ours)** **40.87%** **51.50%** **42.50%** **30.50%** 39.00%



**External 8B Agent Models**
ToolACE-2-8B  - 37.00% 47.00% 31.00% 28.00% 42.00%
BitAgent-8B  - 37.75% 46.50% 37.50% 24.00% 43.00%



Qwen3-8B



Base 28.75% 35.50% 37.00% 22.50% 20.00%
GRPO 42.75% 50.50% 41.00% 36.00% 43.50%
DAPO 43.12% 54.50% 44.50% 29.00% 44.50%
Dr.GRPO 44.88% 55.00% **45.00%** 32.50% 47.00%
**FISSION-GRPO (Ours)** **46.75%** **57.50%** 43.50% **38.00%** **48.00%**



**Table 1:** Performance comparison on BFCL V4 Multi-Turn benchmark across model scales and training methods.



based on these schemas, followed by generating
full interaction trajectories that fulfill the requests.

(3) **Hierarchical Filtering and Factorization** :
To ensure rigorous quality control, we applied a
hierarchical protocol. First, raw trajectories underwent a global coherence check via Claude-4Sonnet. Validated trajectories of length _K_ were
then factorized into discrete decision instances
_{_ ( _ht, at_ ) _}_ _[K]_ _t_ =1 [, where] _[ h][t]_ [ denotes the cumulative]
context history. Finally, these decomposed instances underwent a double-blind verification via
Qwen3-235B-A22B-Instruct-2507 (Team, 2025)
and Kimi K2 (Team et al., 2025). Only samples
achieving unanimous consensus were retained, distilling an initial pool of _∼_ 2,000 trajectories down
to 630 high-quality training instances.


**Training Details** All models are trained using
the **Verl** framework (Sheng et al., 2024) on a single node with 8 _×_ H800 80GB GPUs. For GRPO
training, we use a learning rate of 1e-6 with cosine
warmup, a batch size of 8, and sample 8 rollouts
per query ( _G_ = 8). The maximum prompt length
is set to 12,800 tokens and the maximum response
length to 4,096 tokens. We use temperature 0.95
and top- _k_ 50 for sampling. For FISSION-GRPO,
we set the correctness threshold _δ_ corr = 1 for error
identification (Eq. 3), determined empirically as a
stable threshold across multiple runs.


**Benchmarks.** We evaluate on the BFCL V4
Multi-Turn benchmark (Patil et al., 2025), specifi


cally chosen for its stress-testing of state tracking
and robustness. A distinctive feature of this benchmark, crucial to our study, is its interactive error
feedback mechanism: upon execution error, the
environment provides explicit error traces and permits the agent up to 20 retries to correct its action.
This setup directly aligns with our research objective, allowing us to measure how improved error
recovery dynamics translate to overall success rates
in tool use.


**Baselines.** We compare FISSION-GRPO against
advanced RL baselines implemented on the Qwen3
series (1.7B/4B/8B), including: (1) **GRPO** (Shao
et al., 2024), utilizing group-normalized advantages; (2) **DAPO** (Yu et al., 2025), incorporating dynamic sampling constraints; and (3)
**Dr.GRPO** (Liu et al., 2025), employing meancentered estimators to mitigate length bias. For
broader context, we also report performance of specialized 8B-scale tool agents such as ToolACE (Liu
et al., 2024) and BitAgent.


**4.2** **Main Results**


Table 1 presents the performance on the BFCL
v4 Multi-Turn benchmark. As shown, FISSIONGRPO achieves consistent state-of-the-art (SOTA)
performance across all Qwen3 model scales (1.7B,
4B, and 8B) when compared to other GRPO-based
post-training methods.


**Scalability** **and** **Subset** **Performance.** Our
method demonstrates superior scalability and ro


6


bustness. On Qwen3-1.7B, FISSION-GRPO yields
a substantial improvement, elevating overall accuracy to **20.38%**, a relative gain of over 160%
compared to the Base model (7.80%) and surpassing standard GRPO (17.12%). As the model scale
increases, the performance gap remains distinct: on
Qwen3-4B and Qwen3-8B, our method achieves
**40.87%** and **46.75%** accuracy respectively, outperforming strong baselines like DAPO and Dr.GRPO.
Notably, FISSION-GRPO excels in the _Base_ and
_Miss Param_ categories, achieving the highest scores
across most settings (e.g., 57.50% on 8B Base and
30.50% on 4B Miss Param), indicating a precise
understanding of function calls and parameters.


**Comparison with Specialized Agents.** Furthermore, we compare our generalist approach
with specialized 8B-scale tool agents. FISSIONGRPO (Qwen3-8B) significantly outperforms
both ToolACE-2-8B (37.00%) and BitAgent-8B
(37.75%) by margins of 9.75 and 9.00 percentage
points, respectively. This underscores the efficacy
of our method in enhancing tool-use capabilities
beyond varying baselines.


**4.3** **Error Recovery Analysis**


To identify the source of performance gains, we
decouple the overall success rate into two components: _One-Shot Success_ (success achieved without triggering any errors) and _Error Recovery Rate_
(conditional probability of success after an error
occurs).
Figure 3 illustrates this breakdown for the
Qwen3-8B model. The results clearly indicate
that the performance improvement is primarily
driven by enhanced error recovery capabilities.
FISSION-GRPO yields an average improvement of
**5.7%** in Error Recovery Rate across all categories,
with particularly substantial gains in _Long Context_
(+11.8%) and _Base_ (+5.5%) scenarios. This enhanced recovery capability serves as the primary
contributor to the overall accuracy improvement
from 42.75% to 46.75%.
Crucially, this gain does not come at the expense
of fundamental capabilities. The One-Shot Success
Rate is preserved and even modestly improved by
an average of 1.75%, with notable gains in _Long_
_Context_ (+3.5%) and _Base_ (+2.0%). This confirms
that our framework effectively equips the model
with robust recovery skills while simultaneously enhancing its standard tool-use competence, demonstrating that the fission mechanism provides com


**Figure 3:** Performance decomposition on BFCL v4 MultiTurn (Qwen3-8B).


**Method** **Avg.** **Base** **M.Func M.Param Long**


_**Qwen3-1.7B**_
GRPO 17.12 22.00 18.50 15.50 12.50
Static 17.75 23.50 21.00 15.50 11.00
**Dynamic 20.38 29.00** **18.50** **16.00** **18.00**


_**Qwen3-4B**_
GRPO 36.38 46.50 34.50 27.50 37.00
Static 37.25 50.50 34.00 28.50 36.00
**Dynamic 40.87 51.50** **42.50** **30.50** **39.00**


_**Qwen3-8B**_
GRPO 42.75 50.50 41.00 36.00 43.50
Static 44.00 53.50 43.00 35.50 44.00
**Dynamic 46.75 57.50** **43.50** **38.00** **48.00**


**Table 2: Ablation on Feedback Quality.** _Static_ denotes
Fission training with generic error prompts; _Dynamic_ uses our
simulated feedback. _Avg._ denotes Overall Accuracy; _M.Func_
and _M.Param_ denote Missing Function and Missing Parameter
errors, respectively.


plementary benefits to both error prevention and
error correction.


**4.4** **Impact of Feedback Quality**


To disentangle the contribution of the _Fission_ mechanism from the informational gain of the Error
Simulator, we conduct an ablation study across
three settings: (1) **GRPO** : The standard baseline
without explicit recovery training. (2) **Fission-**
**Static** : Applies the fission update but uses a fixed,
generic error message for all errors. [1] (3) **Fission-**
**Dynamic** : Our full method using the Error Simulator for context-aware feedback.
Results in Table 2 reveal a hierarchical improvement. First, Fission-Static consistently outperforms
GRPO. Even with uninformative feedback, the explicit process of re-sampling and penalizing failed
trajectories forces the model to refine its internal
state tracking (e.g., +1.25% on Qwen3-8B Overall
Acc). This validates that the _structural_ intervention


1The static prompt is: “ERROR: Function call failed.
Please verify your output format, function name, required
parameters, and parameter values are correct.”



7


**Figure 4:** Multi-turn performance across different correction
trigger intervals ( _N_ ) on BFCL v4 Multi-Turn.


of the fission mechanism is inherently valuable.
Second, Fission-Dynamic yields significant
marginal gains. The gap between Static and Dynamic (e.g., +3.62% on Qwen3-4B) underscores
the necessity of precise supervision. Generic signals fail to guide the model through complex errors,
whereas simulated feedback effectively directs the
gradient towards correcting specific semantic errors, particularly in the _Miss Param_ and _Long Con-_
_text_ subsets.


**4.5** **Sensitivity to Correction Trigger**
**Frequency**


To investigate how frequently correction updates
should be triggered in training, and to balance
timely suppression of recurring error patterns with
scheduling overhead and potential training instability, we conduct a sensitivity study on the minimum
trigger interval, denoted by _N_ (in global steps).
Here _N_ controls how often correction can be inserted by limiting correction updates to occur no
more than once every _N_ global steps. We fix the
total training budget to 234 global steps and vary
only _N_, enforcing the constraint that at most one
correction update occurs every _N_ global steps. We
further adopt a LIFO sampling strategy to prioritize the most recent correction samples, which
keeps updates better aligned with the current policy
distribution and therefore closer to the on-policy
assumption.


**Results.** Figure 4 shows Multi-turn Overall Accuracy and category-wise metrics as a function of
_N_ . Performance remains relatively stable when
correction is triggered frequently, corresponding
to small _N_, but declines noticeably as updates become sparse, corresponding to large _N_, with consistent degradation across subcategories. The drop
is most pronounced on Miss Param and is also evident on Long Context, indicating that parameter


level errors and long-context interactions particularly benefit from timely correction signals. These
results suggest that standard policy optimization
alone does not reliably prevent error patterns from
accumulating and reappearing over long horizons,
whereas correction updates serve as a stabilizing mechanism for multi-turn reliability. At the
same time, the stability observed over a small-tomoderate range of _N_ indicates that correction does
not need to be inserted extremely often to remain
effective, highlighting a practical trade-off between
correction timeliness and scheduling cost.


**4.6** **Case Study: Error Recovery Behaviors**


To qualitatively illustrate the robustness improvements, we compare three Qwen3-8B variants (Base,
GRPO, FISSION-GRPO) on a representative multiturn file manipulation task (from BFCL V4 MultiTurn Base) requiring state tracking across directory
changes and file moves (full logs in Appendix C).
We observe three distinct error-recovery patterns.
The **Base** model exhibits _collapse_ : it fails to update internal state after partial command success,
entering repetitive invalid retries until conversation
breakdown. **GRPO** shows _hallucination_ : it recognizes errors but lacks grounding—when a file
path becomes invalid, it invents non-existent parameters (e.g., a path argument for ls) rather than
verifying the actual state. In contrast, **FISSION-**
**GRPO** demonstrates _active diagnosis_ : it employs
a diagnose-then-correct strategy, deploying verification tools (e.g., find) to resolve state uncertainty before reattempting the task. This comparison shows that FISSION-GRPO transforms error
signals into active diagnostic capabilities rather
than brittle heuristics.


**5** **Conclusion**


We presented FISSION-GRPO, a framework that
transforms execution errors into on-policy corrective supervision for multi-turn tool use. By intercepting errors, augmenting them with simulated
feedback, and resampling recovery attempts, our
approach enables smaller models to learn robust
self-correction rather than collapsing into repetitive loops. On BFCL v4 Multi-Turn, Qwen3-8B
achieves a 5.7% gain in error recovery and 4%
overall accuracy improvement, outperforming specialized tool agents. The fission paradigm may generalize to other iterative refinement domains such
as code debugging and mathematical reasoning.



8


**Limitations**


Our work has several limitations that suggest directions for future research.


**Evaluation Scope.** We evaluate FISSION-GRPO
exclusively on the BFCL v4 Multi-Turn benchmark. We chose this benchmark because it is one of
the few that features an interactive error feedback
mechanism permitting retry attempts, which directly aligns with our focus on error recovery. Most
existing tool-use benchmarks emphasize singleturn correctness without explicit retry mechanisms.
Extending evaluation to other domains with errorretry dynamics (e.g., interactive code debugging
or web navigation with fallback) is a promising
direction for future work.


**Computational Overhead.** The fission mechanism introduces additional computational cost by
resampling _G_ _[′]_ rollouts for each intercepted error.
We partially mitigate this through a configurable
trigger interval _N_ (Section 4.5), which allows trading off correction frequency against training efficiency. Our analysis shows that moderate intervals maintain effectiveness while reducing overhead, though further optimization of this trade-off
remains an open direction.


**References**


Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan
Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine
Lin, and Pavlo Molchanov. 2025. Small language
models are the future of agentic ai. _arXiv preprint_
_arXiv:2506.02153_ .


Wenlong Deng, Yi Ren, Muchen Li, Danica J Sutherland, Xiaoxiao Li, and Christos Thrampoulidis. 2025.
On the effect of negative gradient in group relative
deep reinforcement optimization. _arXiv preprint_
_arXiv:2505.18830_ .


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning. _arXiv preprint_
_arXiv:2501.12948_ .


Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang,
Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and
Yang Liu. 2024. Stabletoolbench: Towards stable
large-scale benchmarking on tool learning of large
language models. In _ACL (Findings)_ .


Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, and Feng Zhao.



2025. Critictool: Evaluating self-critique capabilities of large language models in tool-calling error
scenarios. _arXiv preprint arXiv:2506.13977_ .


Aviral Kumar, Vincent Zhuang, Rishabh Agarwal,
Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,
Shariq Iqbal, Colton Bishop, Rebecca Roelofs, and
1 others. 2024. Training language models to selfcorrect via reinforcement learning. _arXiv preprint_
_arXiv:2409.12917_ .


Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao,
Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan,
Zhengying Liu, Yuanqing Yu, and 1 others. 2024.
Toolace: Winning the points of llm function calling.
_arXiv preprint arXiv:2409.00920_ .


Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi,
Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.
2025. Understanding r1-zero-like training: A critical
perspective. _arXiv preprint arXiv:2503.20783_ .


Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu,
Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou
Zeng, Qixuan Zhou, Yadong Li, and 1 others. 2025.
Ngrpo: Negative-enhanced group relative policy optimization. _arXiv preprint arXiv:2509.18851_ .


Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, and 1
others. 2022. Training language models to follow instructions with human feedback. _Advances in neural_
_information processing systems_, 35:27730–27744.


Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang,
Wei Wu, Chenlin Ming, H Vicky Zhao, Conghui
He, and Lijun Wu. 2025. Lemma: Learning from
errors for mathematical advancement in llms. _arXiv_
_preprint arXiv:2503.17439_ .


Shishir G Patil, Huanzhi Mao, Fanjia Yan, Charlie
Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E
Gonzalez. 2025. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of
large language models. In _Forty-second International_
_Conference on Machine Learning_ .


Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
Gonzalez. 2024. Gorilla: Large language model
connected with massive apis. _Advances in Neural_
_Information Processing Systems_, 37:126544–126565.


Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, and 1 others. 2023. Toolllm: Facilitating
large language models to master 16000+ real-world
apis. _arXiv preprint arXiv:2307.16789_ .


Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
Toolformer: Language models can teach themselves
to use tools. _Advances in Neural Information Pro-_
_cessing Systems_, 36:68539–68551.



9


John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint_
_arXiv:1707.06347_ .


Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, YK Li, and 1 others. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open
language models. _arXiv preprint arXiv:2402.03300_ .


Raghav Sharma and Manan Mehta. 2025. Small language models for agentic systems: A survey of architectures, capabilities, and deployment trade offs.
_arXiv preprint arXiv:2510.03847_ .


Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin
Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin
Lin, and Chuan Wu. 2024. Hybridflow: A flexible
and efficient rlhf framework. _arXiv preprint arXiv:_
_2409.19256_ .


Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement
learning. _Advances in Neural Information Process-_
_ing Systems_, 36:8634–8652.


Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi,
Tianyang Han, Junfeng Luo, and Yurui Qiu. 2025.
Failure makes the agent stronger: Enhancing accuracy through structured reflection for reliable tool
interactions. _arXiv preprint arXiv:2509.18847_ .


Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen,
Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru
Chen, Yuankun Chen, Yutian Chen, and 1 others.
2025. Kimi k2: Open agentic intelligence. _arXiv_
_preprint arXiv:2507.20534_ .


[Qwen Team. 2025. Qwen3 technical report.](https://arxiv.org/abs/2505.09388) _Preprint_,
arXiv:2505.09388.


Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, and
Maheep Chaudhary. 2025. Paladin: Self-correcting
language model agents to cure tool-failure cases.
_arXiv preprint arXiv:2509.25238_ .


Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng,
Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng
Shang, Qun Liu, and Wenjie Li. 2025. Subtle errors
in reasoning: Preference learning via error-injected
self-editing. In _Proceedings of the 63rd Annual Meet-_
_ing of the Association for Computational Linguistics_
_(Volume 1: Long Papers)_, pages 31184–31203.


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. In _The eleventh international conference on_
_learning representations_ .


Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,
Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,
Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo:



An open-source llm reinforcement learning system
at scale. _arXiv preprint arXiv:2503.14476_ .


Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu,
Thai Quoc Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, and 1 others. 2025a. xlam: A family of large action models
to empower ai agent systems. In _Proceedings of_
_the 2025 Conference of the Nations of the Americas_
_Chapter of the Association for Computational Lin-_
_guistics: Human Language Technologies (Volume 1:_
_Long Papers)_, pages 11583–11597.


Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan
Lu, Weiwen Liu, Weinan Zhang, and Yong Yu. 2025b.
Looptool: Closing the data-training loop for robust
llm tool calls. _arXiv preprint arXiv:2511.09148_ .


**A** **Prompt Template for the Error**
**Simulator**


To improve reproducibility, we provide the prompting template used to query the error simulator
_Sϕ_ . We use a two-message chat format: a system prompt that specifies the simulator role and
output constraints, followed by a user prompt that
injects the original context, ground-truth tool calls,
and the model’s failed attempt.


**B** **Training Algorithm Details**


Algorithm 1 outlines the detailed execution flow
of the FISSION-GRPO framework. The process
alternates between standard exploration (to maintain general capability and mine errors) and fissionbased updates (to learn specific recovery strategies).


**C** **Extended Case Study Analysis**


In this section, we provide a detailed breakdown of
the case study referenced in Section 4.6. Figure 6
visualizes the trajectories of Qwen3-8B under three
training conditions on a multi-turn file manipulation task (Sample ID: multi_turn_base_1).


**Scenario Overview.** The user requests to verify
the current directory, move a log.txt file into a
new archive folder, and then search for a keyword
within that file. The key challenge arises in Turn
2: mkdir archive fails (directory already exists),
but cd workspace and mv log.txt succeed. This
partial-success state requires careful tracking—in
Turn 3, since the file was moved to archive, a
direct grep will fail, requiring the agent to locate
the file first.



10


**Detailed Behavioral Comparison.**


**1. Qwen3-8B (Base): State Awareness Col-**
**lapse.** The Base model correctly issues the initial
batch command [cd, mkdir, mv]. However, it
fails to update its internal state to reflect that it
is already inside workspace after the successful
cd. When attempting to handle the mkdir error,
it redundantly retries cd workspace, which fails
(“No such directory” within the current directory).
Confused by this feedback, it spirals into a loop of
invalid operations, ultimately failing to realize the
file was already moved.
**2. Qwen3-8B + GRPO: Latent State Mis-**
**match & Hallucination.** The GRPO model
succeeds in Turn 2 (the file is moved), but
fails to track the consequence—specifically, that
log.txt is no longer in the current directory
but in the archive subdirectory. This latent
state mismatch surfaces in Turn 3: it first tries
grep("log.txt") (fails), then attempts a heuristic guess grep("archive/log.txt") (also fails).
Lacking a grounded fallback strategy, it resorts to
hallucination, inventing a non-existent path parameter for ls.
**3.** **Qwen3-8B + FISSION-GRPO: Active**
**Diagnosis.** Our model handles the Turn 2
state transition correctly. More importantly, in
Turn 3, when faced with the same “No such
file” error, it demonstrates a superior recovery mechanism: instead of guessing, it deploys
find(name="log.txt", path="workspace") to
empirically verify the file’s location. Using the
confirmed path, it performs a precise state update
via cd(folder="archive"), then executes grep
successfully. This confirms that FISSION-GRPO
learns to bridge state gaps through active diagnosis
rather than relying on fragile internal memory or
hallucinated corrections.



12: _θ ←_ _θ_ + _η∇θL_ GRPO
13: // STAGE 2: SYNTHESIS & ACCUMULATION
14: Identify error set _E_ = _{τi | R_ corr( _τi_ ) _< R_ thresh _∨_
_R_ fmt( _τi_ ) = 0 _}_
15: **for** each error trajectory _τ_ err _∈E_ **do**
16: **if** _R_ fmt( _τ_ err) == 0 **then**
17: _f ←_ GetFormatError( _τ_ err)
18: **else**
19: _f ←Sϕ_ ( _x, τ_ err) _▷_ Generate diagnostic
feedback
20: **end if**
21: Construct corrective context _x_ corr _←_ [ _x_ ; _τ_ err; _f_ ]
22: Compute Deduplication Key _k_ _←_
Hash( _x, τ_ err _, f_ )
23: **if** _k /∈_ Keys( _B_ ) **then**
24: Push( _x_ corr) _→B_ _▷_ LIFO Push
25: **end if**
26: **end for**
27: // STAGE 3: FISSION-BASED REMEDIAL UPDATE
28: **if** _|B| ≥_ _B_ trig **then**
29: _X_ batch _←_ Pop( _B_ trig) items from top of _B ▷_ LIFO:
Fetch freshest errors
30: Initialize batch loss _L_ total _←_ 0
31: **for** each corrective context _x_ corr _∈_ _X_ batch **do**
32: _Fission Resampling:_
33: Generate recovery group _{τj_ _[′][}][G]_ _j_ =1 _[′]_ _∼_
_πθ_ ( _·|x_ corr)
34: Compute rewards _{rj_ _[′]_ _[}]_ [ for recovery at-]
tempts
35: _Compute Corrective Advantages:_
36: _µ_ _[′]_ _R_ _[←]_ _G_ 1 ~~_[′]_~~ - _rj′_ _[,]_ _σR_ _[′]_ _[←]_ [Std][(] _[r]_ _j_ _[′]_ [)]

37: _A_ ˆ _[′]_ _j_ _[←]_ _rσj_ _[′]_ _R_ ~~_[′]_~~ _[−]_ [+] _[µ][′]_ _R_ _[ϵ]_ _▷_ Variance restored via
Fission
38: _Accumulate Gradients:_
39: _L_ corr _←_
_G_ 1 ~~_[′]_~~  - _Gj_ =1 _′_ �min( _ρ_ _[′]_ _j_ _[A]_ [ˆ] _j_ _[′]_ _[, . . .]_ [ )] _[ −]_ _[β]_ [D][KL]  
40: _L_ total _←L_ total + _L_ corr
41: **end for**
42: _θ ←_ _θ_ + _η∇θL_ total _▷_ Apply corrective update
43: **end if**
44: **end for**



**Algorithm 1** Detailed Training Procedure of
FISSION-GRPO
**Require:** Policy _πθ_, Reference Policy _π_ ref, Error Simulator
_Sϕ_
**Require:** Training dataset _D_
**Require:** Hyperparameters: Learning rate _η_, KL coefficient
_β_, Clip ratio _ϵ_
**Require:** Group sizes: _G_ (Exploration), _G_ _[′]_ (Fission/Correction)
**Require:** Thresholds: Buffer trigger _B_ trig, Success score
_R_ thresh = 1 _._ 0
1: Initialize Corrective Sample Pool _B ←∅_ _▷_ Implemented
as LIFO Stack
2: **for** iteration _k_ = 1 _, . . ., K_ **do**
3: // STAGE 1: STANDARD EXPLORATION & MINING
4: Sample batch of user queries _x ∼D_
5: Generate exploration group _{τi}_ _[G]_ _i_ =1 _[∼]_ _[π][θ]_ [(] _[·|][x]_ [)]
6: Compute rewards for each trajectory: _ri_ _←_
_R_ corr( _τi_ ) + _R_ fmt( _τi_ )
7: _Compute GRPO Advantages:_
8: _µR ←_ [1] - _ri,_ _σR ←_ Std( _ri_ )



8: _µR ←_ _G_ [1] - _ri,_ _σR ←_ Std( _ri_ )

9: _A_ ˆ _i ←_ _[r][i][−][µ][R]_



_σR_ + _ϵ_
10: _Update Policy (Standard):_
11: _L_ GRPO _←_
_G_ 1  - _Gi_ =1 �min( _ρiA_ [ˆ] _i,_ clip( _ρi,_ 1 _± ϵ_ ) _A_ [ˆ] _i_ ) _−_ _β_ DKL( _πθ∥π_ ref)�



11


**Prompt Templates**


**Prompt 1:** System prompt for querying the error simulator.


You are a Runtime Environment Simulator for an AI Agent.
Your role is to act as the API Server or Operating System that executes tool calls.


IMPORTANT CONTEXT:
You are receiving a tool call from an Agent that has ALREADY FAILED validation or logic checks against the Ground Truth.
Your task is NOT to judge correctness. Your task is to generate the specific ERROR MESSAGE that the system would return

to the Agent.


GOAL:
Generate a short, realistic, and actionable error message (starting with "ERROR: ")
that will help the Agent understand why its call failed compared to the expected Ground Truth.


PRIORITY ERROR CATEGORIES:

- Dependency & Sequence Violations

- Parameter Hallucination

- Schema & Parameter Errors

- Business Logic Errors


EVALUATION LOGIC:

- Reference the Ground Truth: ground_truth is usually correct and serves as the primary standard.

- Verify Context: cross-check the Agent output against the User Request in the Original Context.

- Ambiguity Rule: if the Agent output differs from ground_truth but is still plausible, note missing validation.


OUTPUT RULES:

- Start with "ERROR: " (case-sensitive)

- Be specific: mention actual parameter names/values from the failed attempt

- Sound like a system/API response

- Keep it concise (1--2 sentences)

- Return ONLY the error string (no JSON, no markdown, no extra explanation)


ERROR MESSAGE EXAMPLES (Real error style):
<<ERROR_EXAMPLES_SNIPPET>>


CRITICAL REMINDERS:

- Do NOT output JSON like {"error": "..."}; output plain text only

- Do NOT add any preamble; only the "ERROR: ..." line

- Do NOT fabricate placeholders unless they appear in the failed attempt

- The error should be what the runtime system returns, not an analysis


**Prompt 2:** User prompt template (simulation input).


## Simulation Task
The Agent attempted to execute a tool call, but it was INCORRECT compared to the Ground Truth.
Generate the system error message triggered by the Agent's specific mistake.


1) Original Context (what the Agent saw)


[System instructions & tools]
<<SYSTEM_AND_TOOLS>>


[Dialogue history before this attempt (non-system turns)]
<<DIALOGUE_HISTORY>>


2) Execution Comparison


[Ground-truth tool call(s)]
<<GROUND_TRUTH_TOOL_CALLS>>


[Failed tool call(s) extracted from the model output]
<<FAILED_TOOL_CALLS>>


3) Instruction


Compare the failed attempt against the ground truth under the given context.
Identify the first critical failure and generate the runtime error.


Output:
Return ONLY one error string starting with "ERROR:".


**Figure 5:** Two-message prompting format used to query the error simulator _Sϕ_ .


12


**TURN 1**



**Qwen3-8B**
Collapse: Loop & Context Loss


**USER: I am alex. Check if the current directory**
**is under my name and list all the visible and**
**hidden contents in the current directory now,**
**please.**


**ASSISTANT:** [pwd(), ls(a=True)]


_TOOL:{"current_working_directory":"/ale_
_x"}_


_TOOL:{"current_directory_content":_

_["workspace"]}_

**ASSISTANT:** The current working directory is
"/alex", which is under your name "alex". The
visible and hidden contents in the current
directory are listed as ["workspace"]



**TURN 1**


**TURN 2**



**TURN 1**


**TURN 2**



**Qwen3-8B + GRPO**

Partial: Hallucination


**USER:** I am alex. Check if the current directory is
under my name and list all the visible and
hidden contents in the current directory now,
please.


**ASSISTANT:** [ls(a=true)]


TOOL:{"current_directory_content":["wor
kspace"]}


**ASSISTANT:** [pwd()]


TOOL:{"current_working_directory":
"/alex"}


**USER:** Go to workspace directory and move one
of the 'log.txt' files into a new directory 'archive'.


**ASSISTANT:** [cd(folder="workspace"),mkdir(dir_
name="archive"), mv(destination="archive",
source="log.txt")]


TOOL:{"current_working_directory":"wor
kspace"}

TOOL: {"error": "mkdir: cannot create
directory 'archive': File exists"}


TOOL:{"result": "'log.txt' moved to
'archive /log. txt'"}



**Qwen3-8B + Fission-GRPO**

Success: Robust Recovery


**USER:** I am alex. Check if the current directory is
under my name and list all the visible and
hidden contents in the current directory now,
please.


**ASSISTANT:** [ls(a=true)]


TOOL:{"current_directory_content":["wor
kspace"]}


**USER:** Go to workspace directory and move one
of the 'log.txt' files into a new directory 'archive'.


**ASSISTANT:** [cd(folder="workspace")]


TOOL:{"current_working_directory":"wor
kspace"}


**ASSISTANT:** [mkdir(dir_name="archive")]


TOOL:{"error": "mkdir: cannot create
directory 'archive': File exists"}


**ASSISTANT:** [mv(destination="archive",
source="log.txt")]


TOOL:{"result": "'log.txt' moved to
'archive/log. txt'"}



**TURN 2 ：FAILURE TRIGGER**

**USER:** Go to workspace directory and move one
of the 'log.txt' files into a new directory 'archive'.


**ASSISTANT:** [ cd(folder="workspace"), mkdir
(dir_name="archive"), mv(source="log.txt",
destination="archive") ]

TOOL:{"current_working_directory":
"workspace"}

TOOL: {"error": "mkdir: cannot create
directory 'archive': File exists"}


TOOL: {"result": "'log.txt' moved to
'archive /log.txt'"}


**ASSISTANT:** [cd(folder="workspace"),mv(source=
"log.txt", destination="archive")]


TOOL:{"error": "cd: workspace: No such
directory. You cannot use path to change
directory."}


TOOL: {"error": "mv: cannot move
'log.txt': No such file or directory"}


**ASSISTANT:** [ls(),mv(source="log.txt",destination
="archive")]


TOOL:{"current_directory_content":

["archive"]}


TOOL: {"error": "mv: cannot move
'log.txt': No such file or directory"}


**ASSISTANT** : {"error":"log.txt not found in the
current directory."}


**TURN 3 : GIVE UP**


**USER:** Investigate within 'log.txt' for the
occurrence of the keyword 'Error'.


**ASSISTANT:** [grep(file_name="log.txt",
pattern="Error")]


TOOL: {"error": "grep: log.txt: No such
file or directory"}


**ASSISTANT:** {"error": "log.txt not found in the
current directory. Please verify the file name and
ensure it exists."}



**TURN 3 : HALLUCINATION**


**USER:** Investigate within 'log.txt' for the
occurrence of the keyword 'Error'.


**ASSISTANT:** [grep(file_name=“log.txt”,
pattern="Error")]


TOOL:{"error": "grep: log.txt: No such file
or directory"}


**ASSISTANT:** [grep(file_name="archive/log.txt",pa
ttern="Error")]


TOOL:{"error": "grep: archive/log.txt: No
such file or directory"}


**ASSISTANT:** [ls(a=false, path="archive")]


TOOL:Error during execution:
GorillaFileSystem.ls() got an unexpected
keyword argument ‘path’



**TURN 3 : ERROR RECOVERY**


**USER:** Investigate within 'log.txt' for the
occurrence of the keyword 'Error'.

**ASSISTANT:** [grep(file_name=“log.txt”,
pattern="Error")]


TOOL:{"error": "grep: log.txt: No such file
or directory"}


**ASSISTANT:** [find(name="log.txt",
path="workspace")]


TOOL:{"matches":["workspace/archive/lo
g.txt"]}


**ASSISTANT:** [cd(folder="archive")]


TOOL:{"current_working_directory":
"archive"}


**ASSISTANT:** [grep(file_name="log.txt",
pattern="Error")]


TOOL:{"matching_lines": ["This is a log
file. No errors found. Another line. Yet
another line. Error: Something went
wrong. Final line."]



**Figure 6: Detailed visualization of Multi-turn Error Recovery.** Comparisons of trajectories generated by Qwen3-8B under
different training regimes. The Base model collapses due to immediate state loss; the GRPO model suffers from latent state
mismatch leading to hallucination in later turns; FISSION-GRPO overcomes this by employing diagnostic tools (find) to
actively resolve state uncertainties.


13


