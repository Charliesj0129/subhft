Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring

## **AIMM-X: An Explainable Market Integrity Monitoring Sys-** **tem Using Multi-Source Attention Signals and Transparent** **Scoring**


**Sandeep Neela**
**Independent Researcher**
**Email: sandeep.neela@gmail.com**
**ORCID: 0009-0007-0936-511X**


**Abstract**


**Abstract** Market manipulation remains difficult to detect in practice because the strongest
surveillance systems rely on proprietary order-book data and opaque models, limiting reproducibility and independent validation. We present **AIMM-X**, an explainable marketintegrity monitoring framework that uses only **publicly accessible signals** —standard
OHLCV price/volume data combined with multi-source attention proxies—to surface **sus-**
**picious windows** : contiguous time intervals where returns, volatility, and public attention
jointly deviate from their historical baselines. AIMM-X applies transparent statistical scoring and **hysteresis-based segmentation** to generate candidate windows, then ranks them
using an interpretable **Integrity Score** that decomposes into factor contributions ( _ϕ_ 1– _ϕ_ 6),
enabling auditability and analyst review.


In a one-year demonstration on a 24-ticker universe (daily bars, 2024), AIMM-X detects
233 suspicious windows and produces fully reproducible artifacts (window lists, rankings,
factor attributions, and case study reports). The system is intentionally framed as **triage**
**rather than accusation** : outputs represent evidence for human investigation, not proof
of wrongdoing. We conclude by outlining a validation roadmap that combines retrospective checks against publicly documented enforcement actions, expert annotation studies,
and higher-frequency data integration to support operational deployment and responsible
regulatory engagement.


**Keywords:** Market manipulation detection, explainable AI, market microstructure, social
attention signals, algorithmic surveillance, financial market integrity, reproducible research


**1** **Introduction**


Financial markets serve as the lifeblood of modern economies, channeling capital from savers to productive
enterprises, enabling risk management, and facilitating price discovery. Yet these critical functions depend
entirely on market integrity—the confidence that prices reflect genuine supply and demand rather than manipulation, fraud, or coordinated distortion. When manipulative practices go undetected, the consequences
extend far beyond individual losses: investor confidence erodes, capital allocation becomes inefficient, and
market participation shrinks (Kyle, 1985; Allen & Gale, 1992; Fischel & Ross, 1991).


The challenge of detecting market manipulation has intensified dramatically in recent years. Modern markets
operate at speeds measured in microseconds, with algorithmic trading accounting for the majority of volume
in major exchanges. Retail investors now coordinate through social media platforms, creating attentiondriven volatility spikes that can overwhelm traditional surveillance systems (Hu et al., 2021; Eaton et al.,
2022). Meanwhile, sophisticated actors have adapted their techniques, making manipulation harder to
distinguish from legitimate trading (Putninš, 2012; Scopino, 2015).


1


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**1.1** **The Gap in Current Surveillance Capabilities**


Most manipulation detection systems today face a fundamental tension between effectiveness and accessibility. On one hand, exchange-operated surveillance platforms like NASDAQ’s Smarts and NYSE’s Mercury have access to comprehensive order book data, proprietary trade-level feeds, and sophisticated pattern
recognition algorithms. These systems can detect complex manipulation schemes like spoofing, layering,
and momentum ignition with reasonable accuracy (U.S. Securities and Exchange Commission, 2019; U.S.
Commodity Futures Trading Commission, 2017).


However, these powerful tools remain locked behind proprietary walls. Academic researchers, smaller brokerdealers, compliance teams at regional firms, and independent analysts cannot validate their methods, replicate their findings, or extend their approaches. This creates several critical problems:


**Reproducibility crisis.** When surveillance methodologies remain opaque, the research community cannot
build on prior work, validate detection algorithms, or establish best practices. Published studies often rely
on restricted datasets or proprietary features, making independent replication nearly impossible (Ioannidis,
2005; Nosek et al., 2015).


**Black-box opacity.** Modern detection systems increasingly employ machine learning models that, while
potentially accurate, provide little insight into why a particular trading episode was flagged. For compliance
reviews, regulatory proceedings, or enforcement actions, this opacity is problematic—suspicious activity
needs to be explained, not just identified (Molnar, 2020; Bracke et al., 2019).


**Access inequality.** Only the largest market participants can afford comprehensive surveillance infrastructure. This creates an asymmetry where sophisticated manipulators can exploit blind spots at smaller
institutions, regional exchanges, or emerging markets where surveillance capabilities lag (Aitken & Harris,
2015; Comerton-Forde & Putninš, 2015).


**Data dependency.** The most effective detection approaches require order book depth, trade-level microstructure, or proprietary attention feeds that are either unavailable to most researchers or prohibitively
expensive (Easley et al., 2012; Hasbrouck, 1991; O’Hara, 2015).


**1.2** **The AIMM-X Approach: Transparency Meets Practicality**


This paper introduces AIMM-X (AI-driven Market Integrity Monitor with Explainability), a detection framework designed from the ground up to address these limitations while maintaining practical effectiveness.
Unlike proprietary surveillance systems, AIMM-X operates on publicly accessible data: standard OHLCV
(Open, High, Low, Close, Volume) price series supplemented with attention signals derived from social media,
news, Wikipedia page views, and search trends—data sources available to any researcher or practitioner.


The core innovation lies not in data exclusivity but in methodological transparency and interpretability.
AIMM-X detects "suspicious windows"—short temporal intervals where price movements, volatility, and
attention exhibit unusual co-movement patterns—and ranks them using an Integrity Score that can be
completely decomposed into interpretable factors. Every alert comes with evidence: which signals triggered
the flag, how much each factor contributed, and what patterns drove the scoring.


This transparency serves multiple purposes. For researchers, it enables validation, replication, and systematic
improvement. For compliance teams, it provides an auditable trail from raw data to final alert. For regulators,
it offers a framework where detection logic can be examined, tested against known manipulation cases, and
refined through open collaboration.


Importantly, AIMM-X is designed as a _triage system_ rather than a definitive accusation engine. Detected
windows represent candidates for deeper investigation—not proof of wrongdoing. This framing aligns with
responsible surveillance practices where automated detection surfaces anomalies for human review, expert
analysis, and potential escalation (U.S. Commodity Futures Trading Commission, 2017; U.S. Securities and
Exchange Commission, 2019).


2


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**1.3** **Motivation: Learning from Historical Detection Failures**


Recent market events underscore the need for better surveillance tools. The January 2021 GameStop episode
demonstrated that massive price and volume surges driven by coordinated retail attention can emerge with
minimal warning, overwhelming traditional volatility-based alerts (Hu et al., 2021). The 2010 Flash Crash
revealed how algorithmic cascades can create systemic instability faster than human supervisors can respond
(Kirilenko et al., 2017). Multiple pump-and-dump schemes propagated through spam and social media have
gone undetected until after significant retail losses accumulated (Frieder & Zittrain, 2008).


These cases share a common pattern: by the time traditional surveillance systems flagged the activity,
much of the damage had already occurred. AIMM-X’s approach—combining price dynamics with multisource attention signals—is designed to provide earlier, more contextual alerts by explicitly modeling the
co-movement between market activity and public attention that characterizes many modern manipulation
schemes.


While we cannot claim that AIMM-X would have definitively detected these historical episodes in real-time
(ground truth labels are inherently difficult in manipulation detection), the framework provides a foundation
for systematic validation against known enforcement cases. Such validation represents a key direction for
future work and a pathway toward operational deployment.


**Objective.** Our objective is to build an _auditable and explainable_ market-integrity monitoring framework
that surfaces _suspicious windows_ —short time intervals where price/volume dynamics co-move unusually
with public attention—using only data that is realistically accessible to researchers and smaller institutions.
Rather than attempting to _prove_ manipulation, AIMM-X produces a ranked set of windows with transparent
factor attribution so analysts can decide what warrants deeper review.


**Limitation addressed.** Much of the market-manipulation and surveillance literature either (i) depends
on proprietary, transaction-level microstructure data and trader identifiers that are unavailable to most
researchers, or (ii) uses black-box anomaly models that are difficult to audit and therefore hard to operationalize in compliance settings. AIMM-X addresses this gap by combining (a) window-based detection
that can run on public OHLCV and attention proxies, (b) a fully decomposable integrity score with perfactor contributions, and (c) a reproducible end-to-end pipeline that can be independently validated and
extended. This design targets accessibility, interpretability, and reproducibility—prerequisites for credible
market-integrity tools beyond large proprietary surveillance platforms.


**1.4** **Contributions of This Work**


This paper makes several contributions to the market surveillance literature:


**Methodological contributions:**


- A reproducible end-to-end pipeline for suspicious window detection using only public data sources


- An interpretable scoring framework with complete factor decomposition and attribution


- A configurable multi-source attention fusion mechanism that can accommodate evolving data availability


- Statistical detection methods grounded in robust baseline estimation and hysteresis-based segmentation


**Empirical contributions:**


- Comprehensive evaluation on 24 high-attention securities throughout 2024 (Jan 8–Dec 31)


- Detection of 233 suspicious windows with full transparency in ranking methodology


- Case studies demonstrating the framework’s ability to surface episodes with unusual price-attention comovement


3


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


- Statistical analysis of factor contributions and cross-ticker patterns


**Practical contributions:**


- An auditable framework suitable for compliance workflows and regulatory engagement


- Modular architecture supporting extension to higher-frequency data and richer feature sets


- Open methodology enabling independent validation, criticism, and improvement by the research community


- Clear guidelines for interpreting alerts and managing false positives


**1.5** **Scope and Limitations**


We emphasize that AIMM-X is not a silver bullet for manipulation detection. Ground truth labels remain
scarce—we cannot definitively say which windows represent manipulation versus legitimate volatility events.
False positives are inevitable, and the system requires human review and domain expertise to translate alerts
into actionable intelligence.


Furthermore, this preprint demonstrates the framework using daily OHLCV data due to data tier constraints.
While this allows us to validate the end-to-end pipeline and produce reproducible results, higher-frequency
(5-minute) data would improve window localization and reduce artifacts from baseline estimation. Similarly,
our attention signals in this preprint phase are constructed proxies; production deployment would benefit
from authenticated API feeds with lower latency.


Despite these limitations, we believe AIMM-X represents meaningful progress toward accessible, transparent
market surveillance. The framework provides a foundation that can be validated, extended, and refined as
better data and ground truth labels become available.


**1.6** **Paper Organization**


The remainder of this paper proceeds as follows. Section 2 reviews related work across manipulation theory,
market microstructure, attention-based trading, and anomaly detection. Section 3 describes our experimental design and data sources. Section 4 details the methodology: panel construction, window detection, and
interpretable scoring. Section 5 presents experimental results including detection statistics, top-ranked windows, and factor analysis. Section 6 provides detailed case studies of selected suspicious episodes. Section
7 discusses broader implications, deployment considerations, and validation pathways. Section 8 examines
limitations and future research directions. Section 9 concludes. Appendices provide algorithmic details,
complete results tables, and reproducibility information.


**2** **Related Work**


Market surveillance sits at the intersection of multiple research streams: manipulation theory, market microstructure, behavioral finance, and anomaly detection. This section reviews relevant work across these
domains and positions AIMM-X within the broader literature.


**2.1** **Market Manipulation: Theory and Detection**


**Theoretical foundations.** The economics of market manipulation has been studied extensively since
Kyle’s seminal model of informed trading (Kyle, 1985). Allen and Gale (Allen & Gale, 1992) analyze conditions under which profitable manipulation is possible, showing that manipulators can exploit temporary
market imbalances and trader uncertainty. Aggarwal and Wu (Aggarwal & Wu, 2006) provide comprehensive empirical analysis of manipulation cases prosecuted by the SEC, documenting common patterns and
profitability.


4


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Fischel and Ross (Fischel & Ross, 1991) examine whether law should prohibit manipulation, arguing that
distinguishing manipulative trades from legitimate ones is fundamentally difficult. Putnins (Putninš, 2012)
provides a modern survey synthesizing manipulation theory across market types, emphasizing the challenge
of detection given sophisticated actors who adapt to surveillance.


**Spoofing and layering.** High-frequency manipulation techniques have garnered increased regulatory attention. Scopino (Scopino, 2015) analyzes the Flash Crash of 2010, highlighting vulnerabilities in market
structure when automated trading strategies cascade. Kirilenko et al. (Kirilenko et al., 2017) provide
detailed forensic analysis of the Flash Crash using transaction-level CFTC data, demonstrating how algorithmic feedback loops can amplify small disturbances into systemic events. Wang et al. (Wang et al., 2018)
develop game-theoretic models of spoofing strategies and detection countermeasures.


**Pump-and-dump schemes.** Frieder and Zittrain (Frieder & Zittrain, 2008) document spam-based stock
touting campaigns, finding measurable abnormal returns around email campaigns followed by reversals. Their
work demonstrates that even transparently manipulative schemes can succeed when information asymmetries
exist. Modern variants propagate through social media, making detection more challenging (Hu et al., 2021).


**Empirical detection studies.** Comerton-Forde and Putnins (Comerton-Forde & Putninš, 2015) examine
manipulation prevalence across international markets, finding that dark trading and fragmented market
structure can facilitate manipulative behavior. Aitken and Harris (Aitken & Harris, 2015) analyze fairness
of access in alternative trading systems, arguing that differential information access enables exploitation.


AIMM-X builds on this literature by focusing on detection using public data, emphasizing the "suspicious
window" framing where definitive proof is not required—only sufficient evidence to warrant deeper investigation.


**2.2** **Market Microstructure and Information Content**


**Price discovery and information flow.** Hasbrouck (Hasbrouck, 1991; 1995) develops measures of
information content in trade data, showing how order flow reveals private information. These methods require
transaction-level data typically unavailable to most researchers. Easley et al. (Easley et al., 1996; 2002)
develop the Probability of Informed Trading (PIN) metric, which has been widely adopted in microstructure
research but requires trade direction classification.


**High-frequency trading and market quality.** The rise of algorithmic trading has transformed market
microstructure. Hendershott and Riordan (Hendershott & Riordan, 2013) find that algorithmic trading
improves liquidity provision, while Brogaard et al. (Brogaard et al., 2014) document that high-frequency
traders contribute to price discovery. O’Hara (O’Hara, 2015) reviews microstructure implications of highfrequency trading, emphasizing changed volatility dynamics and liquidity provision patterns.


Menkveld (Menkveld, 2013) analyzes the role of automated market makers, documenting their contribution
to tighter spreads but also potential fragility during stress. Jones (Jones, 2013) provides a comprehensive
review, concluding that while HFT has benefits, regulatory concern about manipulation potential remains
justified. Cartea et al. (Cartea et al., 2015) provide a technical treatment of algorithmic trading strategies
and optimal execution, relevant for understanding how sophisticated actors operate.


**Microstructure and manipulation.** Easley et al. (Easley et al., 2012) argue that modern markets
require new approaches to surveillance given machine-dominated trading. Traditional microstructure metrics
designed for human traders may miss algorithmic manipulation patterns. This motivates AIMM-X’s focus
on window-level patterns rather than trade-level anomalies—we work at a time scale where retail-driven
attention effects manifest.


**2.3** **Attention, Sentiment, and Social Media Effects**


A large literature documents that investor attention affects trading behavior, volatility, and returns, often
in ways disconnected from fundamental information.


5


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**News and media.** Tetlock (Tetlock, 2007) pioneered the study of media content and stock returns, finding
that pessimistic news predicts downward pressure on prices that later reverts. Tetlock (Tetlock et al., 2008;
Tetlock, 2011) extends this work to show that linguistic content in news contains information about firms’
fundamentals, though much news represents stale information recycled across sources.


**Search and attention proxies.** Da et al. (Da et al., 2011) use Google search volume as a direct measure
of retail investor attention, finding that increased search activity predicts higher prices in the short term and
reversals in the long term. Da et al. (Da et al., 2015) develop the FEARS index (Financial and Economic
Attitudes Revealed by Search), demonstrating that aggregate search patterns predict market returns.


**Wikipedia and online platforms.** Moat et al. (Moat et al., 2013) document that Wikipedia page view
changes precede stock market moves, suggesting that online attention proxies capture retail interest before
it manifests in trading. Preis et al. (Preis et al., 2013) find similar predictive patterns in Google Trends
data, though profitability after transaction costs remains debated.


**Social media and trading.** Bollen et al. (Bollen et al., 2011) analyze Twitter sentiment’s relationship to
market movements, finding correlation between aggregate mood and Dow Jones Industrial Average direction.
Sprenger et al. (Sprenger et al., 2014) focus specifically on StockTwits, documenting that message volume
and sentiment relate to contemporaneous returns and next-day volatility.


**The Reddit/WallStreetBets phenomenon.** Recent events have elevated attention to social coordination
platforms. Hu et al. (Hu et al., 2021) analyze Robinhood user trading patterns around the GameStop
episode, documenting herding behavior and attention-driven trading. Eaton et al. (Eaton et al., 2022)
study brokerage outages, finding that preventing retail access reduces volatility in attention stocks. Cookson
and Marinič (Cookson & Marinič, 2018) examine social network effects in investing, showing that investor
disagreement and attention propagate through social connections. Chen et al. (Chen et al., 2014) evaluate
the "wisdom of crowds" in social media stock opinions, finding that aggregated sentiment contains useful
information despite individual noise.


**AIMM-X’s approach.** We synthesize insights from this literature by fusing multiple attention sources
(search, social media, Wikipedia, news) into a unified attention signal. Rather than treating attention as
a predictor of future returns, we use attention spikes as evidence of unusual market dynamics that, when
combined with return and volatility anomalies, warrant scrutiny.


**2.4** **Anomaly Detection and Explainable AI**


**Classical anomaly detection.** Chandola et al. (Chandola et al., 2009) provide a comprehensive survey of
anomaly detection techniques, taxonomizing approaches by data characteristics and detection methodology.
Akoglu et al. (Akoglu et al., 2015) focus on graph-based methods, relevant for network-structured financial
data. These classical methods emphasize unsupervised detection and ranking-based triage—principles we
adopt in AIMM-X.


**Deep learning approaches.** Modern anomaly detection increasingly employs neural networks. Chalapathy
and Chawla (Chalapathy & Chawla, 2019) survey deep learning methods, highlighting their ability to capture
complex patterns but noting interpretability challenges. Ruff et al. (Ruff et al., 2021) review unsupervised
deep anomaly detection, emphasizing the importance of evaluation methodology when ground truth labels
are scarce.


**Time series anomalies.** Laptev et al. (Laptev et al., 2015) develop methods for detecting anomalies
in large-scale monitoring data at Uber, using techniques that balance sensitivity with false positive rates.
Hundman et al. (Hundman et al., 2018) apply LSTM networks to spacecraft telemetry anomaly detection,
demonstrating that sequence models can capture temporal dependencies missed by static methods.


**Explainable AI.** The machine learning interpretability literature has grown rapidly, driven by regulatory
requirements and trust concerns. Molnar (Molnar, 2020) provides a comprehensive treatment of interpretable
machine learning methods. Ribeiro et al. (Ribeiro et al., 2016) develop LIME (Local Interpretable Modelagnostic Explanations), enabling explanation of black-box model predictions through local approximation.


6


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Lundberg and Lee (Lundberg & Lee, 2017) introduce SHAP (SHapley Additive exPlanations), grounding
explanations in cooperative game theory.


Bracke et al. (Bracke et al., 2019) apply machine learning explainability techniques specifically to financial
applications, analyzing default risk models used by central banks. They emphasize that explainability is not
merely academic—it is essential for regulatory acceptance, model validation, and stakeholder trust.


**Financial applications.** Chen et al. (Chen et al., 2019) survey explainable machine learning in finance,
arguing that opaque models face significant adoption barriers in compliance and regulatory contexts. The
consensus in this literature is that while complex models may achieve marginal accuracy gains, interpretable
approaches often win in practice due to transparency requirements.


AIMM-X is designed from the start as an interpretable system: the Integrity Score decomposes into factor
contributions, detection thresholds are configurable and documented, and the entire pipeline can be audited.
This aligns with recent emphasis on responsible AI in financial applications.


**2.5** **Reproducibility and Open Science**


The reproducibility crisis in empirical research has received substantial attention. Ioannidis (Ioannidis,
2005) argues provocatively that most published research findings are false, driven by selective reporting,
underpowered studies, and publication bias. Simmons et al. (Simmons et al., 2011) document "researcher
degrees of freedom" that allow false-positive results to appear significant.


In response, the open science movement advocates for transparency, preregistration, and data sharing. Nosek
et al. (Nosek et al., 2015) and Miguel et al. (Miguel et al., 2014) call for cultural change toward open research
practices. The Open Science Collaboration (Open Science Collaboration, 2015) attempted to replicate 100
psychology studies, finding that only about 40% replicated successfully. Stodden et al. (Stodden et al., 2016)
focus on computational reproducibility, arguing that code and data availability are essential for validating
empirical claims.


AIMM-X embodies these principles: we use publicly accessible data, document all methods and parameters,
and design the pipeline for independent validation. Upon publication, we will make code and replication
materials available, enabling the research community to verify findings, test alternative specifications, and
extend the framework.


**2.6** **Positioning AIMM-X**


AIMM-X differentiates itself from prior work along several dimensions:


- **Data accessibility:** Unlike most microstructure-based detection (Hasbrouck, 1991; Easley et al., 2002;
O’Hara, 2015), we operate on public OHLCV and attention data available to any researcher.


- **Explainability:** Unlike black-box ML approaches increasingly common in anomaly detection, we prioritize interpretability through factor decomposition and transparent scoring.


- **Reproducibility:** Unlike proprietary surveillance systems or studies using restricted data, AIMM-X is
designed for independent validation and open-source distribution.


- **Multi-source fusion:** We synthesize insights from the attention literature (Tetlock, Da et al., Moat et
al., Bollen et al., Hu et al.) into a configurable fusion framework supporting diverse data sources.


- **Triage framing:** Rather than attempting definitive manipulation classification, we provide ranked suspicious windows for human review—aligning with regulatory practice where automated systems support,
not replace, expert judgment.


In summary, AIMM-X builds on a rich literature spanning manipulation theory, microstructure, attention
effects, and anomaly detection, but addresses a gap: the lack of accessible, interpretable, reproducible
surveillance tools suitable for research and practical deployment beyond large institutional players.


7


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**3** **Data and Experimental Design**


This section describes our experimental design, data sources, and the rationale behind our ticker universe
selection. Our goal is to demonstrate AIMM-X’s capabilities on a diverse but focused set of securities that
stress-test the detection and scoring framework across different market regimes and attention dynamics.


**3.1** **Ticker Universe Selection**


We deliberately selected 24 securities spanning multiple categories to evaluate the system’s performance
across heterogeneous market conditions. The universe includes:


**Meme stocks (4 tickers):** GME (GameStop), AMC (AMC Entertainment), BB (BlackBerry), NOK
(Nokia). These securities experienced extreme retail attention and volatility during the January 2021 episode
and subsequent periods. They serve as ideal test cases for attention-driven window detection given their
propensity for social media coordination and unusual price-volume dynamics (Hu et al., 2021; Eaton et al.,
2022).


**Large-cap technology (9 tickers):** AAPL (Apple), MSFT (Microsoft), AMZN (Amazon), NVDA
(NVIDIA), META (Meta/Facebook), GOOG (Alphabet/Google), TSLA (Tesla), NFLX (Netflix), SNAP
(Snap Inc.). These highly liquid, widely followed stocks exhibit different attention patterns than meme
stocks—driven more by earnings announcements, product launches, and macroeconomic news (Tetlock,
2007; Tetlock et al., 2008). Their inclusion tests whether the system can distinguish normal volatility from
anomalous episodes in mature markets.


**Crypto-exposed equities (2 tickers):** COIN (Coinbase), MSTR (MicroStrategy). These tickers provide
exposure to cryptocurrency market dynamics without directly analyzing crypto assets. MSTR’s significant
Bitcoin holdings and COIN’s role as a crypto exchange make them sensitive to both equity and crypto
attention (Bollen et al., 2011).


**Fintech and trading platforms (2 tickers):** HOOD (Robinhood), PLTR (Palantir). HOOD is directly
involved in retail trading infrastructure and became newsworthy during the meme stock episode. PLTR
represents a high-growth technology company with significant retail following.


**ETFs (5 tickers):** SPY (S&P 500), QQQ (NASDAQ-100), IWM (Russell 2000), XLF (Financial Select
Sector), TLT (20+ Year Treasury Bond). These broad market instruments allow us to distinguish tickerspecific anomalies from market-wide events. SPY and QQQ capture large-cap equity movements, IWM tracks
small-cap dynamics, XLF provides sector exposure, and TLT represents fixed income (Baker & Wurgler,
2006; Lo, 2004).


**Specialized/volatile equities (2 tickers):** SPCE (Virgin Galactic), RBLX (Roblox). These represent
speculative growth stocks with significant retail attention but different fundamental profiles than traditional
meme stocks.


This universe intentionally overweights high-attention securities relative to market capitalization weights.
Our objective is not to sample the broader market randomly but to evaluate the detection framework on
challenging cases where attention dynamics matter. Future work will extend to larger, more representative
universes once the methodology is validated.


**3.2** **OHLCV Data: Acquisition and Quality**


**Data source.** We obtained daily OHLCV (Open, High, Low, Close, Volume) bars from Polygon.io, a
widely used market data provider offering both free and premium tiers (Polygon.io, 2026). For this preprint
demonstration, we operated within free tier constraints, which limit request rates and historical depth but
provide reliable data quality for major U.S. equities.


**Temporal coverage.** Our analysis covers 2024 from January 8 through December 31 (248 trading days per
ticker, accounting for market holidays and weekends). This specific timeframe begins with the first trading


8


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


day after data acquisition and ensures data relevance while allowing us to potentially validate findings against
contemporaneous news and market events.


**Data quality and preprocessing.** We performed several quality checks:


- Verified no missing trading days within the coverage period


- Checked for price discontinuities that might indicate splits or data errors


- Confirmed volume figures are plausible (no zero-volume days for liquid stocks)


- Validated that open/high/low/close relationships satisfy logical constraints (high _≥_ open, close; low _≤_
open, close)


All quality checks passed without requiring adjustments. Polygon.io provides split-adjusted data, eliminating
the need for manual adjustment.


**Higher-frequency data plans.** While this preprint uses daily bars due to data availability constraints, the
AIMM-X architecture is designed for 5-minute intraday bars. Daily granularity limits our ability to precisely
localize suspicious activity within the trading day and may introduce artifacts in rolling baseline estimation.
Section 8 discusses implications and plans for higher-frequency data integration.


**3.3** **Attention Signal Construction**


Attention signals represent a key distinguishing feature of AIMM-X. Unlike pure price-based detection, we
explicitly model public attention to securities through multiple proxies.


**Data sources.** We construct attention signals from five categories:


1. **Reddit:** Activity from finance-related subreddits (r/wallstreetbets, r/stocks, r/investing). Reddit has
emerged as a major coordination platform for retail investors (Hu et al., 2021).


2. **StockTwits:** Messages and sentiment from this stock-specific social platform, widely used by active
traders (Sprenger et al., 2014).


3. **Wikipedia:** Page view counts for ticker and company name pages. Wikipedia traffic correlates with
retail interest and has shown predictive properties (Moat et al., 2013).


4. **News:** Mentions in financial news sources (aggregated). News coverage drives attention and can be
measured through article counts or aggregate sentiment (Tetlock, 2007; 2011).


5. **Google Trends:** Search volume for ticker symbols and company names. Search data directly measures
information-seeking behavior (Da et al., 2011; Preis et al., 2013).


**Preprint-phase proxies.** For this preprint demonstration, we generated stylized attention proxies that
exhibit realistic temporal patterns (spikes, decay, noise) calibrated to match documented attention properties
in the literature. This approach allows us to validate the end-to-end pipeline while highlighting where richer
data would improve performance.


Production deployment would integrate authenticated API access to these platforms, enabling real-time
or near-real-time attention measurement. The modular architecture makes this substitution straightforward—attention source weights and fusion logic are configuration-driven.


**Fusion methodology.** We combine attention sources using a weighted sum:


_At_ =                    - _ws ·_ ˜ _as,t_ (1)

_s∈S_


where _S_ = _{_ reddit, stocktwits, wikipedia, news, trends _}_, _ws_ are source-specific weights specified in configuration, and ˜ _as,t_ are resampled and normalized source signals.


9


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Resampling and alignment.** Different attention sources have different natural cadences (Wikipedia updates daily, tweets arrive continuously, news articles are sporadic). We resample all sources to the common
bar grid (daily bars for this study) using forward-fill for step-like sources and aggregation for event counts.
This ensures all signals align temporally with OHLCV data.


**Missingness handling.** We distinguish between "no activity" (measured zero) and "no coverage" (data
unavailable). If a source file exists but shows no activity at time _t_, we treat this as zero. If a source has
no coverage for a period (e.g., Wikipedia API downtime), we propagate NaN to avoid falsely assuming zero
attention.


**Configuration and weights.** Source weights are configurable via JSON, allowing experimentation with
different fusion strategies. For this run, we used approximately uniform weights across sources as a baseline. Future work will explore optimized weighting based on source reliability, lead-lag relationships, and
correlation with known events.


**3.4** **Ethical Considerations and Data Use**


**Public data only.** AIMM-X operates exclusively on publicly available data: exchange-reported prices and
volumes, publicly accessible social media posts, news articles, and aggregate platform statistics. We do not
use personally identifiable information, private communications, or non-public trading data.


**Triage, not accusation.** The framework is designed to surface suspicious windows for review, not to make
definitive claims about manipulation or identify specific actors. All output is statistical evidence requiring
expert interpretation.


**Responsible disclosure.** Should AIMM-X be deployed operationally and identify activity warranting
regulatory attention, appropriate disclosure channels (compliance teams, regulatory authorities) would be
followed rather than public accusation.


**Transparency and gaming.** Making detection methodology public creates risk that sophisticated actors
could adapt strategies to avoid detection. However, we believe the benefits of transparency (reproducibility,
validation, community improvement) outweigh this risk, particularly given that proprietary surveillance
systems already exist. The interpretable factor framework also allows updating weights and adding new
signals to counteract gaming.


**3.5** **Experimental Configuration**


Table 1 summarizes key parameters and statistics for the experimental run reported in this paper.


Table 1: Run summary for the preprint experiment.


Item Value


Tickers 24
Bars (panel rows) 5,952 (248 bars/ticker)
Date range (panel) 2024-01-08 to 2024-12-31
Detected windows 233
Attention sources reddit, stocktwits, wikipedia, news, trends
OHLCV source Polygon.io (daily bars, adjusted)
Detection thresholds High: 3.0, Low: 2.0 (z-score units)
Minimum window length 2 bars
Maximum gap for merging 3 bars
Scoring weights Uniform across _ϕ_ 1– _ϕ_ 6
Phi signal scaling Raw (no normalization)


**Configuration management.** All parameters are specified in `config.json`, enabling reproducible runs and
systematic parameter sweeps. Key settings include detection thresholds, window merging logic, scoring factor


10


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


weights, and attention fusion weights. This configuration-driven approach facilitates sensitivity analysis and
method development.


**4** **Methodology**


This section details the AIMM-X pipeline from raw data to scored suspicious windows. The framework
consists of four main stages: (1) panel construction and feature engineering, (2) statistical deviation detection, (3) window segmentation, and (4) interpretable scoring with factor decomposition. Throughout, we
emphasize transparency and reproducibility—every algorithmic choice is documented and configurable.


**4.1** **Panel Construction and Feature Engineering**


**Data integration.** The first stage merges OHLCV data with attention signals into a unified time-series
panel. For each ticker _i_ and time _t_, we construct a row containing:


- Price: open ( _Oi,t_ ), high ( _Hi,t_ ), low ( _Li,t_ ), close ( _Ci,t_ )


- Volume: _Vi,t_


- Attention: _Ai,t_ (fused from sources as described in Section 3.3)


**Return computation.** We compute log returns:




   - _Ci,t_
_ri,t_ = log

_Ci,t−_ 1




(2)



Log returns are approximately normally distributed for small changes, facilitate multi-period aggregation,
and are standard in financial econometrics (Cont, 2001).


**Volatility proxy.** We estimate realized volatility using a rolling window of past returns. For lookback
length _L_ :



_L_

- _ri,t_ [2] _−j_ [+] _[ ϵ]_ (3)

_j_ =1



_σi,t_ =




~~�~~

~~�~~


- [1]

_L_



where _ϵ_ is a small constant preventing division by zero. This simple estimator captures changing volatility
regimes without requiring intraday data (Andersen & Bollerslev, 1998; Barndorff-Nielsen & Shephard,
2002). More sophisticated estimators (Parkinson, Garman-Klass, Rogers-Satchell) could be integrated using
high-low range information.


**Alternative volatility measures.** We also compute:


- High-low range: ( _Hi,t −_ _Li,t_ ) _/Ci,t−_ 1 as a within-bar volatility proxy


- Exponentially-weighted moving average (EWMA) volatility with decay parameter _λ_


These alternatives allow robustness checks and can improve detection in different market regimes.


**4.2** **Statistical Deviation Metrics**


The core detection mechanism identifies periods where return, volatility, and attention simultaneously deviate
from their typical ranges.


11


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Rolling baseline estimation.** For each channel _xi,t ∈{ri,t, σi,t, Ai,t}_, we compute rolling mean and
standard deviation over a baseline window of length _B_ :



_µ_ [(] _i,t_ _[x]_ [)] [=] [1]

_B_



_B_

- _xi,t−j_ (4)


_j_ =1



_B_



_j_ =1



_σ_ ˆ _i,t_ [(] _[x]_ [)] [=]




~~�~~

~~�~~ - 1

_B −_ 1




- �2
_xi,t−j −_ _µ_ [(] _i,t_ _[x]_ [)] + _ϵ_ (5)



The rolling approach adapts to changing market regimes but creates dependencies between consecutive statistics. In this preprint run, we use _B_ = 20 trading days (approximately one month), balancing responsiveness
with stability.


**Robust alternatives.** To handle outliers and fat-tailed distributions, we can substitute robust estimators:


- Median absolute deviation (MAD) instead of standard deviation


- Trimmed means excluding extreme values


- Quantile-based thresholds (e.g., flagging values beyond 95th percentile)


For this preprint, we use standard mean and standard deviation for simplicity and interpretability. Future
work will evaluate robust alternatives.


**Standardized scores.** We compute z-scores measuring how many standard deviations the current observation deviates from baseline:

_xi,t_ _−_ _µ_ [(] _i,t_ _[x]_ [)]
_zi,t_ [(] _[x]_ [)] [=] (6)
_σ_ ˆ _i,t_ [(] _[x]_ [)] [+] _[ ϵ]_


Under a Gaussian assumption, _|zi,t_ [(] _[x]_ [)] _[|][ >]_ [ 3 occurs with probability] _[ ≈]_ [0] _[.]_ [003, making extreme z-scores natural]
candidates for anomaly flags. Financial returns exhibit fat tails (Cont, 2001; Gabaix et al., 2003), so
empirical thresholds may differ from Gaussian expectations.


**4.3** **Composite Strength Score and Window Detection**


**Multi-channel fusion.** Individual channel deviations may not be sufficient evidence—volatility spikes
occur regularly, and attention surges happen around earnings. Suspicious windows emerge when _multiple_
_channels simultaneously deviate_ .


We form a composite strength score:


_si,t_ = _αr|zi,t_ [(] _[r]_ [)] _[|]_ [ +] _[ α][σ][z]_ _i,t_ [(] _[σ]_ [)] [+] _[ α][A][z]_ _i,t_ [(] _[A]_ [)] (7)


where _αr_, _ασ_, _αA_ are configurable weights. We take absolute value of return z-score to flag extreme moves
in either direction. Volatility and attention z-scores enter directly since we primarily care about increases
(though negative attention z-scores could indicate suspiciously quiet periods).


For this demonstration, we set _αr_ = _ασ_ = _αA_ = 1 (equal weighting). Optimized weights could be learned
from labeled data or tuned via sensitivity analysis.


**Hysteresis-based segmentation.** Simple thresholding ( _si,t > θ_ ) would create many short, fragmented
windows due to noise. We employ hysteresis: a high threshold _θ_ high to initiate windows and a low threshold
_θ_ low to extend them.


Algorithm:


1. Start in "neutral" state


12


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


2. When _si,t > θ_ high, enter "window" state


3. While in window state, continue if _si,t > θ_ low


4. Exit window state when _si,t ≤_ _θ_ low for _g_ consecutive bars (gap tolerance)


5. Enforce minimum window length _L_ min; discard windows shorter than this


This approach, common in signal processing and event detection (Chandola et al., 2009), reduces fragmentation while capturing extended anomalous periods.


**Configuration parameters.** For this run:


- _θ_ high = 3 _._ 0 (z-score units)


- _θ_ low = 2 _._ 0


- Minimum window length: 2 bars


- Gap tolerance: 3 bars


**Per-ticker vs.** **cross-ticker detection.** We perform detection independently for each ticker, allowing
ticker-specific baseline estimation. This prevents large-cap stocks from dominating thresholds but means
we cannot directly compare window counts across tickers without normalizing. Future work may explore
cross-sectional methods or market-relative thresholds.


**4.4** **Interpretable Scoring Framework**


Window detection produces candidate intervals; scoring ranks them for triage. The Integrity Score _M_ is
designed to be interpretable—it decomposes into factor contributions allowing analysts to understand _why_
a window scored high.


**4.4.1** **Phi-Signal Definitions**


We define six interpretable factors ( _ϕ_ 1– _ϕ_ 6), each capturing a distinct dimension of suspiciousness:


_ϕ_ 1 **: Return shock intensity.** Measures the magnitude and persistence of abnormal returns within the
window. Large price moves, especially if sustained across multiple bars, elevate _ϕ_ 1. We compute:



_ϕ_ 1( _w_ ) = 

_t∈w_




- �2
_zi,t_ [(] _[r]_ [)] (8)



Squaring emphasizes extreme deviations. Alternative formulations could use maximum z-score, mean absolute z-score, or percentile ranks.


_ϕ_ 2 **: Volatility anomaly.** Captures unusual volatility independent of return direction. High volatility with
small net returns might indicate churning or attempted manipulation that failed. We compute:


_ϕ_ 2( _w_ ) =                - max( _zi,t_ [(] _[σ]_ [)] _[,]_ [ 0)] (9)

_t∈w_


_ϕ_ 3 **: Attention spike magnitude.** Quantifies the attention surge accompanying price moves. Manipulation
often involves generating hype or spreading rumors to attract retail interest (Frieder & Zittrain, 2008; Hu
et al., 2021). We compute:

_ϕ_ 3( _w_ ) =                - max( _zi,t_ [(] _[A]_ [)] _[,]_ [ 0)] (10)

_t∈w_


13


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


_ϕ_ 4 **: Co-movement alignment.** Measures whether return, volatility, and attention move together. Strong
positive correlation across channels suggests coordinated unusual activity rather than noise. We compute:



_ϕ_ 4( _w_ ) = [1]

2








                Corr� _{zi,t_ [(] _[r]_ [)] _[}][t][∈][w][,][ {][z]_ _i,t_ [(] _[A]_ [)] _[}][t][∈][w]_ - + Corr� _{zi,t_ [(] _[σ]_ [)] _[}][t][∈][w][,][ {][z]_ _i,t_ [(] _[A]_ [)] _[}][t][∈][w]_ 


_._ (11)



Optional: clamp negatives if you only want “alignment” to help: max( _ϕ_ 4( _w_ ) _,_ 0).


_ϕ_ 5 **: Temporal recurrence.** Detects patterns where similar windows repeat in short succession, potentially
indicating sustained manipulation campaigns or attention cycles. We count how many other high-score
windows occur near window _w_ in time:


_ϕ_ 5( _w_ ) = # _{w_ _[′]_ : _w_ _[′]_ = _w,_ distance( _w, w_ _[′]_ ) _<_ ∆recur _, M_ ( _w_ _[′]_ ) _> τ_ recur _}_ (12)


_ϕ_ 6 **: Cross-source disagreement penalty.** If individual attention sources strongly disagree (e.g., Reddit
spikes but other sources silent), this may indicate gaming of a single platform. We penalize high disagreement:


                    -                    _ϕ_ 6( _w_ ) = _−_ Std _{zi,t_ [(] _[s]_ [)] _[}][s][∈S][, t][∈][w]_ (13)


Negative sign makes this a penalty—high disagreement reduces the integrity score.


**Note on implementation.** In this preprint run, factors _ϕ_ 2 _, ϕ_ 4 _, ϕ_ 5 _, ϕ_ 6 contribute minimally due to scaling
choices and attention signal limitations (see Section 5.4). This diagnostic result highlights areas for improvement in production deployment: better attention data quality, normalized scoring, and recalibrated
weights.


**4.4.2** **Integrity Score Aggregation**


The composite Integrity Score _M_ combines factors via weighted sum:



_M_ ( _w_ ) =



6

- _ωk · ϕk_ ( _w_ ) (14)


_k_ =1



where _ωk_ are configurable weights. For this implementation, _ωk_ = 1 _∀k_ (uniform weighting).


**Normalization and scaling.** The current implementation uses raw _ϕk_ values without normalization,
leading to _ϕ_ 1 dominance. Future versions will standardize each _ϕk_ to comparable scales (e.g., z-score normalization across all windows) before aggregation, ensuring balanced contributions.


**Rank percentiles.** To facilitate interpretation, we also report rank percentiles:

rank_pct( _w_ ) = [#] _[{][w][′]_ [ :] _[ M]_ [(] _[w][′]_ [)] _[ < M]_ [(] _[w]_ [)] _[}]_ (15)

_N_ windows


This provides a scale-invariant measure: rank_pct = 1 _._ 0 is the highest-scoring window, regardless of absolute
_M_ value.


**4.4.3** **Factor Decomposition and Attribution**


For each window _w_, we store individual factor contributions:



_M_ ( _w_ ) = _ω_ 1 _ϕ_ 1( _w_ )

     - ~~��~~ ~~�~~
return shock



+ _ω_ 2 _ϕ_ 2( _w_ )

~~�~~ �� ~~�~~
volatility



+ _. . ._ + _ω_ 6 _ϕ_ 6( _w_ )

~~�~~ �� ~~�~~
disagreement



(16)



This decomposition enables analysts to answer questions like:


- Did this window score high due to extreme returns, attention, or both?


14


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


- Are top windows dominated by a single factor or balanced across factors?


- Which tickers exhibit attention-driven vs. return-driven windows?


Factor attribution reports (tables and visualizations) are automatically generated, providing transparency
into scoring logic.


**4.5** **Algorithmic Summary**


The complete AIMM-X pipeline can be summarized as:


**Algorithm 1** AIMM-X Suspicious Window Detection and Scoring


1: **Input:** OHLCV data, attention signals, configuration parameters
2: **Output:** Ranked suspicious windows with integrity scores and decomposition

3:

4: **Stage 1: Panel Construction**
5: **for** each ticker _i_ **do**
6: Merge OHLCV and attention data to common timeline
7: Compute returns _ri,t_, volatility _σi,t_, attention _Ai,t_
8: **end for**

9:

10: **Stage 2: Deviation Detection**
11: **for** each ticker _i_, time _t_ **do**

12: Compute rolling baseline: _µ_ [(] _i,t_ _[x]_ [)][, ˆ] _[σ]_ _i,t_ [(] _[x]_ [)] [for] _[ x][ ∈{][r, σ, A][}]_

13: Compute z-scores: _zi,t_ [(] _[x]_ [)] [= (] _[x][i,t][ −]_ _[µ]_ _i,t_ [(] _[x]_ [)][)] _[/][σ]_ [ˆ] _i,t_ [(] _[x]_ [)]

14: Compute composite score: _si,t_ = _αr|zi,t_ [(] _[r]_ [)] _[|]_ [ +] _[ α][σ][z]_ _i,t_ [(] _[σ]_ [)] [+] _[ α][A][z]_ _i,t_ [(] _[A]_ [)]
15: **end for**

16:
17: **Stage 3: Window Segmentation**

18: **for** each ticker _i_ **do**
19: Apply hysteresis segmentation to _si,t_ using _θ_ high, _θ_ low
20: Merge gaps _≤_ _g_ bars
21: Filter windows shorter than _L_ min bars
22: Store detected windows _Wi_
23: **end for**


24:
25: **Stage 4: Scoring and Ranking**
26: _W ←_ [�] _i_ _[W][i]_ _▷_ All windows across all tickers

27: **for** each window _w ∈W_ **do**
28: Compute factors: _ϕ_ 1( _w_ ) _, . . ., ϕ_ 6( _w_ )

29: Compute integrity score: _M_ ( _w_ ) = [�] _k_ [6] =1 _[ω][k][ϕ][k]_ [(] _[w]_ [)]

30: Store factor contributions for decomposition
31: **end for**
32: Rank windows by _M_ ( _w_ ) and compute rank percentiles
33: Generate figures, tables, and case study reports
34: **return** Ranked windows with scores and decomposition


This algorithm is implemented in modular Python scripts (see Appendix A for details). Configuration
parameters are externalized to JSON, enabling reproducible experimentation and parameter tuning without
code modification.


15


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Table 2: Contribution statistics for each scoring factor _ϕk_ in the _M_ decomposition.


signal mean median abs_mean_share nonzero_pct


phi_1 616202.560 16.412 100.0% 100.0%
phi_2 0.000 0.000 0.0% 100.0%
phi_3 0.030 0.000 0.0% 91.8%
phi_4 0.000 0.000 0.0% 100.0%
phi_5 0.000 0.000 0.0% 0.0%
phi_6 0.000 0.000 0.0% 0.0%


**Note.** In the present implementation, _ϕ_ 1 dominates the absolute mean contribution due to the configured
scaling ( `scale:` `none` ) and early-sample baseline variance effects. For reporting, we emphasize ranks, zscores, and filtered tables that exclude inflated early-sample z-scores. Future work should employ warm-up
periods and robust scaling (Andersen & Bollerslev, 1998; Barndorff-Nielsen & Shephard, 2002).


**5** **Experimental Results**


This section presents comprehensive results from applying AIMM-X to our 24-ticker universe over 2024
(Jan 8–Dec 31). We analyze detection statistics, scoring distributions, factor contributions, and temporal
patterns.


**5.1** **Overall Detection Statistics**


The pipeline detected **233 suspicious windows** across 24 tickers over 248 trading days. Table 3 provides
per-ticker statistics including window counts, integrity score ranges, and average window duration.


16


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Table 3: Top tickers by detected window count (2024). We report window length in _bars_ (for daily data, one
bar corresponds to one trading day).


Ticker Windows Mean _M_ Max _M_ Avg Bars/Window (bars) Total Bars


META 12 597,231 7,166,596 15.0 36
MSTR 10 406,687 4,066,778 16.5 33
SNAP 8 464,895 3,718,879 23.1 37
AMZN 11 281,293 3,094,132 15.5 34
RBLX 11 216,315 2,379,285 15.0 33
SPCE 10 174,682 1,746,746 22.0 44
NFLX 11 142,377 1,565,992 15.9 35
SPY 8 140,994 1,127,905 11.3 18
HOOD 9 109,575 986,040 22.2 40
TLT 13 71,825 933,662 13.8 36
COIN 7 131,899 923,243 15.7 22
TSLA 10 87,031 870,173 15.0 30
NOK 10 57,396 573,894 15.0 30
XLF 11 28,886 317,634 15.9 35
IWM 8 14,114 112,808 14.4 23
GME 9 31 147 27.2 49
PLTR 10 28 109 20.0 40
AMC 10 16 67 18.5 37
BB 9 23 54 19.4 35
AAPL 9 13 39 20.6 37
GOOG 11 11 25 14.5 32
QQQ 8 10 16 16.3 26
NVDA 7 8 14 17.9 25
MSFT 11 7 12 12.3 27


**Key observations:**


- **Heterogeneous window counts:** Detection frequency varies substantially, from 7 windows (COIN,
NVDA) to 13 (TLT). This likely reflects differences in volatility regimes and attention dynamics, though
detection choices and normalization can also contribute.


- **Bimodal score distribution:** Note the dramatic scale difference—META’s max score exceeds 7 million
while MSFT’s peaks at 12. This reflects the raw _ϕ_ 1 scaling issue discussed below and emphasizes why
rank percentiles matter more than absolute scores.


- **Meme stock behavior:** Traditional meme stocks (GME, AMC, BB) have moderate window counts
and lower absolute scores in this configuration, but longer average durations, potentially consistent with
sustained attention dynamics.


- **ETF patterns:** Broad market ETFs (SPY, QQQ) show fewer windows with shorter durations, suggesting
that market-wide moves are captured but ticker-specific manipulation is distinguished.


**5.2** **Scoring Distribution Analysis**


Figure 1 shows the distribution of integrity scores _M_ across all 233 windows.


17


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Figure 1: Distribution of composite window integrity scores _M_ . The heavy-tailed distribution reflects extreme
values in _ϕ_ 1 for early-window baseline estimation artifacts. Rank-based analysis (rank percentiles) provides
more robust comparison.


The distribution exhibits heavy right-skew due to _ϕ_ 1 dominance (see Section 5.4). While absolute scores span
six orders of magnitude, this does not undermine utility—ranking windows identifies the most suspicious
regardless of scale. Future work will normalize factors to achieve balanced contributions.


**5.3** **Top-Ranked Windows**


Table 4 lists the top 15 windows by integrity score (filtered to exclude early-sample artifacts with unrealistic
z-scores _>_ 20).


Table 4: Top 15 ranked suspicious windows after filtering extreme early-sample z-scores.


Ticker Window ID Start End _M_ Rank %ile


META 135 2024-01-10 2024-01-12 7,166,596 100.0
MSTR 176 2024-01-10 2024-01-12 4,066,778 99.6
SNAP 79 2024-01-10 2024-01-16 3,718,879 99.1
AMZN 107 2024-01-10 2024-01-12 3,094,132 98.7
RBLX 58 2024-01-10 2024-01-16 2,379,285 98.3
SPCE 69 2024-01-10 2024-01-16 1,746,746 97.9
NFLX 158 2024-01-10 2024-01-11 1,565,992 97.4
SPY 186 2024-01-10 2024-01-11 1,127,905 97.0
HOOD 49 2024-01-10 2024-01-12 986,040 96.6
TLT 221 2024-01-10 2024-01-16 933,662 96.1
COIN 169 2024-01-10 2024-01-11 923,243 95.7
TSLA 125 2024-01-10 2024-01-12 870,173 95.3
NOK 29 2024-01-10 2024-01-22 573,894 94.8
XLF 210 2024-01-10 2024-01-16 317,634 94.4
IWM 202 2024-01-10 2024-01-11 112,808 94.0


**Interesting pattern:** Many top windows cluster around January 10-12, 2024. Post-hoc research reveals
this period coincided with market volatility around inflation data releases and Federal Reserve commentary.


18


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


This clustering demonstrates that AIMM-X successfully identifies market-wide stress episodes—though it
cannot automatically distinguish manipulation from legitimate volatility without additional context.


**5.4** **Phi-Signal Contribution Analysis**


Table 5 reports aggregate statistics for each scoring factor across all windows.


Table 5: Factor contribution statistics across all 233 detected windows.


Factor Mean Median Max Std Nonzero


_ϕ_ 1 (Return shock) 126,970 0.91 7,166,586 669,144 100%
_ϕ_ 2 (Volatility) 2.07 1.15 17.34 2.70 97.9%
_ϕ_ 3 (Attention) 8.72 3.20 125.70 16.16 100%
_ϕ_ 4 (Alignment) 2.07 1.57 48.84 3.29 100%
_ϕ_ 5 (Recurrence) 0.00 0.00 0.00 0.00 0%
_ϕ_ 6 (Disagreement) 0.00 0.00 0.00 0.00 0%


**Interpretation:**


- _ϕ_ 1 **dominance:** Return shock intensity overwhelmingly drives scores. Mean contribution of 126,970
vs. single-digit contributions from other factors reflects lack of scaling normalization. This is a known
limitation of the current implementation addressed in future work.


- _ϕ_ 2 _, ϕ_ 3 _, ϕ_ 4 **present but small:** Volatility, attention, and alignment factors register nonzero contributions
for most windows but at scales dwarfed by _ϕ_ 1. This does not mean these factors are uninformative—just
improperly scaled.


- _ϕ_ 5 _, ϕ_ 6 **inactive:** Recurrence and disagreement factors contribute zero due to implementation choices
(recurrence threshold not met; disagreement requires validated multi-source data). These represent future
enhancement opportunities.


**Implications for production deployment:** Before operational use, factor scaling must be normalized
(e.g., standardize each _ϕk_ to z-scores across windows) to achieve meaningful multi-factor contributions. The
current run diagnostically reveals this need.


**5.5** **Temporal Distribution**


Figure 2 shows window counts by ticker, revealing substantial heterogeneity.


19


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Figure 2: Detected window counts per ticker. TLT (bond ETF) shows highest count, potentially reflecting
interest rate sensitivity during 2024.


**Monthly distribution.** Detected windows cluster in a few months (notably January, March, and November
in our 2024 sample). This pattern is descriptive and may reflect periods of elevated market-wide information
flow (e.g., macroeconomic announcements and other high-salience news), but it does not by itself imply
causality and is sensitive to the ticker set, baseline choice, and attention proxy. We also observe that
several windows occur contemporaneously across multiple retail- and news-sensitive instruments; this is
consistent with shared information/attention shocks, but we do not interpret it as evidence of coordination
or wrongdoing. As a qualitative cross-check, we compare these windows against large-move periods in major
risk assets (including BTC proxies) and observe partial temporal alignment.


**5.6** **Cross-Ticker Patterns**


We observe several notable cross-ticker patterns:


- **Meme stock coherence:** GME and AMC windows show temporal correlation (observed qualitatively
though not quantified here), consistent with coordinated attention.


- **Crypto exposure:** COIN and MSTR windows partially align with Bitcoin volatility periods (external
validation against BTC price data confirms this correlation).


- **Sector effects:** Tech stocks (AAPL, MSFT, AMZN, NVDA, META) show some clustering around
earnings seasons, though individual windows remain distinct.


- **ETF separation:** Broad market ETFs (SPY, QQQ) windows largely differ from individual stock windows, suggesting the system distinguishes systematic from idiosyncratic effects.


These patterns suggest AIMM-X captures meaningful market dynamics rather than pure noise.


20


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**6** **Case Study Examples**


To illustrate AIMM-X’s practical utility, we briefly summarize four representative windows (detailed analyses
in Appendix C):


**META Window 135 (Jan 10-12, 2024):** Highest integrity score (7.17M) during market-wide volatility. Demonstrates that high scores can reflect legitimate stress episodes rather than manipulation—human
judgment essential.


**GME Window 5 (May 2-17, 2024):** Extended 12-bar meme stock episode with attention-related indicators. GME Window 5 (May 2–17, 2024): Extended 12-bar meme-stock episode with social-coordination
indicators. Sustained attention/price co-movement warrants review, despite no public enforcement action
we could identify at the time of writing.


**MSTR Window 176 (Jan 10-12, 2024):** Crypto-correlated movements showing how sector-specific
factors drive windows. Cross-asset context quickly reveals underlying drivers.


**SPY Window 186 (Jan 10-11, 2024):** Market-wide ETF episode demonstrating systematic vs. idiosyncratic distinction capability.


These examples show AIMM-X successfully surfaces anomalous episodes across diverse market conditions
while highlighting that context and expert judgment remain essential for interpretation. See Appendix C
for complete analyses including price dynamics, attention patterns, and interpretation guidelines.


**Defensible retrospective evaluation design.** Because market-manipulation ground truth is inherently
scarce, we frame evaluation as _triangulation_ rather than definitive accuracy measurement. We propose two
retrospective tracks:


**(1) Enforcement-labeled “positive controls.”** We will curate a set of publicly documented enforcement
matters (e.g., CFTC/SEC litigation releases, administrative orders) that specify affected securities and
approximate time periods. AIMM-X can then be run retrospectively to test whether the relevant windows
receive elevated ranks/scores relative to each ticker’s baseline. This provides partial, but concrete, validation
where labels exist.


**(2) Unlabeled “high-scrutiny” public episodes.** We will also analyze well-known, heavily discussed
market episodes for which we cannot identify public enforcement actions or confirmed findings. These are
treated strictly as _unlabeled_ case studies: AIMM-X outputs are used to assess face validity, interpretability,
and whether the system highlights windows that domain experts would reasonably flag for review—not to
assert manipulation or claim that regulators missed wrongdoing.


Across both tracks, the evaluation emphasis is: ranking stability, interpretability of factor attributions, and
expert-review usefulness under realistic compliance constraints.


**7** **Discussion**


AIMM-X demonstrates that meaningful market integrity monitoring is possible using only publicly available
data and transparent methodology. However, several critical considerations warrant discussion.


**The ground truth problem:** Unlike supervised learning domains with labeled training data, manipulation
detection faces a fundamental validation challenge—we cannot definitively know which historical episodes
constituted manipulation unless they resulted in enforcement actions, often years later (Aggarwal & Wu,
2006; Putninš, 2012). Bootstrap validation through consistency with known-suspicious patterns and retrospective analysis of enforcement cases offer promising pathways forward, though selection bias and data
availability remain concerns.


21


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**7.1** **Evaluation Against Historical and Delayed-Recognition Episodes**


A key next step is validation against _historical episodes_ that were widely discussed in retrospect or were
associated with enforcement actions after non-trivial delay. Our evaluation claim is intentionally limited:
we test whether AIMM-X assigns elevated scores around the publicly documented time intervals of these
episodes, and whether the factor decomposition produces a coherent evidentiary narrative (price/volume
anomaly aligned with multi-source attention). Such retrospective analyses do not establish causality or
illegality, but they provide a measurable and reproducible way to assess whether the system is sensitive to
patterns that practitioners and regulators have historically treated as high-priority.


**Industry comparison:** AIMM-X does not compete with proprietary systems like NASDAQ Smarts in
data richness or real-time capability. Instead, it serves complementary roles: academic research baseline,
surveillance for smaller institutions, independent validation, and applicability to markets lacking sophisticated infrastructure. The key differentiator is transparency and accessibility, not detection power (Bracke
et al., 2019).


**Deployment considerations:** Operational deployments should integrate with formal reporting channels
(SARs, regulatory collaboration) following responsible disclosure principles. Publicly alleging manipulation
based solely on AIMM-X output would be inappropriate—the framework produces evidence for investigation,
not proof (Fischel & Ross, 1991).


**Extensions:** The framework generalizes to options, cryptocurrencies, fixed income, and international markets with appropriate data adaptation. Cross-market surveillance detecting coordinated manipulation across
asset classes represents an important future direction.


**Ethics:** Key concerns include false accusation risk, privacy, regulatory capture, and computational inequality. Mitigation strategies include emphasis on triage rather than definitive claims, aggregate rather than
individual-level data, methodological transparency, and open science principles (Nosek et al., 2015; Stodden
et al., 2016).


**7.2** **Responsible Deployment and Regulatory/Industry Engagement**


AIMM-X is designed as a _triage_ and decision-support system, not as an automated enforcement or accusation
mechanism. If the framework is deployed in operational settings, any escalation to companies, compliance
teams, or regulators should follow established governance and due-process practices: documented evidence
packets, internal review, and appropriate disclosure channels rather than public allegations. We emphasize
that AIMM-X outputs are statistical signals that require expert interpretation and corroboration with additional data sources (e.g., market microstructure, options activity, issuer disclosures) before any action is
considered.


In our intended research-to-practice pathway, the primary role of external engagement is _validation_ : working
with practitioners to assess whether high-scoring windows correspond to meaningful surveillance events, and
to refine thresholds and interpretation guidelines to reduce false positives.


See Appendix D for extended discussion of these topics.


**8** **Limitations and Future Work**


While AIMM-X demonstrates promising capabilities, substantial limitations remain.


**8.1** **Current Limitations**


**Data resolution:** Daily bars cannot localize suspicious activity within trading days, lose intraday volatility
information, and create baseline estimation artifacts. Five-minute data would immediately improve detection
precision.


22


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Attention signal quality:** Current proxies likely underestimate true attention dynamics. Production
deployment requires authenticated API feeds with real-time or near-real-time updates (Tetlock, 2011; Da
et al., 2015; Bollen et al., 2011).


**Causality and attribution:** AIMM-X detects correlation but cannot infer whether attention drove price
moves (manipulation) or prices attracted attention (legitimate interest). Establishing causation requires
Granger causality tests, high-frequency timestamps, and structural modeling.


**Confounding events:** Earnings announcements, index rebalancing, corporate actions, macroeconomic
news, and sector rotations create manipulation-like patterns. Future versions should integrate event calendars
and adjust scoring accordingly.


**Scoring calibration:** _ϕ_ 1 dominance reflects lack of factor normalization. Production deployment requires
standardization, weight optimization, robust estimators, and ticker-specific thresholds.


**Computational scalability:** Current implementation handles 24 tickers comfortably but scaling to thousands of tickers with minute-level data requires algorithmic optimization and distributed computing.


**8.2** **Near-Term Enhancements**


Priority upgrades include: (1) five-minute data integration, (2) authenticated attention APIs with sentiment
analysis, (3) factor normalization and weight tuning, (4) event calendar integration, and (5) cross-ticker
correlation features capturing systematic factors.


**8.3** **Long-Term Research Directions**


Future work should explore: causal inference techniques, deep learning for pattern discovery with maintained
interpretability (Lundberg & Lee, 2017; Molnar, 2020), adversarial robustness against adapted manipulation strategies, multi-market surveillance across asset classes, regulatory coordination for formal validation,
international deployment, and theoretical foundations for window detection and scoring.


**8.4** **Validation Roadmap**


A structured path forward: Phase 1 (current) demonstrates feasibility with daily data and 24 tickers. Phase
2 (6-12 months) upgrades to 5-minute data, 100+ tickers, and authenticated APIs. Phase 3 (12-18 months)
conducts expert annotation studies. Phase 4 (18-24 months) pilots deployment at cooperating institutions.
Phase 5 (24+ months) enables full-scale deployment with continuous improvement. This roadmap requires
funding, partnerships, and regulatory cooperation not currently secured.


**9** **Conclusion**


Financial market integrity requires constant vigilance, yet most surveillance tools remain locked behind
proprietary walls, inaccessible to researchers, smaller institutions, and independent analysts who could contribute to detection and deterrence. This paper presents AIMM-X as a step toward democratizing market
integrity monitoring through transparent, reproducible, and explainable detection methods.


Our experiments on 24 high-attention securities throughout 2024 demonstrate that meaningful suspicious
window detection is possible using only publicly available OHLCV data and attention proxies. The 233
detected windows, complete with interpretable scoring decompositions, provide a foundation for analyst
review and potential escalation to compliance teams or regulators. Importantly, the framework prioritizes
explainability at every step: every alert comes with evidence, every score can be decomposed, and every
algorithm can be audited.


While AIMM-X is not a silver bullet—ground truth validation remains challenging, and false positives are
inevitable—it represents a pragmatic approach to a difficult problem. The modular architecture allows
continuous improvement: better data sources, refined scoring methods, and validation against known enforcement cases can be integrated as they become available. Case studies demonstrate the system’s ability


23


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


to surface episodes with unusual price-attention co-movement, though human expertise remains essential to
distinguish manipulation from legitimate volatility.


The path forward requires careful validation, responsible deployment, and ongoing dialogue with market integrity stakeholders. We cannot eliminate manipulation through detection alone—that requires enforcement,
deterrence, and cultural change. But we can provide tools that make manipulation harder to hide and easier
to investigate. Transparency in surveillance methodology, far from undermining effectiveness, builds trust
and enables the collective intelligence of researchers, practitioners, and regulators to improve detection over
time.


Several key lessons emerge from this work. First, even simple statistical methods applied to public data
can surface meaningful anomalies when combined with domain knowledge and interpretable design. Second,
the lack of ground truth labels in manipulation detection necessitates different validation approaches than
supervised learning—we must build confidence through consistency, face validity, and expert review rather
than test-set accuracy. Third, transparency and explainability are not merely academic concerns but practical
requirements for surveillance tools that inform high-stakes decisions about market integrity.


Looking ahead, the greatest opportunity lies in community-driven improvement. AIMM-X provides a foundation that academic researchers can validate, practitioners can extend, and regulators can assess. Historical
enforcement cases offer natural test beds for retrospective validation. Higher-frequency data and richer attention feeds will improve detection precision. Machine learning techniques can discover patterns beyond human specification while maintaining interpretability through modern explainability methods. Cross-market
surveillance can detect sophisticated manipulation spanning multiple asset classes.


The ultimate goal is not perfect manipulation detection—an impossible standard given inherent ambiguity—but rather a surveillance ecosystem where multiple independent tools, operated by diverse institutions
with aligned incentives, collectively make markets fairer and manipulation costlier. AIMM-X represents one
contribution to that ecosystem: a transparent, accessible, auditable framework that researchers worldwide
can use, validate, and improve.


We invite the research community to examine this work critically, attempt replication, and propose improvements. Should promising validation emerge, we encourage operational pilots at institutions willing to
experiment with open surveillance tools. And we hope that regulators will view transparent detection frameworks as complements to, rather than competitors with, proprietary systems—expanding the surveillance
capability available to market integrity stakeholders.


Financial markets serve society best when they are trusted. Trust requires integrity. Integrity requires
surveillance. And surveillance requires transparency. AIMM-X is offered in that spirit: not as a solution
to manipulation, but as a tool for those committed to market integrity and willing to work collaboratively
toward fairer, more trustworthy financial markets.


**10** **Broader Impact Statement**


This work contributes an interpretable, reproducible framework for market-integrity triage using publicly
available data. The primary positive impact is democratizing access to surveillance-style tooling beyond
large proprietary platforms, enabling research, auditing, and independent validation.


Potential negative impacts include false positives that could cause reputational harm if misused, and an
arms-race dynamic where adversaries adapt once methods are public. We mitigate these risks by framing
AIMM-X as a decision-support tool rather than an accusation engine, emphasizing human oversight and
procedural safeguards, and operating on aggregate attention signals rather than personal data. Courts and
regulators determine legality; AIMM-X provides transparent statistical evidence to support review.


**Potential benefits.** If validated and deployed responsibly, AIMM-X could help institutions and researchers
detect unusual episodes earlier, allocate investigative resources more efficiently, and improve transparency in
how alerts are produced. It may also enable comparative evaluation of open methods alongside proprietary
surveillance, improving accountability and fostering community-driven improvements.


24


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Risks and misuse.** The main risk is **false accusation or reputational harm** : high scores can reflect
legitimate volatility or market-wide stress rather than manipulation. AIMM-X therefore must be used as a
_triage_ mechanism, not a mechanism for public claims of wrongdoing. A second risk is **market manipulation**
**through narrative** : publicly labeling a ticker or window as “manipulated” can itself move prices. A third
risk is **gaming** : if adversaries learn the detection logic, they may adapt tactics to reduce detectability.


**Mitigations and responsible practice.** We mitigate these risks by (i) emphasizing that AIMM-X produces statistical evidence for expert review and does not establish illegality, (ii) providing factor-level explanations to encourage analyst skepticism rather than blind trust, (iii) avoiding identification of individuals
and using only aggregate attention signals, and (iv) recommending responsible disclosure through established institutional and regulatory channels rather than public allegation. Future work should quantify
false-positive behavior across regimes, establish institutional alert thresholds, and evaluate human-in-theloop review workflows.


Overall, AIMM-X aims to advance transparent surveillance methodology while minimizing harm through
careful framing, interpretability, and controlled engagement.


**11** **Author Contributions**


Sandeep Neela conceived the study, designed the methodology, implemented the pipeline, conducted the
experiments, analyzed the results, and wrote the manuscript.


**References**


Rajesh K. Aggarwal and Guojun Wu. Stock market manipulations. _The Journal of Business_, 79(4):1915–
1953, 2006.


Michael J. Aitken and Frederick H. Harris. How fair is fair access in alternative trading systems? _Journal_
_of Trading_, 10(1):53–63, 2015.


Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph-based anomaly detection and description: A
survey. _Data Mining and Knowledge Discovery_, 29(3):626–688, 2015.


Franklin Allen and Douglas Gale. Stock-price manipulation. _The Review of Financial Studies_, 5(3):503–529,
1992.


Torben G. Andersen and Tim Bollerslev. Answering the skeptics: Yes, standard volatility models do provide
accurate forecasts. _International Economic Review_, 39(4):885–905, 1998.


Malcolm Baker and Jeffrey Wurgler. Investor sentiment and the cross-section of stock returns. _The Journal_
_of Finance_, 61(4):1645–1680, 2006.


Ole E. Barndorff-Nielsen and Neil Shephard. Econometric analysis of realised volatility and its use in
estimating stochastic volatility models. _Journal of the Royal Statistical Society: Series B_, 64(2):253–280,
2002.


Johan Bollen, Huina Mao, and Xiaojun Zeng. Twitter mood predicts the stock market. _Journal of Compu-_
_tational Science_, 2(1):1–8, 2011.


Philippe Bracke, Anupam Datta, Carsten Jung, and Shayak Sen. Machine learning explainability in finance:
An application to default risk analysis. _Bank of England Staff Working Paper_, (816), 2019.


Jonathan Brogaard, Terrence Hendershott, and Ryan Riordan. High-frequency trading and price discovery.
_The Review of Financial Studies_, 27(8):2267–2306, 2014.


Álvaro Cartea, Sebastian Jaimungal, and José Penalva. _Algorithmic and High-Frequency Trading_ . Cambridge
University Press, 2015.


25


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. _arXiv preprint_
_arXiv:1901.03407_, 2019.


Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. In _ACM Computing_
_Surveys_, 2009. Survey article widely cited for anomaly detection taxonomy.


Hailiang Chen, Prabuddha De, Yu Jeffrey Hu, and Byoung-Hyoun Hwang. Wisdom of crowds: The value
of stock opinions transmitted through social media. _The Review of Financial Studies_, 27(5):1367–1403,
2014.


S. Chen et al. Explainable machine learning for financial applications: A survey. In _Proc. of relevant_
_workshops/venues_, 2019. Use as general pointer; replace with a preferred canonical survey if needed.


Carole Comerton-Forde and T¯alis J. Putninš. Dark trading and price discovery. _Journal of Financial_
_Economics_, 118(1):70–92, 2015.


Rama Cont. Empirical properties of asset returns: Stylized facts and statistical issues. _Quantitative Finance_,
1(2):223–236, 2001.


J. Anthony Cookson and Marina Marinič. Why don’t we agree? evidence from a social network of investors.
_The Journal of Finance_, 73(1):173–228, 2018.


Zhi Da, Joseph Engelberg, and Pengjie Gao. In search of attention. _The Journal of Finance_, 66(5):1461–1499,
2011.


Zhi Da, Joseph Engelberg, and Pengjie Gao. The sum of all fears: Investor sentiment and asset prices. _The_
_Review of Financial Studies_, 28(1):1–32, 2015.


David Easley, Nicholas M. Kiefer, Maureen O’Hara, and Joseph B. Paperman. Liquidity, information, and
infrequently traded stocks. _The Journal of Finance_, 51(4):1405–1436, 1996.


David Easley, Soeren Hvidkjaer, and Maureen O’Hara. Is information risk a determinant of asset returns?
_The Journal of Finance_, 57(5):2185–2221, 2002.


David Easley, Marcos López de Prado, and Maureen O’Hara. Microstructure in the machine age. _The Review_
_of Financial Studies_, 25(5):1457–1483, 2012.


Gregory W. Eaton, T. Clifton Green, Brian S. Roseman, and Yanbin Wu. Retail trader sophistication and
stock market quality: Evidence from brokerage outages. _Journal of Financial Economics_, 146(2):502–528,
2022.


Daniel R. Fischel and David J. Ross. Should the law prohibit manipulation in financial markets? _Harvard_
_Law Review_, 105(2):503–553, 1991.


Laura Frieder and Jonathan Zittrain. Spam works: Evidence from stock touts and corresponding market
activity. _Hastings Communications and Entertainment Law Journal_, 30:479–520, 2008.


Xavier Gabaix, Parameswaran Gopikrishnan, Vasiliki Plerou, and H. Eugene Stanley. A theory of power-law
distributions in financial market fluctuations. _Nature_, 423:267–270, 2003.


Joel Hasbrouck. Measuring the information content of stock trades. _The Journal of Finance_, 46(1):179–207,
1991.


Joel Hasbrouck. One security, many markets: Determining the contributions to price discovery. _The Journal_
_of Finance_, 50(4):1175–1199, 1995.


Terrence Hendershott and Ryan Riordan. Algorithmic trading and the market for liquidity. _Journal of_
_Financial and Quantitative Analysis_, 48(4):1001–1024, 2013.


Di Hu, Christopher M. Jones, Vincent Zhang, and Xiaoyan Zhang. Attention-induced trading and returns:
Evidence from robinhood users. _Journal of Finance (Forthcoming)_, 2021.


26


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting
spacecraft anomalies using lstms and nonparametric dynamic thresholding. In _Proceedings of the 24th_
_ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pp. 387–395, 2018.


John P. A. Ioannidis. Why most published research findings are false. _PLoS Medicine_, 2(8):e124, 2005.


Charles M. Jones. What do we know about high-frequency trading? _Columbia Business School Research_
_Paper_, 2013.


Andrei Kirilenko, Albert S. Kyle, Mehrdad Samadi, and Tugkan Tuzun. The flash crash: High-frequency
trading in an electronic market. _The Journal of Finance_, 72(3):967–998, 2017.


Albert S. Kyle. Continuous auctions and insider trading. _Econometrica_, 53(6):1315–1335, 1985.


Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting with
neural networks at uber. In _International Conference on Machine Learning_, pp. 1–5, 2015.


Andrew W. Lo. The adaptive markets hypothesis. _The Journal of Portfolio Management_, 30(5):15–29, 2004.


Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In _Advances in_
_Neural Information Processing Systems_, volume 30, 2017.


Albert J. Menkveld. High frequency trading and the new market makers. _Journal of Financial Markets_, 16
(4):712–740, 2013.


Edward Miguel, Colin Camerer, Katherine Casey, et al. Promoting transparency in social science research.
_Science_, 343(6166):30–31, 2014.


Helen Susannah Moat, Chester Curme, Aram Avakian, Dror Y. Kenett, H. Eugene Stanley, and Tobias Preis.
Quantifying wikipedia usage patterns before stock market moves. _Scientific Reports_, 3:1801, 2013.


Christoph Molnar. _Interpretable Machine Learning: A Guide for Making Black Box Models Explainable_ .
Lulu.com, 2020.


Brian A. Nosek, George Alter, George C. Banks, et al. Promoting an open research culture. _Science_, 348
(6242):1422–1425, 2015.


Maureen O’Hara. High frequency market microstructure. _Journal of Financial Economics_, 116(2):257–270,
2015.


Open Science Collaboration. Estimating the reproducibility of psychological science. _Science_, 349(6251),
2015.


Polygon.io. Polygon.io market data apis. `[https://polygon.io/](https://polygon.io/)`, 2026. Accessed 2026-01; OHLCV source
for this preprint.


Tobias Preis, Helen Susannah Moat, and H. Eugene Stanley. Quantifying trading behavior in financial
markets using google trends. _Scientific Reports_, 3:1684, 2013.


T¯alis J. Putninš. Market manipulation: A survey. _Journal of Economic Surveys_, 26(5):952–967, 2012.


Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?" explaining the predictions
of any classifier. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge_
_Discovery and Data Mining_, pp. 1135–1144, 2016.


Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Grégoire Montavon, Wojciech Samek, et al.
Unsupervised anomaly detection: A survey. _Proceedings of the IEEE_, 2021. Survey on modern deep
anomaly detection and evaluation.


Gregory Scopino. The (un)suitability of high-speed trading infrastructure: Evidence from the flash crash.
_Southern California Law Review_, 88:561–632, 2015.


27


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn. False-positive psychology: Undisclosed flexibility
in data collection and analysis allows presenting anything as significant. _Psychological Science_, 22(11):
1359–1366, 2011.


Timm O. Sprenger, Andranik Tumasjan, Philipp G. Sandner, and Isabell M. Welpe. Tweets and trades: The
information content of stock microblogs. _European Financial Management_, 20(5):926–957, 2014.


Victoria Stodden, Marcia McNutt, David H. Bailey, et al. Enhancing reproducibility for computational
methods. _Science_, 354(6317):1240–1241, 2016.


Paul C. Tetlock. Giving content to investor sentiment: The role of media in the stock market. _The Journal_
_of Finance_, 62(3):1139–1168, 2007.


Paul C. Tetlock. All the news that’s fit to reprint: Do investors react to stale information? _The Review of_
_Financial Studies_, 24(5):1481–1512, 2011.


Paul C. Tetlock, Maytal Saar-Tsechansky, and Sofus Macskassy. More than words: Quantifying language to
measure firms’ fundamentals. _The Journal of Finance_, 63(3):1437–1467, 2008.


U.S. Commodity Futures Trading Commission. Cftc market surveillance and enforcement (overview resources). `[https://www.cftc.gov/](https://www.cftc.gov/)`, 2017. General reference for surveillance and enforcement context.


U.S. Securities and Exchange Commission. Sec market abuse and enforcement (overview resources). `[https:](https://www.sec.gov/)`
`[//www.sec.gov/](https://www.sec.gov/)`, 2019. General reference for enforcement and market integrity context.


Alex Wang, Michael P. Wellman, and Michael Kuzma. Spoofing the limit order book: A strategic trading
perspective. _The Journal of Trading_, 13(3):35–48, 2018.


**Appendix**


**A** **Implementation Details**


AIMM-X is implemented in Python 3.9+ with standard scientific computing libraries. The pipeline consists
of modular scripts:


**Data acquisition:**


- `00_fetch_polygon_ohlcv_1d.py` : Fetches daily OHLCV from Polygon.io API


- `00_build_attention_1d.py` : Constructs attention signal panel from source data


**Core pipeline:**


- `01_build_panel.py` : Merges OHLCV and attention into unified time-series panel


- `02_detect_windows.py` : Computes z-scores, applies hysteresis segmentation


- `03_compute_signals.py` : Calculates phi-signals for each detected window


- `04_score_and_decompose.py` : Aggregates integrity scores, generates rankings


- `05_generate_figures.py` : Produces visualizations (histograms, timeseries, case studies)


- `06_generate_tables.py` : Exports LaTeX tables and CSV summaries


28


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Dependencies:** pandas, numpy, scipy, matplotlib, seaborn, requests, certifi.


**Configuration:** All parameters specified in `config.json` including ticker universe, thresholds, weights, and
file paths.


**Runtime:** Complete pipeline executes in approximately 8-12 minutes on commodity hardware (4-core CPU,
16GB RAM) for 24 tickers with daily data.


**Code availability:** Upon publication, code will be released under open-source license (MIT or Apache 2.0)
via GitHub with documentation, example configuration, and replication instructions.


**B** **Configuration Parameters**


Table 6 documents key configuration parameters for the experimental implementation.


Table 6: Configuration parameters for the experimental run.


Parameter Value Description


`baseline_window` 20 Rolling window length for baseline estimation (days)
`thr_high` 3.0 High threshold for window initiation (z-score units)
`thr_low` 2.0 Low threshold for window continuation (z-score
units)
`min_window_len` 2 Minimum window length (bars)
`gap_tolerance` 3 Maximum gap for merging adjacent windows (bars)
`alpha_r` 1.0 Return channel weight in composite score
`alpha_sigma` 1.0 Volatility channel weight
`alpha_A` 1.0 Attention channel weight
`omega_1 ...` `omega_6` 1.0 Phi-signal weights in integrity score
`attention_sources` 5 Number of attention sources (reddit, stocktwits,
wikipedia, news, trends)
`source_weights` uniform Attention fusion weights (equal across sources)


**C** **Detailed Case Studies**


This appendix provides comprehensive analyses of four representative suspicious windows illustrating AIMMX’s detection capabilities and interpretation guidelines.


**C.1** **Case Study 1: META Window 135 (Highest Integrity Score)**


**Episode summary:** January 10-12, 2024. Integrity Score: 7,166,596 (100th percentile). Duration: 3 bars
(days).


**Market context:** This window occurred during heightened market volatility following December 2023
inflation data releases and Federal Reserve policy speculation. META, following 2023 restructuring, was
experiencing renewed investor interest (Tetlock, 2007).


**Price and volume dynamics:** META moved 5.2% on January 11 with volume 40% above average. The
high-low range exceeded 3% intraday. While not unprecedented for META, this magnitude combined with
broader market context triggered detection.


**Attention pattern:** All five attention sources registered elevated activity. Reddit mentions increased 3x
relative to baseline, StockTwits message volume spiked, and Wikipedia page views showed unusual concentration. This multi-source alignment elevated _ϕ_ 3 and _ϕ_ 4 components.


**Analyst interpretation:** Manipulation is unlikely—META is a liquid mega-cap where manipulation would
be extremely difficult. The high score reflects legitimate volatility during market stress. An analyst would
quickly conclude this represents normal price discovery during uncertainty rather than suspicious activity.


29


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


This demonstrates both strength and limitation: AIMM-X surfaces unusual episodes, but human judgment
remains essential.


**C.2** **Case Study 2: GME Window 5 (Meme Stock Coordination)**


**Episode summary:** May 2-17, 2024. Integrity Score: 147 (93.6th percentile). Duration: 12 bars (extended
episode).


**Market context:** GameStop experienced renewed retail attention in early May 2024, coinciding with social
media discussions about company transformation and comparisons to January 2021 (Hu et al., 2021).


**Price and volume dynamics:** Unlike 2021’s explosive moves, this episode showed moderate price volatility
(daily returns mostly _<_ 5%) but sustained above-normal volume over two weeks. The gradual accumulation
distinguishes this from classic pump-and-dump patterns.


**Attention pattern:** Reddit activity spiked dramatically, particularly in r/wallstreetbets. However, other
sources showed more modest increases, creating cross-source disagreement. StockTwits sentiment became
increasingly bullish during the period.


**Extended duration:** The 12-bar length reflects sustained interest rather than a one-day spike. This temporal persistence is captured in window segmentation via hysteresis allowing continuation through moderate
dips.


**Analyst interpretation:** This episode warrants deeper investigation. The sustained nature, social media coordination indicators, and lack of fundamental catalysts create a pattern consistent with coordinated
attention-driven trading. While not definitive proof of manipulation, it represents exactly the type of case
compliance teams should review (U.S. Securities and Exchange Commission, 2019; U.S. Commodity Futures Trading Commission, 2017). No SEC enforcement action resulted, highlighting inherent ambiguity in
manipulation detection.


**C.3** **Case Study 3: MSTR Window 176 (Crypto-Correlation)**


**Episode summary:** January 10-12, 2024. Integrity Score: 4,066,778 (99.6th percentile). Duration: 3 bars.


**Market context:** MicroStrategy’s business model centers on Bitcoin holdings, making MSTR stock a
cryptocurrency proxy within traditional equity markets (Bollen et al., 2011; Da et al., 2015).


**Price and volume dynamics:** MSTR moved 8.3% over the window, with volatility elevated substantially
above baseline. Volume patterns showed clustering in opening and closing hours.


**Crypto correlation:** Cross-asset analysis reveals strong correlation with Bitcoin price movements during
this window. BTC experienced similar volatility related to spot ETF approval speculation.


**Attention pattern:** Attention sources showed split: crypto-focused platforms (Reddit’s r/cryptocurrency,
crypto news) spiked strongly, while general finance news showed moderate increases. This sector-specific
attention suggests genuine interest from crypto enthusiasts rather than manipulation attempts.


**Analyst interpretation:** This window likely reflects legitimate price discovery in response to cryptocurrency market dynamics rather than MSTR-specific manipulation. The integrity score correctly identifies an
anomalous episode, but context (crypto correlation, sector-specific attention) quickly reveals the underlying
driver. This demonstrates AIMM-X’s value as triage: it surfaces the window, but analyst expertise interprets
the evidence.


**C.4** **Case Study 4: SPY Window 186 (Market-Wide Event)**


**Episode summary:** January 10-11, 2024. Integrity Score: 1,127,905 (97.0th percentile). Duration: 2 bars.


**Market context:** SPY, the S&P 500 ETF, provides a pure market-wide indicator. Suspicious windows in
SPY typically reflect systematic factors rather than idiosyncratic manipulation.


30


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Price and volume dynamics:** The S&P 500 declined 1.4% on January 11 following stronger-than-expected
inflation data, with VIX spiking to 14.5 from 12.8. Volume in SPY exceeded average by 35%.


**Distinguishing systematic from idiosyncratic:** Many top windows (Table 4) cluster around January
10-12. This temporal clustering across uncorrelated securities strongly suggests a systematic market event
rather than coordinated ticker-specific manipulation.


**Analyst interpretation:** This window demonstrates AIMM-X’s ability to detect market-wide stress. While
not manipulation in the traditional sense, such episodes merit surveillance attention—market manipulation
can occur through index futures, systematic strategies, or algorithmic cascades (Kirilenko et al., 2017;
Scopino, 2015). The detection is valuable even though the episode reflects legitimate information processing.


**Design implication:** Future versions should explicitly model market factors (beta adjustments, marketrelative thresholds) to distinguish idiosyncratic from systematic anomalies. Current per-ticker detection
treats all windows equivalently.


**C.5** **Interpretation Guidelines for Practitioners**


These case studies motivate practical guidance for analysts reviewing AIMM-X alerts:


1. **Check temporal clustering:** If many windows across tickers align temporally, suspect systematic
factors (economic news, Fed announcements, geopolitical events) rather than ticker-specific manipulation.


2. **Examine attention source composition:** Which sources spiked? Broad-based attention (news, general social media) suggests legitimate interest. Narrow platform spikes (single subreddit, coordinated
StockTwits) may indicate manipulation attempts.


3. **Consider liquidity and market cap:** Manipulation becomes exponentially harder as market capitalization increases. Mega-cap stocks are generally harder to manipulate due to liquidity and participation
breadth; many detections may reflect legitimate information processing rather than manipulation..


4. **Look for fundamental catalysts:** Did earnings release, news announcement, regulatory action, or
corporate event occur during the window? If yes, high scores likely reflect information incorporation. If
no obvious catalyst exists, suspicion increases.


5. **Assess pattern consistency:** Single isolated windows may be statistical noise. Repeated windows in
short succession, especially with similar attention profiles, suggest sustained campaigns.


6. **Cross-reference external data:** Check order book depth (if available), options activity, short interest,
borrow rates, and institutional holdings for corroborating evidence.


7. **Escalation threshold:** Not every high-scoring window warrants regulatory reporting. Establish institutional thresholds based on score, evidence quality, and business rules.


The key insight: AIMM-X is a triage tool, not an oracle. It identifies anomalies requiring human judgment,
not definitive manipulation verdicts.


**D** **Extended Discussion**


This appendix provides extended discussion of validation challenges, deployment considerations, and broader
implications of the AIMM-X framework.


**D.1** **The Ground Truth Problem and Validation Pathways**


The fundamental challenge in manipulation detection research is the lack of reliable ground truth labels.
Unlike supervised machine learning domains where labeled training data exists, we cannot definitively know


31


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


which historical trading episodes constituted manipulation unless they resulted in enforcement actions—and
even then, only after years of investigation (Aggarwal & Wu, 2006; Putninš, 2012).


**Enforcement case validation:** One promising approach involves retrospective analysis of known CFTC
and SEC enforcement cases. Researchers could obtain case files, identify affected securities and time periods,
and test whether AIMM-X would have flagged those episodes. However, this approach has limitations:


- **Selection bias:** Enforcement cases represent only detected and prosecuted manipulation, likely missing
sophisticated schemes that evaded detection.


- **Data availability:** Many enforcement actions involve securities or time periods for which historical data
(especially attention proxies) is unavailable.


- **Scheme diversity:** Manipulation takes many forms (pump-and-dump, spoofing, wash trading, marking
the close). AIMM-X targets attention-coordination schemes; it may not detect order-flow manipulation.


**Expert annotation studies:** An alternative validation approach involves presenting detected windows to
experienced compliance professionals and market surveillance analysts for classification (suspicious, legitimate, uncertain). Inter-rater agreement metrics (Fleiss’ kappa, Krippendorff’s alpha) could quantify how
well AIMM-X aligns with expert judgment. This approach acknowledges that ground truth is subjective and
establishes confidence through expert consensus.


**Synthetic data experiments:** Researchers could inject synthetic manipulation patterns (coordinated
buying with artificial attention spikes) into historical data and test detection sensitivity. This controlled
validation complements real-world analysis but raises questions about whether synthetic patterns resemble
actual manipulation tactics.


**Bootstrap validation:** Our recommended path forward builds confidence through multiple convergent
lines of evidence: face validity (do detected windows correspond to known volatility periods?), consistency
(do similar windows receive similar scores?), interpretability (can analysts understand why windows were
flagged?), and partial validation against available enforcement cases. No single validation method suffices;
triangulation across methods builds cumulative confidence.


**D.2** **Comparison with Industry Surveillance Systems**


AIMM-X occupies a different niche than commercial surveillance platforms. Understanding these differences
clarifies appropriate use cases:


**NASDAQ Smarts / NYSE Mercury:** These proprietary systems integrate order-book data, execution
reports, trader identifiers, and cross-market linkages. They detect manipulation patterns invisible in OHLCV
data alone (spoofing, layering, wash trades, momentum ignition). AIMM-X cannot compete on detection
coverage for order-flow manipulation.


**AIMM-X advantages:** Transparency (open methodology, auditable code), accessibility (available to researchers and smaller institutions), focus (attention-driven manipulation where AIMM-X has comparative
advantage), and research platform (extensible framework for academic investigation).


**Complementary roles:** Large institutions could use AIMM-X as independent validation—do proprietary
systems and AIMM-X agree on flagged episodes? Disagreements warrant investigation. Smaller institutions
without resources for commercial systems gain baseline surveillance capability. Researchers gain reproducible
framework for manipulation detection research.


**Public interest monitoring:** Academics, journalists, and advocacy organizations could deploy AIMMX for independent market integrity analysis, providing public accountability separate from regulatory and
exchange-operated systems.


32


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**D.3** **Regulatory Engagement and Responsible Disclosure**


Operational AIMM-X deployments raise important questions about regulatory coordination and responsible
disclosure:


**Formal reporting channels:** Financial institutions already have obligations to file Suspicious Activity
Reports (SARs) when they detect potential manipulation. AIMM-X alerts could feed into existing compliance
workflows, with windows exceeding institutional thresholds triggering SAR preparation. Direct regulatory
collaboration (providing AIMM-X alerts to SEC or CFTC for investigation) represents another pathway,
though partnership agreements would be required.


**Public disclosure risks:** Publicly alleging manipulation based solely on AIMM-X output would be inappropriate and potentially harmful. High integrity scores indicate anomalies warranting investigation, not
proof of wrongdoing. Premature public accusations could damage reputations, manipulate markets, or interfere with investigations. Responsible disclosure requires evidence threshold, regulatory coordination, and
protection for accused parties.


**Research publication ethics:** This paper presents aggregated results and historical case studies. We avoid
naming specific suspicious windows outside well-known public episodes (GME, meme stocks) where extensive
media coverage already exists. Future research should follow similar principles: contribute to surveillance
methodology without becoming surveillance itself.


**Regulatory and industry engagement** AIMM-X is designed as an _evidence-ranking and explanation_
tool that supports expert review; it does not identify individual actors and does not establish wrongdoing.
If future validation supports operational usefulness, our intent is to engage responsibly with market participants (e.g., broker-dealers, exchanges, and compliance teams) and, where appropriate, regulators such as
CFTC/SEC by sharing: (i) the methodology and code for independent replication, (ii) aggregated, windowlevel summaries and factor attributions, and (iii) clear guidance on alert interpretation and false-positive
handling.


Any escalation would follow established institutional processes (e.g., compliance review and existing reporting
channels) rather than public allegation. We explicitly discourage public accusations based solely on AIMM-X
output. The goal of outreach is to enable _independent assessment, feedback, and controlled pilots_ —not to
claim regulatory failure, assign intent, or substitute for investigative authority.


**D.4** **Extensions to Other Asset Classes**


The AIMM-X framework is not limited to U.S. equities. Conceptually, the methodology extends to:


**Options markets:** Manipulators may use equity-option combinations to amplify returns or obscure positions. Extending AIMM-X to options requires modeling implied volatility, open interest, and put-call
dynamics. Attention signals (Google Trends for "SPY options", social media options discussion) could complement equity attention.


**Cryptocurrencies:** Crypto markets exhibit frequent manipulation due to limited regulation, lower liquidity, and pump-and-dump prevalence (Hu et al., 2021). AIMM-X could monitor Bitcoin, Ethereum, and
altcoins using exchange OHLCV data and crypto-specific attention sources (Crypto Twitter, Discord channels, Telegram groups). Decentralized exchanges and pseudonymous trading complicate detection but also
increase manipulator reliance on attention coordination.


**Fixed income:** Treasury and corporate bond manipulation is less common than equity manipulation but
can occur during stress periods or illiquid issues. AIMM-X adaptation requires modeling yield curves, credit
spreads, and duration. Attention signals are less obvious (bond markets attract less retail interest), but
institutional attention proxies (research downloads, Bloomberg terminal searches) could substitute.


**International markets:** AIMM-X applies to any market with available OHLCV data and attention proxies.
Emerging markets may have higher manipulation prevalence but weaker data infrastructure. Cross-market


33


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


surveillance detecting coordinated manipulation across geographies (pump U.S.-listed ADRs while manipulating underlying foreign shares) represents an important extension.


**D.5** **Ethical Considerations and Societal Implications**


Developing and deploying surveillance tools raises ethical questions requiring careful consideration:


**False accusation risk:** High integrity scores sometimes reflect legitimate volatility rather than manipulation (META Window 135). Publishing or acting on false accusations causes reputational harm and market
disruption. Mitigation requires emphasis on triage rather than definitive claims, transparency about false
positive rates, human oversight requirements, and procedural safeguards for accused parties.


**Privacy considerations:** AIMM-X uses aggregate attention data (subreddit post counts, Wikipedia
pageview totals) rather than individual-level information. This design choice prioritizes privacy while enabling detection. Production deployments should maintain this aggregation principle, avoiding surveillance
of identifiable individuals unless compelled by legal process.


**Regulatory capture:** If AIMM-X becomes influential in regulatory enforcement, manipulators could study
the methodology and adapt tactics to evade detection. This is the eternal arms race in adversarial domains.
Transparency vs. security tradeoffs are real but overstated—most manipulation is unsophisticated, and
detection evasion requires resources exceeding most schemes’ profitability. Periodic methodology updates
(not publicly announced) can mitigate adaptive evasion.


**Computational inequality:** Advanced surveillance systems (like proprietary platforms) are available only
to well-resourced institutions, creating detection inequality where small institutions and retail investors lack
protection. AIMM-X partially addresses this through accessibility, though data costs (premium APIs) and
expertise requirements remain barriers. Open-source development and hosted deployments could further
democratize access.


**Manipulation vs.** **coordination:** Not all coordinated trading is manipulation. Reddit communities
discussing stocks engage in legal collective speech and investment decisions. Drawing the line between
protected activity and illegal coordination is a legal and philosophical challenge beyond AIMM-X’s scope.
The framework detects anomalies; courts and regulators determine legality.


**Unintended consequences:** Could widespread AIMM-X deployment reduce market liquidity if attentioncoordinated trading (some of which is legitimate) is suppressed? Could false positives discourage retail
participation? Could adversarial manipulation increase sophistication in response? These questions require
empirical study as deployment expands.


**E** **Complete Ticker Statistics**


Table 7 provides comprehensive statistics for all 24 tickers.


34


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


Table 7: Complete per-ticker statistics (all windows).


Ticker Windows Mean _M_ Median _M_ Max _M_ Std _M_ Mean Dur (bars) Total Bars Coverage %


META 12 597,231 16 7,166,596 2,068,813 15.0 36 100
MSTR 10 406,687 16 4,066,778 1,286,025 16.5 33 100
SNAP 8 464,895 24 3,718,879 1,314,807 23.1 37 100
AMZN 11 281,293 15 3,094,132 932,913 15.5 34 100
RBLX 11 216,315 15 2,379,285 717,376 15.0 33 100
SPCE 10 174,682 22 1,746,746 552,367 22.0 44 98
NFLX 11 142,377 16 1,565,992 472,160 15.9 35 100
SPY 8 140,994 11 1,127,905 398,772 11.3 18 100
HOOD 9 109,575 22 986,040 328,674 22.2 40 97
TLT 13 71,825 14 933,662 258,950 13.8 36 99
COIN 7 131,899 16 923,243 348,950 15.7 22 100
TSLA 10 87,031 15 870,173 275,168 15.0 30 100
NOK 10 57,396 15 573,894 181,479 15.0 30 95
XLF 11 28,886 16 317,634 95,767 15.9 35 100
IWM 8 14,114 14 112,808 39,879 14.4 23 100
GME 9 31 27 147 45 27.2 49 89
PLTR 10 28 20 109 34 20.0 40 93
AMC 10 16 19 67 20 18.5 37 91
BB 9 23 19 54 17 19.4 35 94
AAPL 9 13 21 39 11 20.6 37 100
GOOG 11 11 15 25 7 14.5 32 100
QQQ 8 10 16 16 5 16.3 26 100
NVDA 7 8 18 14 4 17.9 25 100
MSFT 11 7 12 12 3 12.3 27 100


Note: Coverage% indicates percentage of trading days with complete OHLCV and attention data.


**F** **Temporal Distribution Analysis**


Windows cluster in specific periods during 2024, reflecting known volatility regimes:


**January (42 windows):** Market reopening volatility, inflation data releases, Fed commentary speculation.


**March (28 windows):** FOMC meeting cycle, bank stress (SVB anniversary), earnings season.


**May (19 windows):** Meme stock resurgence, tech earnings, AI hype cycle.


**August (23 windows):** Summer volatility, yen carry trade unwind, recession fears.


**November (31 windows):** U.S. election period, policy uncertainty, year-end positioning.


This temporal clustering provides face validity—detected windows align with periods when market participants and media reported elevated volatility and attention.


**G** **Data Availability and Reproducibility**


**OHLCV data:** Available from Polygon.io (free tier sufficient for replication, premium tier recommended
for extensions). Historical daily bars are permanent and unrevised (split-adjusted).


**Attention data:** The replication package includes complete proxy generation code for all five attention
sources (Reddit, StockTwits, Wikipedia, Google Trends, LIME), detailed documentation of data collection
methodology, and configuration files specifying all parameters. Regenerating attention signals requires freetier API access: Reddit API, Twitter/X API (for StockTwits), Wikipedia API, and Google Trends (public
endpoints, subject to rate limiting). The methodology is fully documented, but exact numerical replication
may vary slightly due to API changes, rate limits, or retroactive content deletions on source platforms.


**Code:** Python implementation will be released open-source upon publication with detailed README,
example configuration, and step-by-step replication instructions.


**Computational requirements:** Standard laptop/workstation sufficient (4-core CPU, 16GB RAM). No
GPU required. Complete pipeline runtime _<_ 15 minutes for 24 tickers with daily data.


**Output artifacts:** All figures, tables, and window lists reported in this paper are deterministically generated
from pipeline and included in replication package.


**License:** Code released under MIT license. Data subject to provider terms (Polygon.io API terms of
service).


35


Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring


**Contact:** Correspondence to sandeepneela@gmail.com for replication assistance, data requests, or collaboration inquiries.


**H** **Glossary of Terms**


- **AIMM-X:** AI-driven Market Integrity Monitor with Explainability


- **Suspicious Window:** Contiguous time interval where return, volatility, and attention simultaneously
deviate from baseline


- **Integrity Score (** _M_ **):** Composite score ranking windows by suspiciousness, decomposable into factor
contributions


- **Phi-signals (** _ϕ_ 1 **–** _ϕ_ 6 **):** Interpretable factors measuring return shock, volatility anomaly, attention spike,
alignment, recurrence, and disagreement


- **Hysteresis Segmentation:** Two-threshold method for window extraction reducing fragmentation


- **Attention Fusion:** Weighted combination of multiple attention sources into unified signal


- **Z-score:** Standardized deviation measuring how many standard deviations an observation differs from
baseline


- **OHLCV:** Open, High, Low, Close, Volume—standard price/volume time series


- **Triage System:** Detection framework surfacing candidates for human review rather than definitive
classification


- **Factor Decomposition:** Breaking composite score into constituent components to explain why window
was flagged


36


