## **Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit** **Logic and Ambiguity**



**Hangle Hu, Chenyu Hou, Bin Cao**
Zhejiang University of Technology
Hangzhou, China


**Abstract**


While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of
general-purpose programming languages such
as Python or Pandas to manage file-based data
and complex analytical workflows. Despite this
growing need, the reliability of Text-to-Python
in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address
this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation.
We systematically refined the original dataset
to reduce annotation noise and align execution
semantics, thereby establishing a consistent
and standardized baseline for comparison. Our
analysis reveals a fundamental paradigmatic
divergence: whereas SQL leverages implicit
DBMS behaviors through its declarative structure, Python requires explicit procedural logic,
making it highly sensitive to underspecified
user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF),
which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these
gaps are addressed, Text-to-Python achieves
performance parity with Text-to-SQL. These
findings establish Python as a viable foundation
for analytical agents—provided that systems effectively ground ambiguous natural language
inputs in executable logical specifications.
[Resources are available at https://github.](https://github.com/1050727345hu-web/Bird-Python)
[com/1050727345hu-web/Bird-Python.](https://github.com/1050727345hu-web/Bird-Python)


**1** **Introduction**


The rapid advancement of Large Language Models (LLMs) has fundamentally reshaped Natural
Language Interface (NLI) research. By allowing
users to retrieve information through intuitive natural language queries (Qin et al., 2022; Huang et al.,



**Ruizhe Li**
University of Aberdeen
Aberdeen, Scotland, UK


2025), NLIs significantly lower technical barriers
and enhance the efficiency of human-computer interaction. This paradigm offers substantial potential for broadening data access and analysis across
diverse domains.
Driven by this promise, the research community
has focused extensively on Text-to-SQL technology, which translates natural language questions
into executable queries for structured databases.
This field has developed rapidly, supported by the
emergence of comprehensive benchmarks like Spider (Yu et al., 2018) and BIRD (Li et al., 2024b),
and a progression in techniques from early semantic parsing to modern end-to-end neural generation
and schema linking (Ruan et al., 2023; Kong et al.,
2023; Sui et al., 2023; Zhang et al., 2024; Cao et al.,
2024; Maamari et al., 2024; Safdarian et al., 2025).
However, an exclusive reliance on the Text-to-SQL
paradigm presents critical limitations in real-world
scenarios. First, vast quantities of practical data
reside in standalone files (e.g., CSV, Excel, JSON)
rather than managed databases; these formats are
not directly queryable via SQL without cumbersome preprocessing. Second, and perhaps more
critically, modern analytical needs often require
flexible procedural logic and library integration that
exceed the expressive capacity of standard declarative SQL, necessitating the computational breadth
of general-purpose programming languages.
To address these challenges, the Text-to-Python
paradigm has emerged as a promising alternative
(Luo et al., 2025; Jiang et al., 2024; Li et al.,
2025). By generating executable Python code,
this approach can directly manipulate file-based
data while leveraging powerful libraries like Pandas for robust data manipulation, effectively establishing a foundation for advanced analytics. Consequently, Text-to-Python inherently overcomes the
inherent constraints of the Text-to-SQL route, offering a more flexible framework for natural languagedriven data exploration (see Figure 1).



1


Despite its theoretical advantages, the Text-toPython path for data querying and analysis remains
largely unexplored. Existing work in code generation predominantly focuses on general programming tasks such as code snippet generation, completion, and repair, as exemplified by benchmarks
like HumanEval (Chen et al., 2021; Zhuo et al.,
2024) and SWE-Bench (Yu et al., 2025; Dou et al.,
2024). While recent agentic frameworks (Lei et al.,
2024b; Huo et al., 2025; Zhou et al., 2025; Gu
et al., 2025) and data science benchmarks (Jing
et al., 2024; Egg et al., 2025) have begun to explore broader capabilities, there is a notable absence of a dedicated benchmark to systematically
evaluate a model’s ability to translate natural language requests into Python code specifically tailored for core data retrieval and analysis. This gap
raises a fundamental research question: Can Textto-Python effectively serve as a viable and more
flexible replacement for Text-to-SQL in practical
data interaction scenarios?
To investigate this, we conduct a thorough empirical study by adapting the BIRD benchmark (Li
et al., 2024b) to evaluate Text-to-Python performance. To align with real-world scenarios, we
transformed the underlying relational databases
into standalone file formats (i.e., CSVs) and
mapped original SQL-based objectives into equivalent Python execution logic. This process highlights a key paradigmatic distinction: unlike SQL’s
declarative nature which relies on implicit database
rules (e.g., automatic null handling), Python requires explicit, step-by-step procedural logic to
define execution behaviors. This fundamental
difference exposes the Text-to-Python paradigm
to greater challenges regarding ambiguous constraints, necessitating strict definition to ensure precise execution.
Using this refined benchmark, we compare stateof-the-art Text-to-SQL and Text-to-Python models.
Our analysis reveals that while Text-to-Python offers comparable performance given sufficient context, its reliance on explicit procedural logic makes
it significantly more sensitive to knowledge gaps
than SQL. Both paradigms struggle when essential
background knowledge is missing (B et al., 2024;
Liu et al., 2024; Ouyang et al., 2025; Zhu et al.,
2025), but this deficiency is particularly detrimental to Python’s need for precise operational steps.
This finding suggests that the focus of NLI research
should shift toward supplementing models with adequate contextual knowledge. Consequently, we



Figure 1: Functional comparison of SQL-based and
code-based approaches across key data processing
stages. SQL handles structured data and basic queries;
code supports these tasks and adds flexibility—direct
raw data processing, complex logic, machine learning
integration, and automated dynamic output.


propose the Logic Completion Framework (LCF),
which explicitly supplements latent domain knowledge and markedly improves query accuracy across
both paradigms.
The contributions of this paper are threefold.
(1) First, we propose the first dedicated benchmark BIRD-Python for evaluating Text-to-Python
in data querying tasks on file-based environments,
enabling a detailed comparative analysis with Textto-SQL. (2) Second, we uncover the pivotal insight
that the need for explicit logic formulation makes
knowledge grounding—rather than the choice of
paradigm alone—the primary bottleneck in flexible analytics scenarios. (3) Finally, we introduce
the LCF, a practical methodology for context completion that explicitly supplements latent domain
knowledge and markedly improves the query accuracy of both Text-to-SQL and Text-to-Python
systems.


**2** **Related Work**


Text-to-SQL approaches have evolved from rulebased matching (Zhong et al., 2017) to advanced
systems leveraging LLMs with multi-agent architectures. MAC-SQL (Wang et al., 2023) decomposes queries among specialized agents, while
SQL-of-Thought (Chaturvedi et al., 2025) mitigates logical errors through iterative refinement.
As the scope of tasks expands beyond traditional
database querying to broader data science workflows, Text-to-Python systems have attracted in


**Code-Based**


**Diverse Inputs**


**Loop** **Complex state**
**iteration** **management**



**Input**

**Data**



**SQL-Based**


**Structured Inputs**



**Processing**



**Logic**
**Extract-Transform**



**-Load**



**Machine**

**learning**
**Limited**
**MLP/Statistical Modeling**



**Final**
**output**



**Static Result**

**Table**



**Automated**

**Reporting**



**Dynamic**
**Visualization**



2


**Input Context** **Semantic Parsing &**
**Program Generation**

**Declarative**
**Program Psql**



**Execution and Results**



d



**SELECT avg(col)**



**DAtabase: DRelational db** **LLM** **WHERE...** **DBMS**



**LLM**



**FROM T**
**WHERE...**



**Query: Q** **Procedural**
**Program Pcode**



**Dataframe: Ddf** **LLM** **Interpreter**



**Answer**


**Answer**



**LLM**



**df = pd.load(...);**
**df_c = df.dropna();**
**res = df_c.mean() ...**



Figure 2: Comparative framework of NLI semantic
parsing paradigms. Top: Text-to-SQL maps queries
to declarative relational algebra, delegating execution
optimization to the DBMS. Bottom: Text-to-Python
generates explicit procedural workflows, requiring the
model to handle computational reasoning directly.


creasing attention (Luo et al., 2025; Jiang et al.,
2024). Python code generation offers greater flexibility than SQL but operates without the schemaenforced constraints of databases, which can compromise semantic precision.


Benchmarks have advanced to reflect realworld challenges. Early datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al.,
2018) emphasize schema complexity, whereas
BIRD (Li et al., 2024b) incorporates large-scale
databases and external knowledge to better represent enterprise-level scenarios. However, evaluating reasoning capabilities remains problematic:
execution-based metrics may penalize semantically
valid outputs when natural language inputs are ambiguous (Pourreza and Rafiei, 2023), indicating
that performance limitations often stem from underspecified user intent rather than syntactic inaccuracies.


Contextual ambiguity significantly impacts
model performance. Retrieving excessive schema
elements introduces noise that degrades accuracy
(Cao et al., 2024; Maamari et al., 2024), while insufficient constraints can lead to invalid logic during
error correction attempts (B et al., 2024). More recent benchmarks like Spider 2.0 (Lei et al., 2024b)
and BIRD-INTERACT (Huo et al., 2025) simulate
multi-turn interactions but conflate information retrieval with reasoning processes. In contrast, our
approach explicitly separates these components.
By aligning SQL and Python execution environments and employing our proposed Logic Completion Framework under controlled conditions, we
isolate procedural reasoning from confounding factors such as implicit assumptions and underspecified context.



**3** **Task Formalization and Experimental**
**Framework**


To systematically evaluate whether Text-to-Python
can effectively replace Text-to-SQL, we establish a
unified experimental framework. This framework
addresses the structural differences between the
two programming languages by introducing a standardized task formulation, a corrected dataset to
ensure reliability, and consistent evaluation criteria.


**3.1** **Problem Formulation**


We unify Text-to-SQL and Text-to-Python as executable program generation tasks (Figure 2). Given
a query _Q_ and data environment _D_, the model synthesizes a program _P_ such that _Exec_ ( _P, D_ ) _→_
_A_ . In Text-to-SQL, _Ddb_ is a relational database
schema _S_, and the output _Psql_ is a declarative
query. Conversely, in Text-to-Python, _D_ represents
in-memory dataframes _Ddf_, and the output _Pcode_ is
an imperative script requiring explicit procedural
logic.


**3.2** **Benchmark Construction**


**3.2.1** **Dataset Correction**


Our study employs the development set of the
BIRD benchmark (Li et al., 2024b), comprising
**1,534** query-SQL pairs. Each instance is a triplet
( _Q, K, Ysql_ ) consisting of the natural language
question, external knowledge, and the ground truth
SQL query.
However, our preliminary error analysis revealed
that evaluation metrics were distorted by annotation noise, where valid model outputs were unfairly
penalized due to inaccuracies in the reference SQL.
To establish a reliable evaluation foundation for
our comparative study, we build upon recent advances in leveraging LLMs for dataset curation
(Li et al., 2024a) and the double-blind annotation
protocol established in (Li et al., 2024b). We implemented a two-stage purification pipeline that integrates Model-in-the-Loop verification with expert
review. First, 1,193 consistent queries were automatically verified through consensus among three
state-of-the-art LLMs (Qwen3-238B, DeepSeekR1, and Qwen-Max). The remaining divergent
cases underwent double-blind expert review, resulting in 259 corrections for logical inconsistencies
or outdated values and the confirmation of 82 original annotations as valid. This rigorous process effectively distinguishes genuine model errors from
dataset artifacts.



3


**Structure & Schema**


**Data Values &**

**Formats**


**Logic & Business**

**Rules**


**OptimizationOutput &** ~~**29 (11.2%)**~~



~~**57 (22%)**~~



~~**92 (35.5%)**~~


~~**81 (31.3%)**~~


**Ratio (%)**

**35**

**30**

**25**

~~**20**~~

**15**



**0** **30** **60** **90**
**Number of Correction Cases**


Figure 3: Distribution of difference categories for verified SQL queries.



**Data Types &**

**Formats**


**Structure &**
**Redundancy**


**Logic & Constraints**



~~**40 (10.8%)**~~



~~**177 (47.8%**~~


~~**120 (32.4%)**~~


**Ratio (%)**


**40**

**30**

**20**

**10**



**Sorting** ~~**33 (8.9%)**~~



**0** **50** **100** **150** **200**
**Number of Correction Cases**


Figure 4: Distribution of difference categories in SQLto-Python conversion.


We categorized corrections into four dimensions,
as illustrated in Figure 3. First, we addressed _Struc-_
_ture & Schema_ errors by correcting column hallucinations and missing joins, ensuring the SQL
structure faithfully reflects the entity relationships
described in natural language. Second, we refined
_Data Values & Formats_ by adjusting filter conditions to match actual database values and explicitly handling NULLs, thereby resolving logic gaps
caused by dirty data. Third, we performed semantic
rectification on _Logic & Business Rules_, restructuring nested queries and set operations to resolve
scope confusion between clauses. Finally, we standardized _Formatting & Constraints_ by adding missing ORDER BY and LIMIT clauses, eliminating
non-deterministic outputs to ensure reproducible
evaluation.


**3.2.2** **SQL-to-Python Conversion**


To address the execution paradigm mismatch, we
reconstructed SQL logic into Python by explicitly implementing the database’s implicit behaviors
(e.g., null handling and sort stability). This ensures
that the Python ground truth faithfully replicates



the SQL execution outcomes within a procedural
environment.
**Code Reconstruction** To establish a valid ground
truth for the Text-to-Python task, we reconstructed
the SQL logic using Python, ensuring the code
explicitly replicated the DBMS-level operations.
We employed a similar verification pipeline: after
generating code candidates via three SOTA LLMs,
we filtered the results to ensure consistency with
SQL.
**Manual Refinement** We identified 370 instances
requiring manual intervention, which were processed via a double-blind review (see Figure 4).
The primary divergence involves _Data Types &_
_Formats_ ; we implemented strict type safety and
explicit null semantics to bridge the gap between
SQL’s permissible implicit casting and Python’s
strict type safety. Additionally, we addressed _Struc-_
_ture & Redundancy_ by removing redundant structures such as unnecessary joins—often ignored by
SQL optimizers but computationally significant in
Python. This phase also involved _Logic & Con-_
_straints_ refactoring to convert ambiguous assumptions or specific value filters into complete procedural steps that accurately reflect the intent of the
user. Finally, we implemented _Sorting_ by defining
consistent ordering rules for entries with identical
values, to ensure the stability of the output results.


**3.2.3** **LLM-based Evaluation**


We built an LLM-based evaluation method to evaluate the results of the model execution. Standard
metrics such as Execution Accuracy (EX) rely on
exact matches between execution outcomes. This
approach may produce false negatives in Text-toPython tasks, where semantically correct outputs
are incorrectly classified as erroneous due to superficial discrepancies, such as differences in column
ordering or data type representation.
**Semantic Validator** To address this, we implemented an LLM-based semantic validator ( _Vsem_ )
for both SQL and Code contexts. Following the
LLM-as-a-Judge methodology (Zheng et al., 2023;
Kim et al., 2024; Huo et al., 2025), _Vsem_ determines if the predicted execution result is semantically equivalent to the ground truth, regardless of
format.
**Human Verification** To validate the reliability of
_Vsem_, we conducted a human audit on a stratified
sample of _N_ = 600 instances. Three domain
experts independently assessed each predictionground truth pair, with final judgments based on



4


**Input context**



**No-Leakage Barrier**



**C**


**Augmented**
**Knowledge (K** **[’]** **)**



**Question**

**(Q)**



**Input**


**Schema**

**(S)**


**Question**

**(Q)**


**External**
**Knowledge**

**(K)**



**Oracle LLM**


**Logical**
**Translation**



**Schema**

**(S)**



**External**
**Knowledge**

**(K)**



**Schema**

**(S)**



**Question**

**(Q)**



**Target LLM**

**(Subject)**


**try {**
**SELECT ***
**FROM table**
**WHERE rate =X/Y;**
**}**



**{**
**Classification:**
**Calculation &**
**Aggregation Logic,**
**Answer: Rate is**
**defined as…**
**}**


**Natural Language**

**Constraints (C)**



**Target LLM**

**(Subject)**


**Is "eligible rate"calculated**

**as X/Y or Z?**


**Logical Inquiry (I)**
**Phase 1: Logic Probing**
**(Ambiguity ldentification)**



**Gold Code/SQL**
**(Ground Truth)**



**Predicted Code/SQL (y)**
**Phase 2: Ground Truth Injection** **Phase 3: Execution**
**(ldealized Context Construction)** **(Logic Chain Verification)**



**Phase 3: Execution**
**(Logic Chain Verification)**



Figure 5: The Logic Completion Framework (LCF). Standard semantic parsing models the probability of a program
_P_ as _P_ ( _P_ _|Q, S, K_ ). LCF makes latent domain knowledge explicit by providing logic clarifications ( _Clogic_ ), refining
the task to _P_ ( _P_ _|Q, S, K ∪_ _Clogic_ ).



consensus. The manual review confirmed that the
automated evaluation results are consistent with expert judgments. This demonstrates that our framework accurately assesses reasoning capability regardless of output format.


**4** **Logic Completion Framework**


To address ambiguity in natural language queries,
we introduce LCF. Unlike traditional models that
generate code directly, LCF first clarifies intent
and then generates code. It identifies ambiguous
or incomplete requirements and interacts with a
domain expert to resolve them. This ensures the
final code is based on a complete and logically
sound specification.


**4.1** **Model Role Design**


The _Subject_ (the evaluated model) is responsible
for detecting ambiguity (Phase 1) and generating
code (Phase 3). Crucially, the Subject generates
inquiries independently to test its intrinsic ambiguity detection, rather than relying on a proxy. The
_Oracle_, implemented by Qwen3-max, simulates a
domain expert. It accesses the ground truth to provide accurate natural language hints ( _Clogic_ ) that
semantically align with the ground truth of SQL
( _H_ ( _Clogic_ ) _≈_ _H_ ( _Ygold_ )).


**4.2** **Dialogue Paradigm**


The interaction follows a three-phase paradigm
(Figure 5).



**Phase 1: Logic Probing.** The Subject ( _Msubject_ )
analyzes inputs ( _S, K, Q_ ) to identify ambiguity. Instead of immediate code generation, it formulates a
clarifying question ( _Qambiguity_ ) targeting unclear
definitions or logic gaps.


**Phase 2: Ground Truth Injection.** The Oracle ( _Moracle_ ) acts as an expert analyst. Using the
inquiry ( _Qambiguity_ ) and ground truth ( _Ygold_ ), it
translates the gold logic into natural language hints
( _Clogic_ ). This ensures task requirements are fully
specified without leaking code.


**Phase 3: Execution.** The Subject generates the
final program ( _Ppred_ ) conditioned on the augmented context: _S, Q, K, Clogic_ . This isolates errors caused by reasoning deficiencies from those
caused by information deficits. Appendix E details
the prompts.
We utilized this framework to construct a dataset
that addresses information deficits. Rather than relying on human assumptions, we identified missing
specifications directly from the model’s inquiries in
Phase 1. By incorporating the Oracle’s responses,
we generated an extra dataset that provides the specific context required for the model’s reasoning.


**5** **Experiments**


**5.1** **Experimental Setup**


We benchmark Text-to-Python paradigm against
the traditional Text-to-SQL approach, employing
the **LLM-based Execution Accuracy (EX)** as the



5


primary evaluation metric. Following the standard
evaluation protocol of the BIRD benchmark (Li
et al., 2024b), our input prompts incorporate the
database schema, the natural language question,
and external knowledge. Specifically for the Textto-Python setting, we impose additional constraints
to ensure the generated code is executable and capable of returning valid results. The detailed prompts
are presented in Appendix A. We conduct comparative evaluations on both the original BIRD dataset
and our refined BIRD-Python dataset to assess the
impact of our corrections.


**5.2** **Baselines**


To ensure a comprehensive evaluation, we select a diverse range of open-source and closedsource LLMs. All open-source models are deployed locally on a server equipped with six
NVIDIA A6000 GPUs. We utilize default inference parameters, with a temperature of 0 _._ 7
and a top- _p_ of 0 _._ 95. Our baseline selection includes: (1) **StarCoder2** (Lozhkov et al., 2024)
(7B), **AutoCoder** (Lei et al., 2024a) (7B), and
**TableGPT2** (Su et al., 2024) (7B); (2) The extensive **Qwen** family, covering the **Qwen-Coder-2.5**
series (Hui et al., 2024), the **Qwen3** series (Yang
et al., 2025) (including both reasoning and coder
variants), and the latest closed-source **Qwen3-**
**Max** ; and (3) The reasoning-oriented **DeepSeek-**
**R1** (Guo et al., 2025).


**5.3** **Main Results**


Table 1 presents the performance of baseline models. First, we observe a performance gap between the two paradigms. For smaller models,
Python generation accuracy is lower than SQL;
for instance, Qwen2.5-Coder (7B) decreases from
52 _._ 93% on SQL to 35 _._ 95% on Python. This performance drop empirically validates our hypothesis
that the requirement for explicit procedural definition in Python creates a higher reasoning barrier for
smaller models compared to the declarative nature
of SQL.
However, this gap decreases with larger or
reasoning-enhanced models. The Qwen3 series
and DeepSeek-R1 show smaller performance drops
between SQL and Python tasks. Qwen3-Max
achieves 63 _._ 43% accuracy in Python generation,
slightly higher than DeepSeek-R1 (62 _._ 52%). In
contrast, models specialized for specific tasks
show lower transferability. While AutoCoder and
TableGPT2 achieve scores comparable to other



**SQL** **Code**


Figure 6: Four Dimensions of Semantic Mismatch causing Execution Errors.


baselines on SQL, their performance on Python
tasks is lower (5 _._ 45% and 19 _._ 82%, respectively),
suggesting that SQL-specific or general codecompletion training may not directly translate to
the Pandas-based data analysis task.
Finally, comparing the Verified and Original settings indicates the impact of data quality.
Models such as Qwen3-Max and DeepSeek-R1
show higher accuracy on the Verified benchmark
(+8 _._ 15% and +6 _._ 78%, respectively). This increase suggests that the original dataset contained
noise that affected evaluation scores, while the verified set provides a more accurate assessment of
model outputs.


**5.3.1** **Error Analyze**


We analyze execution failures using the error categories defined in our framework. Figure 6 shows
two trends: shared difficulties in natural language
understanding and paradigm-specific error patterns.
Misunderstanding the question’s intent constitutes the primary source of error. We observe that
_Filter Condition Errors_ are the most prevalent in
both SQL (32.7%) and Code (31.7%). This consistency indicates that natural language understanding
plays a critical role in both paradigms. Models frequently misinterpret ambiguous user intents—such
as vague temporal references or implicit numerical
thresholds—translating them into incorrect logical
predicates. This implies that, in the absence of
explicit clarification, models in both settings tend
to resolve linguistic ambiguity incorrectly by mapping it onto schema elements.
Beyond this common challenge, error types differ by paradigm. In SQL, errors frequently involve _Row Selection_ (21.2%) and _Column Selec-_
_tion_ (16.6%), often related to clauses like LIMIT



6


**Model** **Reasoning** **Verified** **Code Generation Accuracy (%)** **SQL Generation Accuracy (%)**
**Simple** **Mod.** **Hard** **Total** **Simple** **Mod.** **Hard** **Total**
StarCoder2  - ✓ 1.95 0.86 0.00 1.12 0.97 1.08 0.00 0.91
(7B)  - 1.62 0.43 0.00 0.85 0.65 0.86 0.69 0.72
AutoCoder  - ✓ 8.65 4.31 2.07 5.45 13.84 3.88 7.59 10.23
(7B)  - 7.50 3.88 1.72 4.88 12.54 3.23 6.90 9.19
TableGPT2  - ✓ 25.46 17.24 13.10 19.82 40.97 25.00 18.62 34.03
(7B)  - 23.15 15.09 11.03 17.45 39.68 22.84 18.62 32.59
Qwen2.5-Coder  - ✓ 41.91 26.51 28.28 35.95 61.41 42.67 31.72 52.93
(7B)  - 39.89 18.97 23.45 31.94 60.65 41.16 31.03 51.96
Qwen3 ✓ ✓ 59.46 45.04 39.31 53.19 68.98 52.92 38.19 61.22
(7B)  - 54.70 36.21 34.48 47.20 62.15 46.00 32.64 54.48
Qwen3 ✓ ✓ 54.70 37.93 37.24 47.98 65.84 48.28 42.76 58.34
(14B)  - 65.84 32.33 29.66 42.89 59.46 44.18 39.31 52.93
Qwen2.5-Coder  - ✓ 60.76 45.69 40.69 54.30 63.78 51.29 50.34 58.74
(14B)  - 51.24 33.84 35.86 44.52 63.24 46.55 44.14 56.39
Qwen2.5-Coder  - ✓ 62.16 49.78 42.76 56.58 66.70 50.65 41.38 59.45
(32B)  - 58.38 41.81 37.93 51.43 64.11 47.84 39.31 56.84
Qwen3-Coder  - ✓ 49.51 41.59 35.86 45.83 67.35 55.82 43.45 61.60
(30B)  - 46.92 34.05 32.41 41.53 64.11 51.29 43.54 58.28
Qwen3 ✓ ✓ 65.33 51.94 54.48 60.10 69.08 54.09 40.00 61.80
(32B)  - 60.22 42.24 45.52 53.32 61.84 47.84 35.17 55.08
Qwen-Max  - ✓ 65.73 53.66 52.41 60.82 68.32 51.72 40.00 59.20

        - 59.57 45.04 42.76 53.59 63.14 45.47 34.48 55.08
Qwen3-Coder-Plus  - ✓ 65.95 55.82 53.10 61.60 65.30 **56.25** 50.34 61.15

        - 60.32 45.26 42.07 53.98 62.92 53.45 50.34 58.87
DeepSeek-R1 ✓ ✓ **69.95** 53.02 45.52 62.52 67.54 53.48 **57.93** 62.32

        - 62.38 45.04 48.28 55.74 62.81 44.40 40.00 55.08
Qwen3-Max  - ✓ 67.89 **59.91** 46.21 **63.43** **69.41** 55.82 46.21 **63.10**

        - 62.27 43.75 47.59 55.28 65.51 52.59 52.41 60.37


Table 1: Experimental results on the BIRD-Python benchmark (Python) and original BIRD benchmark (SQL). We
report Execution Accuracy (EX) across Simple, Moderate (Mod.), and Challenging (Hard) subsets. "✓" denotes our
verified dataset; "�" denotes the original BIRD dev set.



or DISTINCT. In contrast, Python generation exhibits a higher rate of _Logic Errors_ (17.5%) compared to SQL (0.3%). This reflects the difference
in execution: SQL engines manage execution plans
implicitly, whereas Python requires explicit implementation of data manipulation steps, increasing
the likelihood of procedural errors.
These results illustrate the tension between underspecified user intent and the precision required
for program execution. While SQL abstracts away
procedural details, Python’s requirement for explicit logic definition tends to increase the risk of
hallucinated specifications. This suggests that current evaluations may conflate ambiguity resolution
with code generation capability, highlighting the
need to distinguish missing specifications from logical reasoning ability.


**5.4** **LCF Test**


To separate reasoning capability from information deficits, we apply the LCF to the verified



**SQL** **Code**


Figure 7: Distribution of inquiry categories for SQL and
Code generation across Qwen3 model scales.


BIRD-Python dataset. We select the Qwen3 family
(7B, 32B, Max), which shows strong performance
across scales, to evaluate gains when latent specifications are resolved. LCF enriches queries with
explicit specifications (see Section E), isolating execution failures caused by underspecified requirements.



**Structural &**
**Scope Ambiguity**


**Join &**
**Relationship**

**Logic**


**Domain Concept**

**Definition**



**Calculation &**

**Aggregation**

**Logic**



**Constraint &**

**Boundary**
**Specification**



**Structural &**
**Scope Ambiguity**



**Join &**
**Relationship**



**Output**
**Structure**
**Requirements**



**Logic**



**Domain Concept**

**Definition**



**Calculation &**

**Aggregation**

**Logic**


**Output**
**Structure**
**Requirements**


**Constraint &**

**Boundary**
**Specification**



7


**Code Generation Accuracy (%)** **SQL Generation Accuracy (%)**
**Model** **Setting**
**Simple** **Mod.** **Hard** **Total** **Simple** **Mod.** **Hard** **Total**
**Qwen3-7B** LCF **79.46** **61.64** **48.97** **71.19** **80.65** **60.99** **45.52** **71.38**
(7B) Standard 59.46 45.04 39.31 53.19 68.98 52.92 38.19 61.22
**Qwen3-32B** LCF **78.16** **63.15** **66.21** **72.49** **80.54** **62.93** **54.48** **72.75**
(32B) Standard 65.33 51.94 54.48 60.10 69.08 54.09 40.00 61.80
**Qwen3-Max** LCF **77.84** **66.16** **66.90** **73.21** **85.95** **72.63** **66.21** **80.05**
( _∼_ 1T) Standard 67.89 59.91 46.21 63.43 69.41 55.82 46.21 63.10


Table 2: Ablation study of the LCF. We evaluate performance on the sanitized BIRD-Python dataset in two settings:
"Standard" (baseline without LCF) and "LCF" (with logical clarifications). The highest score for each model is
bolded.



**5.4.1** **Ablation Study Results**


Table 2 compares model performance under Standard and LCF settings. Resolving ambiguity improves accuracy across all models. For Qwen3-7B,
performance increases in both Code (53 _._ 19% _→_
71 _._ 19%) and SQL (61 _._ 22% _→_ 71 _._ 38%). This suggests that incomplete inputs contribute to performance limitations in the Standard setting.

Additionally, the performance gap between
SQL and Code generation decreases under wellspecified clarifications. With LCF, Qwen3-32B
achieves similar accuracy in Code (72 _._ 49%) and
SQL (72 _._ 75%), indicating that Python-based generation performs comparably to SQL when semantic
specifications are clear.

Finally, LCF clarifies the impact of model scaling. In the Standard setting, ambiguity appears
to obscure performance differences across model
sizes. Under LCF, larger models show clearer advantages: Qwen3-Max outperforms Qwen3-7B by
approximately 9 percentage points in SQL (80 _._ 05%
vs. 71 _._ 38%). This demonstrates that larger models
more effectively leverage explicit specifications.


**5.4.2** **Analysis of Clarification Inquiries**


We categorize the clarification inquiries from Phase
1 to identify sources of ambiguity (Figure 7). The
distribution of inquiry types remains consistent
across model sizes. "Constraint & Boundary Specification" is the dominant category, whereas inquiries regarding "Output Structure Requirements"
are minimal. This suggests that models generally
handle syntactic requirements effectively but have
difficulty resolving ambiguous business logic without explicit context. Notably, this pattern persists
across scales: larger models like Qwen3-Max identify logical gaps more precisely rather than attempting implicit resolution. This suggests that model
scaling alone may not fully resolve domain-specific
ambiguity.



**6** **Conclusion**


This paper evaluates the Text-to-Python paradigm
as a viable and flexible alternative to Text-to-SQL
for data interaction. Using the BIRD-Python benchmark, we demonstrate that Python can effectively
replicate SQL’s core data retrieval capabilities in
file-based environments. However, the transition
from SQL’s declarative syntax to Python’s procedural formulation increases sensitivity to underspecified user intent. While Python enables greater
flexibility for advanced analytics, this benefit depends on precise specification of execution logic.
Through application of the LCF, we identify the
primary performance bottleneck not in code generation itself, but in gaps in latent domain knowledge. Therefore, Text-to-Python can serve as a robust alternative to Text-to-SQL—provided that systems are capable of translating ambiguous natural
language inputs into logically precise, executable
code.


**Limitations**


Although BIRD-Python and LCF provide a controlled framework for cross-paradigm evaluation,
limitations exist. First, we provided explicit DDL
schemas to Text-to-Python models to ensure fair
comparison. While this isolates procedural reasoning, it may yield higher performance than scenarios
involving raw, schema-less files.
Second, LCF utilizes an Oracle with access to
ground truth. While useful for diagnosis, this reliance assumes high-quality annotations; practical
deployments would likely depend on human feedback, introducing latency not modeled here.
Third, the evaluation focuses on logical complexity using clean data. Consequently, it does not
fully address issues related to data quality, such as
type inference noise common in production. Future
work could examine the interaction between data
quality and intent ambiguity.



8


**References**


Zhang B, Ye Y, Du G, and 1 others. 2024. Benchmarking the text-to-sql capability of large language
models: A comprehensive evaluation. _Computing_
_Research Repository_, arXiv:2403.02951.


Z Cao, Y Zheng, Z Fan, and 1 others. 2024. Rsl-sql: Robust schema linking in text-to-sql generation. _arXiv_
_preprint arXiv:2411.00073_ .


Saumya Chaturvedi, Aman Chadha, and Laurent Bindschaedler. 2025. Sql-of-thought: Multi-agentic textto-sql with guided error correction. _arXiv preprint_
_arXiv:2509.00581_ .


Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, and 1 others. 2021. Evaluating large language models trained on code. _Computing Research_
_Repository_, arXiv:2107.03374.


Shihan Dou and 1 others. 2024. What’s wrong with
your code generated by large language models? an
extensive study. _arXiv preprint arXiv:2407.06153_ .


Alex Egg and 1 others. 2025. Dabstep: Data agent
benchmark for multi-step reasoning. _arXiv preprint_
_arXiv:2506.23719_ .


Zhouhong Gu, Haoning Ye, Xinyu Chen, and 1 others. 2025. Structext-eval: Evaluating large language
model’s reasoning ability in structure-rich text. In
_Proceedings of the 63rd Annual Meeting of the As-_
_sociation for Computational Linguistics (Volume 1:_
_Long Papers)_, pages 223–244.


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Zhang,
Kai Chen, Kushuai Zhang, Qihao Liang, Ruisong
Xu, Yuwang Xu, Xingkai Yu, and 1 others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning. _arXiv preprint_
_arXiv:2501.12948_ .


C Huang, Y Deng, W Lei, and 1 others. 2025. How
to enable effective cooperation between humans and
nlp models: A survey of principles, formalizations,
and beyond. In _Proceedings of the 63rd Annual Meet-_
_ing of the Association for Computational Linguistics_
_(Volume 1: Long Papers)_, pages 466–488.


Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang,
Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun
Zhang, Bowen Yu, Kai Lu, and 1 others. 2024.
Qwen2.5-coder technical report. _arXiv preprint_
_arXiv:2409.12186_ .


Nan Huo, Xiaohan Xu, Jinyang Li, Per Jacobsson,
Shipei Lin, Bowen Qin, Binyuan Hui, Xiaolong Li,
Ge Qu, Shuzheng Si, and 1 others. 2025. Birdinteract: Re-imagining text-to-sql evaluation for large
language models via lens of dynamic interactions.
_arXiv preprint arXiv:2510.05318_ .



Xue Jiang and 1 others. 2024. Self-planning code generation with large language models. _ACM Transactions_
_on Software Engineering and Methodology_, 33(7):1–
30.


Liqiang Jing and 1 others. 2024. Dsbench: How far
are data science agents from becoming data science
experts? _arXiv preprint arXiv:2409.07703_ .


H Kim, T Jeon, S Choi, and 1 others. 2024.
Flex: Expert-level false-less execution metric for
reliable text-to-sql benchmark. _arXiv preprint_
_arXiv:2409.19014_ .


Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang,
Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu,
Hangyu Mao, Ziyue Li, and 1 others. 2023. Tptuv2: Boosting task planning and tool usage of large
language model-based agents in real-world systems.
_Computing Research Repository_, arXiv:2311.11315.


Bo Lei, Yong Li, and Qian Chen. 2024a. Autocoder:
Enhancing code large language model withAIEVINSTRUCT. _arXiv preprint arXiv:2405.14906_ .


Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng
Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo,
Hongcheng Gao, Wenjing Hu, Pengcheng Yin, and 1
others. 2024b. Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows.
_arXiv preprint arXiv:2411.07763_ .


C Li, Y Wang, Z Wu, and 1 others. 2024a. Multisql: A
schema-integrated context-dependent text2sql dataset
with diverse sql operations. In _Findings of the As-_
_sociation for Computational Linguistics ACL 2024_,
pages 13857–13867.


Jia Li and 1 others. 2025. Structured chain-of-thought
prompting for code generation. _ACM Transactions_
_on Software Engineering and Methodology_, 34(2):1–
23.


Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li,
Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,
Nan Huo, and 1 others. 2024b. Can llm already
serve as a database interface? a big bench for largescale database grounded text-to-sqls. In _Advances in_
_Neural Information Processing Systems_, volume 36.


Fang Liu and 1 others. 2024. Exploring and evaluating
hallucinations in llm-powered code generation. _arXiv_
_preprint arXiv:2404.00971_ .


Anton Lozhkov, Raymond Li, Loubna Ben Allal, Qin
Zi, Niklas Muennighoff, Leandro von Werra, Thomas
de Vries, Suraj Kosaraju, Xiangru Liu, Harm de
Petronio, and 1 others. 2024. Starcoder 2 and
the stack v2: The next generation. _arXiv preprint_
_arXiv:2402.19173_ .


Ziyang Luo, K Li, H Lin, and 1 others. 2025. Treeof-evolution: Tree-structured instruction evolution
for code generation in large language models. In
_Proceedings of the 63rd Annual Meeting of the As-_
_sociation for Computational Linguistics (Volume 1:_
_Long Papers)_, pages 297–316.



9


K Maamari, F Abubaker, D Jaroslawicz, and 1 others.
2024. The death of schema linking? text-to-sql in
the age of well-reasoned language models. _arXiv_
_preprint arXiv:2408.07702_ .


Shuyin Ouyang and 1 others. 2025. An empirical study
of the non-determinism of chatgpt in code generation. _ACM Transactions on Software Engineering_
_and Methodology_, 34(2):1–28.


Mohammadreza Pourreza and Davood Rafiei. 2023.
Evaluating cross-domain text-to-sql models and
benchmarks. _Computing Research Repository_,
arXiv:2310.18538.


Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang,
Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao,
Jian Sun, Luo Si, and 1 others. 2022. A survey
on text-to-sql parsing: Concepts, methods, and future directions. _Computing Research Repository_,
arXiv:2208.13629.


Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu,
Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu
Mao, Xingyu Zeng, and Rui Zhao. 2023. Tptu: Task
planning and tool usage of large language modelbased ai agents. _Computing Research Repository_,
arXiv:2308.03427.


A H Safdarian, M Mohammadi, E Jahanbakhsh, and
1 others. 2025. Schemagraphsql: Efficient schema
linking with pathfinding graph algorithms for textto-sql on large-scale databases. _arXiv preprint_
_arXiv:2505.18363_ .


Aochuan Su, An Wang, Chen Ye, Dong Liu, Guochao
Li, Haiyang Zhang, Heng Chen, Huajun Xu, Jiawei
Zhu, Jun Yin, and 1 others. 2024. Tablegpt2: A
large multimodal model with tabular data integration.
_arXiv preprint arXiv:2411.02059_ .


Guanghu Sui, Zhishuai Li, Ziyue Li, Sun Yang, Jingqing
Ruan, Hangyu Mao, and Rui Zhao. 2023. Reboost large language model-based text-to-sql, text-topython, and text-to-function–with real applications
in traffic domain. _Computing Research Repository_,
arXiv:2310.18752.


Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang,
Jiaqi Bai, Qian-Wen Zhang, Zhao Yan, and Zhoujun
Li. 2023. Mac-sql: Multi-agent collaboration for
text-to-sql. volume arXiv:2312.11242.


Aiyuan Yang, An Li, Bo Yang, Binyuan Chen,
Chen Zhang, Chen Li, Cheng Wang, Chengming
Chen, Chuyu Chen, Damai Ding, and 1 others.
2025. Qwen3 technical report. _arXiv preprint_
_arXiv:2505.09388_ .


B Yu, Y Zhu, P He, and 1 others. 2025. Utboost: Rigorous evaluation of coding agents on swe-bench. _arXiv_
_preprint arXiv:2506.09289_ .


Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, and 1 others. 2018. Spider: A large-scale human-labeled dataset for complex



and cross-domain semantic parsing and text-to-sql
task. In _Computing Research Repository_, volume
arXiv:1809.08887.


Qinggang Zhang and 1 others. 2024. Structure guided
large language model for sql generation. _arXiv_
_preprint arXiv:2402.13284_ .


L Zheng, W L Chiang, Y Sheng, and 1 others. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. In _Advances in Neural Information Processing_
_Systems_, volume 36, pages 46595–46623.


Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries from
natural language using reinforcement learning. _Com-_
_puting Research Repository_, arXiv:1709.00103.


R Zhou, W Hua, L Pan, and 1 others. 2025. Rulearena:
A benchmark for rule-guided reasoning with llms
in real-world scenarios. In _Proceedings of the 63rd_
_Annual Meeting of the Association for Computational_
_Linguistics (Volume 1: Long Papers)_, pages 550–572.


Yuqi Zhu and 1 others. 2025. Uncertainty-guided chainof-thought for code generation with llms. _arXiv_
_preprint arXiv:2503.15341_ .


Terry Yue Zhuo and 1 others. 2024. Bigcodebench:
Benchmarking code generation with diverse function calls and complex instructions. _arXiv preprint_
_arXiv:2406.15877_ .


**A** **Baseline Prompt Specifications**


This appendix provides the prompt templates for
the baseline Text-to-SQL and Text-to-Python generation tasks (Phase 3 in the LCF framework utilizes the Text-to-Python template). Both paradigms
receive identical schema information (Li et al.,
2024b).


**A.1** **Text-to-SQL Prompt**


We follow the standard BIRD benchmark protocol.
The system instruction and prompt template are
defined in Table 3 and Table 4, respectively.


**Text-to-SQL System Instruction**


You are a SQL assistant. Only return the SQL query without any explanation.


Table 3: System instruction for Text-to-SQL generation.


**A.2** **Text-to-Python Prompt**


To evaluate procedural reasoning, we require a
Pandas-based generation format. The prompt
excludes SQL syntax and requires the use of
pd.read_csv to simulate file-based workflows.
The system instruction and prompt template are
provided in Table 5 and Table 6.



10


**Text-to-SQL Prompt Template**


/* [Schema Info: DDL] */
CREATE TABLE ‘table_name‘ (...);
/*
...


 - External Knowledge: {knowledge}

 - Using valid SQLite and understanding External Knowledge,

 - answer the following questions for the tables provided
above.

 - {question}
SELECT


Table 4: Complete Text-to-SQL prompt template including schema information.


**Text-to-Python System Instruction**


You are an expert Python code generator specializing in
pandas data analysis.
Return runnable Python code only. No explanations or
markdown.
Strictly use ’import pandas as pd’ and
pd.read_csv(’<table>.csv’).
Do NOT use or mention
SQL/SELECT/JOIN/CREATE/WHERE/etc.
Do NOT define functions or classes (no ’def’, ’lambda’,
’class’).


Table 5: System instruction for Text-to-Python generation.


**Text-to-Python Prompt Template**


/* [Schema Info: Same as Text-to-SQL] */
...


# Task Description:
# Generate runnable pandas code only. No explanations,
# no markdown, no JSON.
# Requirements:
# 1) Use: import pandas as pd
# 2) Read tables strictly via pd.read_csv(’<table>.csv’)
# 3) Do NOT use or mention SQL/ SELECT/ JOIN/ CREATE/ WHERE/ etc.
# 4) Do NOT define functions or classes (no ’def’, ’lambda’,
’class’)
# 5) Prefer clear variable names; keep code executable endto-end
# 6) Use result to record the final result, and finally
# print(result) to print the final result.


# External Knowledge: {knowledge}
# Question: {question}
CODE


Table 6: Complete Text-to-Python prompt template with
pandas-specific requirements.


**B** **Rectification Case Studies**


We present case studies for the four error categories
defined in Section 3.2. These examples illustrate
logical inconsistencies in the original BIRD dataset



**Case Study: Structural Alignment & Join Semantics**


**Natural Language Query:**
Of the schools that offers a magnet program serving a
grade span of Kindergarten to 8th grade, how many offers
Multiple Provision Types? List the number of cities that
offers a Kindergarten to 8th grade span and indicate how
many schools are there serving such grade span for each
city.


**Original SQL:**
SELECT T2.City, COUNT(T2.CDSCode) FROM frpm AS
T1 INNER JOIN schools AS T2 ON T1.CDSCode =
T2.CDSCode WHERE T2.Magnet = 1 AND T2.GSoffered
= ’K-8’ AND T1.‘NSLP Provision Status‘ =
’Multiple Provision Types’ GROUP BY T2.City
_Defect:_ The use of INNER JOIN with the WHERE filter restricts the result set to cities containing schools with "Multiple Provision Types", excluding cities that satisfy the K-8
Magnet condition but lack this specific provision. Additionally, the query omits the calculation for the total school
count per city.


**Rectified SQL (Gold):**
SELECT T2.City, COUNT(DISTINCT CASE WHEN
T1.‘NSLP Provision Status‘ = ’Multiple
Provision Types’ THEN T2.CDSCode END),
COUNT(DISTINCT T2.CDSCode) FROM schools AS
T2 LEFT JOIN frpm AS T1 ON T2.CDSCode =
T1.CDSCode WHERE T2.GSoffered = ’K-8’ AND
T2.Magnet = 1 GROUP BY T2.City
_Correction:_ Uses LEFT JOIN to retain all cities meeting the
primary K-8 Magnet criteria. Applies conditional aggregation to calculate both the subset (Multiple Provision) and
the total school count, addressing the dual-quantification
requirement of the prompt.


Table 7: Comparison of Original and Rectified SQL for
a complex aggregation task. The rectification resolves
structural misalignment caused by incorrect filtering and
join selection.


and the corrections implemented for BIRD-Python.


**B.1** **Category 1: Schema Linking &**
**Structural Alignment**


The "Schema Linking & Structural Alignment"
category accounts for 35.5% of errors. It involves
cases where the SQL structure does not match the
logic required by the query, particularly regarding
join semantics (e.g., INNER vs. LEFT joins) and
aggregation scope.
Table 7 shows a representative instance. The
original annotation used an INNER JOIN combined
with a WHERE clause. This structure removed cities
that satisfied the primary condition (K-8 Magnet
schools) but lacked the secondary attribute (Multiple Provision Types), and failed to include the
"Total Schools" column. The rectified SQL uses
a LEFT JOIN and conditional aggregation (CASE
WHEN), retaining all relevant cities and calculating
the required statistics.



11


**Case Study: Data Consistency & Value Alignment**


**Natural Language Query:**
How many schools in merged Alameda have number of
test takers less than 100?


**Original SQL:**
SELECT COUNT(T1.CDSCode) FROM schools AS T1
INNER JOIN satscores AS T2 ON T1.CDSCode
= T2.cds WHERE T1.StatusType = ’Merged’ AND
T2.NumTstTakr < 100 AND T1.County = ’Lake’
_Defect:_ The query filters by ’Lake’ instead of ’Alameda’,
using a literal value that does not appear in the prompt.


**Rectified SQL (Gold):**
SELECT COUNT(T1.CDSCode) FROM schools AS T1
INNER JOIN satscores AS T2 ON T1.CDSCode
= T2.cds WHERE T1.StatusType = ’Merged’ AND
T2.NumTstTakr < 100 AND T1.County = ’Alameda’
_Correction:_ Maps the token "Alameda" to the corresponding database value ’Alameda’, ensuring the query filters
the correct data partition.


Table 8: Comparison of Original and Rectified SQL
demonstrating a Data Consistency error. The rectification resolves the mismatch between the user’s constraint
and the SQL literal.


**B.2** **Category 2: Data Consistency & Integrity**


The "Data Consistency & Integrity" category
(31.3%) involves cases where the SQL logic incorrectly maps natural language entities to database
values. Common issues include incorrect literals,
string matching errors, or format mismatches.
Table 8 presents an example of a value mismatch.
The user queries for "Alameda" county, but the
original SQL filters for "Lake", a value not present
in the query. This error likely stems from incorrect
entity mapping. The rectification process aligns the
SQL literal with the information provided in the
natural language hints.


**B.3** **Category 3: Semantic Logic & Business**
**Rules**


The "Semantic Logic & Business Rules" category
(22 _._ 0%) covers errors where the SQL implementation does not match the query’s business intent,
particularly regarding conditional logic and scope.
These errors often involve incorrect translation of
natural language quantifiers (e.g., "all", "only") into
SQL predicates like NOT EXISTS or HAVING.
Table 9 shows a "Quantification Scope" error.
The user queries for "single bond molecules," implying that _all_ bonds in the molecule must be of
the single type. However, the original SQL uses a
JOIN with a filter, which functions as an existential
check (selecting molecules with _at least one_ single
bond). This logic incorrectly includes molecules



**Case Study: Semantic Logic & Quantification Scope**


**Natural Language Query:**
Among the single bond molecule id, which molecules are
not carcinogenic?


**Original SQL:**
SELECT DISTINCT T1.molecule_id FROM bond AS T1
INNER JOIN molecule AS T2 ON T1.molecule_id
= T2.molecule_id WHERE T2.label = ’-’ AND
T1.bond_type = ’-’
_Defect:_ The query uses an existential filter (bond_type
= ’-’), selecting molecules with _at least one_ single bond
instead of those composed _exclusively_ of single bonds.


**Rectified SQL (Gold):**
SELECT T2.molecule_id FROM molecule T2 WHERE
T2.label = ’-’ AND NOT EXISTS (SELECT 1 FROM
bond WHERE molecule_id = T2.molecule_id AND
bond_type != ’-’)
_Correction:_ Uses NOT EXISTS to ensure no non-single
bonds exist for the molecule, matching the definition of a
"single bond molecule."


Table 9: Comparison of Original and Rectified SQL
illustrating a Semantic Logic error. The rectification
fixes the quantification scope from existential ("any") to
universal ("all") using negative existential logic.


with mixed bond types. The rectification uses NOT
EXISTS to exclude molecules containing non-single
bonds, satisfying the universal quantifier.


**B.4** **Category 4: Formatting & Operational**
**Constraints**


The "Formatting & Operational Constraints" category (11 _._ 2%) covers cases where the SQL logic is
correct but the output formatting does not match
requirements, such as column ordering or tiebreaking. These issues can lead to false negatives
in automated evaluation.

Table 10 shows a "Column Ordering" error. The
user requests the address components in the sequence: "Street, City, Zip and State". The original
SQL retrieves the correct fields but swaps the last
two, returning "State" before "Zip". Although semantically minor, this discrepancy affects evaluation accuracy. The rectification reorders the projection clause to match the requested sequence.


**C** **Structure-Agnostic Validator**
**Specifications**


Table 11 provides the specifications for the
Structure-Agnostic Validator ( _Vsem_ ). The prompt
is designed to verify semantic equivalence, ignoring format variations to focus on content accuracy.



12


**Case Study: Formatting & Output Determinism**


**Natural Language Query:**
What is the complete address of the school with the lowest
excellence rate? Indicate the Street, City, Zip and State.


**Original SQL:**
SELECT T2.Street, T2.City, T2.State, T2.Zip
FROM satscores AS T1 INNER JOIN schools AS T2 ON
T1.cds = T2.CDSCode ORDER BY CAST(T1.NumGE1500
AS REAL) / T1.NumTstTakr ASC LIMIT 1
_Defect:_ The query projects columns in the order (Street,
City, State, Zip), inconsistent with the user instruction
to present Zip before State.


**Rectified SQL (Gold):**
SELECT T2.Street, T2.City, T2.Zip, T2.State
FROM satscores AS T1 INNER JOIN schools AS T2 ON
T1.cds = T2.CDSCode ORDER BY CAST(T1.NumGE1500
AS REAL) / T1.NumTstTakr ASC LIMIT 1
_Correction:_ Reorders the SELECT clause to match the requested output format (Street, City, Zip, State).


Table 10: Comparison of Original and Rectified SQL
demonstrating a Formatting error. The rectification fixes
the column permutation to comply with the output order
requirement.


**Structure-Agnostic Validator Instruction**


**System Role:**
You are a professional output-equivalence validator. Decide whether Output 1 and Output 2 are semantically and
materially equivalent. Follow these rules strictly:


**Core Equivalence Rules:**
1. **Content Over Format:** Ignore data presentation (e.g.,
CSV vs. JSON, whitespace) and metadata (e.g., Pandas
Index). Focus solely on the actual content.
2. **Structural Invariance:** Flatten nested structures. A
scalar, a single-element list, and a 1xN array are equivalent
if they contain the same value (e.g., [’apple’] == ’apple’).
3. **Order Insensitivity:** Unless explicit sorting is required,
treat lists and table rows as multisets. Order does not
matter.
4. **Type Normalization:** Normalize numeric types (1.0 ==
1), booleans (True/yes/1), and date formats before comparison.
5. **Superset Validity:** If one output contains extra descriptive columns (labels) and the other is value-only, they are
equivalent if the value projection matches exactly.
6. **Numeric Tolerance:** Percentages and fractions are
equivalent (e.g., 0.227 == 22.7%) within standard rounding
tolerance.
7. **De-duplication:** If one output contains duplicates and
the other is unique, compare the sets of unique values.


**Evaluation Task:**
Output 1: {predicted_output}
Output 2: {ground_truth}
Respond with exactly one word: "Correct" if equivalent,
"Incorrect" otherwise.


Table 11: Prompt specifications for the StructureAgnostic Validator ( _Vsem_ ). The system applies normalization rules (summarized above) to distinguish semantic correctness from format variations.



**Type** **Content**


**Question** Name schools in Riverside where the average
math score is greater than 400, and what is the
funding type?


**Gold** SELECT T1.sname, T2.Funding FROM ...
**SQL** WHERE T2.District LIKE ’Riverside%’
**GROUP BY T1.sname, T2.Funding**
HAVING ... > 400


**Prediction** df_merged = pd.merge(df_sat, df_frpm,
**(Pandas)** ...)
# Implicitly drops NaNs in grouping keys
**grouped** **=** **df_merged.groupby([’sname’,**
**’Funding’])**
result = grouped[’AvgScrMath’].mean()


Table 12: A case study of semantic mismatch between
SQL and Pandas. While the SQL GROUP BY retains
NULL keys, the generated Pandas code uses the default
groupby which drops NaN values, reducing recall.


**D** **Detailed Case Studies of Semantic**
**Mismatch**


This appendix presents case studies supporting
the discrepancy taxonomy in the main text. We
analyze examples across three dimensions—Null
Semantics, Operational Determinism, and Granularity Mismatch—to demonstrate execution divergence. These cases illustrate the mechanisms
behind model failures and link the concept of
impedance mismatch to empirical results.


Table 12 illustrates the mismatch in null-value
handling. The SQL standard GROUP BY clause
preserves NULL keys, whereas Pandas groupby defaults to dropna=True, removing rows with NaN
keys. The model failed to set dropna=False, causing valid records to be discarded.


Table 13 demonstrates discrepancies caused by
nondeterministic sorting. With identical values at
the limit boundary ( _N_ = 5) and no secondary sort
key, SQL and Pandas rely on different default behaviors. This leads to inconsistencies in the result
set.


Table 14 highlights granularity mismatches
driven by constraint inference. The term "district"
led the model to apply a filter (rtype=’D’), assuming district-level aggregation. However, the ground
truth SQL selects the global maximum without this
constraint. Since the highest score belonged to a
school-level entry, the model’s overly restrictive
logic failed to retrieve the correct result.



13


**Type** **Content**


**Question** Which cities have the top 5 lowest enrollment
number for students in grades 1 through 12?


**Gold** SELECT T2.City FROM ...
**SQL** **ORDER BY SUM(T1.‘Enrollment‘) ASC LIMIT**
**5**
_Execution Result (Top 5):_

[’Coulterville’, ’Pinecrest’, ’Shaver Lake’, **’Emi-**
**grant Gap’, ’Hyampom’** ]



**Type** **Content**


**Question** Which active **district** has the highest average
score in Reading?


**Gold** SELECT T1.District FROM schools ...
**SQL** WHERE T1.StatusType = ’Active’
**ORDER BY T2.AvgScrRead DESC LIMIT 1**
_Result:_ **Palo Alto Unified** (Score: 642.0, Type:

S)


**Prediction** # Explicitly filtering for
**(Pandas)** District-level records
**df_scores** **=** **df_sat[df_sat[’rtype’]** **==**
**’D’]**
df_merged = pd.merge(df_schools,
df_scores, ...)
_Result:_ **Santa Cruz County Office** (Score:

638.0, Type: D)



**Prediction** df_merged = pd.merge(...)
**(Pandas)** **df_sorted** **=**
**df_merged.sort_values(’Enrollment’).head(5)**
_Execution Result (Top 5):_

[’Coulterville’, ’Pinecrest’, ’Shaver Lake’,
**’Hyampom’, ’Woody’** ]
_(Note:_ _’Emigrant Gap’ is excluded due to_
_tie-breaking)_



Table 13: Example of Nondeterministic Sort Stability.
Both SQL and Pandas correctly sorted by enrollment,
but the absence of a secondary sort key caused divergent
results at the cutoff point ( _N_ = 5) for tied values.


**E** **Logic Completion Framework (LCF)**
**Specifications**


This section details the experimental configuration
for the LCF roles and interaction pipeline.


**E.1** **Role Configuration**


**Subject (Evaluated Model).** The Subject is responsible for both ambiguity detection (Phase 1)
and code generation (Phase 3). Assigning the inquiry task to the Subject evaluates its intrinsic ability to identify missing information, avoiding dependence on an external model’s detection capabilities.
**Oracle (Ground Truth Source).** The Oracle
(Qwen3-max) functions as a domain expert. It
utilizes the Ground Truth SQL/Code to address the
Subject’s inquiries. The Oracle provides natural
language hints to guide reasoning without revealing
the target code.


**E.2** **Pipeline Specification**


The LCF process comprises three steps:


1. **Logic Probing:** The Subject identifies ambiguities in the schema or task requirements and
formulates a clarifying question ( _Qambiguity_ ).


2. **Constraint Generation:** The Oracle derives
logic from the ground truth to produce natural language hints ( _Clogic_ ) that answer the
inquiry.



Table 14: Example of Granularity Mismatch. The
model inferred a schema constraint (rtype=’D’) from
the word "district", while the SQL ground truth operated
on a global level, leading to divergent answers.


**Phase 1 Prompt: Logic Probing (System Instruction)**


 - As an expert Data Analyst, identify ambiguity in the
following question regarding the data schema or business
logic.

 - Task: You are a logical analyst. Identify any ambiguity
in the question regarding the database schema, business
logic, or output structure requirements (e.g., exact columns
to return, handling of NULLs). Return ONLY a clarifying
question that addresses the ambiguity. Do not generate
SQL or Code.


Table 15: System prompt for the Subject model in Phase
1 (Logic Probing).


3. **Execution:** The Subject generates the final
program based on the original query and the
provided constraints.


**E.3** **LCF Prompt Templates**


Table 15 and Table 16 present the system prompts
for Phase 1 and Phase 2. Phase 3 uses the standard
generation prompt (Appendix A) augmented with
constraints from Phase 2.


**E.4** **LCF Dialogue Case Study**


We present a case study from the development
set in Table 17 to illustrate the interaction within
the LCF. This example shows how the protocol
resolves schema ambiguity and computational constraints.
In Phase 1, the Subject identifies ambiguity regarding the definition of "continuation school" (specifically, the choice between
schools.StatusType and frpm.Educational
Option Type) and questions boundary conditions



14


**Phase 2 Prompt: Constraint Generation (Oracle In-**
**struction)**


 - Task: You are a Data Analyst Proxy. Based strictly on the
Ground Truth provided below, answer the Model’s Inquiry
by explaining the business constraints or logic used in the
Gold implementation.

 - Ground Truth: {GOLD_QUERY}

 - Model’s Inquiry: {PHASE1_INQUIRY}
Return your response in JSON format with "classification"
and "answer".

 - Constraints:
1. Do NOT output code.
2. Extract the business logic into implementation-agnostic
natural language hints (e.g., "Filter by status ’Active’"
instead of "WHERE status = ’Active’"). Do not leak implementation details or syntactic tokens.
3. Answer ONLY what is explicitly asked in the Inquiry.
4. Explicitly mention specific Column Names and Values
required.


Table 16: System prompt for the Oracle model in Phase
2 (Specification Generation).


**Case Study: Eligible Free Rates Calculation**


**Input Question**
"Please list the lowest three eligible free rates for students
aged 5-17 in continuation schools."


**Phase 1 Output: Ambiguity Identification (Model In-**
**quiry)**
"What defines a ’continuation school’ in the StatusType
column of the schools table, and how should division
by zero or NULL values be handled when calculating the
eligible free rate?"
_Observation: The model correctly questions the calculation_
_logic but holds an incorrect assumption about the schema_
_location of the school type definition._


**Phase 2 Output: Constraint Generation (Oracle Re-**
**sponse)**
"A ’continuation school’ is defined by the value ’Continuation School’ in the Educational Option Type column
of the frpm table, not by the StatusType column in the
schools table. When calculating the eligible free rate for
students aged 5–17, only records where Free Meal Count
(Ages 5-17) is not NULL and Enrollment (Ages 5-17)
is greater than zero are included, thereby avoiding division
by zero and excluding undefined rates."


Table 17: Example of the LCF workflow. The interaction moves from initial schema uncertainty to logic
specification, facilitating code generation.


for arithmetic operations. In Phase 2, the Oracle
provides implementation-agnostic constraints derived from the Ground Truth. It clarifies the correct table source and defines the logic for handling
NULL values without revealing SQL syntax.



15


