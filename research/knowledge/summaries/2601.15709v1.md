## **AgentSM: Semantic Memory for Agentic Text-to-SQL**

Asim Biswal [â–³â—¦], Chuan Lei [âˆ—â‹„], Xiao Qin [âˆ—][â–¡], Aodong Li [â—¦],
Balakrishnan Narayanaswamy [â—¦], Tim Kraska [â—¦]

 - _Amazon Web Services_  - _University of California, Berkeley_ â‹„ _Oracle Corporation_  - _Snowflake Inc._
abiswal@berkeley.edu,chuan.lei@oracle.com,xiao.qin@snowflake.com
{aodongli,muralibn,timkrask}@amazon.com



**ABSTRACT**


Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet,
these systems struggle to scale in realistic enterprise settings with
large, complex schemas, diverse SQL dialects, and expensive multistep reasoning. Emerging agentic approaches show potential for
adaptive reasoning but often suffer from inefficiency and instabilityâ€”repeating interactions with databases, producing inconsistent
outputs, and occasionally failing to generate valid answers. To
address these challenges, we introduce Agent Semantic Memory
(AgentSM), an agentic framework for Text-to-SQL that builds and
leverages interpretable semantic memory. Instead of relying on raw
scratchpads or vector retrieval, AgentSM captures prior execution
tracesâ€”or synthesizes curated onesâ€”as structured programs that directly guide future reasoning. This design enables systematic reuse
of reasoning paths, which allows agents to scale to larger schemas,
more complex questions, and longer trajectories efficiently and
reliably. Compared to state-of-the-art systems, AgentSM achieves
higher efficiency by reducing average token usage and trajectory
length by 25% and 35%, respectively, on the Spider 2.0 benchmark.
It also improves execution accuracy, reaching a state-of-the-art
accuracy of 44.8% on the Spider 2.0 Lite benchmark.


**1** **INTRODUCTION**


Text-to-SQL seeks to enable interaction with structured data by
translating natural language tasks into executable SQL queries. This
capability is particularly valuable in enterprise data analytics and
business intelligence, where non-technical users need to extract
insights from large, complex databases without mastering SQL or
detailed schema knowledge.
Recent advances in large language models (LLMs) [37], prompting strategies [21], and post-training techniques [34] have driven
notable progress in Text-to-SQL performance across benchmarks
such as BIRD [16] and Spider [35]. However, most existing Text-toSQL systems remain difficult to scale in realistic enterprise settings,
where challenges such as deep nested schemas, diverse SQL dialects,
and domain-specific business logic lead to degraded accuracy and
efficiency.
To evaluate Text-to-SQL systems under more realistic conditions,
the Spider 2.0 benchmark [14] was recently introduced, featuring
complex, multi-dialect databases and extremely long contexts. Traditional Text-to-SQL approachesâ€”including vector-based schema
retrieval [15], candidate generation with majority voting [21], and
self-consistency decoding [13, 27]â€”struggle when applied on their
own to this benchmark, revealing fundamental limitations in their
ability to generalize and _scale_ . These shortcomings have led to


*Work done at Amazon Web Services.



growing interest in agentic Text-to-SQL methods [7, 26, 30], where
agents iteratively interact with databases to inspect schemas, validate partial queries, and adapt to dialectal differences.
While agentic systems demonstrate improved adaptability to
large and complex schemas, there is a challenging tradeoff between
computational cost and accuracy that makes these systems difficult
to scale. Agents often incur substantial **redundancy**, repeatedly
retracing identical exploration steps across queries on the same database, which inflates execution cost and latency. They are also prone
to **planning variance**, where suboptimal initial reasoning paths
lead to inconsistent or failed outcomes. Moreover, the inherently
iterative nature of these systems results in **high computational**
**cost and latency**, as each step consumes additional tokens and
time. Under practical constraints such as step limits, token limits,
or latency budgets, these inefficiencies significantly reduce both
overall accuracy and efficiency [17].
To address these limitations, we present Agent Semantic Memory (AgentSM), a scalable, stable, and efficient agentic framework
for Text-to-SQL. Instead of treating each query as an isolated interaction, AgentSM introduces a structured semantic memory that
captures and reuses trajectories from prior executions. Each trajectory is enriched and stored with semantic annotations to support
future retrieval and reasoning. When a new query arrives on a
known database, AgentSM reuses relevant portions of prior trajectories, eliminating redundant exploration and ensuring more
consistent behavior. In addition, frequently co-occurring tool sequences are automatically coupled into composite tools, shortening
trajectories and improving execution efficiency.
Beyond Text-to-SQL, AgentSM provides a generalizable foundation for other agentic data tasksâ€”including information extraction,
data cleaning, and data transformationâ€”where structured trajectory
reuse is critical for scalable and high-quality performance.
In summary, our contributions are as follows:
(1) We present AgentSM, an agentic framework that captures
and leverages structured semantic memory for enterprise-level
Text-to-SQL.
(2) We design a structured semantic memory that encodes prior
trajectories in an interpretable and retrievable format. This
memory enables agents to retrieve relevant past experiences,
improving efficiency and reasoning consistency.
(3) We introduce composite tools to AgentSM that streamline
decision-making, reduce latency, and alleviate hallucination
in agent planning and tool usage during complex multi-step
query generation. These tools reduce both agent turns and
token usage.
(4) We conduct extensive experiments demonstrate that AgentSM
achieves state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite


Asim Biswal [â–³â—¦], Chuan Lei [âˆ—â‹„], Xiao Qin [âˆ—][â–¡], Aodong Li [â—¦],
Balakrishnan Narayanaswamy [â—¦], Tim Kraska [â—¦]



























**Figure 1: The standard workflow for Text-to-SQL agents is a**

**series of steps alternating between data exploration, query**

**generation, and answer validation.**


benchmark. Ablation studies further show that AgentSM effectively shortens average trajectory length by 25% and improves
execution accuracy by 35%.


**2** **BACKGROUND AND PROBLEM**

**FORMULATION**

**2.1** **Agentic Text-to-SQL**


Modern ReAct-based agents [33] follow a standard pattern of reasoning and action steps. At each step, the agent observes the current
state of the task, reasons about the next appropriate action, and
executes an action that often involves tool use. For example, a code
agent observes intermediate program states or execution outputs,
reasons about modifications or next steps, and acts by generating
and executing code to advance toward the final solution.
Figure 1 shows the standard workflow of an agentic Text-to-SQL
system, which generally follows three phases: data exploration,
SQL query generation/execution, and response validation. During
the data exploration phase, the agent explores and understands the
database schema, identifying relevant tables, columns, and values
needed for the task. In enterprise environments, schema information is often parsed and stored offline in files, which the agent can
reference alongside simple exploratory SQL queries to understand
the data. The agent then attempts to generate SQL queries to answer the given task, often in parts, before synthesizing a final query.
Without encountering any errors, the agent can choose to validate
its response or directly output it. When errors occur, such as syntax
violations or unexpected query results, the agent can revise the
SQL or return to data exploration for additional context, depending
on the nature of the error.
We identify three key opportunities within these standard workflows that motivate our proposed solution.


_2.1.1_ _Repeated exploration._ Analysis of agent reasoning patterns
reveals that data exploration is _inherently repetitive_ across queries
on the same database. In fact, Liu et al. [17] highlight the repetitive
nature of agent trajectories on the BIRD benchmark [16], reporting
that fewer than 10â€“20% of the trajectories are distinct.



**Figure 2: Distribution of the first seven steps in agent trajec-**

**tories on the firebase from Spider 2 [14], reporting the per-**

**centage of instances in which each specific action is taken.**


We observe the same behavior for agents operating in general
Text-to-SQL settings. In the initial steps, the agent consistently performs a set of basic exploration actions. It begins by reading schema
files or performing PRAGMA queries to obtain understanding of
the database schema, including table structures, column names, and
data types. If external knowledge files are available, the agent reads
them for additional context about the database or task. If the agent
requires additional help dealing with particularly large schemas,
then it consults the help of tools, such as vector search, or issues
additional SQL queries to understand the contents of candidate
tables.


_**Opportunity 1.**_ _Reuse prior trajectories_ to eliminate redundant
exploration and improve efficiency on new questions over the same
database.


_2.1.2_ _Strategy selection._ Traditional Text-to-SQL systems have converged toward highly precise, pipeline-style solutions that consist
of a fixed sequence of steps to every question [21, 27]. Similarly,
agentic Text-to-SQL approaches remain heavily reliant on prompt
engineering [7], where carefully designed instructions define the
agentâ€™s workflow and tool use to ensure consistency.
However, enforcing a single dominant strategy across all queries
can be suboptimal in terms of both accuracy and efficiency. While
standardized routines can be effective for common cases, they often
fail on out-of-distribution queries. For example, Figure 2 reveals that
the agent chose to perform vector search in only 30% of questions on
the â€˜firebaseâ€™ database in Spider 2.0, likely due to the nested schema
and sufficient prior knowledge gained from prior exploration steps.
In such cases, adhering to a rigid strategy can introduce irrelevant
context, unnecessary computation, and wasted steps that otherwise
contribute to more targeted actions to solve the given query.


_**Opportunity 2.**_ _Dynamically adapt trajectories_ according to the
characteristics of each task and database, rather than relying on a
fixed, one-size-fits-all strategy.


_2.1.3_ _Reducing variance._ Agent behavior in Text-to-SQL generation inherently exhibits variance across runs. Small deviations,
including trivial syntax errors, in intermediate steps can derail the


AgentSM: Semantic Memory for Agentic Text-to-SQL


reasoning process, causing the agent to abandon a correct reasoning path and pursue an alternative one. This instability leads to
inconsistent query generation and, consequently, variation in final
results [31].
Deterministic behavior cannot be enforced reliably even by setting the model temperature to zero for agents. Moreover, even if
a strictly deterministic setting was possible, it prevents the agent
from exploring alternative reasoning paths, limiting its ability to
discover solutions to previously unseen or complex tasks [36]. Common Text-to-SQL strategies, such as candidate generation and query
refinement, explicitly rely on controlled model variance to improve
the likelihood of a correct SQL being generated. However, once new
contexts are introduced into the reasoning workflow, subsequent
steps can diverge unpredictably, even under deterministic settings.


_**Opportunity 3.**_ Optimize the _trade-off between tool complexity and_
_trajectory length_ to minimize variance and enhance accuracy and
robustness.


**2.2** **Problem Statement**


In this paper, we aim to to develop an agentic Text-to-SQL system
that, given a natural language query _ğ‘_, a database _ğ·_ and a set of
tools U, produces an optimized reasoning trajectory


_ğœ_ ( _ğ‘, ğ·,ğ‘¢_ ) = arg max
_ğœ_ âˆˆ _ğ‘‡_ ( _ğ‘,ğ·,_ U) [Acc][(] _[ğœ]_ [)]


where _ğ‘‡_ ( _ğ‘, ğ·,ğ‘¢_ ) denotes the space of executable reasoning trajectories composed of tool usages and intermediate reasoning steps,
and Acc( _ğœ_ ) measures the accuracy of the generated SQL query.


**3** **METHODOLOGY**


Figure 3 depicts a full overview of the AgentSM framework, which
consists of two agents with a trajectory-reading method and careful
tool design to improve agent performance on Text-to-SQL tasks.


**3.1** **AgentSM Architecture**


_**Agents.**_ We introduce two main agents in our framework, the
planner agent and the schema linking agent. Inspired by the ReAct
framework, agents alternate between observation, reasoning, and
action cycles. In our implementation, both agents are coding agents
that take actions by writing Python code. Unlike prior methods,
however [14], we do not restrict the functions or code that the
agent can write to a limited, predefined action space. Instead, we
provide a broad range of authorized libraries which the agent may
use to help with data exploration and analysis, including Pandas
and NumPy.
The **planner agent** serves as the systemâ€™s core reasoning component. It constructs a high-level execution plan, iteratively issues SQL
queries, performs reasoning and data wrangling, and applies query
refinements before producing the final answer. Unlike multi-agent
pipelines that delegate SQL generation to a dedicated agent[29],
we intentionally integrate SQL generation within the plannerâ€™s
reasoning loop. This design choice ensures that the planner agent
directly leverages the schema and data understanding gathered
through exploration, avoiding the context loss commonly observed
when control is handed off to specialized sub-agents. Moreover, by



letting the planner generate and execute SQL, AgentSM avoids
unnecessary inter-agent communication overhead and latency.
The **schema linking agent** is managed by the planner agent,
and its role is to perform fine-grained data exploration for the task,
when the planner agent requires deeper inspection of candidate
tables. It has access to specialized tools including a vector search
tool, which performs a similarity search on a precomputed index of
schema information to narrow a list of tables and columns relevant
for a given question. The schema linking agent operates within
a small budget (e.g., 5 steps) to perform basic queries to interpret
nested structures, probe candidate columns, and validate potential
join paths, before returning its findings (i.e., mapping question
tokens to schema elements) to the planner agent. Delegating deeper
exploration to a separate agent allows the planner agent to not drift
in its reasoning and keep only relevant information in context.
Although prior work has proposed complex multi-agent systems
(e.g., introducing query fixers, decomposers, or evaluators [7]), we
deliberately keep AgentSMâ€™s architecture simple with two tightly
coupled agents. This decision is guided by empirical findings [9,
28] that excessive agent decomposition leads to communication
overhead, fragmented memory, and increased behavioral variance.
In Text-to-SQL tasks, contextual knowledge is often highly localized:
understanding a schemaâ€™s nested hierarchy requires maintaining
persistent state over multiple exploration and reasoning cycles.
Therefore, introducing additional agents would necessitate a shared
memory [4, 24] to prevent context fragmentation. However, shared
memory introduces nontrivial challenges in retrieval efficiency,
consistency management, and error propagation, which we leave
as future work.


_**Tools.**_ Each agent in AgentSM is equipped with a carefully selected
set of tools designed to balance flexibility, robustness, and efficiency.
The planner agent is provided with tools for trajectory retrieval,
output validation, and result saving. The trajectory retrieval tool
allows the planner to review reasoning traces from semantically
similar questions before starting a new task, helping it avoid redundant exploration steps. The validation and saving tools ensure
that the final SQL and results follow consistent formatting and
can be correctly persisted. The schema linking agent, on the other
hand, is equipped with tools specialized for fine-grained schema
exploration, particularly the vector search tool that retrieves semantically relevant tables and columns from a precomputed index.
Restricting this capability to the schema linking agent prevents
redundant searches between the two agents.
Both agents share a set of general-purpose tools for local directory exploration and SQL execution on supported backends such as
BigQuery, Snowflake, and SQLite. To improve execution robustness,
we incorporate self-refinement into the SQL execution tools. When
a query fails or returns an empty result, the SQL execution tools
automatically attempt a correction based on the received feedback.
This mechanism reduces the likelihood that a single execution error
derails the reasoning trajectory.
In addition to these basic tools, we introduce _composite tools_, special types of tool that combine the logic of single-purpose tools that
frequently co-occur in consecutive steps. These composite tools,
described in Section 3.3, simplify planning, reduce unnecessary tool
usage, and improve consistency across runs.


### **Trajectory Synthesis**

Example Qs Ext. Knowledge


DB schema


Qsyn: Synthetic Question Set


+


**Raw Trajectories**


Step Classification +
Semantic Annotations


Trajectory Store



```python posts_wiki.. SELECT *
ddl_content = read_file(...) (sim: 0.42) FROM
``` - - - - - - - - - - Columns: posts

**Retrieve most similar question** **Execution tools** creation_date LIMIT 10
**on same db** edit_date



Asim Biswal [â–³â—¦], Chuan Lei [âˆ—â‹„], Xiao Qin [âˆ—][â–¡], Aodong Li [â—¦],
Balakrishnan Narayanaswamy [â—¦], Tim Kraska [â—¦]

### **AGENTSM in Action**


What is the highest number of answers received for a single Python 2 specific

question on Stack Overflow, excluding any discussions that involve Python 3?


Planner Agent Schema Linking Agent



**Exploration**
**Table Retrieval** **SQL**



**Trajectory Reuse**



**Iterative query**

**generation**



Found similar question on dbâ€¦
**# Question**
â€œWhat is â€¦â€
**## Schema Analysis**
**Step 1**
```python
ddl_content = read_file(...)
```



WITH py3posts AS
(
SELECT *
...
)



Relevant tables
===============
posts_wiki..
(sim: 0.42)

- - - - - - - - - Columns:
creation_date
edit_date
...



**Execution tools**



**Figure 3: An overview of the AgentSM architecture that leverages trajectories in structured semantic memory.**



**3.2** **Trajectory Synthesis and Retrieval**


_**Synthetic question exploration.**_ We make a key insight that data
exploration is inherently reusable: once an agent has examined the
schema, inspected columns, and issued exploratory queries, those
interaction traces remain valuable for subsequent questions on the
same database. Crucially, this property extends to _synthetic ques-_
_tions_, questions that can be formulated and executed offline that are
deliberately designed to elicit data exploration. By enriching the
trajectory store with such exploration-rich traces, AgentSM effectively reduces redundant probing and enhances agent performance
on real queries.
Algorithm 1 outlines the process for generating the full synthetic
question set _ğ‘„_ syn. First, we ensure that each database present in
our query distribution is covered by at least one synthetic question (Lines 2-4). We allocate additional synthetic questions across
databases proportionally to the distribution these databases on the
query set, up to a fixed total question budget _ğ‘›_ (Line 6). This allocation strategy naturally emphasizes large, complex schemas, where
additional exploration traces are most beneficial (Lines 7-11).
For each selected schema _ğ‘†_, we prompt an LLM with the schema,
external knowledge file, and a set of existing questions to generate
diverse candidate questions that involves different operators, tables,
and columns. The number of questions generated per schema follows the same distribution as the full query set. The prompts used
to generate candidate questions can be found in the repository [1] .
Each synthetic question _ğ‘_ âˆˆ _ğ‘„_ syn is then consumed by an agent
equipped with SQL and file-reading tools only to produce a trajectory _ğ‘‡_ ( _ğ‘_ ), which is stored for reuse. Although the answers to
these synthetic questions are not evaluated, the resulting trajectories provide dense and diverse exploration traces. When the agent


[1https://tinyurl.com/xcey6h33](https://tinyurl.com/xcey6h33)



**Algorithm 1:** Synthetic Question Generation.
GenQ( _ğ‘†, ğ¾,ğ‘„_ ) generates a new question using an LLM over
the database schema _ğ‘†_, external knowledge _ğ¾_, and existing
questions _ğ‘„_ for _ğ‘†_ âˆˆD.
Allocation( _ğ‘†,ğ‘›, ğ‘_ ) returns an integer budget _ğ‘˜_ for the max
number of questions on _ğ‘†_ based on the query distribution
_ğ‘_ (Q) and the remaining global budget _ğ‘›_ .


**Input:** Databases D, target count _ğ‘›_, query distribution _ğ‘_ (Q)
**Output:** Synthetic question set _ğ‘„_ syn

**1** _ğ‘„_ syn â†âˆ…;

**2 foreach** _schema ğ‘†_ âˆˆD **do**

**3** _ğ‘_ â† GenQ [ï¿½] _ğ‘†, ğ¾, ğ‘„_ [ï¿½] ;

**4** _ğ‘„_ syn â† _ğ‘„_ syn âˆª{ _ğ‘_ };


**5 while** | _ğ‘„syn_ | _< ğ‘›_ **do**

**6** _ğ‘˜_ â† Allocation [ï¿½] _ğ‘†, ğ‘›, ğ‘_ (Q) [ï¿½] ;

**7** **for** _ğ‘–_ â† 1 **to** _ğ‘˜_ **do**

**8** **if** | _ğ‘„syn_ | = _ğ‘›_ **then**


**9** **break**


**10** _ğ‘_ â† GenQ [ï¿½] _ğ‘†, ğ¾, ğ‘„_ [ï¿½] ;

**11** _ğ‘„_ syn â† _ğ‘„_ syn âˆª{ _ğ‘_ };


**12 return** _ğ‘„_ syn;


encounters new queries in future, these traces provide rich context,
allowing it to leverage past exploration rather than repeating it.


_**Step classification.**_ Agent workflows are non-linear, consisting
of sequence of distinct states that the agent enters and exits during
execution. In practice, a Text-to-SQL agent alternates among three
primary phases depicted in Figure 1: (1) exploring the database
to understand the data, (2) formulating and executing candidate


AgentSM: Semantic Memory for Agentic Text-to-SQL


**Method** **Avg Steps** **Accuracy (%)**


No trajectory 22.62 25
Naive 22.62 25
Markdown 16.50 50
JSON 15.12 50


**Table 1: Average trajectory length (in steps) and execution**

**accuracy on a sample of Spider 2.0 Lite questions, under dif-**

**ferent trajectory formats. Reading raw, unstructured trajec-**

**tories yield no improvement compared to without using any**

**trajectory. In contrast, structured representations, in either**

**Markdown or JSON format, significantly reduce the average**

**number of steps and improve the overall execution accuracy.**


queries, and (3) validating results or producing an answer. Similar
stage decompositions have been observed in prior work [17].
Grounding trajectory reuse in these phases is essential: exploration steps are often generalizable across questions, execution
steps encode database-specific reasoning, and validation steps highlight patterns of error correction and output formatting. Step classification helps avoid presenting the agent with a noisy sequence
of mixed tool invocations and rendering reuse ineffective.
To classify steps, we apply a lightweight text-pattern matching
using regular expressions rather than relying on step position in
the trajectory. While exploration often occurs early, agents may
re-enter this exploration phase later when encountering execution
errors or missing context. We therefore identify intent based on
tool usage and query patterns. For example, file listing and reading operations are matched as exploration, while complex SQL
queries beginning with â€˜WITHâ€˜ (common table expressions) are almost always recognized as main query execution. We also explored
LLM-based step classification and observed comparable accuracy
but substantially higher cost and latency.
Each classified step is parsed and saved in a separate trajectory
file. During trajectory reading, the agent selectively loads only
the trajectory segment corresponding to the relevant phase for
reference. For example, during data exploration phase, the agent
retrieves and reads the relevant exploration trajectories that include
basic schema analysis and external knowledge reading actions,
enabling efficient reuse of prior context without redundant steps.


_**Trajectory structure.**_ Agent trajectories are typically long, complex sequences that interleave text, code, reasoning traces, and
execution outputs. Reading raw trajectories introduces significant
noise and increases the _lost-in-the-middle_ effect, where valuable
information is buried amid verbose agent debugging logs and intermediate outputs.
To help the agent extract useful knowledge more effectively, we
impose a structured representation on each trajectory. In particular,
we store agent trajectories in markdown format, with semantically
meaningful headers automatically generated by a lightweight LLM
(e.g., Claude Haiku 4.5). This structure segments the trajectory into
interpretable sections, improving both retrieval and readability.
As shown in Table 1, structured trajectories, whether represented in markdown or JSON, yield substantial improvements in



both execution accuracy and average step reduction compared to
unstructured logs, based on a sample of 20 benchmark questions.
We adopt markdown as the format for its consistency with external
knowledge files and human readability. The implementation details
of step classification and trajectory structure generation can be
found in the repository [2] .


_**Trajectory selection.**_ Having established a structured representation for storing agent trajectories, we next describe how these stored
trajectories are retrieved and reused during inference. Among the
synthetic questions _ğ‘„ğ‘ ğ‘¦ğ‘›_ for which trajectories are synthesized and
saved, we first restrict retrieval to those associated with the same
database as the given question _ğ‘_ . This filtering narrows the search
space to relevant trajectories:


_ğ‘„ğ‘‘_ = { _ğ‘_ [â€²] âˆˆ _ğ‘„_ syn | db( _ğ‘_ [â€²] ) = db( _ğ‘_ )} (1)


From this filtered subset _ğ‘„ğ‘‘_, we then select the most relevant
trajectory _ğ‘‡_ based on its associated question _ğ‘_ [â€²] that has the highest
semantic similarity to the given question _ğ‘_ :


_ğ‘_ [âˆ—] = arg max sim( _ğ‘_ [â€²] _,ğ‘_ ) (2)
_ğ‘_ [â€²] âˆˆ _ğ‘„ğ‘‘_


_ğ‘‡_ = traj( _ğ‘_ [âˆ—] ) (3)


This strategy is effective for data exploration reuse, as semantically similar questions tend to probe the same set of tables during
their initial exploration steps. The agent naturally prioritizes tables with their schema semantically similar to the question before
performing deeper searches. Consequently, selecting trajectories
based on question similarity leads to effective trajectory reuse. Further improvements in retrieval quality could be achieved through
finer-grained alignment between questions and stored trajectories.
For example, schema-level retrieval could identify which tables
are relevant to a question and restrict trajectory retrieval to those
involving the same tables. Alternatively, trajectory-level alignment
could leverage the agentâ€™s initial plan to retrieve similar prior executions and adjust the reasoning strategy before execution. We
leave these directions to future work, as fine-grained retrieval risks
missing relevant context essential for accurate reasoning.


**3.3** **Composite Tools**


_**Composite tool construction.**_ Across trajectories, we observe
that certain tool pairs consistently co-occur in sequence. Figure 4b
illustrates how the get_ext â†’ get_ddl pattern frequently appears
when the agent needs to access an external knowledge file and the
database schema at the beginning of the exploration phase. As
such, these tools can be combined into a single composite tool
local_exploration.
As shown in Figure 4a, agents rarely chain multiple tool invocations within a single reasoning step, instead issuing one call per step,
even when such sequences are highly regular and deterministic.
This pattern leads to inefficiency: the agent spends valuable steps
on executing repetitive exploration routines rather than focusing
on higher-level reasoning and problem solving.


[2https://tinyurl.com/492ymhcn](https://tinyurl.com/492ymhcn)


**(a) Dist. of tool calls per step over 327 steps**







**Step 1**
extknowledge =

get_ext(local009)
print(extknowledge)


## Haversine formula
...



**Step 2**
dbschema =
get_ddl(local009)

print(dbschema)


{ tables: flights, â€¦
columns: â€¦}



get_ext + get_ddl â‡’ local_exploration (new tool)


exploration = local_exploration(local009)

print(exploration)


**# External knowledge**
## Haversine formula
â€¦
**# Database schema**
{tables: â€¦, columns:...}


**(b) Composite tool formation.**


**Figure 4: Analysis of tool usage and composite tool construc-**

**tion. (a) shows that most steps use zero or one tool, motivat-**

**ing composition. (b) illustrates how frequently co-occurring**

**tools are merged into a higher-level composite tool.**


To mitigate this overhead, we introduce _composite tools_, which
combine commonly co-occurring tool sequences into single higherlevel operations. Formally, we construct a composite tool:


_ğ‘“_ ( _ğ‘¡_ 1 _,ğ‘¡_ 2 _, . . .,ğ‘¡ğ‘˜_ )
_ğ¶_ = âŸ¨ _ğ‘¡_ 1 _,ğ‘¡_ 2 _, . . .,ğ‘¡ğ‘˜_ âŸ© if â‰¥ _ğœ_

_ğ‘_


where _ğ‘¡ğ‘–_ âˆˆT, T is the tool set, _ğ‘“_ (Â·) counts the trajectories containing the subsequence ( _ğ‘¡_ 1 _,ğ‘¡_ 2 _, . . .,ğ‘¡ğ‘˜_ ), _ğ‘_ is the total number of
trajectories, and _ğœ_ is a chosen support threshold.
In practice, AgentSM follows several heuristics to construct
effective composite tools while preserving modularity. First, tools
are combined only when they co-occur within the same reasoning
phase (e.g., data exploration or validation), ensuring that composite
tools remain semantically coherent. Second, tools that appear across
multiple phases are excluded from composition to preserve their
agility for reuse in different contexts. Finally, to avoid excessive
aggregation, we constrain the maximum size of composite tools,
preventing all tools collapse into a single monolithic operator.
Each composite tool is then assigned a descriptive name and natural language summary by an LLM, which the agent can reference



Asim Biswal [â–³â—¦], Chuan Lei [âˆ—â‹„], Xiao Qin [âˆ—][â–¡], Aodong Li [â—¦],
Balakrishnan Narayanaswamy [â—¦], Tim Kraska [â—¦]


during prompting. At inference time, the agent uses these composite
tools directly, effectively replacing repeated multi-step exploration
with single high-level actions and freeing more trajectory budget
for reasoning.


**4** **EVALUATION**


In this section, we evaluate the effectiveness of AgentSM on Spider
2, a widely recognized benchmark for agentic Text-to-SQL systems.
Our evaluation is designed to answer the following key questions:
(1) How do trajectory synthesis and retrieval influence the agentâ€™s
efficiency and accuracy?
(2) How does the optimized tool design improve agentâ€™s efficiency
and accuracy?


**4.1** **Evaluation Setup**


_**Dataset.**_ Spider 2.0 [14] is a large-scale benchmark of 547 Text-toSQL workflow problems that emulate the challenges of enterprisescale data analysis. Unlike traditional Text-to-SQL benchmarks,
Spider 2.0 requires solving tasks that involve extremely long contexts, nested schemas, and diverse SQL dialects. The databases in
Spider 2.0 often span thousands of columns and are deployed in
production systems such as BigQuery and Snowflake.
This makes Spider 2.0 a natural workload for agentic approaches,
where agents must iteratively plan and issue queries to reach a
final answer. We select this benchmark to evaluate our method, as
our framework is specifically designed to operate in such complex
environments that reward iterative reasoning and tool use.


_**Metrics.**_ We measure execution accuracy as the percentage of
execution output matches to the gold output using the evaluation
function provided by the authors of the original benchmark. We
adopt this metric as per the original benchmark. We report accuracy for BigQuery, Snowflake, and SQLite questions, along with an
overall result.
We also report the average agent trajectory length in both reasoning steps, as well as the number of input and output tokens. We
also report average end-to-end latency for a single execution.


_**Experimental setup.**_ We evaluate our method using the Claude
3-7 [1] and Claude 4 Sonnet [2] models. Several agent frameworks
have risen in popularity, including LangChain [12], smolagents [8],
etc. While our approach is framework-agnostic, for evaluation we
implement it using smolagents as the foundation of our agentic
Text-to-SQL system.
For all evaluated systems, we employ the MiniLM-L6-v2 model [23]
from SentenceTransformers for generating embeddings and use the
FAISS [10] library for vector similarity search. We run ArcticSQL-7B
with vLLM [11] on a cluster of 8Ã—A100 80GB GPUs.


_**Baselines.**_ For our main experiments on Spider 2.0 Lite, we compare
AgentSM against two baselines: SpiderAgent [14] and a standard
coding agent (CodingAgent) implemented using the smolagents [8]
framework equipped with basic SQL execution tools described in
Section 2. We evaluate both baselines with Claude 3-7 [1] and
Claude 4 Sonnet [2] as the underlying LLMs. The performance of


AgentSM: Semantic Memory for Agentic Text-to-SQL


EX (%) Avg. Length
Method Avg Latency (s)
BigQuery Snowfake SQLite Overall Steps Input Toks Output Toks
SpiderAgent (claude-3-7-sonnet) 37.6 12.6 40.7 28.7 18.9 200K 4K 363.2
SpiderAgent (claude-4-sonnet) - - - 27.8 [âˆ—] - - - CodingAgent (claude-4-sonnet) 27.3 11.1 42.2 24.7 18.1 200K 4K 325.5
AgentSM (claude-3-7-sonnet) 40.5 33.3 42.2 38.4 16.8 299K 5K 226.4
AgentSM (claude-4-sonnet) **52.2** **35.0** **51.9** **44.8** 16.4 300K 5K 247.1
_AgentSM (claude-4-sonnet, gold tables)_ 62.0 44.0 79.2 57.6 15.8 268K 5K 194.3

**Table 2: Comparison of execution accuracy, step/token length, and latency for methods on the Spider 2.0 Lite dataset.** âˆ— **This**

**result is quoted directly from the original SpiderAgent paper.**



other state-of-the-art systems is available on the official Spider 2.0
leaderboard [3] .


**4.2** **Main Results**


Table 2 contains the execution accuracy for each method on the full
Spider 2.0 Lite benchmark with 547 questions. We report execution
accuracy for BigQuery instances, Snowflake instances, and SQLite
instances along with overall accuracy.


_**Accuracy.**_ AgentSM substantially outperforms the prior agentic baseline SpiderAgent [14], achieving higher accuracies across
each SQL dialect and an overall 14.1% improvement in execution
accuracy. AgentSM also outperforms a standard coding agent implemented using the same agent framework[8] and model [2] by
over 20%. Based on our evaluation, AgentSM would rank No. 1 on
the Spider 2.0 Lite leaderboard, with an overall accuracy of 44.8%
at the time of paper submission. Notably, we achieve this result
without using a powerful reasoning model such as Qwen3 [3] or
OpenAIâ€™s o3 [19], which other top systems rely on.
Across all SQL dialects, the questions on the Snowflake database
are the most challenging for the agents. The platform has more
questions on databases with larger and nested schemas. For this
platform alone, our method performs better with an older model
Claude 3-7 Sonnet than Claude-4 Sonnet.


_**Efficiency.**_ To understand how AgentSM improves agent efficiency, we revisit the standard agentic workflow (i.e., CodingAgent)
and analyze how our method reshapes the agentâ€™s progression
through three key stages: database exploration, query execution,
and answer validation. We randomly sample 75 questions from
the Spider 2.0 Lite dataset and classify each step in the agentâ€™s
trajectory as one of the three stages, based on the reasoning trace
and executed queries. We aggregate these stage annotations across
instances to measure the proportion of each trajectory devoted to
exploration versus execution.
As shown in Figure 5, AgentSM transitions from exploration to
execution significantly earlier, completing tasks with fewer overall
steps. In contrast, a standard coding agent not utilizing synthesized
trajectories repeatedly issues exploratory queries and spends most
of its steps analyzing schemas rather than composing or validating
SQL queries. By leveraging structured semantic memory, AgentSM
alleviates redundant data exploration, maintains consistent reasoning efficiency, and scales gracefully to larger databases and more
complex questions.


[3https://spider2-sql.github.io/](https://spider2-sql.github.io/)



**Figure 5: Trajectory composition: median steps split into**

**exploration vs. execution and validation.**


**4.3** **Ablation Studies**


_4.3.1_ _Effectiveness of trajectory reading and composite tools._ We
conduct ablation studies on two key components of AgentSM,
the synthesized trajectory and the composite tools, to report their
respective effects on accuracy and efficiency.
Table 3 reports the average trajectory length and accuracy for
a sample of 75 examples from Spider 2.0 Lite with and without
trajectory reading. We observe that enabling trajectory reading
reduces the average trajectory length of the agent by 25% while
improving accuracy by 35%. This is consistent with the results
reported in Section 4.2.
Composite tools have a similarly pronounced impact on agent
trajectory length and accuracy. Without composite tools, the agent
requires the largest number of steps across all sample instances,
yet achieving the lowest overall execution accuracy.
The observed accuracy gains naturally stem from the reduction
in average trajectory length, which can be attributed to two factors:
(1) longer trajectories often correspond to inherently more complex
questions, and (2) as trajectory length increases, the agent becomes
more likely to reach the maximum step threshold before solving the
question. By reducing the number of steps spent on data exploration,
AgentSM effectively mitigates the issues, allowing the agent to
allocate more steps toward reasoning and to iteratively refine its
final queries before reaching the maximum steps.


_4.3.2_ _Impact of gold tables._ We further evaluate AgentSM on Spider 2.0 Lite with gold tables (as provided by Spider 2.0). Using the
gold tables and columns, our method achieves **55.3%** execution












Asim Biswal [â–³â—¦], Chuan Lei [âˆ—â‹„], Xiao Qin [âˆ—][â–¡], Aodong Li [â—¦],
Balakrishnan Narayanaswamy [â—¦], Tim Kraska [â—¦]


Execution Accuracy Steps Avg. Length Latency
Method
EX (%) Î”EX Total Avg Î”Steps Input Toks Output Toks Î”Input Time (s) Î”Time
No trajectory reading 17.3 â€“34.7 1527 20.36 +4.37 360K 5.6K +8K 348.6 +96.2
No composite tools 16.2 â€“35.8 1557 20.76 +4.77 398K 5.8K +46K 523.7 +271.3

**Full method** **52.0** **â€“** **1167** **15.99** **â€“** **352K** **5.2K** **â€“** **252.4** **â€“**

**Table 3: Effects of synthesized trajectory and composite tools on execution accuracy, step count, token usage, and latency over**

**75 sampled questions. Deltas are computed relative to the full method. Removing either component increases token usage and**

**latency while sharply decreasing execution accuracy.**



accuracy on the question subset (listed in Table 2). The results indicate a substantial improvement in execution accuracy compared to
the base configuration of our method. This highlights that schema
linking is still a significant hurdle in these complex database scenarios, as agents often struggle to identify the right tables and columns
when data and the given question are ambiguous.


**4.4** **Error Analysis**


We provide a detailed error analysis on the full Spider 2.0 Lite evaluation to better understand the remaining failure cases of AgentSM.
Among the 245 incorrect instances, 34% occur on BigQuery, 22% on
SQLite, and 44% on Snowflake, which shows the highest relative
error rate. Within Snowflake, 5% of failures stem from exceeding
the step budget, 30% from schema-linking errors caused by nested
schemas, and the remainder from logical or dialect-specific syntax
errors during query generation.
We further examine how structured semantic memory influences
the agentâ€™s success rate. When relevant trajectories are available,
AgentSM can reuse past reasoning steps to provide useful context
for new queries. This is particularly effective for schema-linking,
where prior trajectories often highlight useful table and join patterns, helping the agent quickly identify the relevant portions of a
large schema. In contrast, the benefit is limited for queries that demand complex mathematical operations or intricate CTE reasoning.
These tasks typically require deeper reasoning or problem decomposition that cannot be easily inferred from previous trajectories.
Finally, we observe that AgentSMâ€™s performance varies across
domains. Commonly used databases (e.g., city, weather, census)
achieve 60â€“78% accuracy, whereas heterogeneous databases (e.g.,
github_repos, idc) lag behind at 14â€“40%. These results suggest
that AgentSM excels in domains with consistent schema patterns
and recurring reasoning structures, but remains challenged by
highly domain-specialized data sources.


**5** **RELATED WORK**


_**Text-to-SQL.**_ Recent Text-to-SQL work has spanned supervised
fine-tuning, prompt engineering, and reinforcement learning based
solutions. Supervised systems such as DIN-SQL [22] and MACSQL [29] utilize a pipeline with structured decoding, schema linking,
and decomposition components to generate SQL queries. Prompting based methods [7, 21] leverage LLMs in few-shot or zero-shot
settings, often augmented with modular pipelines that include candidate generation, majority voting, and execution verification. More
recently, reinforcement techniques using group relative policy optimization (GRPO) engineer rewards to improve model performance
on Text-to-SQL tasks [5]. Multi-agent systems, such as AgenticData, explore utilizing a combination of planning, exploration, and
validation agents to iteratively work towards an answer [26]. These



techniques have steadily improved overall performance on the Spider [35] and BIRD [16] benchmarks, however they often assume
clean, well-aligned schemas and struggle with ambiguity, nested
queries, and relational complexity found in enterprise settings.


_**Coding Agents.**_ Recent work on coding agents explore how LLMs
can use external tools, such as web-search and code, to solve tasks
through iterative reasoning. Approaches like ReACT [32] and Reflexion [25] highlight the effectiveness of planning, execution, and
feedback stages for complex tasks. In line with these approaches,
agentic Text-to-SQL systems decompose query generation into
steps of interleaved tool executions and reasoning, allowing models
to inspect schemas, validate partial queries, and iteratively refine
the final answer. SpiderAgent [14] formalizes this trend as an initial
solution for Spider2.0. Recent works [18] have also extended this
broad strategy with memory-guided refinement. While effective,
most of these solutions rely on a fresh state for the agent or maintain some intra-task memory, unlike our solution which leverages
inter-task memory to re-use database exploration reasoning across
examples.


_**Agent Memory.**_ Research on agentic memory can be broadly categorized into action-oriented memory, focused on persisting agent
state, and knowledge-oriented memory, focused on persisting the
knowledge agents gain from interaction. The most basic approach
treats the language modelâ€™s context window as a scratchpad-like
working memory, where agents maintain notes. MemGPT [20] addresses the challenge of limited context windows by implementing
a memory hierarchy that mimics operating system memory management, effectively bridging the previous two approaches. More
recently, Mem0[6] utilized graph representations of memory, maintaining memory with standard graph knowledge base structure: a
collection of nodes (entities) and edges (relationships between entities). These forms of memory are limited in the amount of structure
they can represent in their information for the agent.

**6** **CONCLUSION**


In this paper, we introduce AgentSM, a framework that enables
agents to reuse reasoning steps across related Text-to-SQL tasks
within the same database. By exploiting the inherent repetition
in data exploration, AgentSM synthesizes, stores and retrieves
structured trajectories to substantially reduce redundant exploration and improve execution efficiency. Moreover, our solution,
equipped with semantic memory, enhances scalability, allowing
agents to handle larger schemas and more complex queries. As a result, AgentSM delivers significant gains in efficiency across agent
turns, token usage, and latency. On the Spider 2.0 Lite benchmark,
AgentSM achieves an execution accuracy of 44.8%.


AgentSM: Semantic Memory for Agentic Text-to-SQL


**REFERENCES**


[[1] Anthropic. 2025. Claude 3.7 Sonnet. https://www.anthropic.com/claude. Large](https://www.anthropic.com/claude)
Language Model.

[[2] Anthropic. 2025. Claude 4 Sonnet. https://www.anthropic.com/claude. Large](https://www.anthropic.com/claude)
Language Model.

[3] Jiawei Bai, Wei Zhang, Yifei Li, et al. 2025. Qwen3 Technical Report. _arXiv_
_preprint arXiv:2503.06749_ [(2025). https://arxiv.org/abs/2503.06749](https://arxiv.org/abs/2503.06749)

[4] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li,
Xu Chen, and Ji-Rong Wen. 2024. Reflective Multi-Agent Collaboration
based on Large Language Models. In _Advances in Neural Information Pro-_
_cessing Systems_, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37. Curran Associates, Inc.,
138595â€“138631. [https://proceedings.neurips.cc/paper_files/paper/2024/file/](https://proceedings.neurips.cc/paper_files/paper/2024/file/fa54b0edce5eef0bb07654e8ee800cb4-Paper-Conference.pdf)
[fa54b0edce5eef0bb07654e8ee800cb4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/fa54b0edce5eef0bb07654e8ee800cb4-Paper-Conference.pdf)

[5] Kai Cheng, Xiaojun Zhou, and Weidong Chen. 2024. Text-to-SQL with Reinforcement Learning via Execution-Guided Reward Modeling. _arXiv preprint_
_arXiv:2403.12345_ (2024).

[6] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav.
2025. Mem0: Building Production-Ready AI Agents with Scalable Long-Term
[Memory. arXiv:2504.19413 [cs.CL] https://arxiv.org/abs/2504.19413](https://arxiv.org/abs/2504.19413)

[7] Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei
Yao, Anupam Datta, and Hao Zhang. 2025. ReFoRCE: A Text-to-SQL
Agent with Self-Refinement, Consensus Enforcement, and Column Exploration.
[arXiv:2502.00675 [cs.CL] https://arxiv.org/abs/2502.00675](https://arxiv.org/abs/2502.00675)

[8] Hugging Face. 2025. _SmolAgents_ [. https://github.com/huggingface/smolagents](https://github.com/huggingface/smolagents)
Lightweight framework for building language model agents.

[9] Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, and
Christina Mack. 2025. Agentic AI for Scientific Discovery: A Survey of Progress,
[Challenges, and Future Directions. arXiv:2503.08979 [cs.CL] https://arxiv.org/](https://arxiv.org/abs/2503.08979)
[abs/2503.08979](https://arxiv.org/abs/2503.08979)

[10] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-Scale Similarity
Search with FAISS. In _IEEE Transactions on Big Data_ [. IEEE. https://doi.org/10.](https://doi.org/10.1109/TBDATA.2019.2921572)
[1109/TBDATA.2019.2921572](https://doi.org/10.1109/TBDATA.2019.2921572)

[11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
Memory Management for Large Language Model Serving with PagedAttention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems_
_Principles_ .

[12] LangChain. 2025. _LangChain_ [. https://github.com/langchain-ai/langchain Large](https://github.com/langchain-ai/langchain)
Language Model application framework.

[13] Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. 2024. MCS-SQL:
Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL
[Generation. arXiv:2405.07467 [cs.CL] https://arxiv.org/abs/2405.07467](https://arxiv.org/abs/2405.07467)

[14] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su,
Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider
2.0: Evaluating language models on real-world enterprise text-to-sql workflows.
_arXiv preprint arXiv:2411.07763_ (2024).

[15] Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang,
Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, Hong Chen, and Cuiping
Li. 2025. OmniSQL: Synthesizing High-Quality Text-to-SQL Data at Scale. _Proc._
_VLDB Endow._ [18, 11 (Sept. 2025), 4695â€“4709. https://doi.org/10.14778/3749646.](https://doi.org/10.14778/3749646.3749723)
[3749723](https://doi.org/10.14778/3749646.3749723)

[16] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang,
Bowen Qin, Ruiying Geng, Nan Huo, et al. 2024. Can llm already serve as a
database interface? a big bench for large-scale database grounded text-to-sqls.
_Advances in Neural Information Processing Systems_ 36 (2024).

[17] Shu Liu, Soujanya Ponnapalli, Shreya Shankar, Sepanta Zeighami, Alan Zhu,
Shubham Agarwal, Ruiqi Chen, Samion Suwito, Shuo Yuan, Ion Stoica, Matei
Zaharia, Alvin Cheung, Natacha Crooks, Joseph E. Gonzalez, and Aditya G.
Parameswaran. 2025. Supporting Our AI Overlords: Redesigning Data Systems
[to be Agent-First. arXiv:2509.00997 [cs.AI] https://arxiv.org/abs/2509.00997](https://arxiv.org/abs/2509.00997)

[18] Zihan Liu, Junhao Xu, Dongjie Guo, et al. 2024. CodeAct: LLM-Based
Autonomous Programming via Tool-Use and Reflection. _arXiv preprint_
_arXiv:2402.16055_ [(2024). https://arxiv.org/abs/2402.16055](https://arxiv.org/abs/2402.16055)

[[19] OpenAI. 2025. OpenAI o3 Reasoning Model. https://platform.openai.com/docs/](https://platform.openai.com/docs/models)
[models. Large Language Model.](https://platform.openai.com/docs/models)

[20] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion
Stoica, and Joseph E. Gonzalez. 2024. MemGPT: Towards LLMs as Operating
[Systems. arXiv:2310.08560 [cs.AI] https://arxiv.org/abs/2310.08560](https://arxiv.org/abs/2310.08560)

[21] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei,
Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan O. Arik.
2024. CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate
[Selection in Text-to-SQL. arXiv:2410.01943 [cs.LG] https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.01943)
[01943](https://arxiv.org/abs/2410.01943)

[22] Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL: Decomposed In[Context Learning of Text-to-SQL with Self-Correction. arXiv:2304.11015 [cs.CL]](https://arxiv.org/abs/2304.11015)
[https://arxiv.org/abs/2304.11015](https://arxiv.org/abs/2304.11015)




[23] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In _Proceedings of the 2019 Conference on Em-_
_pirical Methods in Natural Language Processing_ . Association for Computational
[Linguistics. https://www.sbert.net/](https://www.sbert.net/)

[24] Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, and Yujia Bao.
2025. Collaborative Memory: Multi-User Memory Sharing in LLM Agents with
[Dynamic Access Control. arXiv:2505.18279 [cs.MA] https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.18279)
[18279](https://arxiv.org/abs/2505.18279)

[25] Noah Shinn, Joseph Labash, and Ashwin Gopinath. 2023. Reflexion: Language
Agents with Verbal Reinforcement Learning. In _Advances in Neural Information_
_Processing Systems (NeurIPS) Workshop on LLMs_ [. https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)

[26] Ji Sun, Guoliang Li, Peiyao Zhou, Yihui Ma, Jingzhe Xu, and Yuan Li. 2025.
AgenticData: An Agentic Data Analytics System for Heterogeneous Data.
[arXiv:2508.05002 [cs.DB] https://arxiv.org/abs/2508.05002](https://arxiv.org/abs/2508.05002)

[27] Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and
Amin Saberi. 2024. CHESS: Contextual Harnessing for Efficient SQL Synthesis.
[arXiv:2405.16755 [cs.LG] https://arxiv.org/abs/2405.16755](https://arxiv.org/abs/2405.16755)

[28] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry
Oâ€™Sullivan, and Hoang D. Nguyen. 2025. Multi-Agent Collaboration Mechanisms:
[A Survey of LLMs. arXiv:2501.06322 [cs.AI] https://arxiv.org/abs/2501.06322](https://arxiv.org/abs/2501.06322)

[29] Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, LinZheng Chai,
Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, and Zhoujun Li. 2025. MAC-SQL: A
[Multi-Agent Collaborative Framework for Text-to-SQL. arXiv:2312.11242 [cs.CL]](https://arxiv.org/abs/2312.11242)
[https://arxiv.org/abs/2312.11242](https://arxiv.org/abs/2312.11242)

[30] Yihan Wang, Peiyu Liu, and Xin Yang. 2025. LinkAlign: Scalable
Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL.
[arXiv:2503.18596 [cs.CL] https://arxiv.org/abs/2503.18596](https://arxiv.org/abs/2503.18596)

[31] Junde Wu, Jiayuan Zhu, Yuyuan Liu, Min Xu, and Yueming Jin. 2025. Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with
[Agentic Tools. arXiv:2502.04644 [cs.AI] https://arxiv.org/abs/2502.04644](https://arxiv.org/abs/2502.04644)

[32] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Tom Griffiths, Yuan
Cao, and Karthik Narasimhan. 2022. ReAct: Synergizing Reasoning and Acting in
Language Models. In _Advances in Neural Information Processing Systems (NeurIPS)_ .
[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)

[33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language
[Models. arXiv:2210.03629 [cs.CL] https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)

[34] Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Zheyu Shen, Minghang Deng,
Bohan Zhai, Hao Zhang, Ang Li, and Yuxiong He. 2025. Arctic-Text2SQL-R1:
[Simple Rewards, Strong Reasoning in Text-to-SQL. arXiv:2505.20315 [cs.CL]](https://arxiv.org/abs/2505.20315)
[https://arxiv.org/abs/2505.20315](https://arxiv.org/abs/2505.20315)

[35] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,
James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
Radev. 2019. Spider: A Large-Scale Human-Labeled Dataset for Complex and
[Cross-Domain Semantic Parsing and Text-to-SQL Task. arXiv:1809.08887 [cs.CL]](https://arxiv.org/abs/1809.08887)
[https://arxiv.org/abs/1809.08887](https://arxiv.org/abs/1809.08887)

[36] Bingxi Zhao, Lin Geng Foo, Ping Hu, Christian Theobalt, Hossein Rahmani,
and Jun Liu. 2025. LLM-based Agentic Reasoning Frameworks: A Survey from
[Methods to Scenarios. arXiv:2508.17692 [cs.AI] https://arxiv.org/abs/2508.17692](https://arxiv.org/abs/2508.17692)

[37] Glenn Zorpette. 2025. Large language models are improving exponentially.
[https://spectrum.ieee.org/large-language-model-performance](https://spectrum.ieee.org/large-language-model-performance)


