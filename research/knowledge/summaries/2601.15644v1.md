1


### SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction

Zichen Yu, Quanli Liu, _Member, IEEE_, Wei Wang, _Senior Member, IEEE_,
Liyong Zhang, _Member, IEEE_, and Xiaoguang Zhao


|46|Col2|Col3|
|---|---|---|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|~~SuperOcc~~<br><br>SuperOcc~~-~~L|~~-S~~|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|~~Super~~<br>SuperOcc~~-~~M<br>ALOcc~~-~~<br><br><br>US~~-~~L|~~Occ-T~~<br>2D~~-~~mini|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|OPUS~~-~~S<br>~~OPUS-M~~<br><br>P~~-~~Flas<br>FB~~-~~Occ|hOcc(8f)|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|~~OPUS-T~~<br>~~SparseOcc(16f)~~<br>|~~P-FlashOcc(1f)~~<br>P~~-~~FlashOcc(2f)|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|SparseOcc(8f)<br>8f)|~~P-FlashOcc-T(1f~~|
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|||
|~~0~~<br>~~5~~<br>28<br>30<br>32<br>34<br>36<br>38<br>40<br>42<br>44<br>46<br>RayIoU (%)<br>ALOcc~~-~~2D<br>ALOcc~~-~~3D<br>OP<br>BEVD<br>BEVDetOcc(<br>~~BEVFormer~~|~~10~~<br>~~15~~<br>~~20~~<br>~~25~~<br>~~3~~<br> <br>etOcc(2f)|~~35~~<br>~~40~~<br>~~4~~<br>|



|26|Col2|Col3|
|---|---|---|
|~~0~~<br>~~5~~<br>16<br>18<br>20<br>22<br>24<br><br>mIoU (%)<br>GaussianWo<br>~~Qua~~<br>GaussianForm<br>GaussianForm<br>SurroundOcc<br>TPVFormer*<br>BEVFormer|SuperOcc~~-~~M<br>SuperOcc~~-~~L||
|~~0~~<br>~~5~~<br>16<br>18<br>20<br>22<br>24<br><br>mIoU (%)<br>GaussianWo<br>~~Qua~~<br>GaussianForm<br>GaussianForm<br>SurroundOcc<br>TPVFormer*<br>BEVFormer||SuperOcc~~-T~~<br>SuperOcc~~-~~S|
|~~0~~<br>~~5~~<br>16<br>18<br>20<br>22<br>24<br><br>mIoU (%)<br>GaussianWo<br>~~Qua~~<br>GaussianForm<br>GaussianForm<br>SurroundOcc<br>TPVFormer*<br>BEVFormer|rld<br>~~dricFormer~~||
|~~0~~<br>~~5~~<br>16<br>18<br>20<br>22<br>24<br><br>mIoU (%)<br>GaussianWo<br>~~Qua~~<br>GaussianForm<br>GaussianForm<br>SurroundOcc<br>TPVFormer*<br>BEVFormer|e~~r-~~2<br>er||
|~~0~~<br>~~5~~<br>16<br>18<br>20<br>22<br>24<br><br>mIoU (%)<br>GaussianWo<br>~~Qua~~<br>GaussianForm<br>GaussianForm<br>SurroundOcc<br>TPVFormer*<br>BEVFormer|~~10~~<br>~~15~~<br>~~20~~|~~25~~<br>~~30~~<br>|


(b) mIoU vs. FPS on SurroundOcc Benchmark



_**Abstract**_ **—3D occupancy prediction plays a pivotal role in**
**the realm of autonomous driving, as it provides a comprehen-**
**sive understanding of the driving environment. Most existing**
**methods construct dense scene representations for occupancy**
**prediction, overlooking the inherent sparsity of real-world driving**
**scenes. Recently, 3D superquadric representation has emerged as**
**a promising sparse alternative to dense scene representations**
**due to the strong geometric expressiveness of superquadrics.**
**However, existing superquadric frameworks still suffer from**
**insufficient temporal modeling, a challenging trade-off between**
**query sparsity and geometric expressiveness, and inefficient**
**superquadric-to-voxel splatting. To address these issues, we**
**propose SuperOcc, a novel framework for superquadric-based 3D**
**occupancy prediction. SuperOcc incorporates three key designs:**
**(1) a cohesive temporal modeling mechanism to simultaneously**
**exploit view-centric and object-centric temporal cues; (2) a multi-**
**superquadric decoding strategy to enhance geometric expres-**
**siveness without sacrificing query sparsity; and (3) an efficient**
**superquadric-to-voxel splatting scheme to improve computational**
**efficiency. Extensive experiments on the SurroundOcc and Occ3D**
**benchmarks demonstrate that SuperOcc achieves state-of-the-art**
**performance while maintaining superior efficiency. The code is**
**available at https://github.com/Yzichen/SuperOcc.**


_**Index Terms**_ **—3D occupancy prediction, autonomous driving,**
**temporal modeling, computer vision**


I. INTRODUCTION

ISION-CENTRIC 3D scene understanding is a core
research direction in autonomous driving. Existing ap# **V**
proaches primarily focused on 3D object detection [1]–[5],
predicting 3D bounding boxes of traffic participants to provide object-level representations. However, they struggle to
handle out-of-vocabulary or arbitrarily shaped objects, which
significantly compromises the safety and reliability of driving
systems [6], [7]. To overcome this limitation, 3D occupancy
prediction divides the scene into voxel grids and jointly
predicts the occupancy state and semantic label of each voxel.
Compared with 3D object detection, occupancy prediction


Zichen Yu, Quanli Liu, Wei Wang and Liyong Zhang are with the School
of Control Science and Engineering, Dalian University of Technology, Dalian
116024, China, and also with the Dalian Rail Transmit Intelligent Control and
Intelligent Operation Technology Innovation Center, Dalian 116024, China (email: yuzichen@mail.dlut.edu.cn; liuql@dlut.edu.cn; wangwei@dlut.edu.cn;
zhly@dlut.edu.cn).
Xiaoguang Zhao is with the Dalian Rail Transmit Intelligent Control and
Intelligent Operation Technology Innovation Center, Dalian 116024, China,
and also with the Dalian Seasky Automation Co., Ltd, Dalian 116024, China
(e-mail: xiaoguang.zhao@dlssa.com).
This work was supported in part by the National Natural Science Foundation of China under Grant 62373077, in part by the Science and Technology
Joint Plan of Liaoning Province under Grant 2024JH2/102600014, in part
by the Fundamental Research Funds for the Central Universities under Grant
DUTZD25203 and in part by the Key Field Innovation Team Support Plan of
Dalian, China, under Grant 2021RT02. _(Corresponding author: Quanli Liu)._



Fig. 1. Comparison of speed-accuracy trade-offs of different methods on
(a) Occ3D and (b) SurroundOcc benchmarks. SuperOcc achieves the optimal
trade-off, delivering high accuracy while maintaining efficient inference.


offer a unified representation of all scene elements, enabling
more comprehensive and fine-grained scene understanding.

3D occupation prediction is essentially a 3D semantic
segmentation task. Therefore, early methods [6]–[10] directly
generate dense volumetric feature representations followed
by per-voxel classification. These methods are intuitive and
capable of capturing sufficient geometric details, but they
suffer from high computational complexity and memory consumption. To alleviate this burden, subsequent studies have
extensively explored alternative dense representations [11]–

[13] and more efficient learning schemes [14], [15]. Although
these methods effectively reduce computational costs, they
still fail to fully exploit the spatial sparsity of real-world
scenes, i.e., the vast majority of 3D space is actually free.
Consequently, performing uniform computation and prediction
across these free regions introduces considerable computa






















(a) RayIoU vs. FPS on Occ3D Benchmark






tional redundancy and resource waste.

To align with the inherent sparsity of driving scenes, recent research [16]–[21] has begun exploring sparse scene
representations. Gaussian-based methods [18], [19] describe
scenes sparsely using a set of 3D semantic gaussians, and
each of them is responsible for modeling a flexible region
of interest. However, the strong elliptical priors limit their
ability to model diverse geometric shapes. Superquadric-based
methods [21], [22] address this limitation by introducing
geometrically expressive superquadrics as scene primitives.
With a compact parametric representation, superquadrics can
generate a rich variety of geometric shapes, allowing complex
structures to be modeled effectively without dense stacking.
Despite the significant progress made by superquadric-based
methods in scene representation, they still suffer from the
following key limitations:


_•_ **Insufficient Temporal Modeling** . Existing temporalaware methods are restricted to a singular modeling
paradigm, i.e., view-centric or object-centric temporal
modeling. The former exploits historical image features
to provide fine-grained spatio-temporal context, while
the latter propagates temporal information via sparse
queries focusing on high-level semantic and geometric
abstractions. The absence of a unified temporal modeling
strategy limits the effective utilization of temporal cues
in dynamic scenes.

_•_ **Challenging Trade-off between Query Sparsity and**
**Geometric fidelity** . Current frameworks decode each
query into a single superquadric primitive, which severely
constrains the geometric expressiveness of individual
queries. As a result, it is difficult to accurately reconstruct
complex driving scenarios with a limited number of
queries, forcing a challenging trade-off between geometric fidelity and query sparsity.

_•_ **Inefficient Superquadric-to-Voxel Splatting** . The implementation of the superquadric-to-voxel splatting lacks
operator-level optimization tailored for the CUDA architecture, resulting in redundant memory usage and inefficient computation. This constrains the model’s deployment potential in resource-constrained in-vehicle systems.


To address these limitations, we propose a novel framework
SuperOcc for superquadric-based 3D occupancy prediction.
It mainly contains three key designs: 1) The **cohesive tem-**
**poral modeling mechanism** is developed to simultaneously
leverage view-centric and object-centric temporal modeling
within a unified framework. For the former, we maintain
a memory queue to cache historical image features, which
are sampled and aggregated to provide fine-grained spatiotemporal context. For the latter, sparse queries serve as hidden
states for temporal propagation. Historical queries are stored
in a memory queue and propagated to the subsequent frame
following temporal alignment. They can provide informative
spatial and semantic priors for subsequent predictions, since
the evolution of the driving scenarios is continuous. This dualpath integration enables the model to comprehensively exploit
temporal cues, improving both the accuracy and stability of
occupancy predictions in dynamic driving scenarios. 2) The



2


**multi-superquadric decoding strategy** is designed to enhance the model’s geometric expressiveness while preserving
query sparsity. Built upon the superquadric-based scene representation, this strategy decodes each query into a local cluster
of superquadrics in a coarse-to-fine manner. This allows a
single query to model intricate geometric structures with high
fidelity. Consequently, complex 3D scenes can be accurately
represented even with only a few hundred sparse queries.
3) The **efficient superquadric-to-voxel splatting scheme**
is implemented to accelerate the transformation from 3D
superquadric representations to 3D semantic occupancy. By
introducing a tile-level binning strategy and leveraging GPU
shared memory, this optimization substantially reduces computational complexity and memory access overhead, significantly
improving both training and inference efficiency.
Overall, our contributions are summarized as follows:


_•_ We propose SuperOcc, a novel 3D occupancy prediction
framework that introduces a cohesive temporal modeling
mechanism. This mechanism captures fine-grained spatiotemporal context from historical image features and efficiently leverages informative historical priors through
query propagation, thereby enhancing occupancy modeling accuracy in dynamic driving scenarios.

_•_ We design a multi-superquadric decoding strategy to
boost the geometric expressiveness of sparse queries,
alongside an efficient superquadric-to-voxel splatting implementation to significantly reduce training costs and
improve inference efficiency.

_•_ We evaluate SuperOcc on the challenging Occ3D [6]
and SurroundOcc [9] benchmarks, achieving state-ofthe-art (SOTA) performance while maintaining superior
efficiency, as illustrated in Fig. 1.


II. RELATED WORKS


_A. Camera-based 3D Occupancy Prediction_


_1) Dense Scene Representation:_ Dense scene representation is the most straightforward modeling paradigm. Existing methods [6], [8]–[10], [14], [15], [23]–[26] typically
discretizes the 3D space into voxel grids to construct a
dense volume representation. FB-OCC [8] employs depthaware back-projection to complete and enhance the initial
3D volume generated by forward projection. SurroundOcc [9]
adopts a 2D-3D spatial attention mechanism to integrate multicamera information into 3D volume queries and construct
3D volume features in a multi-scale manner. Although these
methods are capable of capturing rich geometric details, they
suffer from high computational cost and memory overhead.
Subsequent works therefore aim to improve the efficiency of
dense volume construction and processing. OccFormer [10]
decomposes heavy 3D processing into local and global transformer pathways along the horizontal plane to efficiently process the 3D volume. CTF-Occ [6] introduces an incremental
token selection strategy to mitigate the computational burden
caused by dense sampling. To better balance performance and
efficiency, COTR [15], PanoOcc [14] first generate a compact
low-resolution volume and then recover the geometric details
via voxel upsampling.


In addition to directly constructing the 3D volume, some
methods [11]–[13], [27], [28] seek more efficient dense
representations. TPVFormer [13] proposes a Tri-perspective
view (TPV) representation, where each TPV plane aggregates
image features through deformable attention, and each voxel
is modeled as the sum of its projected features onto the
three planes. To further enhance deployment friendliness,
FlashOcc [11] compresses the 3D scene into a BEV representation and adopts a channel-to-height decoding strategy to
directly infer voxel-level occupancy from it.
_2) Sparse Scene Representation:_ Given the spatial sparsity of driving scenarios, sparse scene representation [16]–

[21], [29], [30] focuses on modeling non-free areas, thereby
avoiding redundant computations for large free spaces.
SparseOcc [16] proposes a sparse voxel decoder that progressively reconstructs sparse 3D voxel representations of
occupied regions through multi-stage sparsification and refinement. However, the sparsification process heavily relies on the
accurate estimation of voxel occupancy scores. OPUS [17]
predicts the locations and semantic classes of occupied points
with a set of learnable queries, eliminating the need for explicit
space modeling or complex sparsification procedures. Recent
advances adopt parametric geometric primitives as the fundamental units for scene representation. GaussianFormer [18]
and GaussianFormer-2 [19] utilize a set of 3D semantic
gaussians to sparsely describe a 3D scene, where each gaussian
is responsible for modeling a flexible region of interest. However, the inherent ellipsoidal shape priors limit their ability
to model diverse structures. Therefore, QuadricFormer [21]
further proposes the 3D semantic superquadric representation,
which can efficiently represent complex structures with fewer
primitives due to the stronger geometric expressiveness of
superquadrics.


_B. Camera-based Temporal Modeling_


Camera-based temporal modeling aims to exploit historical
observations to improve spatial understanding and temporal
consistency in 3D perception. Existing methods can be broadly
categorized into view-centric and object-centric temporal modeling paradigms.
The view-centric paradigm extracts historical information
from structured scene representations across frames, such as
perspective view (PV), bird’s-eye-view (BEV), or 3D volume
representations. Specifically, PV-based methods [4], [5], [16],

[17], [31] typically perform temporal modeling by interacting
multi-frame PV features with object queries. For example,
PETRv2 [4] utilizes 3D position embedding to transform
multi-frame, multi-view 2D features into 3D position-aware
features, and then object queries interact with these features using global cross-attention. To improve the interaction
efficiency, SparseBEV [5], Sparse4D [31], SparseOcc [16],
OPUS [17] adopt adaptive sparse sampling and aggregation. In
parallel, BEV/volume-based methods [2], [14], [32]–[41] focus on the fusion of multi-frame BEV/volume features. SOLOFusion [32], PanoOcc [14], GSD-Occ [33] and ALOcc [34]
align historical features to the current frame based on egomotion, and then fuse them with current features via feature



3


concatenation. BEVFormer [2] and ViewFormer [36] utilize
temporal self-attention to extract temporal information from
history BEV features.
The object-centric paradigm propagates temporal information with sparse queries. Representative methods such as
StreamPETR [42], Sparse4Dv2 [43], Sparse4Dv3 [44] propagate temporal information through query propagation and temporal cross-attention between current and historical queries.


III. METHOD


_A. Preliminaries: 3D Semantic Superquadric Representation_


The 3D semantic superquadric representation utilizes a set
of general posed superquadric primitives to describe a 3D
scene:

_S_ = _{_ **S** _[i]_ _}_ _[M]_ _i_ =1 [=] �� **m** _[i]_ _,_ **r** _[i]_ _,_ **s** _[i]_ _, ϵ_ _[i]_ _, σ_ _[i]_ _,_ **c** _[i]_ [��] _[M]_ _i_ =1 _[,]_ (1)

where the center **m** = ( _mx, my, mz_ ) _[T]_ and rotation **r** _∈_ R [4]

specify the pose of the superellipsoid. **s** = ( _sx, sy, sz_ ) _[T]_

determines the scaling factors along the three axes, while the
squareness parameter _ϵ_ = ( _ϵ_ 1 _, ϵ_ 2) _[T]_ controls the squareness of
the shape. Opacity _σ_ and semantic probability **c** are equipped
to incorporate semantic information.
QuadricFormer [21] interprets each superquadric as a continuous occupancy field that describes the occupancy probability in its local neighborhood rather than a hard surface
boundary. Specifically, for a 3D point **p**, the probability of it
being occupied by the superquadric **S** is defined as:



The semantic prediction of the point **p** is formulated as
a weighted aggregation of the semantic predictions of all
contributing superquadrics:


     - _M_
_i_ =1 _[p][o]_ [(] **[p]** [;] **[ S]** _[i]_ [)] _[ σ][i]_ **[c]** _[i]_
_ps_ ( **p** ) = ~~�~~ _M_ _[.]_ (5)
_j_ =1 _[p][o]_ [(] **[p]** [;] **[ S]** _[j]_ [)] _[σ][j]_


Finally, the semantic occupancy prediction is generated as:


_o_ ( **p** ) = [ _po_ ( **p** ) _· ps_ ( **p** ); 1 _−_ _po_ ( **p** )] _,_ (6)


where _po_ ( **p** ) weights the semantic prediction and 1 _−_ _po_ ( **p** )
corresponds to the probability of the empty class.



_ϵ_ 1 + - _|z_ _[′]_ _|_



_,_ (2)







_sy_




- _ϵ_ [2]



_ϵ_ [2] 1 ��




- _ϵ_ [2]




 - _[ϵ]_ _ϵ_ [2] 1
_ϵ_ [2] 2



��� _|x_ _[′]_ _|_

_sx_




- _ϵ_ [2]



_ϵ_ 2 + - _|y_ _[′]_ _|_



_po_ ( **p** ; **S** ) = exp



_−λ_



_sz_



where _λ_ is a temperature parameter controlling the decay rate
of the occupancy probability, and **p** _[′]_ = ( _x_ _[′]_ _, y_ _[′]_ _, z_ _[′]_ ) _[T]_ denotes
the coordinates of point **p** in the local coordinate system of the
superquadric. The local coordinates are obtained via a rotation
and translation:
**p** _[′]_ = **R** _[T]_ ( **p** _−_ **m** ) _,_ (3)


where **R** _∈_ _SO_ (3) is the rotation matrix derived from the
rotation **r** . Assuming that the occupancy events associated with
different superquadrics at point **p** are mutually independent,
the overall occupancy probability is derived as the complement
of the joint non-occupancy probability:



_po_ ( **p** ) = 1 _−_



_M_



_i_ =1



�1 _−_ _po_ ( **p** ; **S** _[i]_ )� _._ (4)


4



























Fig. 2. Framework of SuperOcc. Given multi-view image sequences, SuperOcc constructs a superquadric-based sparse scene representation for 3D occupancy
prediction. To achieve comprehensive temporal modeling, the framework extracts fine-grained spatio-temporal context through the interaction of sparse queries
with multi-frame image features, while efficiently exploiting informative historical priors via query propagation. Furthermore, each updated query is decoded
into a set of semantic superquadrics through a multi-superquadric decoding strategy. Finally, voxel-level occupancy prediction is generated through an efficient
superquadric-to-voxel splatting process.



_B. Overall framework_


As illustrated in Fig. 2, SuperOcc follows an encoderdecoder architecture. The image encoder comprises a backbone (e.g., ResNet50 [45]) and a neck (e.g., FPN [46]). Given
multi-view images at time _t_ as input, the image encoder
extracts multi-scale features denoted as _I_ _[t]_ = _{I_ _[t,v,l]_ _∈_
R _[C][×][H][l][×][W][l]_ _|_ 1 _≤_ _l ≤_ _L,_ 1 _≤_ _v ≤_ _V }_, where _V_ and _L_ represent
the number of camera views and feature scales, respectively.
_Hl_ and _Wl_ denote the feature resolution at the _l_ -th feature
scale. To leverage temporal information, the image features
from the most recent _T_ frames are maintained in a memory
queue, denoted as _I_ = _{I_ _[t]_ _}_ _[t]_ _t_ [0] = _t_ 0 _−T_ [.]
Subsequently, we initialize a set of queries _Q_ = _{_ **q** _[i]_ _∈_
R _[C]_ _}_ _[N]_ _i_ =1 _[q]_ [and a corresponding set of 3D reference points]
_Pref_ = _{_ **p** _[i]_ _ref_ _[∈]_ [R][3] _[}]_ _i_ _[N]_ =1 _[q]_ [in the decoder. To achieve compre-]
hensive temporal modeling, SuperOcc integrates view-centric
and object-centric paradigms within a unified framework.
Specifically, the view-centric path adaptively samples and
aggregates multi-frame image features from a memory queue
to capture detailed spatio-temporal cues guided by the queries.
Simultaneously, the object-centric path propagates historical
queries to the subsequent frame, enabling the preservation and
reuse of object-level semantic and spatial priors across frames.
These queries then interact globally through adaptive selfattention [5]. Finally, the updated query features are decoded
into geometric parameters and semantic logits for a set of
superquadrics, which are then transformed into voxel-level



occupancy prediction via an efficient superquadric-to-voxel
splatting scheme.


_C. Cohesive Temporal Modeling_


_1) View-centric Temporal Modeling:_ The view-centric path
is designed to extract fine-grained temporal cues from historical observation sequences. By leveraging geometric constraints and ego-motion compensation, it adaptively samples
spatio-temporal features from the image feature queue _I_ and
aggregates them into a unified 4D representation.
Specifically, for each query **q** and its corresponding 3D
reference point **p** _ref_ = ( _x, y, z_ ), we first utilize a linear layer
to predict a set of sampling offsets:


_{_ ( _△x_ _[i]_ _, △y_ _[i]_ _, △z_ _[i]_ ) _}_ _[N]_ _i_ =1 _[s]_ [= Linear(] **[q]** [)] _[.]_ (7)


These offsets are subsequently added to the reference point to
generate a set of 3D sampling points:


_Ps_ = _{_ **p** _[i]_ = ( _x_ + _△x_ _[i]_ _, y_ + _△y_ _[i]_ _, z_ + _△z_ _[i]_ ) _}_ _[N]_ _i_ =1 _[s]_ _[.]_ (8)


This sampling strategy allows queries to adaptively adjust the
receptive field to capture more discriminative features.
To sample features from any time _t_ within the memory
queue _I_, we project each sampling point onto the hit camera
views based on ego-motion compensation and camera param

eters. The sampled feature _f_ _[t,i]_ for the _i_ -th point at time _t_ is
extracted via bilinear interpolation as follows:



1
_f_ _[t,i]_ =
_|Vhit|_






_v∈Vhit_



_L_

- _w_ _[i,l]_ _· BS_ ( _I_ _[t,v,l]_ _, πv_ ( _Tt_ 0 _→tp_ _[i]_ )) _,_ (9)


_l_ =1



where _Vhit_ is the set of hit camera views, _Tt_ 0 _→t ∈_ R [4] _[×]_ [4]

denotes the ego-motion transformation matrix that transforms
the 3D coordinates from the current coordinate frame at time
_t_ 0 to the target frame at time _t_ . _πv_ ( _·_ ) denotes the projection
function onto the _v_ -th image plane, and _BS_ ( _·_ ) denotes the
bilinear sampling function. The term _w_ _[i,l]_ represents the weight
predicted by the query for the _i_ -th sampling point on the _l_ -th
feature scale.
To aggregate the spatio-temporal evidence from different
timestamps and sampling points, we organize the extracted
features into _fst ∈_ R _[M]_ _[×][C]_, where _M_ = _T × Ns_ represents
the total number of sampled features across _T_ frames. Then,
adaptive mixing [5], [17] is employed to transform _fst_ along
the channel and spatial dimensions under the guidance of the
query. The resulting features are then aggregated into the query
feature to produce an updated representation.
_2) Object-centric Temporal Modeling:_ Sparse queries compactly encode the geometric and semantic information of the
scene. Given the inherent temporal continuity of the evolution
of driving scenarios, sparse query representations in adjacent
frames exhibit strong correlation. Based on this observation,
object-centric temporal modeling propagates these queries
across frames, enabling efficient exploitation of informative
historical spatial and semantic priors.
Specifically, we maintain a memory queue of size _Np_
to store selected queries from the previous frame. Since
these selected queries are expected to predominantly cover
foreground regions, an effective selection strategy is crucial.
In our formulation, the occupancy field associated with each
query is determined by its predicted superquadric parameters.
In particular, the scale vector **s** = ( _sx, sy, sz_ ) _[T]_ controls the
spatial extent of the field and thus reflects the geometric
contribution of the query to the foreground. A larger scale
typically indicates a more substantial contribution. Thus, we
define the foreground score _ζ_ of each query as the maximum
scale among its _K_ predicted superquadrics:



5


_D. Multi-Superquadric Decoding Strategy_


Existing superquadric-based methods [21], [22] typically
decode each query into a single superquadric primitive. However, a single superquadric struggles to accurately represent
irregular local geometry in real-world driving scenes, which
forces the model to rely on high-density queries to achieve
adequate geometric fidelity. To amplify the geometric expressiveness of the model without compromising the sparsity of
the query representation, we propose the multi-superquadric
decoding strategy. Under this strategy, each query **q** predicts a local superquadric cluster _Slocal_ composed of _K_
superquadrics:

_Slocal_ = _{_ **S** _[k]_ _}_ _[K]_ _k_ =1 [=] _[ {]_ [(] **[p]** _[ref]_ [ +] **[ o]** _[k][,]_ **[ r]** _[k][,]_ **[ s]** _[k][, ϵ][k][, σ][k][,]_ **[ c]** [)]  - _Kk_ =1 _[,]_ [ (11)]

where **o** _[k]_ represents the predicted offset of the _k_ -th superquadric center relative to the reference point **p** _ref_ . All
geometric parameters together with the opacity are generated
by a regression head _ϕ_ reg:

_{_ ( **o** _[k]_ _,_ **r** _[k]_ _,_ **s** _[k]_ _, ϵ_ _[k]_ _, a_ _[k]_ ) _}_ _[K]_ _k_ =1 [=] _[ ϕ]_ [reg][(] **[q]** [)] _[.]_ (12)


The semantic logits **c** are shared across the cluster and
predicted by a classification head _ϕ_ cls:


**c** = _ϕ_ cls( **q** ) _._ (13)


Furthermore, we employ a coarse-to-fine prediction mechanism to guide the decoder in smoothly transitioning from
coarse structural approximation to fine-grained geometric refinement. Specifically, as the decoding depth increases, the
number of superquadrics _K_ predicted by each query is progressively increased.


_E. Efficient Superquadric-to-Voxel Splatting_


The superquadric-to-voxel splatting aims to transform the
predicted superquadrics into a volumetric semantic occupancy
representation via a local aggregation operator. Considering the locality of the superquadric distribution, QuadricFormer [21] first estimates the splatting radius based on the
superquadric scale **s**, and then determines the contributing
superquadrics for each voxel through a matching and sorting
procedure. During the splatting phase, a dedicated thread is
assigned to each voxel to compute its occupancy probability
and semantic logits.
However, this voxel-level binning strategy suffers from
two major bottlenecks: 1) The massive number of matching
pairs formed between superquadrics and voxels significantly
escalates the computational complexity of the sorting process.
2) Neighboring voxels typically interact with highly similar
sets of superquadrics. However, their corresponding threads
necessitate redundant global memory transactions, leading to
substantial memory bandwidth overhead.
To address these issues, inspired by the tile-based rasterization in gaussian splatting [47], we partition the entire 3D space
into cubic tiles of size 4 _×_ 4 _×_ 4 voxels and perform binning
operation at the tile level. This fundamentally reduces the
population of matching pairs, thereby alleviating the sorting
complexity. During the splatting stage, each tile is processed
by a CUDA thread block, and each thread within the block



_ζ_ = max
_k∈{_ 1 _,...,K}_



�max( _s_ _[k]_ _x_ _[, s][k]_ _y_ _[, s][k]_ _z_ [)] - _._ (10)



At each frame, the top- _Np_ queries with the highest foreground scores and their reference points _Pp_ are pushed into a
memory queue. After temporal alignment, they are propagated
to the next frame. To alleviate potential spatial redundancy
between propagated queries and newly initialized ones, we
discard initialized queries whose reference points lie within
a distance threshold _τ_ (e.g., _τ_ = 1 _._ 0 m) of any propagated
reference point. Then, the query set is complemented by
stochastically sampling _Nq −_ _Np_ candidates from the residual
initialization queries to maintain a constant budget _Nq_ . This
ensures that the hybrid queries provide both stable temporal
priors for persistent objects and sufficient exploratory coverage
for newborn objects.


still handles one voxel. Crucially, all superquadrics associated
with a tile are coalesced and loaded into shared memory for
collective reuse by all threads in the block. This mechanism
drastically suppresses global memory traffic and significantly
enhances overall computational throughput.


IV. EXPERIMENTS


_A. Datasets and Evaluation Metric_


nuScenes [48] is a large-scale autonomous driving dataset
that provides multi-sensor data collected from six cameras, one
LiDAR, and five radars. It consists of 1,000 driving sequences,
which are divided into 700 for training, 150 for validation, and
150 for testing. Each sequence lasts approximately 20 seconds,
with a keyframe sampled every 0.5 seconds for annotation. The
Occ3D [48] and SurroundOcc [9] benchmarks provide dense
semantic occupancy annotations for the nuScenes dataset. The
occupancy annotations in Occ3D span [-40m, 40m] along
the X and Y axes, [-1m, 5.4m] along the Z axis, with a
resolution of 200 _×_ 200 _×_ 16. It contains 17 semantic classes
and one empty class. In comparison, SurroundOcc extends
the spatial range to [-50m, 50m] along the X and Y axes
and [-5m, 3m] along the Z axis, while maintaining the same
200 _×_ 200 _×_ 16 resolution. Each voxel is labeled as one of 18
classes, including 16 semantic classes, one empty class, and
one unknown class.
We adopt two evaluation metrics for SurroundOcc: the
Intersection-over-Union (IoU) of occupied voxels ignoring
semantic classes, and the mean IoU (mIoU) across all semantic
classes. They are defined as:

IoU = _TP_ = _c_ 0 _,_ (14)
_TP_ = _c_ 0 + _FP_ = _c_ 0 + _FN_ = _c_ 0



6


TABLE I
CONFIGURATIONS FOR DIFFERENT VARIANTS

|Model|Nq Ns Np|superquadric number<br>K1 K2 K3 K4 K5 K6|
|---|---|---|
|SuperOcc-T<br>SuperOcc-S<br>SuperOcc-M<br>SuperOcc-L|600<br>4<br>500<br>1200<br>2<br>1000<br>2400<br>2<br>2000<br>3600<br>2<br>3000|2<br>2<br>4<br>4<br>8<br>8<br>2<br>2<br>4<br>4<br>8<br>8<br>1<br>1<br>2<br>2<br>4<br>4<br>1<br>1<br>2<br>2<br>4<br>4|



models are trained for 48 epochs on Occ3D [6] and 24 epochs
on SurroundOcc [9], respectively. For ablation studies, all
experiments are conducted using the SuperOcc-T variant and
trained for 24 epoch on Occ3D. FPS is evaluated on a single
NVIDIA RTX 4090 GPU using the PyTorch FP32 backend
with a batch size of 1.


_C. Main Results_


_1) Comparasion on Occ3D:_ We compare SuperOcc with
recent occupancy prediction methods on the Occ3D [6] benchmark. As reported in Tab. II, SuperOcc establishes new SOTA
performance while achieving a superior trade-off between
prediction accuracy and inference efficiency.

Compared with the representative sparse predictor
OPUS [17], SuperOcc-T achieves 42.5% RayIoU and
30.3 FPS, surpassing OPUS-L in prediction accuracy and
outperforming OPUS-T in inference speed. By scaling up
the model, SuperOcc-S/M/L achieve further performance
improvements. In particular, SuperOcc-L reaches 38.1%
mIoU and 44.0% RayIoU, outperforming the previous SOTA
ALOcc-3D [34] by 0.3% RayIoU while maintaining nearly
twice the inference throughput of ALOcc-3D (12.7 vs.
6.0 FPS). Moreover, these results demonstrate the strong
scalability of SuperOcc: by simply adjusting parameters such
as the number of queries, the model achieves a flexible and
controllable trade-off between accuracy and efficiency. This
enables SuperOcc to meet diverse real-world requirements
for both real-time performance and high prediction accuracy.


_2) Comparasion on SurroundOcc:_ In Tab. III, we present a
comprehensive comparison with other SOTA 3D semantic occupancy prediction methods on the SurroundOcc benchmark.
Our SuperOcc family consistently demonstrates a commanding
performance lead over existing approaches.

Specifically, compared with the advanced temporal-aware
predictor GaussianWorld [20], SuperOcc achieves higher IoU
and mIoU even under a weaker setting (ResNet-50 [45]
vs. ResNet101-DCN [45], and input resolution 256 _×_ 704
vs. 900 _×_ 1600). Furthermore, compared with the recent
superquadric-based predictor QuadricFormer [21], our method
also exhibits significant performance margins. In particular,
SuperOcc-T outperforms QuadricFormer by 3.69% IoU and
2.36% mIoU, while providing substantially higher inference
throughput. These results collectively establish SuperOcc as
a more effective and efficient superquadric-based occupancy
prediction framework.



mIoU = [1]




 _C_ _[′]_



_i∈C_ _[′]_



_TPi_
_,_ (15)
_TPi_ + _FPi_ + _FNi_



where _C_ _[′]_, _c_ 0, _TP_, _FP_, _FN_ denote the nonempty classes, the
empty class, the number of true positive, false positive and
false negative predictions, respectively. For Occ3D, we report
the mIoU and the Ray-based IoU (RayIoU) [16]. RayIoU is
computed at three distance thresholds of 1, 2, and 4 meters,
denoted as RayIoU1 _m_, RayIoU2 _m_, and RayIoU4 _m_ . The final
RayIoU score is obtained by averaging these three values.


_B. Implementation Details_


Following previous works [16], [17], the input images are
resized to 704 _×_ 256. We adopt a ResNet-50 [45] backbone
with a FPN [46] neck as the image encoder, where the
backbone is initialized by nuImages [48] pre-training. To
explore the trade-off between efficiency and performance,
we scale our model into four variants, namely SuperOcc-T,
SuperOcc-S, SuperOcc-M, and SuperOcc-L. These variants
employ 0.6K, 1.2K, 2.4K, and 3.6K queries, respectively. The
specific configurations for each variant are detailed in Tab. I.
During training, we employ the AdamW [49] optimizer with
a weight decay of 1 _×_ 10 _[−]_ [2] and a batch size of 8. The learning
rate is initialized at 2 _×_ 10 _[−]_ [4] and decayed with a cosine
annealing schedule. For comparison with SOTA methods, the


7


TABLE II
**OCCUPANCY PREDICTION PERFORMANCE ON OCC3D [6] BENCHMARK** ”8F” AND ”16F” DENOTE MODELS FUSING TEMPORAL INFORMATION FROM 8
OR 16 FRAMES, RESPECTIVELY. FPS RESULTS ARE MEASURED ON A SINGLE RTX 4090 GPU

|Methods|Backbone Image Size Epoch|Vis.Mask|mIoU|RayIoU RayIoU RayIoU<br>1m 2m 4m|RayIoU|FPS|
|---|---|---|---|---|---|---|
|RenderOcc [50]<br>BEVFormer [2]<br>BEVDer-Occ (2f) [1]<br>BEVDer-Occ (8f) [1]<br>SparseOcc (8f) [16]<br>SparseOcc (16f) [16]<br>P-FlashOcc-Tiny (1f) [12]<br>P-FlashOcc (1f) [12]<br>P-FlashOcc (2f) [12]<br>P-FlashOcc (8f) [12]<br>GSD-Occ (16f) [33]<br>FB-Occ (16f) [8]<br>OPUS-T (8f) [17]<br>OPUS-S (8f) [17]<br>OPUS-M (8f) [17]<br>OPUS-L (8f) [17]<br>STCOcc (8f) [29]<br>ALOcc-2D-mini (16f) [34]<br>ALOcc-2D (16f) [34]<br>ALOcc-3D (16f) [34]|Swin-B<br>1408_ ×_ 512<br>12<br>R101<br>1600_ ×_ 900<br>24<br>R50<br>704_ ×_ 256<br>90<br>R50<br>704_ ×_ 384<br>90<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>24<br>R50<br>704_ ×_ 256<br>100<br>R50<br>704_ ×_ 256<br>100<br>R50<br>704_ ×_ 256<br>100<br>R50<br>704_ ×_ 256<br>100<br>R50<br>704_ ×_ 256<br>36<br>R50<br>704_ ×_ 256<br>54<br>R50<br>704_ ×_ 256<br>54<br>R50<br>704_ ×_ 256<br>54|✓<br>✓<br>✓<br>✓<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-<br>-|24.5<br>39.3<br>36.1<br>39.3<br>30.9<br>30.6<br>29.1<br>29.4<br>30.3<br>31.6<br>31.8<br>31.1<br>33.2<br>34.2<br>35.6<br>36.2<br>-<br>33.4<br>37.4<br>38.0|13.4<br>19.6<br>25.5<br>26.1<br>32.9<br>38.0<br>23.6<br>30.0<br>35.1<br>26.6<br>33.1<br>38.2<br>28.0<br>34.7<br>39.4<br>29.1<br>35.8<br>40.3<br>29.1<br>35.7<br>39.7<br>29.4<br>36.0<br>40.1<br>31.2<br>37.6<br>41.5<br>32.8<br>39.3<br>43.4<br>-<br>-<br>-<br>33.0<br>40.0<br>44.0<br>31.7<br>39.2<br>44.3<br>32.6<br>39.9<br>44.7<br>33.7<br>41.1<br>46.0<br>34.7<br>42.1<br>46.7<br>36.2<br>42.7<br>46.4<br>32.9<br>40.1<br>44.8<br>37.1<br>43.8<br>48.2<br>37.8<br>44.7<br>48.8|19.5<br>32.4<br>29.6<br>32.6<br>34.0<br>35.1<br>34.8<br>35.2<br>36.8<br>38.5<br>38.9<br>39.0<br>38.4<br>39.1<br>40.3<br>41.2<br>41.7<br>39.3<br>43.0<br>43.7|-<br>4.4<br>9.0<br>5.6<br>20.9<br>15.9<br>41.9<br>33.8<br>29.8<br>26.8<br>-<br>10.3<br>25.1<br>24.5<br>17.8<br>8.6<br>-<br>30.5<br>8.2<br>6.0|
|SuperOcc-T (8f)<br>SuperOcc-S (8f)<br>SuperOcc-M (8f)<br>SuperOcc-L (8f)|R50<br>704_ ×_ 256<br>48<br>R50<br>704_ ×_ 256<br>48<br>R50<br>704_ ×_ 256<br>48<br>R50<br>704_ ×_ 256<br>48|-<br>-<br>-<br>-|36.1<br>36.9<br>37.4<br>38.1|36.6<br>43.4<br>47.6<br>37.2<br>43.7<br>48.0<br>37.8<br>44.4<br>48.6<br>38.3<br>44.8<br>48.9|42.5<br>43.0<br>43.6<br>44.0|30.3<br>28.2<br>18.8<br>12.7|



TABLE III
**3D SEMANTIC OCCUPANCY PREDICTION RESULTS ON SURROUNDOCC [9] BENCHMARK** . * MEANS SUPERVISED BY DENSE OCCUPANCY
ANNOTATIONS AS OPPOSED TO ORIGINAL LIDAR SEGMENTATION LABELS. FPS RESULTS ARE MEASURED ON A SINGLE RTX 4090 GPU








|Method|IoU mIoU|■barrier ■bicycle ■bus ■car ■const. veh. ■motorcycle ■pedestrian ■traf i fc cone ■trailer ■truck ■drive. suf. ■other l fat ■sidewalk ■terrain ■manmade ■vegetation|FPS|
|---|---|---|---|
|MonoScene [23]<br>Atlas [51]<br>BEVFormer [2]<br>TPVFormer [13]<br>TPVFormer* [13]<br>OccFormer [10]<br>SurroundOcc [9]<br>GaussianFormer [18]<br>GaussianFormer-2 [19]<br>QuadricFormer [21]<br>GaussianWorld [20]|23.96<br>7.31<br>28.66 15.00<br>30.50 16.75<br>11.51 11.66<br>30.86 17.10<br>31.39 19.03<br>31.49 20.30<br>29.83 19.10<br>  30.56 20.02<br>31.22 20.12<br>33.40 22.13|4.03<br>0.35<br>8.00<br>8.04<br>2.90<br>0.28<br>1.16<br>0.67<br>4.01<br>4.35<br>27.72<br>5.20<br>15.13 11.29<br>9.03<br>14.86<br>  10.64<br>5.68<br>19.66 24.94<br>8.90<br>8.84<br>6.47<br>3.28<br>10.42 16.21 34.86 15.46 21.89 20.95 11.21 20.54<br>  14.22<br>6.58<br>23.46 28.28<br>8.66<br>10.77<br>6.64<br>4.05<br>11.20 17.78 37.28 18.00 22.88 22.17 13.80 22.21<br>  16.14<br>7.17<br>22.63 17.13<br>8.83<br>11.39 10.46<br>8.23<br>9.43<br>17.02<br>8.07<br>13.64 13.85 10.34<br>4.90<br>7.37<br>  15.96<br>5.31<br>23.86 27.32<br>9.79<br>8.74<br>7.09<br>5.20<br>10.97 19.22 38.87 21.25 24.26 23.15 11.73 20.81<br>  18.65 10.41 23.92 30.29 10.31 14.19 13.59 10.13 12.49 20.77 38.78 19.79 24.19 22.21 13.48 21.35<br>  20.59 11.68 28.06 30.86 10.70 15.14 14.09 12.06 14.38 22.26 37.29 23.70 24.49 22.77 14.89 21.86<br>  19.52 11.26 26.11 29.78 10.47 13.83 12.58<br>8.67<br>12.74 21.57 39.63 23.28 24.46 22.99<br>9.59<br>19.12<br>    20.15 12.99 27.61 30.23 11.19 15.31 12.64<br>9.63<br>13.31 22.26 39.68 23.47 25.62 23.20 12.25 20.73<br>  19.58 13.11 27.27 29.64 11.25 16.26 12.65<br>9.15<br>12.51 21.24 40.20 24.34 25.69 24.24 12.95 21.86<br>  21.38 14.12 27.71 31.84 13.66 17.43 13.66 11.46 15.09 23.94 42.98 24.86 28.84 26.74 15.69 24.74|-<br>-<br>3.3<br>2.9<br>2.9<br>-<br>3.3<br>2.7<br>2.8<br>-<br>4.4|
|SuperOcc-T<br>SuperOcc-S<br>SuperOcc-M<br>SuperOcc-L|34.91 22.48<br>35.63 23.15<br>36.99 23.95<br>38.13 24.55|21.35 11.90 28.35 32.12 15.46 15.84 13.20 11.83 11.87 23.71 47.30 29.19 30.29 28.22 13.23 25.75<br>  21.96 12.14 28.46 32.86 16.98 16.62 13.60 13.06 12.40 24.52 47.16 29.07 30.81 28.46 14.90 26.87<br>  23.73 12.07 28.46 33.13 17.42 17.68 14.62 14.20 12.87 25.27 47.84 29.33 31.58 29.15 17.28 28.55<br>  23.92 12.43 29.77 33.62 17.36 17.74 14.95 14.79 13.42 26.25 48.22 29.22 32.16 30.20 18.67 30.04|30.3<br>                  28.2<br>                  18.8<br>                  12.7|



TABLE IV
ABLATION OF DIFFERENT COMPONENTS IN SUPEROCC.
MDS: MULTI-SUPERQUADRIC DECODING; CTM: COHESIVE TEMPORAL
MODELING


MSD CTM mIoU RayIoU RayIoU1m, 2m, 4m FPS


27.9 33.3 26.9 34.0 39.0 **33.0**
✓ 29.1 34.9 28.6 35.6 40.5 31.7
✓ 33.8 39.5 33.4 40.4 44.8 31.2
✓ ✓ **35.4** **41.2** **35.0** **41.9** **46.5** 30.3


_D. Ablation Studies_

_1) Component analysis:_ We conduct a comprehensive ablation study in Tab. IV to quantify the individual contribution



of each proposed component in the SuperOcc framework.

The baseline relies solely on single-frame image inputs and
single-superquadric decoding. Starting from the baseline, incorporating the proposed multi-superquadric decoding (MSD)
strategy yields an improvement of 1.2% mIoU and 1.6%
RayIoU, with only a marginal increase in inference latency.
This indicates that MSD effectively enhances geometric expressiveness while completely preserving the query sparsity.
When equipping the baseline with the cohesive temporal
modeling (CTM) mechanism, we observe a substantial performance gain of 5.9% mIoU and 6.2% RayIoU. This remarkable
performance leap underscores the effectiveness and necessity


TABLE V
ABLATION OF MULTI-SUPERQUADRIC DECODING


K1-K6 mIoU RayIoU RayIoU1m, 2m, 4m


[1 _,_ 1 _,_ 1 _,_ 1 _,_ 1 _,_ 1] 27.9 33.3 26.9 34.0 39.0

[2 _,_ 2 _,_ 2 _,_ 2 _,_ 2 _,_ 2] 28.4 33.9 27.4 34.6 39.6

[4 _,_ 4 _,_ 4 _,_ 4 _,_ 4 _,_ 4] 28.8 34.6 28.4 35.3 40.2

[8 _,_ 8 _,_ 8 _,_ 8 _,_ 8 _,_ 8] 28.8 34.9 **28.7** 35.6 40.4

[16 _,_ 16 _,_ 16 _,_ 16 _,_ 16 _,_ 16] 28.7 34.5 28.2 35.1 40.0


[2 _,_ 2 _,_ 4 _,_ 4 _,_ 8 _,_ 8] **29.1** **34.9** 28.6 **35.6** **40.5**


TABLE VI
ABLATION OF COHESIVE TEMPORAL MODELING

|View-centric Object-centric|mIoU RayIoU|RayIoU|
|---|---|---|
|View-centric<br>Object-centric|mIoU<br>RayIoU|RayIoU1m, 2m, 4m|
|✓<br>✓<br>✓<br>✓|29.1<br>34.9<br>34.5<br>39.6<br>31.6<br>37.6<br>**35.4**<br>**41.2**|28.6<br>35.6<br>40.5<br>33.6<br>40.4<br>44.9<br>31.3<br>38.3<br>43.2<br>**35.0**<br>**41.9**<br>**46.5**|



of our CTM mechanism. By combining all components, SuperOcc achieves a significant overall improvement of 7.5%
mIoU and 7.9% RayIoU over the baseline. Although this
configuration incurs a moderate reduction in FPS, it maintains
a real-time inference speed of 30.3 FPS, offering a favorable
trade-off between accuracy and efficiency.
_2) Effect of Multi-Superquadric Decoding:_ The number of
superquadrics predicted per query is a critical factor in our
MSD strategy. As summarized in Tab. V, we conduct an
ablation study to analyze its impact.
The first five rows report the performance when varying
the number of predicted superquadrics without the coarse-tofine mechanism. We observe that occupancy prediction performance improves steadily with the number of superquadrics,
and begins to saturate at _K_ = 8. This demonstrates that
while MSD effectively improves geometric expressiveness, an
optimal number of primitives is essential to balance representational capacity and optimization stability, with _K_ = 8
providing the best trade-off in our settings.
In the final row, we incorporate the coarse-to-fine strategy,
where the number of predicted superquadrics is progressively
increased across decoding stages. This configuration further
boosts performance, achieving the highest mIoU of 29.1%
and RayIoU of 34.9%. This suggests that the coarse-tofine strategy facilitates the learning process by allowing the
network to progressively optimize geometric representations
from coarse structures to fine details.
_3) Effect of Cohesive Temporal Modeling:_ To explore the
impact of different temporal modeling strategies, we conduct
a comprehensive ablation study as shown in Tab. VI. Without
temporal modeling, the baseline achieves limited performance,
reflecting the inherent difficulty of accurately predicting 3D
occupancy from single-frame inputs. It can be observed that
incorporating view-centric temporal modeling significantly
boosts mIoU and RayIoU by 5.4% and 4.7%, respectively. This
demonstrates that the rich spatio-temporal context provided
by historical observations is crucial for inferring the current
occupancy. Meanwhile, object-centric temporal modeling also
yields noticeable gains (+2.5% mIoU and +2.7% RayIoU),



8


TABLE VII
ABLATION STUDY ON DIFFERENT QUERY SELECTION STRATEGIES IN
OBJECT-CENTRIC TEMPORAL MODELING


Selection Strategy mIoU RayIoU RayIoU1m, 2m, 4m


No selection 25.3 31.1 24.9 31.7 36.6
Semantic logit-based 29.5 35.4 29.0 36.1 41.1
Opacity-based 29.3 35.6 29.2 36.2 41.2
Scale-based (ours) **31.6** **37.6** **31.3** **38.3** **43.2**


TABLE VIII
ABLATION OF THE EFFICIENT SUPERQUADRIC-TO-VOXEL SPLATTING

|ESS|Training Phase<br>Mem (GB) Time (h)|Inference Phase<br>Mem (GB) Splatting Latency (ms) FPS|
|---|---|---|
|21.4<br>82<br>3.1<br>6.2<br>25.2<br>✓<br>**17.3** (-19%)<br>**20** (-76%)<br>**2.8** (-9%)<br>**1.3** (-79%)<br>**30.3** (+20%)|21.4<br>82<br>3.1<br>6.2<br>25.2<br>✓<br>**17.3** (-19%)<br>**20** (-76%)<br>**2.8** (-9%)<br>**1.3** (-79%)<br>**30.3** (+20%)|21.4<br>82<br>3.1<br>6.2<br>25.2<br>✓<br>**17.3** (-19%)<br>**20** (-76%)<br>**2.8** (-9%)<br>**1.3** (-79%)<br>**30.3** (+20%)|



suggesting that the spatial and semantic priors propagated
via historical queries can effectively guide the current decoding process. When view-centric and object-centric temporal
modeling are jointly applied, the model achieves the best
performance across all evaluation metrics. This result strongly
validates the complementarity of the two modeling strategies,
demonstrating that their integration is essential for achieving
accurate 3D occupancy prediction.
_4) Effect of Scale-based Query Selection:_ Table VII investigates various query selection strategies for object-centric
temporal modeling. Directly propagating all queries results
in a noticeable performance drop, as the propagated queries
tend to dominate the optimization process and impede the
effective learning of initialization queries. While strategies
based on semantic logits or opacity partially mitigate this,
they achieve suboptimal performance because neither semantic
confidence nor opacity reliably reflects a query’s foreground
contribution. In contrast, our scale-based selection achieves the
best performance. This indicates that scale-based selection is
more effective at identifying foreground queries, thereby providing more stable and efficient temporal priors for subsequent
frames.
_5) Effect of Efficient Superquadric-to-Voxel Splatting:_ To
evaluate the impact of the proposed efficient superquadric-tovoxel splatting (ESS), we conduct a comparative analysis as
summarized in Tab. VIII. The experimental results demonstrate
that ESS significantly reduces computational costs across both
the training and inference stages.
In the training phase, the integration of ESS significantly
boosts training efficiency. Notably, it achieves a 76% reduction in wall-clock training time (from 82 h to 20 h)
while simultaneously lowering the GPU memory footprint by
19% (from 21.4 GB to 17.3 GB). This drastic acceleration
demonstrates that our optimized splatting operator effectively
alleviates the computational bottlenecks in the superquadricto-voxel transformation during both forward and backward
propagation. In the inference phase, ESS slashes the splatting
latency by 79%, from 6.2 ms to 1.3 ms. Consequently, the
overall system throughput is increased by 20%, reaching 30.3
FPS. This reduction in computational overhead significantly
enhances the deployment potential of our framework.


9


**CAM_FRONT_LEFT** **CAM_FRONT** **CAM_FRONT_RIGHT** **3D Superquadrics** **Occupancy Prediction** **Occupancy Ground Truth**


**CAM_FRONT_LEFT** **CAM_FRONT** **CAM_FRONT_RIGHT** **3D Superquadrics** **Occupancy Prediction** **Occupancy Ground Truth**


**CAM_FRONT_LEFT** **CAM_FRONT** **CAM_FRONT_RIGHT** **3D Superquadrics** **Occupancy Prediction** **Occupancy Ground Truth**


**driveable surface** **car** **bus** **truck** **terrain** **vegetation** **sidewalk** **other flat** **pedestrian** **bicycle**


**manmade** **motorcycle** **barrier** **construction vehicle** **trailer** **traffic cone**


Fig. 3. Qualitative results of SuperOcc on the SurroundOcc [9] benchmark. For each scene, the predicted superquadrics, resulting occupancy prediction, and
ground-truth occupancy are shown. Colors indicate semantic categories for both superquadrics and voxels.



_E. Qualitative Results_


Qualitative prediction results of SuperOcc on the SurroundOcc [9] benchmark are shown in Fig. 3. For each scene,
we visualize the predicted superquadrics, the resulting occupancy prediction, and the corresponding ground-truth occupancy. Each superquadric and voxel is color-coded according
to its semantic category.
The predicted superquadric primitives effectively model
the underlying 3D scene geometry in a compact manner.
The occupancy predictions derived from these superquadrics
exhibit high fidelity to the ground truth, preserving clear
structural boundaries between occupied and free space. These
results demonstrate that SuperOcc can accurately reconstruct
fine-grained scene geometries, highlighting its effectiveness in
complex urban environments.


_F. Limitations_


Although our proposed method achieves significant
progress, several limitations remain to be addressed.
First, SuperOcc is restricted to semantic occupancy prediction and has not yet been extended to panoptic occupancy
prediction. Specifically, while it can accurately predict the
semantic category of each occupied voxel, it lacks the ability to



distinguish different individual instances within the same category. However, such instance-level discrimination is crucial
for fine-grained scene understanding in autonomous driving.
Second, SuperOcc primarily relies on ego-motion compensation for temporal alignment, without explicitly modeling the
motion of dynamic objects. As a result, historical features
sampled for dynamic objects may exhibit spatio-temporal
misalignment. Moreover, the temporal propagation of queries
may introduce less reliable spatial priors, which can adversely
affect the occupancy modeling of dynamic objects.
In future work, we plan to extend SuperOcc to panoptic
occupancy prediction by incorporating instance-level representations into the superquadric-based framework. Additionally,
we aim to integrate explicit object motion modeling to enhance
the spatio-temporal consistency for dynamic objects.


V. CONCLUSION


In this paper, we propose SuperOcc, a novel superquadricbased framework for 3D occupancy prediction. By unifying
view-centric and object-centric temporal paradigms, SuperOcc effectively exploits fine-grained spatio-temporal context
together with historical priors, enabling accurate predictions
in dynamic driving environments. Furthermore, SuperOcc


introduces a multi-superquadric decoding strategy that decodes each query into a cluster of superquadrics. This design
significantly bolsters the capacity to model complex scenes
while preserving query sparsity. Additionally, we optimize the
superquadric-to-voxel splatting operation through a CUDAfriendly implementation, dramatically reducing training overhead and splatting latency. Extensive evaluations on public benchmarks validate the effectiveness and efficiency of
SuperOcc, demonstrating its clear advantages over existing
occupancy prediction methods. We hope that SuperOcc can
provide valuable insights into superquadric-based occupancy
prediction for the community.


REFERENCES


[1] J. Huang, G. Huang, Z. Zhu, Y. Yun, and D. Du, “BEVDet: Highperformance multi-camera 3d object detection in bird-eye-view,” 2021,
_arXiv:2112.11790_ .

[2] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and
J. Dai, “BEVFormer: Learning bird’s-eye-view representation from
multi-camera images via spatiotemporal transformers,” in _Proc. Eur._
_Conf. Comput. Vis._ Cham, Switzerland: Springer, Oct. 2022, pp. 1–18.

[3] Y. Liu, T. Wang, X. Zhang, and J. Sun, “PETR: Position embedding
transformation for multi-view 3d object detection,” in _Proc. Eur. Conf._
_Comput. Vis._ Cham, Switzerland: Springer, Oct. 2022, pp. 531–548.

[4] Y. Liu, J. Yan, F. Jia, S. Li, A. Gao, T. Wang, and X. Zhang, “PETRv2:
A unified framework for 3d perception from multi-camera images,” in
_Proc. IEEE/CVF Int. Conf. Comput. Vis._, Oct. 2023, pp. 3262–3272.

[5] H. Liu, Y. Teng, T. Lu, H. Wang, and L. Wang, “SparseBEV: Highperformance sparse 3d object detection from multi-camera videos,” in
_Proc. IEEE/CVF Int. Conf. Comput. Vis._, Oct. 2023, pp. 18580–18590.

[6] X. Tian, T. Jiang, L. Yun, Y. Mao, H. Yang, Y. Wang, Y. Wang, and
H. Zhao, “Occ3d: A large-scale 3d occupancy prediction benchmark for
autonomous driving,” _Proc. Adv. Neural Inform. Process. Syst._, vol. 36,
pp. 64318–64330, Dec. 2023.

[7] W. Tong, C. Sima, T. Wang, L. Chen, S. Wu, H. Deng, Y. Gu, L. Lu,
P. Luo, D. Lin _et al._, “Scene as occupancy,” in _Proc. IEEE/CVF Int._
_Conf. Comput. Vis._, Oct. 2023, pp. 8406–8415.

[8] Z. Li, Z. Yu, D. Austin, M. Fang, S. Lan, J. Kautz, and J. M. Alvarez,
“FB-OCC: 3d occupancy prediction based on forward-backward view
transformation,” 2023, _arXiv:2307.01492_ .

[9] Y. Wei, L. Zhao, W. Zheng, Z. Zhu, J. Zhou, and J. Lu, “Surroundocc:
Multi-camera 3d occupancy prediction for autonomous driving,” in _Proc._
_IEEE/CVF Int. Conf. Comput. Vis._, Oct. 2023, pp. 21729–21740.

[10] Y. Zhang, Z. Zhu, and D. Du, “Occformer: Dual-path transformer for
vision-based 3d semantic occupancy prediction,” in _Proc. IEEE/CVF Int._
_Conf. Comput. Vis._, Oct. 2023, pp. 9433–9443.

[11] Z. Yu, C. Shu, J. Deng, K. Lu, Z. Liu, J. Yu, D. Yang, H. Li, and
Y. Chen, “Flashocc: Fast and memory-efficient occupancy prediction
via channel-to-height plugin,” 2023, _arXiv:2311.12058_ .

[12] Z. Yu, C. Shu, Q. Sun, Y. Bian, X. Wei, J. Yu, Z. Liu, D. Yang, H. Li,
and Y. Chen, “Panoptic-flashocc: An efficient baseline to marry semantic
occupancy with panoptic via instance center,” 2024, _arXiv:2406.10527_ .

[13] Y. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu, “Tri-perspective view
for vision-based 3d semantic occupancy prediction,” in _Proc. IEEE/CVF_
_Conf. Comput. Vis. Pattern Recog._, Jun. 2023, pp. 9223–9232.

[14] Y. Wang, Y. Chen, X. Liao, L. Fan, and Z. Zhang, “Panoocc: Unified
occupancy representation for camera-based 3d panoptic segmentation,”
in _Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog._, Jun. 2024, pp.
17158–17168.

[15] Q. Ma, X. Tan, Y. Qu, L. Ma, Z. Zhang, and Y. Xie, “Cotr: Compact
occupancy transformer for vision-based 3d occupancy prediction,” in
_Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog._, Jun. 2024, pp.
19936–19945.

[16] H. Liu, Y. Chen, H. Wang, Z. Yang, T. Li, J. Zeng, L. Chen, H. Li, and
L. Wang, “Fully sparse 3d occupancy prediction,” in _Proc. Eur. Conf._
_Comput. Vis._ Cham, Switzerland: Springer, Oct. 2024, pp. 54–71.

[17] J. Wang, Z. Liu, Q. Meng, L. Yan, K. Wang, J. Yang, W. Liu, Q. Hou,
and M. Cheng, “Opus: occupancy prediction using a sparse set,” in _Proc._
_Adv. Neural Inform. Process. Syst._, vol. 37, Dec. 2024, pp. 119861–
119885.



10


[18] Y. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu, “Gaussianformer:
Scene as gaussians for vision-based 3d semantic occupancy prediction,”
in _Proc. Eur. Conf. Comput. Vis._ Cham, Switzerland: Springer, 2024,
pp. 376–393.

[19] Y. Huang, A. Thammatadatrakoon, W. Zheng, Y. Zhang, D. Du, and
J. Lu, “Gaussianformer-2: Probabilistic gaussian superposition for efficient 3d occupancy prediction,” in _Proc. IEEE/CVF Conf. Comput. Vis._
_Pattern Recog._, Jun. 2025, pp. 27477–27486.

[20] S. Zuo, W. Zheng, Y. Huang, J. Zhou, and J. Lu, “Gaussianworld:
Gaussian world model for streaming 3d occupancy prediction,” in _Proc._
_IEEE/CVF Conf. Comput. Vis. Pattern Recog._, Jun. 2025, pp. 6772–
6781.

[21] S. Zuo, W. Zheng, X. Han, L. Yang, Y. Pan, and J. Lu, “Quadricformer:
Scene as superquadrics for 3d semantic occupancy prediction,” 2025,
_arXiv:2506.10977_ .

[22] S. Hayes, R. Mohandas, T. Brophy, A. Boulch, G. Sistu, and C. Eising, “Superquadricocc: Multi-layer gaussian approximation of superquadrics for real-time self-supervised occupancy estimation,” 2025,
_arXiv:2511.17361_ .

[23] A.-Q. Cao and R. De Charette, “Monoscene: Monocular 3d semantic
scene completion,” in _Proc. IEEE/CVF Conf. Comput. Vis. Pattern_
_Recog._, Jun. 2022, pp. 3991–4001.

[24] Y. Li, Z. Yu, C. Choy, C. Xiao, J. M. Alvarez, S. Fidler, C. Feng,
and A. Anandkumar, “Voxformer: Sparse voxel transformer for camerabased 3d semantic scene completion,” in _Proc. IEEE/CVF Conf. Comput._
_Vis. Pattern Recog._, Jun. 2023, pp. 9087–9098.

[25] G. Oh, S. Kim, H. Ko, H.-g. Chi, J. Kim, D. Lee, D. Ji, S. Choi,
S. Jang, and S. Kim, “3d occupancy prediction with low-resolution
queries via prototype-aware view transformation,” in _Proc. IEEE/CVF_
_Conf. Comput. Vis. Pattern Recog._, Jun. 2025, pp. 17134–17144.

[26] J. Kim, C. Kang, D. Lee, S. Choi, and J. W. Choi, “Protoocc: Accurate,
efficient 3d occupancy prediction using dual branch encoder-prototype
query decoder,” in _Proc. AAAI Conf. Artif. Intell._, vol. 39, no. 4, Feb.
2025, pp. 4284–4292.

[27] J. Zhang, Y. Zhang, Q. Liu, and Y. Wang, “Lightweight spatial embedding for vision-based 3d occupancy prediction,” 2024,
_arXiv:2412.05976_ .

[28] J. Hou, X. Li, W. Guan, G. Zhang, D. Feng, Y. Du, X. Xue, and J. Pu,
“Fastocc: Accelerating 3d occupancy prediction by fusing the 2d bird’seye view and perspective view,” May 2024, pp. 16425–16431.

[29] Z. Liao, P. Wei, S. Chen, H. Wang, and Z. Ren, “Stcocc: Sparse
spatial-temporal cascade renovation for 3d occupancy and scene flow
prediction,” in _Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog._, Jun.
2025, pp. 1516–1526.

[30] Y. Shi, T. Cheng, Q. Zhang, W. Liu, and X. Wang, “Occupancy as set of
points,” in _Proc. Eur. Conf. Comput. Vis._ Cham, Switzerland: Springer,
Oct. 2024, pp. 72–87.

[31] X. Lin, T. Lin, Z. Pei, L. Huang, and Z. Su, “Sparse4D: Multiview 3d object detection with sparse spatial-temporal fusion,” 2022,
_arXiv:2211.10581_ .

[32] J. Park, C. Xu, S. Yang, K. Keutzer, K. M. Kitani, M. Tomizuka, and
W. Zhan, “Time will tell: New outlooks and a baseline for temporal
multi-view 3d object detection,” in _Proc. Int. Conf. Learn. Represent._,
May 2023.

[33] Y. He, W. Chen, S. Wang, T. Xun, and Y. Tan, “Achieving speedaccuracy balance in vision-based 3d occupancy prediction via geometricsemantic disentanglement,” in _Proc. AAAI Conf. Artif. Intell._, vol. 39,
no. 3, Feb. 2025, pp. 3455–3463.

[34] D. Chen, J. Fang, W. Han, X. Cheng, J. Yin, C. Xu, F. S. Khan, and
J. Shen, “Alocc: Adaptive lifting-based 3d semantic occupancy and cost
volume-based flow predictions,” in _Proc. IEEE/CVF Int. Conf. Comput._
_Vis._, Oct. 2025, pp. 4156–4166.

[35] D. Chen, H. Zheng, J. Fang, X. Dong, X. Li, W. Liao, T. He, P. Peng,
and J. Shen, “Rethinking temporal fusion with a unified gradient descent
view for 3d semantic occupancy prediction,” in _Proc. IEEE/CVF Conf._
_Comput. Vis. Pattern Recog._, Jun. 2025, pp. 1505–1515.

[36] J. Li, X. He, C. Zhou, X. Cheng, Y. Wen, and D. Zhang, “Viewformer:
Exploring spatiotemporal modeling for multi-view 3d occupancy perception via view-guided transformers,” in _Proc. Eur. Conf. Comput. Vis._
Cham, Switzerland: Springer, Oct. 2024, pp. 90–106.

[37] Z. Leng, J. Yang, W. Yi, and B. Zhou, “Occupancy learning with
spatiotemporal memory,” in _Proc. IEEE/CVF Int. Conf. Comput. Vis._,
Oct. 2025, pp. 26569–26578.

[38] Z. Zong, D. Jiang, G. Song, Z. Xue, J. Su, H. Li, and Y. Liu, “Temporal
enhanced training of multi-view 3d object detector via historical object
prediction,” in _Proc. IEEE/CVF Int. Conf. Comput. Vis._, Oct. 2023, pp.
3781–3790.


11




[39] Z. Xia, Z. Lin, X. Wang, Y. Wang, Y. Xing, S. Qi, N. Dong, and M.-H.
Yang, “Henet: Hybrid encoding for end-to-end multi-task 3d perception
from multi-view cameras,” in _Proc. Eur. Conf. Comput. Vis._ Cham,
Switzerland: Springer, Oct. 2024, pp. 376–392.

[40] M. Chang, X. Zhang, R. Zhang, Z. Zhao, G. He, and S. Liu, “Recurrentbev: A long-term temporal fusion framework for multi-view
3d detection,” in _Proc. Eur. Conf. Comput. Vis._ Cham, Switzerland:
Springer, Oct. 2024, pp. 131–147.

[41] C. Han, J. Yang, J. Sun, Z. Ge, R. Dong, H. Zhou, W. Mao, Y. Peng, and
X. Zhang, “Exploring recurrent long-term temporal fusion for multi-view
3d perception,” _IEEE Rob. Autom. Lett._, vol. 9, no. 7, pp. 6544-6551,
2024.

[42] S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang, “Exploring objectcentric temporal modeling for efficient multi-view 3d object detection,”
in _Proc. IEEE/CVF Int. Conf. Comput. Vis._, Oct. 2023, pp. 3621–3631.

[43] X. Lin, T. Lin, Z. Pei, L. Huang, and Z. Su, “Sparse4d v2: Recurrent
temporal fusion with sparse model,” 2023, _arXiv:2305.14018_ .

[44] X. Lin, Z. Pei, T. Lin, L. Huang, and Z. Su, “Sparse4d v3: Advancing
end-to-end 3d detection and tracking,” 2023, _arXiv:2311.11722_ .

[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in _Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog._,
Jun. 2016, pp. 770–778.

[46] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in _Proc. IEEE/CVF_
_Conf. Comput. Vis. Pattern Recog._, Jun. 2017, pp. 2117–2125.

[47] B. Kerbl, G. Kopanas, T. Leimk¨uhler, and G. Drettakis, “3d gaussian
splatting for real-time radiance field rendering.” _ACM Trans. Graph._,
vol. 42, no. 4, pp. 139–1, 2023.

[48] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A multimodal
dataset for autonomous driving,” in _Proc. IEEE/CVF Conf. Comput. Vis._
_Pattern Recog._, Jun. 2020, pp. 11621–11631.

[49] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
2017, _arXiv:1711.05101_ .

[50] M. Pan, J. Liu, R. Zhang, P. Huang, X. Li, H. Xie, B. Wang, L. Liu,
and S. Zhang, “Renderocc: Vision-centric 3d occupancy prediction with
2d rendering supervision,” May 2024, pp. 12404–12411.

[51] Z. Murez, T. Van As, J. Bartolozzi, A. Sinha, V. Badrinarayanan, and
A. Rabinovich, “Atlas: End-to-end 3d scene reconstruction from posed
images,” in _Proc. Eur. Conf. Comput. Vis._ Cham, Switzerland: Springer,
Oct. 2020, pp. 414–431.


