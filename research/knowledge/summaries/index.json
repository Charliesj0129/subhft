{
  "total_results": 110,
  "papers": [
    {
      "id": "2512.05734v1",
      "title": "KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books",
      "authors": [
        "Jinfeng Zhong",
        "Emmanuel Bacry",
        "Agathe Guilloux",
        "Jean-Fran\u00e7ois Muzy"
      ],
      "abstract": "This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-05T14:15:02+00:00",
      "url": "https://arxiv.org/pdf/2512.05734v1",
      "resource_uri": "arxiv://2512.05734v1"
    },
    {
      "id": "2512.11765v1",
      "title": "High-Frequency Analysis of a Trading Game with Transient Price Impact",
      "authors": [
        "Marcel Nutz",
        "Alessandro Prosperi"
      ],
      "abstract": "We study the high-frequency limit of an $n$-trader optimal execution game in discrete time. Traders face transient price impact of Obizhaeva--Wang type in addition to quadratic instantaneous trading costs $\u03b8(\u0394X_t)^2$ on each transaction $\u0394X_t$. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval $[0,T]$, the discrete equilibrium inventories converge at rate $1/N$ to the continuous-time equilibrium of an Obizhaeva--Wang model with additional quadratic costs $\\vartheta_0(\u0394X_0)^2$ and $\\vartheta_T(\u0394X_T)^2$ on initial and terminal block trades, where $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case $n=2$ where $\\vartheta_0=\\vartheta_T=1/2$. In particular, we show how the coefficients $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$ arise endogenously in the high-frequency limit: the initial and terminal block costs of the continuous-time model are identified as the limits of the cumulative discrete instantaneous costs incurred over small neighborhoods of $0$ and $T$, respectively, and these limits are independent of $\u03b8>0$. By contrast, when $\u03b8=0$ the discrete-time equilibrium strategies and costs exhibit persistent oscillations and admit no high-frequency limit, mirroring the non-existence of continuous-time equilibria without boundary block costs. Our results show that two different types of trading frictions -- a fine time discretization and small instantaneous costs in continuous time -- have similar regularizing effects and select a canonical model in the limit.",
      "categories": [
        "q-fin.TR",
        "q-fin.MF"
      ],
      "published": "2025-12-12T18:22:36+00:00",
      "url": "https://arxiv.org/pdf/2512.11765v1",
      "resource_uri": "arxiv://2512.11765v1"
    },
    {
      "id": "2512.22476v1",
      "title": "AutoQuant: An Auditable Expert-System Framework for Execution-Constrained Auto-Tuning in Cryptocurrency Perpetual Futures",
      "authors": [
        "Kaihong Deng"
      ],
      "abstract": "Backtests of cryptocurrency perpetual futures are fragile when they ignore microstructure frictions and reuse evaluation windows during parameter search. We study four liquid perpetuals (BTC/USDT, ETH/USDT, SOL/USDT, AVAX/USDT) and quantify how execution delay, funding, fees, and slippage can inflate reported performance. We introduce AutoQuant, an execution-centric, alpha-agnostic framework for auditable strategy configuration selection. AutoQuant encodes strict T+1 execution semantics and no-look-ahead funding alignment, runs Bayesian optimization under realistic costs, and applies a two-stage double-screening protocol across held-out rolling windows and a cost-sensitivity grid. We show that fee-only and zero-cost backtests can materially overestimate annualized returns relative to a fully costed configuration, and that double screening tends to reduce drawdowns under the same strict semantics even when returns are not higher. A CSCV/PBO diagnostic indicates substantial residual overfitting risk, motivating AutoQuant as validation and governance infrastructure rather than a claim of persistent alpha. Returns are reported for small-account simulations with linear trading costs and without market impact or capacity modeling.",
      "categories": [
        "q-fin.TR"
      ],
      "published": "2025-12-27T05:46:43+00:00",
      "url": "https://arxiv.org/pdf/2512.22476v1",
      "resource_uri": "arxiv://2512.22476v1"
    },
    {
      "id": "2512.20850v1",
      "title": "Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal",
      "authors": [
        "Alexey Meteykin"
      ],
      "abstract": "We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.",
      "categories": [
        "q-fin.MF",
        "math.NA"
      ],
      "published": "2025-12-24T00:07:14+00:00",
      "url": "https://arxiv.org/pdf/2512.20850v1",
      "resource_uri": "arxiv://2512.20850v1"
    },
    {
      "id": "2512.21647v1",
      "title": "On the size edge-ordered Ramsey numbers of graphs",
      "authors": [
        "Yanyan Song",
        "Yaping Mao"
      ],
      "abstract": "For edge-ordered graphs $G^{\\prec}$ and $H^{\\prec}$, the size edge-ordered Ramsey number $\\hat{r}_{\\text{edge}}(G^{\\prec}, H^{\\prec})$ is defined as the smallest integer $m$ for which there exists an edge-ordered graph $F^{\\prec}$ (with underlying graph $F$) having $m$ edges, such that every $2$-coloring of the edges of $F^{\\prec}$ contains a monochromatic edge-ordered subgraph isomorphic to $G^{\\prec}$ or a monochromatic edge-ordered subgraph isomorphic to $H^{\\prec}$. Fox and Li posed a foundational question: which families of edge-ordered graphs have linear or near-linear size edge-ordered Ramsey numbers? In this paper, we apply Szemer\u00e9di's regularity lemma to prove that, even for sparse graph families, specifically the well-defined class of edge-ordered book graphs, the size edge-ordered Ramsey numbers of this family exhibit non-linear growth. Furthermore, we show that three families of edge-ordered graphs exhibit linear or near-linear size edge-ordered Ramsey numbers.",
      "categories": [
        "math.CO"
      ],
      "published": "2025-12-25T12:25:15+00:00",
      "url": "https://arxiv.org/pdf/2512.21647v1",
      "resource_uri": "arxiv://2512.21647v1"
    },
    {
      "id": "2512.19838v2",
      "title": "Equilibrium Liquidity and Risk Offsetting in Decentralised Markets",
      "authors": [
        "Fay\u00e7al Drissi",
        "Xuchen Wu",
        "Sebastian Jaimungal"
      ],
      "abstract": "We develop an economic model of decentralised exchanges (DEXs) in which risk-averse liquidity providers (LPs) manage risk in a centralised exchange (CEX) based on preferences, information, and trading costs. Rational, risk-averse LPs anticipate the frictions associated with replication and manage risk primarily by reducing the reserves supplied to the DEX. Greater aversion reduces the equilibrium viability of liquidity provision, resulting in thinner markets and lower trading volumes. Greater uninformed demand supports deeper liquidity, whereas higher fundamental price volatility erodes it. Finally, while moderate anticipated price changes can improve LP performance, larger changes require more intensive trading in the CEX, generate higher replication costs, and induce LPs to reduce liquidity supply.",
      "categories": [
        "q-fin.TR",
        "q-fin.GN",
        "q-fin.MF"
      ],
      "published": "2025-12-22T19:46:23+00:00",
      "url": "https://arxiv.org/pdf/2512.19838v2",
      "resource_uri": "arxiv://2512.19838v2"
    },
    {
      "id": "2512.23515v1",
      "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
      "authors": [
        "Zuoyou Jiang",
        "Li Zhao",
        "Rui Sun",
        "Ruohan Sun",
        "Zhongjian Li",
        "Jing Li",
        "Daxin Jiang",
        "Zuo Bai",
        "Cheng Hua"
      ],
      "abstract": "Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "published": "2025-12-29T14:50:23+00:00",
      "url": "https://arxiv.org/pdf/2512.23515v1",
      "resource_uri": "arxiv://2512.23515v1"
    },
    {
      "id": "2601.03948v2",
      "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
      "authors": [
        "Rui Sun",
        "Yifan Sun",
        "Sheng Xu",
        "Li Zhao",
        "Jing Li",
        "Daxin Jiang",
        "Cheng Hua",
        "Zuo Bai"
      ],
      "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "published": "2026-01-07T14:03:22+00:00",
      "url": "https://arxiv.org/pdf/2601.03948v2",
      "resource_uri": "arxiv://2601.03948v2"
    },
    {
      "id": "2601.04602v1",
      "title": "Forecasting Equity Correlations with Hybrid Transformer Graph Neural Network",
      "authors": [
        "Jack Fanshawe",
        "Rumi Masih",
        "Alexander Cameron"
      ],
      "abstract": "This paper studies forward-looking stock-stock correlation forecasting for S\\&P 500 constituents and evaluates whether learned correlation forecasts can improve graph-based clustering used in basket trading strategies. We cast 10-day ahead correlation prediction in Fisher-z space and train a Temporal-Heterogeneous Graph Neural Network (THGNN) to predict residual deviations from a rolling historical baseline. The architecture combines a Transformer-based temporal encoder, which captures non-stationary, complex, temporal dependencies, with an edge-aware graph attention network that propagates cross-asset information over the equity network. Inputs span daily returns, technicals, sector structure, previous correlations, and macro signals, enabling regime-aware forecasts and attention-based feature and neighbor importance to provide interpretability. Out-of-sample results from 2019-2024 show that the proposed model meaningfully reduces correlation forecasting error relative to rolling-window estimates. When integrated into a graph-based clustering framework, forward-looking correlations produce adaptable and economically meaningfully baskets, particularly during periods of market stress. These findings suggest that improvements in correlation forecasts translate into meaningful gains during portfolio construction tasks.",
      "categories": [
        "q-fin.CP",
        "q-fin.TR"
      ],
      "published": "2026-01-08T05:16:06+00:00",
      "url": "https://arxiv.org/pdf/2601.04602v1",
      "resource_uri": "arxiv://2601.04602v1"
    },
    {
      "id": "2601.04246v2",
      "title": "Technology Adoption and Network Externalities in Financial Systems: A Spatial-Network Approach",
      "authors": [
        "Tatsuru Kikuchi"
      ],
      "abstract": "This paper develops a unified framework for analyzing technology adoption in financial networks that incorporates spatial spillovers, network externalities, and their interaction. The framework characterizes adoption dynamics through a master equation whose solution admits a Feynman-Kac representation as expected cumulative adoption pressure along stochastic paths through spatial-network space. From this representation, I derive the Adoption Amplification Factor -- a structural measure of technology leadership that captures the ratio of total system-wide adoption to initial adoption following a localized shock. A Levy jump-diffusion extension with state-dependent jump intensity captures critical mass dynamics: below threshold, adoption evolves through gradual diffusion; above threshold, cascade dynamics accelerate adoption through discrete jumps. Applying the framework to SWIFT gpi adoption among 17 Global Systemically Important Banks, I find strong support for the two-regime characterization. Network-central banks adopt significantly earlier ($\u03c1= -0.69$, $p = 0.002$), and pre-threshold adopters have significantly higher amplification factors than post-threshold adopters (11.81 versus 7.83, $p = 0.010$). Founding members, representing 29 percent of banks, account for 39 percent of total system amplification -- sufficient to trigger cascade dynamics. Controlling for firm size and network position, CEO age delays adoption by 11-15 days per year.",
      "categories": [
        "econ.EM",
        "econ.TH",
        "q-fin.GN",
        "q-fin.TR"
      ],
      "published": "2026-01-06T08:50:36+00:00",
      "url": "https://arxiv.org/pdf/2601.04246v2",
      "resource_uri": "arxiv://2601.04246v2"
    },
    {
      "id": "2601.11958v1",
      "title": "Autonomous Market Intelligence: Agentic AI Nowcasting Predicts Stock Returns",
      "authors": [
        "Zefeng Chen",
        "Darcy Pu"
      ],
      "abstract": "Can fully agentic AI nowcast stock returns? We deploy a state-of-the-art Large Language Model to evaluate the attractiveness of each Russell 1000 stock daily, starting from April 2025 when AI web interfaces enabled real-time search. Our data contribution is unique along three dimensions. First, the nowcasting framework is completely out-of-sample and free of look-ahead bias by construction: predictions are collected at the current edge of time, ensuring the AI has no knowledge of future outcomes. Second, this temporal design is irreproducible -- once the information environment passes, it can never be recreated. Third, our framework is 100% agentic: we do not feed the model news, disclosures, or curated text; it autonomously searches the web, filters sources, and synthesises information into quantitative predictions. We find that AI possesses genuine stock selection ability, but only for identifying top winners. Longing the 20 highest-ranked stocks generates a daily Fama-French five-factor plus momentum alpha of 18.4 basis points and an annualised Sharpe ratio of 2.43. Critically, these returns derive from an implementable strategy trading highly liquid Russell 1000 constituents, with transaction costs representing less than 10\\% of gross alpha. However, this predictability is highly concentrated: expanding beyond the top tier rapidly dilutes alpha, and bottom-ranked stocks exhibit returns statistically indistinguishable from the market. We hypothesise that this asymmetry reflects online information structure: genuinely positive news generates coherent signals, while negative news is contaminated by strategic corporate obfuscation and social media noise.",
      "categories": [
        "q-fin.GN",
        "q-fin.PM",
        "q-fin.TR"
      ],
      "published": "2026-01-17T08:27:53+00:00",
      "url": "https://arxiv.org/pdf/2601.11958v1",
      "resource_uri": "arxiv://2601.11958v1"
    },
    {
      "id": "2601.03799v1",
      "title": "Optimal execution on Uniswap v2/v3 under transient price impact",
      "authors": [
        "Bastien Baude",
        "Damien Challet",
        "Ioane Muni Toke"
      ],
      "abstract": "We study the optimal liquidation of a large position on Uniswap v2 and Uniswap v3 in discrete time. The instantaneous price impact is derived from the AMM pricing rule. Transient impact is modeled to capture either exponential or approximately power-law decay, together with a permanent component. In the Uniswap v2 setting, we obtain optimal strategies in closed-form under general price dynamics. For Uniswap v3, we consider a two-layer liquidity framework, which naturally extends to multiple layers. We address the problem using dynamic programming under geometric Brownian motion dynamics and approximate the solution numerically using a discretization scheme. We obtain optimal strategies akin to classical ones in the LOB literature, with features specific to Uniswap. In particular, we show how the liquidity profile influences them.",
      "categories": [
        "q-fin.MF"
      ],
      "published": "2026-01-07T10:56:22+00:00",
      "url": "https://arxiv.org/pdf/2601.03799v1",
      "resource_uri": "arxiv://2601.03799v1"
    },
    {
      "id": "2601.11201v1",
      "title": "Fast Times, Slow Times: Timescale Separation in Financial Timeseries Data",
      "authors": [
        "Jan Rosenzweig"
      ],
      "abstract": "Financial time series exhibit multiscale behavior, with interaction between multiple processes operating on different timescales. This paper introduces a method for separating these processes using variance and tail stationarity criteria, framed as generalized eigenvalue problems. The approach allows for the identification of slow and fast components in asset returns and prices, with applications to parameter drift, mean reversion, and tail risk management. Empirical examples using currencies, equity ETFs and treasury yields illustrate the practical utility of the method.",
      "categories": [
        "q-fin.PM",
        "q-fin.CP",
        "q-fin.RM",
        "q-fin.ST",
        "q-fin.TR"
      ],
      "published": "2026-01-16T11:23:13+00:00",
      "url": "https://arxiv.org/pdf/2601.11201v1",
      "resource_uri": "arxiv://2601.11201v1"
    },
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22T18:57:09+00:00",
      "url": "https://arxiv.org/pdf/2601.16206v1",
      "resource_uri": "arxiv://2601.16206v1"
    },
    {
      "id": "2601.16089v1",
      "title": "A forward-only scheme for online learning of proposal distributions in particle filters",
      "authors": [
        "Sylvain Procope-Mamert",
        "Nicolas Chopin",
        "Maud Delattre",
        "Guillaume Kon Kam King"
      ],
      "abstract": "We introduce a new online approach for constructing proposal distributions in particle filters using a forward scheme. Our method progressively incorporates future observations to refine proposals. This is in contrast to backward-scheme algorithms that require access to the entire dataset, such as the iterated auxiliary particle filters (Guarniero et al., 2017, arXiv:1511.06286) and controlled sequential Monte Carlo (Heng et al., 2020, arXiv:1708.08396) which leverage all future observations through backward recursion. In comparison, our forward scheme achieves a gradual improvement of proposals that converges toward the proposal targeted by these backward methods. We show that backward approaches can be numerically unstable even in simple settings. Our forward method, however, offers significantly greater robustness with only a minor trade-off in performance, measured by the variance of the marginal likelihood estimator. Numerical experiments on both simulated and real data illustrate the enhanced stability of our forward approach.",
      "categories": [
        "stat.CO"
      ],
      "published": "2026-01-22T16:40:27+00:00",
      "url": "https://arxiv.org/pdf/2601.16089v1",
      "resource_uri": "arxiv://2601.16089v1"
    },
    {
      "id": "2601.16030v1",
      "title": "Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing",
      "authors": [
        "Jiancheng An",
        "Chau Yuen",
        "Marco Di Renzo",
        "Mehdi Bennis",
        "Merouane Debbah",
        "Lajos Hanzo"
      ],
      "abstract": "Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22T14:58:43+00:00",
      "url": "https://arxiv.org/pdf/2601.16030v1",
      "resource_uri": "arxiv://2601.16030v1"
    },
    {
      "id": "2601.15859v1",
      "title": "Uncertainty-guided Generation of Dark-field Radiographs",
      "authors": [
        "Lina Felsner",
        "Henriette Bast",
        "Tina Dorosti",
        "Florian Schaff",
        "Franz Pfeiffer",
        "Daniela Pfeiffer",
        "Julia Schnabel"
      ],
      "abstract": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-01-22T11:07:19+00:00",
      "url": "https://arxiv.org/pdf/2601.15859v1",
      "resource_uri": "arxiv://2601.15859v1"
    },
    {
      "id": "2601.15766v1",
      "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
      "authors": [
        "Yuhan Chen",
        "Ying Fang",
        "Guofa Li",
        "Wenxuan Yu",
        "Yicui Shi",
        "Jingrui Zhang",
        "Kefei Qian",
        "Wenbo Chu",
        "Keqiang Li"
      ],
      "abstract": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T08:57:36+00:00",
      "url": "https://arxiv.org/pdf/2601.15766v1",
      "resource_uri": "arxiv://2601.15766v1"
    },
    {
      "id": "2601.15729v1",
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "authors": [
        "Rui Yang",
        "Lei Zheng",
        "Ruoyu Yao",
        "Jun Ma"
      ],
      "abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2026-01-22T07:56:36+00:00",
      "url": "https://arxiv.org/pdf/2601.15729v1",
      "resource_uri": "arxiv://2601.15729v1"
    },
    {
      "id": "2601.15687v1",
      "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation",
      "authors": [
        "Khusrav Badalov",
        "Young Yoon"
      ],
      "abstract": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-01-22T06:12:18+00:00",
      "url": "https://arxiv.org/pdf/2601.15687v1",
      "resource_uri": "arxiv://2601.15687v1"
    },
    {
      "id": "2601.15671v1",
      "title": "StreetDesignAI: A Multi-Persona Evaluation System for Inclusive Infrastructure Design",
      "authors": [
        "Ziyi Wang",
        "Yilong Dai",
        "Duanya Lyu",
        "Mateo Nader",
        "Sihan Chen",
        "Wanghao Ye",
        "Zjian Ding",
        "Xiang Yan"
      ],
      "abstract": "Designing inclusive cycling infrastructure requires balancing competing needs of diverse user groups, yet designers often struggle to anticipate how different cyclists experience the same street. We investigate how persona-based multi-agent evaluation can support inclusive design by making experiential conflicts explicit. We present StreetDesignAI, an interactive system that enables designers to (1) ground evaluation in street context through imagery and map data, (2) receive parallel feedback from cyclist personas spanning confident to cautious users, and (3) iteratively modify designs while surfacing conflicts across perspectives. A within-subjects study with 26 transportation professionals demonstrates that structured multi-perspective feedback significantly improves designers' understanding of diverse user perspectives, ability to identify persona needs, and confidence in translating them into design decisions, with higher satisfaction and stronger intention for professional adoption. Qualitative findings reveal how conflict surfacing transforms design exploration from single-perspective optimization toward deliberate trade-off reasoning. We discuss implications for AI tools that scaffold inclusive design through disagreement as an interaction primitive.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-01-22T05:53:05+00:00",
      "url": "https://arxiv.org/pdf/2601.15671v1",
      "resource_uri": "arxiv://2601.15671v1"
    },
    {
      "id": "2601.15649v1",
      "title": "Scaling-Based Quantization of Spacetime Microstructure",
      "authors": [
        "Weihu Ma",
        "Yu-Gang Ma"
      ],
      "abstract": "Planck-scale physics challenges the classical smooth-spacetime picture by introducing quantum fluctuations that imply a nontrivial spacetime microstructure. We present a framework that encodes these fluctuations by promoting local scale factors, rather than the metric tensor, to fundamental dynamical variables while preserving general covariance. The construction employs a two-tiered hierarchy of scale manifolds, comprising a first-order manifold of scale coordinates and a second-order manifold of fluctuation amplitude coordinates. On the first-order manifold, we formulate differential geometry, field equations, and a canonical quantization procedure. The theory yields a geometric renormalization-group flow for scale variables and implies spacetime discreteness at the microscopic level. By constructing a quadratic action and performing spectral decomposition with a stabilizing potential, we obtain discrete modal degrees of freedom quantized as harmonic oscillators. The framework proposes a microscopic description for zero-point energy of spacetime and explores implications for vacuum energy and ultraviolet regularization, suggesting a potential dynamical mechanism that could ameliorate the cosmological constant problem. Main results include a generalized uncertainty relation with scale-dependent coefficients, locally scaled Klein-Gordon and Dirac equations, geodesic equations for scale spacetime, and a microscopic area operator whose state counting is consistent with the Bekenstein-Hawking entropy. This work develops a scale-based quantization procedure, providing a foundation for further mathematical analysis and phenomenological tests of spacetime quantization.",
      "categories": [
        "physics.gen-ph"
      ],
      "published": "2026-01-22T04:54:47+00:00",
      "url": "https://arxiv.org/pdf/2601.15649v1",
      "resource_uri": "arxiv://2601.15649v1"
    },
    {
      "id": "2601.15644v1",
      "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction",
      "authors": [
        "Zichen Yu",
        "Quanli Liu",
        "Wei Wang",
        "Liyong Zhang",
        "Xiaoguang Zhao"
      ],
      "abstract": "3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T04:50:29+00:00",
      "url": "https://arxiv.org/pdf/2601.15644v1",
      "resource_uri": "arxiv://2601.15644v1"
    },
    {
      "id": "2601.15641v1",
      "title": "Machine Failure Detection Based on Projected Quantum Models",
      "authors": [
        "Larry Bowden",
        "Qi Chu",
        "Bernard Cena",
        "Kentaro Ohno",
        "Bob Parney",
        "Deepak Sharma",
        "Mitsuharu Takeori"
      ],
      "abstract": "Detecting machine failures promptly is of utmost importance in industry for maintaining efficiency and minimizing downtime. This paper introduces a failure detection algorithm based on quantum computing and a statistical change-point detection approach. Our method leverages the potential of projected quantum feature maps to enhance the precision of anomaly detection in machine monitoring systems. We empirically validate our approach on benchmark multi-dimensional time series datasets as well as on a real-world dataset comprising IoT sensor readings from operational machines, ensuring the practical relevance of our study. The algorithm was executed on IBM's 133-qubit Heron quantum processor, demonstrating the feasibility of integrating quantum computing into industrial maintenance procedures. The presented results underscore the effectiveness of our quantum-based failure detection system, showcasing its capability to accurately identify anomalies in noisy time series data. This work not only highlights the potential of quantum computing in industrial diagnostics but also paves the way for more sophisticated quantum algorithms in the realm of predictive maintenance.",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2026-01-22T04:43:53+00:00",
      "url": "https://arxiv.org/pdf/2601.15641v1",
      "resource_uri": "arxiv://2601.15641v1"
    },
    {
      "id": "2601.15588v1",
      "title": "YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models",
      "authors": [
        "Junyu Lin",
        "Meizhen Liu",
        "Xiufeng Huang",
        "Jinfeng Li",
        "Haiwen Hong",
        "Xiaohan Yuan",
        "Yuefeng Chen",
        "Longtao Huang",
        "Hui Xue",
        "Ranjie Duan",
        "Zhikai Chen",
        "Yuchuan Fu",
        "Defeng Li",
        "Lingyao Gao",
        "Yitong Yang"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22T02:23:18+00:00",
      "url": "https://arxiv.org/pdf/2601.15588v1",
      "resource_uri": "arxiv://2601.15588v1"
    },
    {
      "id": "2601.15304v1",
      "title": "An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring",
      "authors": [
        "Sandeep Neela"
      ],
      "abstract": "Market integrity monitoring is difficult because suspicious price/volume behavior can arise from many benign mechanisms, while modern detection systems often rely on opaque models that are hard to audit and communicate. We present AIMM-X, an explainable monitoring pipeline that combines market microstructure-style signals derived from OHLCV time series with multi-source public attention signals (e.g., news and online discussion proxies) to surface time windows that merit analyst review. The system detects candidate anomalous windows using transparent thresholding and aggregation, then assigns an interpretable integrity score decomposed into a small set of additive components, allowing practitioners to trace why a window was flagged and which factors drove the score. We provide an end-to-end, reproducible implementation that downloads data, constructs attention features, builds unified panels, detects windows, computes component signals, and generates summary figures/tables. Our goal is not to label manipulation, but to provide a practical, auditable screening tool that supports downstream investigation by compliance teams, exchanges, or researchers.",
      "categories": [
        "q-fin.RM",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-10T22:48:57+00:00",
      "url": "https://arxiv.org/pdf/2601.15304v1",
      "resource_uri": "arxiv://2601.15304v1"
    },
    {
      "id": "2512.06309v1",
      "title": "Wealth or Stealth? The Camouflage Effect in Insider Trading",
      "authors": [
        "Jin Ma",
        "Weixuan Xia",
        "Jianfeng Zhang"
      ],
      "abstract": "We consider a Kyle-type model where insider trading takes place among a potentially large population of liquidity traders and is subject to legal penalties. Insiders exploit the liquidity provided by the trading masses to \"camouflage\" their actions and balance expected wealth with the necessary stealth to avoid detection. Under a diverse spectrum of prosecution schemes, we establish the existence of equilibria for arbitrary population sizes and a unique limiting equilibrium. A convergence analysis determines the scale of insider trading by a stealth index $\u03b3$, revealing that the equilibrium can be closely approximated by a simple limit due to diminished price informativeness. Empirical aspects are derived from two calibration experiments using non-overlapping data sets spanning from 1980 to 2018, which underline the indispensable role of a large population in insider trading models with legal risk, along with important implications for the incidence of stealth trading and the deterrent effect of legal enforcement.",
      "categories": [
        "econ.GN",
        "q-fin.TR"
      ],
      "published": "2025-12-06T05:54:28+00:00",
      "url": "https://arxiv.org/pdf/2512.06309v1",
      "resource_uri": "arxiv://2512.06309v1"
    },
    {
      "id": "2512.10913v1",
      "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
      "authors": [
        "Mohammad Rezoanul Hoque",
        "Md Meftahul Ferdaus",
        "M. Kabir Hassan"
      ],
      "abstract": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
      "categories": [
        "q-fin.CP"
      ],
      "published": "2025-12-11T18:42:19+00:00",
      "url": "https://arxiv.org/pdf/2512.10913v1",
      "resource_uri": "arxiv://2512.10913v1"
    },
    {
      "id": "2512.18098v1",
      "title": "A Games-in-Games Paradigm for Strategic Hybrid Jump-Diffusions: Hamilton-Jacobi-Isaacs Hierarchy and Spectral Structure",
      "authors": [
        "Yunian Pan",
        "Quanyan Zhu"
      ],
      "abstract": "This paper develops a hierarchical games-in-games control architecture for hybrid stochastic systems governed by regime-switching jump-diffusions. We model the interplay between continuous state dynamics and discrete mode transitions as a bilevel differential game: an inner layer solves a robust stochastic control problem within each regime, while a strategic outer layer modulates the transition intensities of the underlying Markov chain. A Dynkin-based analysis yields a system of coupled Hamilton-Jacobi-Isaacs (HJI) equations. We prove that for the class of Linear-Quadratic games and Exponential-Affine games, this hierarchy admits tractable semi-closed form solutions via coupled matrix differential equations. We prove that for the class of Linear-Quadratic games and Exponential-Affine games, this hierarchy admits tractable semi-closed form solutions via coupled matrix differential equations. The framework is demonstrated through a case study on adversarial market microstructure, showing how the outer layer's strategic switching pre-emptively adjusts inventory spreads against latent regime risks, which leads to a hyper-alert equilibrium.",
      "categories": [
        "eess.SY"
      ],
      "published": "2025-12-19T22:10:22+00:00",
      "url": "https://arxiv.org/pdf/2512.18098v1",
      "resource_uri": "arxiv://2512.18098v1"
    },
    {
      "id": "2512.07154v1",
      "title": "Asian option valuation under price impact",
      "authors": [
        "Priyanshu Tiwari",
        "Sourav Majumdar"
      ],
      "abstract": "We study the valuation of Asian options in a binomial market with permanent price impact, extending the Cox-Ross-Rubinstein framework under a modified risk-neutral probability. We obtain an exact pathwise representation for geometric Asian options and derive two-sided bounds for arithmetic Asian options. Our analysis identifies the no-arbitrage region in terms of hedging volumes and shows that permanent price impact systematically raises Asian option prices. Numerical examples illustrate the effect of the impact parameter and hedging volumes on the resulting prices.",
      "categories": [
        "q-fin.MF"
      ],
      "published": "2025-12-08T04:38:02+00:00",
      "url": "https://arxiv.org/pdf/2512.07154v1",
      "resource_uri": "arxiv://2512.07154v1"
    },
    {
      "id": "2512.23386v1",
      "title": "Impact of Volatility on Time-Based Transaction Ordering Policies",
      "authors": [
        "Sunghun Ko",
        "Jinsuk Park"
      ],
      "abstract": "We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model.",
      "categories": [
        "cs.GT",
        "econ.EM",
        "q-fin.TR"
      ],
      "published": "2025-12-29T11:24:08+00:00",
      "url": "https://arxiv.org/pdf/2512.23386v1",
      "resource_uri": "arxiv://2512.23386v1"
    },
    {
      "id": "2512.16411v1",
      "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection",
      "authors": [
        "Matthieu Garcin",
        "Louis Perot"
      ],
      "abstract": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.",
      "categories": [
        "stat.ME",
        "math.ST",
        "q-fin.ST",
        "q-fin.TR",
        "stat.AP"
      ],
      "published": "2025-12-18T11:08:37+00:00",
      "url": "https://arxiv.org/pdf/2512.16411v1",
      "resource_uri": "arxiv://2512.16411v1"
    },
    {
      "id": "2601.10591v1",
      "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
      "authors": [
        "Arundeep Chinta",
        "Lucas Vinh Tran",
        "Jay Katukuri"
      ],
      "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "published": "2026-01-15T17:02:06+00:00",
      "url": "https://arxiv.org/pdf/2601.10591v1",
      "resource_uri": "arxiv://2601.10591v1"
    },
    {
      "id": "2601.03215v1",
      "title": "Trading with market resistance and concave price impact",
      "authors": [
        "Youssef Ouazzani Chahdi",
        "Nathan De Carvalho",
        "Gr\u00e9goire Szymanski"
      ],
      "abstract": "We consider an optimal trading problem under a market impact model with endogenous market resistance generated by a sophisticated trader who (partially) detects metaorders and trades against them to exploit price overreactions induced by the order flow. The model features a concave transient impact driven by a power-law propagator with a resistance term responding to the trader's rate via a fixed-point equation involving a general resistance function. We derive a (non)linear stochastic Fredholm equation as the first-order optimality condition satisfied by optimal trading strategies. Existence and uniqueness of the optimal control are established when the resistance function is linear, and an existence result is obtained when it is strictly convex using coercivity and weak lower semicontinuity of the associated profit-and-loss functional. We also propose an iterative scheme to solve the nonlinear stochastic Fredholm equation and prove an exponential convergence rate. Numerical experiments confirm this behavior and illustrate optimal round-trip strategies under \"buy\" signals with various decay profiles and different market resistance specifications.",
      "categories": [
        "q-fin.TR"
      ],
      "published": "2026-01-06T17:55:32+00:00",
      "url": "https://arxiv.org/pdf/2601.03215v1",
      "resource_uri": "arxiv://2601.03215v1"
    },
    {
      "id": "2601.05085v1",
      "title": "Trading Electrons: Predicting DART Spread Spikes in ISO Electricity Markets",
      "authors": [
        "Emma Hubert",
        "Dimitrios Lolas",
        "Ronnie Sircar"
      ],
      "abstract": "We study the problem of forecasting and optimally trading day-ahead versus real-time (DART) price spreads in U.S. wholesale electricity markets. Building on the framework of Galarneau-Vincent et al., we extend spike prediction from a single zone to a multi-zone setting and treat both positive and negative DART spikes within a unified statistical model. To translate directional signals into economically meaningful positions, we develop a structural and market-consistent price impact model based on day-ahead bid stacks. This yields closed-form expressions for the optimal vector of zonal INC/DEC quantities, capturing asymmetric buy/sell impacts and cross-zone congestion effects. When applied to NYISO, the resulting impact-aware strategy significantly improves the risk-return profile relative to unit-size trading and highlights substantial heterogeneity across markets and seasons.",
      "categories": [
        "q-fin.TR"
      ],
      "published": "2026-01-08T16:31:39+00:00",
      "url": "https://arxiv.org/pdf/2601.05085v1",
      "resource_uri": "arxiv://2601.05085v1"
    },
    {
      "id": "2601.08641v1",
      "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "authors": [
        "Yichen Luo",
        "Yebo Feng",
        "Jiahua Xu",
        "Yang Liu"
      ],
      "abstract": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.\n  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "published": "2026-01-13T15:13:41+00:00",
      "url": "https://arxiv.org/pdf/2601.08641v1",
      "resource_uri": "arxiv://2601.08641v1"
    },
    {
      "id": "2601.11602v1",
      "title": "The Physics of Price Discovery: Deconvolving Information, Volatility, and the Critical Breakdown of Signal during Retail Herding",
      "authors": [
        "Sungwoo Kang"
      ],
      "abstract": "Building on the finding that Market Cap Normalization ($\\SMC$) isolates the ``pure'' directional signal of informed trading \\citep{kang2025}, this paper investigates the physics of how that signal is transmitted -- and how it breaks down. We employ \\textbf{Tikhonov-regularized deconvolution} to recover the impulse response kernels of investor flows, revealing a dual-channel market structure: Foreign and Institutional investors act as ``architects'' of price discovery (positive permanent impact), while Individual investors act as liquidity providers (negative total impact). However, using \\textbf{Multivariate Hawkes Processes}, we demonstrate that this structure is fragile. We find that individual investor order flow exhibits near-critical self-excitation (Branching Ratio $\\approx$ 0.998). During periods of high retail herding, the market undergoes a \\textbf{phase transition} into a ``critical state.'' In this regime, the signal-to-noise ratio collapses, causing the price impact of sophisticated investors to reverse from positive to negative. These findings suggest that retail contagion acts as a physical barrier that temporarily disables efficient price discovery.",
      "categories": [
        "q-fin.ST"
      ],
      "published": "2026-01-08T10:42:22+00:00",
      "url": "https://arxiv.org/pdf/2601.11602v1",
      "resource_uri": "arxiv://2601.11602v1"
    },
    {
      "id": "2601.13421v1",
      "title": "Market Making and Transient Impact in Spot FX",
      "authors": [
        "Alexander Barzykin"
      ],
      "abstract": "Dealers in foreign exchange markets provide bid and ask prices to their clients at which they are happy to buy and sell, respectively. To manage risk, dealers can skew their quotes and hedge in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal market making with execution has previously been addressed within the Almgren-Chriss market impact model, which includes instantaneous and permanent components. However, there is overwhelming empirical evidence of the transient nature of market impact, with instantaneous and permanent impacts arising as the two limiting cases. In this note, we consider an intermediate scenario and study the interplay between risk management and impact resilience.",
      "categories": [
        "q-fin.TR",
        "q-fin.MF"
      ],
      "published": "2026-01-19T22:09:03+00:00",
      "url": "https://arxiv.org/pdf/2601.13421v1",
      "resource_uri": "arxiv://2601.13421v1"
    },
    {
      "id": "2601.15653v1",
      "title": "Distributed Multichannel Active Noise Control with Asynchronous Communication",
      "authors": [
        "Junwei Ji",
        "Dongyuan Shi",
        "Boxiang Wang",
        "Ziyi Yang",
        "Haowen Li",
        "Woon-Seng Gan"
      ],
      "abstract": "Distributed multichannel active noise control (DMCANC) offers effective noise reduction across large spatial areas by distributing the computational load of centralized control to multiple low-cost nodes. Conventional DMCANC methods, however, typically assume synchronous communication and require frequent data exchange, resulting in high communication overhead. To enhance efficiency and adaptability, this work proposes an asynchronous communication strategy where each node executes a weight-constrained filtered-x LMS (WCFxLMS) algorithm and independently requests communication only when its local noise reduction performance degrades. Upon request, other nodes transmit the weight difference between their local control filter and the center point in WCFxLMS, which are then integrated to update both the control filter and the center point. This design enables nodes to operate asynchronously while preserving cooperative behavior. Simulation results demonstrate that the proposed asynchronous communication DMCANC (ACDMCANC) system maintains effective noise reduction with significantly reduced communication load, offering improved scalability for heterogeneous networks.",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "published": "2026-01-22T05:02:06+00:00",
      "url": "https://arxiv.org/pdf/2601.15653v1",
      "resource_uri": "arxiv://2601.15653v1"
    },
    {
      "id": "2601.15599v1",
      "title": "Autonomous Business System via Neuro-symbolic AI",
      "authors": [
        "Cecil Pang",
        "Hiroki Sayama"
      ],
      "abstract": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T02:49:06+00:00",
      "url": "https://arxiv.org/pdf/2601.15599v1",
      "resource_uri": "arxiv://2601.15599v1"
    },
    {
      "id": "2601.15914v1",
      "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars",
      "authors": [
        "Yarin Benyamin"
      ],
      "abstract": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "published": "2026-01-22T12:44:12+00:00",
      "url": "https://arxiv.org/pdf/2601.15914v1",
      "resource_uri": "arxiv://2601.15914v1"
    },
    {
      "id": "2601.15897v1",
      "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
      "authors": [
        "Zhaoqi Su",
        "Shihai Chen",
        "Xinyan Lin",
        "Liqin Huang",
        "Zhipeng Su",
        "Xiaoqiang Lu"
      ],
      "abstract": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T12:24:26+00:00",
      "url": "https://arxiv.org/pdf/2601.15897v1",
      "resource_uri": "arxiv://2601.15897v1"
    },
    {
      "id": "2601.15876v1",
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "authors": [
        "Taofeng Xue",
        "Chong Peng",
        "Mianqiu Huang",
        "Linsen Guo",
        "Tiancheng Han",
        "Haozhe Wang",
        "Jianing Wang",
        "Xiaocheng Zhang",
        "Xin Yang",
        "Dengchang Zhao",
        "Jinrui Ding",
        "Xiandi Ma",
        "Yuchen Xie",
        "Peng Pei",
        "Xunliang Cai",
        "Xipeng Qiu"
      ],
      "abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T11:36:43+00:00",
      "url": "https://arxiv.org/pdf/2601.15876v1",
      "resource_uri": "arxiv://2601.15876v1"
    },
    {
      "id": "2601.15802v1",
      "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation",
      "authors": [
        "Alexandre Albore",
        "Humbert Fiorino",
        "Damien Pellier"
      ],
      "abstract": "Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-01-22T09:34:56+00:00",
      "url": "https://arxiv.org/pdf/2601.15802v1",
      "resource_uri": "arxiv://2601.15802v1"
    },
    {
      "id": "2601.15778v1",
      "title": "Agentic Confidence Calibration",
      "authors": [
        "Jiaxin Zhang",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22T09:08:25+00:00",
      "url": "https://arxiv.org/pdf/2601.15778v1",
      "resource_uri": "arxiv://2601.15778v1"
    },
    {
      "id": "2601.15775v1",
      "title": "Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV",
      "authors": [
        "Amir Habel",
        "Ivan Snegirev",
        "Elizaveta Semenyakina",
        "Miguel Altamirano Cabrera",
        "Jeffrin Sam",
        "Fawad Mehboob",
        "Roohan Ahmed Khan",
        "Muhammad Ahsan Mustafa",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22T09:03:57+00:00",
      "url": "https://arxiv.org/pdf/2601.15775v1",
      "resource_uri": "arxiv://2601.15775v1"
    },
    {
      "id": "2601.15758v1",
      "title": "NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases",
      "authors": [
        "Xieyang Wang",
        "Mengyi Liu",
        "Weijia Yi",
        "Jianqiu Xu",
        "Raymond Chi-Wing Wong"
      ],
      "abstract": "The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22T08:48:32+00:00",
      "url": "https://arxiv.org/pdf/2601.15758v1",
      "resource_uri": "arxiv://2601.15758v1"
    },
    {
      "id": "2601.16012v1",
      "title": "Low-Complexity Sparse Superimposed Coding for Ultra Reliable Low Latency Communications",
      "authors": [
        "Yanfeng Zhang",
        "Xi'an Fan",
        "Xu Zhu",
        "Jinkai Zheng",
        "Hui Liang",
        "Weiwei Yang",
        "Tom H. Luan"
      ],
      "abstract": "Sparse superimposed coding (SSC) has emerged as a promising technique for short-packet transmission in ultra-reliable low-latency communication scenarios. However, conventional SSC schemes often suffer from high encoding and decoding complexity due to the use of dense codebook matrices. In this paper, we propose a low-complexity SSC scheme by designing a sparse codebook structure, where each codeword contains only a small number of non-zero elements. The decoding is performed using the traditional multipath matching pursuit algorithm, and the overall complexity is significantly reduced by exploiting the sparsity of the codebook. Simulation results show that the proposed scheme achieves a favorable trade-off between BLER performance and computational complexity, and exhibits strong robustness across different transmission block lengths.",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-22T14:38:20+00:00",
      "url": "https://arxiv.org/pdf/2601.16012v1",
      "resource_uri": "arxiv://2601.16012v1"
    },
    {
      "id": "2512.12924v1",
      "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
      "authors": [
        "Gagan Deep",
        "Akash Deep",
        "William Lamptey"
      ],
      "abstract": "We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
      "categories": [
        "q-fin.TR",
        "q-fin.CP",
        "stat.ML"
      ],
      "published": "2025-12-15T02:20:42+00:00",
      "url": "https://arxiv.org/pdf/2512.12924v1",
      "resource_uri": "arxiv://2512.12924v1"
    },
    {
      "id": "2512.18648v2",
      "title": "Optimal Signal Extraction from Order Flow: A Matched Filter Perspective on Normalization and Market Microstructure",
      "authors": [
        "Sungwoo Kang"
      ],
      "abstract": "We demonstrate that the choice of normalization for order flow intensity is fundamental to signal extraction in finance, not merely a technical detail. Through theoretical modeling, Monte Carlo simulation, and empirical validation using Korean market data, we prove that market capitalization normalization acts as a ``matched filter'' for informed trading signals, achieving 1.32--1.97$\\times$ higher correlation with future returns compared to traditional trading value normalization. The key insight is that informed traders scale positions by firm value (market capitalization), while noise traders respond to daily liquidity (trading volume), creating heteroskedastic corruption when normalizing by trading volume. By reframing the normalization problem using signal processing theory, we show that dividing order flow by market capitalization preserves the information signal while traditional volume normalization multiplies the signal by inverse turnover -- a highly volatile quantity. Our theoretical predictions are robust across parameter specifications and validated by empirical evidence showing 482\\% improvement in explanatory power. These findings have immediate implications for high-frequency trading algorithms, risk factor construction, and information-based trading strategies.",
      "categories": [
        "q-fin.CP"
      ],
      "published": "2025-12-21T08:50:11+00:00",
      "url": "https://arxiv.org/pdf/2512.18648v2",
      "resource_uri": "arxiv://2512.18648v2"
    },
    {
      "id": "2512.16080v1",
      "title": "Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities",
      "authors": [
        "Tianyi Ma"
      ],
      "abstract": "In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.",
      "categories": [
        "cs.CR",
        "q-fin.TR"
      ],
      "published": "2025-12-18T01:56:11+00:00",
      "url": "https://arxiv.org/pdf/2512.16080v1",
      "resource_uri": "arxiv://2512.16080v1"
    },
    {
      "id": "2512.15720v1",
      "title": "Hidden Order in Trades Predicts the Size of Price Moves",
      "authors": [
        "Mainak Singha"
      ],
      "abstract": "Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p < 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.",
      "categories": [
        "q-fin.TR",
        "q-fin.ST",
        "stat.AP",
        "stat.ME"
      ],
      "published": "2025-12-02T23:20:46+00:00",
      "url": "https://arxiv.org/pdf/2512.15720v1",
      "resource_uri": "arxiv://2512.15720v1"
    },
    {
      "id": "2601.07852v1",
      "title": "Utility-Weighted Forecasting and Calibration for Risk-Adjusted Decisions under Trading Frictions",
      "authors": [
        "Craig S Wright"
      ],
      "abstract": "Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utility-weighted calibration reduces realised decision loss by over 30\\% relative to an uncalibrated baseline ($t$-stat -30.31) for loss differential and improves the Sharpe ratio from -3.62 to -2.29 during a drawdown regime. The mechanism is identified as a structural reduction in the frequency of binding constraints (from 16.0\\% to 5.1\\%), preventing the \"corner solution\" failures that characterize overconfident forecasts in high-friction environments.",
      "categories": [
        "econ.EM",
        "q-fin.CP",
        "q-fin.PM",
        "q-fin.TR"
      ],
      "published": "2026-01-09T01:11:21+00:00",
      "url": "https://arxiv.org/pdf/2601.07852v1",
      "resource_uri": "arxiv://2601.07852v1"
    },
    {
      "id": "2601.13006v1",
      "title": "Realised quantile-based estimation of the integrated variance",
      "authors": [
        "Kim Christensen",
        "Roel Oomen",
        "Mark Podolskij"
      ],
      "abstract": "In this paper, we propose a new jump robust quantile-based realised variance measure of ex-post return variation that can be computed using potentially noisy data. The estimator is consistent for the integrated variance and we present feasible central limit theorems which show that it converges at the best attainable rate and has excellent efficiency. Asymptotically, the quantile-based realised variance is immune to finite activity jumps and outliers in the price series, while in modified form the estimator is applicable with market microstructure noise and therefore operational on high-frequency data. Simulations show that it has superior robustness properties in finite sample, while an empirical application illustrates its use on equity data.",
      "categories": [
        "econ.EM"
      ],
      "published": "2026-01-19T12:34:05+00:00",
      "url": "https://arxiv.org/pdf/2601.13006v1",
      "resource_uri": "arxiv://2601.13006v1"
    },
    {
      "id": "2601.15045v1",
      "title": "Coupled gas and bubble dynamics at the solidification front",
      "authors": [
        "Bastien Isabella",
        "C\u00e9cile Monteux",
        "Sylvain Deville"
      ],
      "abstract": "The formation and entrapment of gas bubbles during solidification significantly influence the microstructure and mechanical properties of materials, from metallic alloys to ice. While gas segregation at the solidification front is well-documented, the real-time dynamics of bubble nucleation, growth, and engulfment-and their dependence on solidification velocity-remain poorly understood. In this study, we use in situ cryo-confocal fluorescence microscopy to investigate the coupled gas-bubble dynamics at the solidification front of carbonated water, systematically varying the solidification velocity ($V = 1-20 \u03bcm/s$) while maintaining a constant thermal gradient ($G = 15 K/mm$). Our experiments reveal that bubble nucleation is governed by a characteristic nucleation time, which emerges from the interplay between gas diffusion ahead of the front, nucleation kinetics, and bubble growth, all competing with the advancing solidification front. These results allow us to estimate the critical gas concentration for bubbles nucleation in carbonated water. These results offer a detailed understanding of the mechanisms controlling bubble nucleation and entrapment during solidification at constant thermal gradient. They contribute to the development of strategies to control bubble formation in industrial processes where the presence of bubbles can either be detrimental or intentionally harnessed.",
      "categories": [
        "cond-mat.soft",
        "cond-mat.mtrl-sci",
        "physics.flu-dyn"
      ],
      "published": "2026-01-21T14:48:16+00:00",
      "url": "https://arxiv.org/pdf/2601.15045v1",
      "resource_uri": "arxiv://2601.15045v1"
    },
    {
      "id": "2601.15685v1",
      "title": "Global regularity for the Navier-Stokes equations with application to global solvability for the Euler equations",
      "authors": [
        "Myong-Hwan Ri"
      ],
      "abstract": "We show that any Leray-Hopf weak solution to the $d$-dimensional Navier-Stokes equations $(d\\geq 3)$ with initial values $u_0\\in H^{s}(\\mathbb R^d)$, $s\\geq -1+\\frac{d}{2}$, belongs to $L^\\infty(0,\\infty; H^{s}(\\mathbb R^d))$ and thus it is globally regular. For the proof, first, we construct a supercritical space which has very sparse inverse logarithmic weight in the frequency domain, compared to the critical homogeneous Sobolev $\\dot{H}^{-1+d/2}$-norm. Then we obtain the energy estimates of high frequency parts of the solution which involve the supercritical norm as a factor of the upper bounds. Finally, we superpose the energy norm of high frequency parts of the solution to get estimates of the critical and subcritical norms independent of the viscosity coefficient for the weak solution via the re-scaling argument.",
      "categories": [
        "math.AP",
        "math-ph"
      ],
      "published": "2026-01-22T06:09:46+00:00",
      "url": "https://arxiv.org/pdf/2601.15685v1",
      "resource_uri": "arxiv://2601.15685v1"
    },
    {
      "id": "2601.15561v1",
      "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems",
      "authors": [
        "Naoya Onizawa",
        "Takahiro Hanyu"
      ],
      "abstract": "This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.",
      "categories": [
        "cs.ET",
        "cs.LG"
      ],
      "published": "2026-01-22T01:01:35+00:00",
      "url": "https://arxiv.org/pdf/2601.15561v1",
      "resource_uri": "arxiv://2601.15561v1"
    },
    {
      "id": "2601.15739v1",
      "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework",
      "authors": [
        "Xinjue Hu",
        "Chi Wang",
        "Boyu Wang",
        "Xiang Zhang",
        "Zhenshan Tan",
        "Zhangjie Fu"
      ],
      "abstract": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T08:07:10+00:00",
      "url": "https://arxiv.org/pdf/2601.15739v1",
      "resource_uri": "arxiv://2601.15739v1"
    },
    {
      "id": "2601.15968v1",
      "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
      "authors": [
        "Xin Xie",
        "Jiaxian Guo",
        "Dong Gong"
      ],
      "abstract": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T13:49:47+00:00",
      "url": "https://arxiv.org/pdf/2601.15968v1",
      "resource_uri": "arxiv://2601.15968v1"
    },
    {
      "id": "2601.16047v1",
      "title": "Enhanced Representation-Based Sampling for the Efficient Generation of Datasets for Machine-Learned Interatomic Potentials",
      "authors": [
        "Moritz Ren\u00e9 Sch\u00e4fer",
        "Johannes K\u00e4stner"
      ],
      "abstract": "In this work, we present Enhanced Representation-Based Sampling (ERBS), a novel enhanced sampling method designed to generate structurally diverse training datasets for machine-learned interatomic potentials. ERBS automatically identifies collective variables by dimensionality reduction of atomic descriptors and applies a bias potential inspired by the On-the-Fly Probability Enhanced Sampling framework. We highlight the ability of Gaussian moment descriptors to capture collective molecular motions and explore the impact of biasing parameters using alanine dipeptide as a benchmark system. We show that free energy surfaces can be reconstructed with high fidelity using only short biased trajectories as training data. Further, we apply the method to the iterative construction of a liquid water dataset and compare the quality of simulated self-diffusion coefficients for models trained with molecular dynamics and ERBS data. Further, we active-learn models for liquid water with and without enhanced sampling and compare the quality of simulated self-diffusion coefficients. The self-diffusion coefficients closely match those simulated with a reference model at a significantly reduced dataset size. Finally, we compare the sampling behaviour of enhanced sampling methods by benchmarking the mean squared displacements of \\ce{BMIM+BF4-} trajectories simulated with uncertainty-driven dynamics and ERBS and find that the latter significantly increases the exploration of configurational space.",
      "categories": [
        "physics.chem-ph"
      ],
      "published": "2026-01-22T15:26:04+00:00",
      "url": "https://arxiv.org/pdf/2601.16047v1",
      "resource_uri": "arxiv://2601.16047v1"
    },
    {
      "id": "2512.05011v1",
      "title": "Risk aversion of insider and dynamic asymmetric information",
      "authors": [
        "Albina Danilova",
        "Valentin Lizhdvoy"
      ],
      "abstract": "This paper studies a Kyle-Back model with a risk-averse insider possessing exponential utility and a dynamic stochastic signal about the asset's terminal fundamental value. While the existing literature considers either risk-neutral insiders with dynamic signals or risk-averse insiders with static signals, we establish equilibrium when both features are present. Our approach imposes no restrictions on the magnitude of the risk aversion parameter, extending beyond previous work that requires sufficiently small risk aversion. We employ a weak conditioning methodology to construct a Schr\u00f6dinger bridge between the insider's signal and the asset price process, an approach that naturally accommodates stochastic signal evolution and removes risk aversion constraints.\n  We derive necessary conditions for equilibrium, showing that the optimal insider strategy must be continuous with bounded variation. Under these conditions, we characterize the market-maker pricing rule and insider strategy that achieve equilibrium. We obtain explicit closed-form solutions for important cases including deterministic and quadratic signal volatilities, demonstrating the tractability of our framework.",
      "categories": [
        "q-fin.MF",
        "q-fin.TR"
      ],
      "published": "2025-12-04T17:21:52+00:00",
      "url": "https://arxiv.org/pdf/2512.05011v1",
      "resource_uri": "arxiv://2512.05011v1"
    },
    {
      "id": "2512.05645v1",
      "title": "On the supremum of a quotient of power sums",
      "authors": [
        "Stefan Gerhold",
        "Friedrich Hubalek"
      ],
      "abstract": "We define a function of two real vectors by a certain homogeneous quotient involving power sums, and show that its supremum grows asymptotically linearly w.r.t. the dimension. From this, we deduce a condition under which a parametric set of real matrices satisfies a set of polynomial positivity constraints. This characterization finds an application in mathematical finance, in a recent study on price impact models.",
      "categories": [
        "math.CA"
      ],
      "published": "2025-12-05T11:44:57+00:00",
      "url": "https://arxiv.org/pdf/2512.05645v1",
      "resource_uri": "arxiv://2512.05645v1"
    },
    {
      "id": "2512.24432v1",
      "title": "Automated Market Making for Energy Sharing",
      "authors": [
        "Michele Fabi",
        "Viraj Nadkarni",
        "Leonardo Leone",
        "Matheus X. V. Ferreira"
      ],
      "abstract": "We develop an axiomatic theory for Automated Market Makers (AMMs) in local energy sharing markets and analyze the Markov Perfect Equilibrium of the resulting economy with a Mean-Field Game. In this game, heterogeneous prosumers solve a Bellman equation to optimize energy consumption, storage, and exchanges. Our axioms identify a class of mechanisms with linear, Lipschitz continuous payment functions, where prices decrease with the aggregate supply-to-demand ratio of energy. We prove that implementing batch execution and concentrated liquidity allows standard design conditions from decentralized finance-quasi-concavity, monotonicity, and homotheticity-to construct AMMs that satisfy our axioms. The resulting AMMs are budget-balanced and achieve ex-ante efficiency, contrasting with the strategy-proof, expost optimal VCG mechanism. Since the AMM implements a Potential Game, we solve its equilibrium by first computing the social planner's optimum and then decentralizing the allocation. Numerical experiments using data from the Paris administrative region suggest that the prosumer community can achieve gains from trade up to 40% relative to the grid-only benchmark.",
      "categories": [
        "econ.TH",
        "cs.GT"
      ],
      "published": "2025-12-30T19:33:37+00:00",
      "url": "https://arxiv.org/pdf/2512.24432v1",
      "resource_uri": "arxiv://2512.24432v1"
    },
    {
      "id": "2601.04687v1",
      "title": "WebCryptoAgent: Agentic Crypto Trading with Web Informatics",
      "authors": [
        "Ali Kurban",
        "Wei Luo",
        "Liangyu Zuo",
        "Zeyu Zhang",
        "Renda Han",
        "Zhaolu Kang",
        "Hao Tang"
      ],
      "abstract": "Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-08T07:55:10+00:00",
      "url": "https://arxiv.org/pdf/2601.04687v1",
      "resource_uri": "arxiv://2601.04687v1"
    },
    {
      "id": "2601.00738v1",
      "title": "Second Thoughts: How 1-second subslots transform CEX-DEX Arbitrage on Ethereum",
      "authors": [
        "Aleksei Adadurov",
        "Sergey Barseghyan",
        "Anton Chtepine",
        "Antero Eloranta",
        "Andrei Sebyakin",
        "Arsenii Valitov"
      ],
      "abstract": "This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agent's DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereum's default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing.",
      "categories": [
        "q-fin.TR",
        "q-fin.CP"
      ],
      "published": "2026-01-02T16:39:25+00:00",
      "url": "https://arxiv.org/pdf/2601.00738v1",
      "resource_uri": "arxiv://2601.00738v1"
    },
    {
      "id": "2512.14134v2",
      "title": "Sources and Nonlinearity of High Volume Return Premium: An Empirical Study on the Differential Effects of Investor Identity versus Trading Intensity (2020-2024)",
      "authors": [
        "Sungwoo Kang"
      ],
      "abstract": "Chae and Kang (2019, \\textit{Pacific-Basin Finance Journal}) documented a puzzling Low Volume Return Premium (LVRP) in Korea -- contradicting global High Volume Return Premium (HVRP) evidence. We resolve this puzzle. Using Korean market data (2020-2024), we demonstrate that HVRP exists in Korea but is masked by (1) pooling heterogeneous investor types and (2) using inappropriate intensity normalization. When institutional buying intensity is normalized by market capitalization rather than trading value, a perfect monotonic relationship emerges: highest-conviction institutional buying (Q4) generates +\\institutionLedQFourDayPlusFiftyCAR\\ cumulative abnormal returns over 50 days, while lowest-intensity trades (Q1) yield modest returns (+\\institutionLedQOneDayPlusFiftyCAR). Retail investors exhibit a flat pattern -- their trading generates near-zero returns regardless of conviction level -- confirming the pure noise trader hypothesis. During the Donghak Ant Movement (2020-2021), however, coordinated retail investors temporarily transformed from noise traders to liquidity providers, generating returns comparable to institutional trading. Our findings reconcile conflicting international evidence and demonstrate that detecting informed trading signals requires investor-type decomposition, nonlinear quartile analysis, and conviction-based (market cap) rather than participation-based (trading value) measurement.",
      "categories": [
        "q-fin.TR"
      ],
      "published": "2025-12-16T06:32:04+00:00",
      "url": "https://arxiv.org/pdf/2512.14134v2",
      "resource_uri": "arxiv://2512.14134v2"
    },
    {
      "id": "2601.02310v1",
      "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
      "authors": [
        "Ahmad Makinde"
      ],
      "abstract": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.",
      "categories": [
        "cs.LG",
        "q-fin.TR"
      ],
      "published": "2026-01-05T17:59:42+00:00",
      "url": "https://arxiv.org/pdf/2601.02310v1",
      "resource_uri": "arxiv://2601.02310v1"
    },
    {
      "id": "2601.06084v1",
      "title": "Who sets the range? Funding mechanics and 4h context in crypto markets",
      "authors": [
        "Habib Badawi",
        "Mohamed Hani",
        "Taufikin Taufikin"
      ],
      "abstract": "Financial markets often appear chaotic, yet ranges are rarely accidental. They emerge from structured interactions between market context and capital conditions. The four-hour timeframe provides a critical lens for observing this equilibrium zone where institutional positioning, leveraged exposure, and liquidity management converge. Funding mechanisms, especially in perpetual futures, act as disciplinary forces that regulate trader behavior, impose economic costs, and shape directional commitment. When funding aligns with the prevailing 4H context, price expansion becomes possible; when it diverges, compression and range-bound behavior dominate. Ranges therefore represent controlled balance rather than indecision, reflecting strategic positioning by informed participants. Understanding how 4H context and funding operate as market governors is essential for interpreting cryptocurrency price action as a rational, power-mediated process.",
      "categories": [
        "q-fin.GN",
        "econ.GN"
      ],
      "published": "2025-12-31T00:19:59+00:00",
      "url": "https://arxiv.org/pdf/2601.06084v1",
      "resource_uri": "arxiv://2601.06084v1"
    },
    {
      "id": "2601.10143v1",
      "title": "History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis",
      "authors": [
        "Haochong Xia",
        "Yao Long Teng",
        "Regan Tan",
        "Molei Qin",
        "Xinrun Wang",
        "Bo An"
      ],
      "abstract": "In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra \"History Is Not Enough\" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.",
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "published": "2026-01-15T07:38:59+00:00",
      "url": "https://arxiv.org/pdf/2601.10143v1",
      "resource_uri": "arxiv://2601.10143v1"
    },
    {
      "id": "2601.14529v1",
      "title": "From Columns to Heaps: Dimensionless Similarity with PSD-Distributed Damk\u00f6hler Numbers and Dual-Porosity Flow",
      "authors": [
        "Juan J. Segura"
      ],
      "abstract": "This work develops a unified, dimensionless framework for comparing geometrically similar reacting porous-flow systems across scale, with emphasis on hydrometallurgical heap leaching, when particle size distribution (PSD) and intraparticle pore structure differ. Under dynamic similarity, the dimensionless liquid residence-time distribution (RTD) is identical, but differences in PSD and internal porosity break microscopic similarity. Using the shrinking-core model (SCM), the analysis shows how a PSD in particle diameter maps to a distribution of particle-scale Damk\u00f6hler numbers that governs heap-averaged conversion.\n  Explicit PSD to Damk\u00f6hler transformations are derived for (i) external film control, (ii) intraparticle diffusion control, and (iii) mixed control via additive rates. Dual-porosity hydrology relevant to sedimentary or strongly stratified ores is then incorporated by coupling SCM kinetics to mobile and immobile liquid domains, introducing additional dimensionless groups that describe interporosity exchange. Two numerical examples map a lognormal PSD into film- and diffusion-controlled Damk\u00f6hler distributions and compare column/heap conversion for different PSDs. A practical workflow is outlined for hydrometallurgical column-test interpretation, combining tracer RTD calibration with PSD-aware kinetic fitting in dimensionless time. The framework clarifies why diffusion-controlled leaching is far more sensitive to PSD tails and dual-porosity structure than film-controlled leaching, and it identifies the compact set of dimensionless groups that must be matched to ensure similarity between laboratory columns and industrial heaps.",
      "categories": [
        "physics.flu-dyn",
        "physics.geo-ph"
      ],
      "published": "2026-01-20T22:52:58+00:00",
      "url": "https://arxiv.org/pdf/2601.14529v1",
      "resource_uri": "arxiv://2601.14529v1"
    },
    {
      "id": "2601.15098v1",
      "title": "Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction",
      "authors": [
        "Yipeng Yin",
        "Rao Yao",
        "Qingying Li",
        "Dazhong Wang",
        "Hong Zhou",
        "Zhijun Fang",
        "Jianing Chen",
        "Longjie Qian",
        "Mingyue Wu"
      ],
      "abstract": "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-21T15:37:38+00:00",
      "url": "https://arxiv.org/pdf/2601.15098v1",
      "resource_uri": "arxiv://2601.15098v1"
    },
    {
      "id": "2601.15085v1",
      "title": "The Hadron-Parton Bridge, From the QCD Vacuum to Partons",
      "authors": [
        "Edward Shuryak",
        "Ismail Zahed"
      ],
      "abstract": "Quantum Chromodynamics (QCD) exhibits complementary descriptions of hadrons: a rest-frame picture based on confinement, chiral symmetry breaking and interquark forces, and a high-energy light-front picture expressed through parton distributions (PDFs,TMDs,GPDs) and form factors. This review develops a unified framework that connects these two domains. It is based mostly on multiple studies by the authors in the past few years. Using the Instanton Liquid Model (ILM) to capture essential nonperturbative features of the QCD vacuum, we derive effective interactions for mesons, baryons, and multiquark states, construct their wave functions in hyperspherical coordinates, and boost them to the light front. The resulting light-front Hamiltonians, incorporating both perturbative and instanton-induced dynamics in the Wilsonian spirit, provide realistic nonperturbative inputs for computing PDFs, DAs, GPDs, quasi-distributions, and gravitational form factors at a well-defined low scale. The connection to perturbative QCD is then established by matching gradient-flow-renormalized operators and LF wave functions to the standard $\\overline{\\rm MS}$ scheme. Perturbative DGLAP and ERBL evolution then connects these predictions to experimentally accessible regimes. % This approach is applied to quarkonia, glueballs, light mesons, baryons, tetraquarks, pentaquarks, and higher multiquark hadrons, yielding consistent descriptions of both their spectra and partonic structure. Special emphasis is placed on the energy-momentum tensor and the mechanical properties of hadrons, which emerge naturally from the same dynamical ingredients. Overall, the framework demonstrates a clear continuity between hadronic spectroscopy and partonic observables, offering a coherent multiscale picture of hadron structure rooted in the underlying dynamics of QCD.",
      "categories": [
        "hep-ph"
      ],
      "published": "2026-01-21T15:26:33+00:00",
      "url": "https://arxiv.org/pdf/2601.15085v1",
      "resource_uri": "arxiv://2601.15085v1"
    },
    {
      "id": "2601.14719v1",
      "title": "Physicochemical properties of lunar regolith simulant for in situ oxygen production",
      "authors": [
        "Alyssa Ang De Guzman",
        "Anish Mathai Varghese",
        "Saif Alshalloudi",
        "Lance Kosca",
        "Kyriaki Polychronopoulou",
        "Marko Gacesa"
      ],
      "abstract": "Permanent lunar settlements will rely on in situ oxygen production from regolith for life support and propulsion. While oxygen is abundant in lunar materials, it is chemically bound within metal oxides whose extractability depends strongly on regolith composition and processing strategy. In this study, we validate and characterize high-fidelity lunar regolith simulants representative of the lunar highlands and south pole using scanning electron microscopy with energy-dispersive X-ray spectroscopy, X-ray diffraction, Brunauer-Emmett-Teller surface area and pore structure analysis, and hydrogen temperature-programmed reduction. The simulants exhibit strong mineralogical and compositional fidelity to returned Apollo and Chang'e samples, with ilmenite confirmed as the most readily reducible oxygen-bearing phase. However, despite low ilmenite abundance, bulk highland simulants display favorable reduction behavior arising from distributed Fe-bearing silicate and glassy phases, as well as surface and microstructural properties that influence gas-solid interactions. Adsorption experiments with gases (H2, CH4, and CO2) and water indicate that mineralogical heterogeneity and pore accessibility influence their uptake in simulants. These results indicate that oxygen extraction behavior in realistic lunar regolith is governed by whole-regolith response rather than ilmenite content alone, supporting the option of whole-regolith processing strategies for oxygen production in lunar in situ resource utilization architectures.",
      "categories": [
        "physics.space-ph",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-01-21T07:19:10+00:00",
      "url": "https://arxiv.org/pdf/2601.14719v1",
      "resource_uri": "arxiv://2601.14719v1"
    },
    {
      "id": "2601.15709v1",
      "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL",
      "authors": [
        "Asim Biswal",
        "Chuan Lei",
        "Xiao Qin",
        "Aodong Li",
        "Balakrishnan Narayanaswamy",
        "Tim Kraska"
      ],
      "abstract": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "published": "2026-01-22T07:31:19+00:00",
      "url": "https://arxiv.org/pdf/2601.15709v1",
      "resource_uri": "arxiv://2601.15709v1"
    },
    {
      "id": "2601.15703v1",
      "title": "Agentic Uncertainty Quantification",
      "authors": [
        "Jiaxin Zhang",
        "Prafulla Kumar Choubey",
        "Kung-Hsiang Huang",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22T07:16:26+00:00",
      "url": "https://arxiv.org/pdf/2601.15703v1",
      "resource_uri": "arxiv://2601.15703v1"
    },
    {
      "id": "2601.15676v1",
      "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
      "authors": [
        "Hengfan Zhang",
        "Yueqian Lin",
        "Hai Helen Li",
        "Yiran Chen"
      ],
      "abstract": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "published": "2026-01-22T05:57:25+00:00",
      "url": "https://arxiv.org/pdf/2601.15676v1",
      "resource_uri": "arxiv://2601.15676v1"
    },
    {
      "id": "2601.15594v1",
      "title": "Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock",
      "authors": [
        "Zhixian Zhou",
        "Bin Chen",
        "Zhe Peng",
        "Zhiming Liang",
        "Ruijun Wu",
        "Chen Sun",
        "Shuo Wang"
      ],
      "abstract": "As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability.",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-01-22T02:40:37+00:00",
      "url": "https://arxiv.org/pdf/2601.15594v1",
      "resource_uri": "arxiv://2601.15594v1"
    },
    {
      "id": "2601.15584v1",
      "title": "Amalgamated CHIRP and OFDM for ISAC",
      "authors": [
        "Pankaj Kumar",
        "Mohammed El-Hajjar",
        "Ibrahim A. Hemadeh",
        "Yasser Mestrah",
        "Suraj Srivastava",
        "Aditya K. Jagannatham",
        "Lajos Hanzo"
      ],
      "abstract": "Integrated Sensing and Communication (ISAC) requires the development of a waveform capable of efficiently supporting both communication and sensing functionalities. This paper proposes a novel waveform that combines the benefits of both the orthogonal frequency division multiplexing (OFDM) and the chirp waveforms to improve both the communication and sensing performance within an ISAC framework. Hence, a new architecture is proposed that utilizes the conventional communication framework while leveraging the parameters sensed at the receiver (Rx) for enhancing the communication performance. We demonstrate that the affine addition of OFDM and chirp signals results in a near constant-envelope OFDM waveform, which effectively reduces the peak-to-average power ratio (PAPR), a key limitation of traditional OFDM systems. Using the OFDM framework for sensing in the conventional fashion requires the allocation of some resources for sensing, which in turn reduces communication performance. As a remedy, the proposed affine amalgam facilitates sensing through the chirp waveform without consuming communication resources, thereby preserving communication efficiency. Furthermore, a novel technique of integrating the chirp signal into the OFDM framework at the slot-level is proposed to enhance the accuracy of range estimation. The results show that the OFDM signal incorporated with chirp has better autocorrelation properties, improved root mean square error (RMSE) of range and velocity, and lower PAPR. Finally, we characterize the trade-off between communications and sensing performance.",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-22T02:07:55+00:00",
      "url": "https://arxiv.org/pdf/2601.15584v1",
      "resource_uri": "arxiv://2601.15584v1"
    },
    {
      "id": "2601.16087v1",
      "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics",
      "authors": [
        "Sukesh Subaharan"
      ],
      "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22T16:34:05+00:00",
      "url": "https://arxiv.org/pdf/2601.16087v1",
      "resource_uri": "arxiv://2601.16087v1"
    },
    {
      "id": "2601.16004v1",
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "authors": [
        "Christopher Altman"
      ],
      "abstract": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
      "categories": [
        "quant-ph",
        "cs.ET"
      ],
      "published": "2026-01-22T14:30:09+00:00",
      "url": "https://arxiv.org/pdf/2601.16004v1",
      "resource_uri": "arxiv://2601.16004v1"
    },
    {
      "id": "2601.15912v1",
      "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
      "authors": [
        "Ariyan Bighashdel",
        "Kevin Sebastian Luck"
      ],
      "abstract": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-01-22T12:42:30+00:00",
      "url": "https://arxiv.org/pdf/2601.15912v1",
      "resource_uri": "arxiv://2601.15912v1"
    },
    {
      "id": "2601.15904v1",
      "title": "Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links",
      "authors": [
        "Hossein Mohammadalizadeh",
        "Holger Karl"
      ],
      "abstract": "Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.",
      "categories": [
        "cs.NI"
      ],
      "published": "2026-01-22T12:31:17+00:00",
      "url": "https://arxiv.org/pdf/2601.15904v1",
      "resource_uri": "arxiv://2601.15904v1"
    },
    {
      "id": "2601.15831v1",
      "title": "Performance Analysis of Digital Beamforming mmWave MIMO with Low-Resolution DACs/ADCs",
      "authors": [
        "Faruk Pasic",
        "Mariam Mussbah",
        "Stefan Schwarz",
        "Markus Rupp",
        "Fredrik Tufvesson",
        "Christoph F. Mecklenbr\u00e4uker"
      ],
      "abstract": "Future wireless communications will rely on multiple-input multiple-output (MIMO) beamforming operating at millimeter wave (mmWave) frequency bands to deliver high data rates. To support flexible spatial processing and meet the demands of latency critical applications, it is essential to use fully digital mmWave MIMO beamforming, which relies on accurate channel estimation. However, ensuring power efficiency in fully digital mmWave MIMO systems requires the use of low-resolution digital-to-analog converters (DACs) and analog-to-digital converters (ADCs). The reduced resolution of these quantizers introduces distortion in both transmitted and received signals, ultimately degrading system performance. In this paper, we investigate the channel estimation performance of mmWave MIMO systems employing fully digital beamforming with low-resolution quantization, under practical system constraints. We evaluate the system performance in terms of spectral efficiency (SE) and energy efficiency (EE). Simulation results demonstrate that a moderate quantization resolutions of 4-bit per DAC/ADC offers a favorable trade-off between energy consumption and achievable data rate.",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-01-22T10:35:03+00:00",
      "url": "https://arxiv.org/pdf/2601.15831v1",
      "resource_uri": "arxiv://2601.15831v1"
    },
    {
      "id": "2601.15803v1",
      "title": "Optimal stochastic impulse control problem with delay with actions decided at the execution time",
      "authors": [
        "Said Hamad\u00e8ne",
        "Ibtissam Hdhiri"
      ],
      "abstract": "In this paper, we consider a class of stochastic impulse control problem when there is a fixed delay $\u0394$ between the decision and execution times. The dynamics of the controlled system between two impulses is an arbitrary adapted stochastic process. Unlike the most existing literature, we consider the problem when the impulse sizes are decided at the execution time in both risk-neutral and risk-sensitive cases. This model fits more, in the real life, for some problems such as the pricing of swing options. The horizon T of the problem can be finite or infinite. In each case we show the existence of an optimal strategy. The main tools we use are the notions of reflected Backward Stochastic Differential Equations (BSDEs for short) and the Snell envelope of processes.",
      "categories": [
        "math.PR"
      ],
      "published": "2026-01-22T09:38:21+00:00",
      "url": "https://arxiv.org/pdf/2601.15803v1",
      "resource_uri": "arxiv://2601.15803v1"
    },
    {
      "id": "2601.15728v1",
      "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity",
      "authors": [
        "Hangle Hu",
        "Chenyu Hou",
        "Bin Cao",
        "Ruizhe Li"
      ],
      "abstract": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "published": "2026-01-22T07:54:45+00:00",
      "url": "https://arxiv.org/pdf/2601.15728v1",
      "resource_uri": "arxiv://2601.15728v1"
    },
    {
      "id": "2601.15938v1",
      "title": "Operating a large-diameter dual-phase liquid xenon TPC in the unshielded PANCAKE facility",
      "authors": [
        "Julia M\u00fcller",
        "Jaron Grigat",
        "Robin Glade-Beucke",
        "Sebastian Lindemann",
        "Tiffany Luce",
        "Gnanesh Chandra Madduri",
        "Jens Reininghaus",
        "Marc Schumann",
        "Adam Softley-Brown",
        "Andrew Stevens"
      ],
      "abstract": "Future liquid-xenon (LXe) based observatories for rare processes, such as XLZD, require testing of large components and sub-assemblies in cryogenic liquid or gaseous xenon environments. Here we present results from the stable operation of a shallow dual-phase LXe TPC with an inner diameter of 133.4\\,cm and a height of 3.1\\,cm in the unshielded PANCAKE platform, without underground suppression of cosmic-ray backgrounds. A total of 340\\,kg of xenon was used in the experiment, of which 127\\,kg constituted the active TPC mass. Measurements of the LXe purity-dependent electron lifetime and the electron drift velocity in LXe demonstrate that sensitive measurements to characterize the TPC performance are possible in a high-background environment, even with a very basic PMT-based light detection system. Improving this will straightforwardly reduce the TPC threshold, which was observed to be around 15\\,keV for electronic recoils in TPC operation.",
      "categories": [
        "physics.ins-det"
      ],
      "published": "2026-01-22T13:18:41+00:00",
      "url": "https://arxiv.org/pdf/2601.15938v1",
      "resource_uri": "arxiv://2601.15938v1"
    },
    {
      "id": "2601.15377v1",
      "title": "Exactly solvable topological phase transition in a quantum dimer model",
      "authors": [
        "Laura Shou",
        "Jeet Shah",
        "Matthew Lerner-Brecher",
        "Amol Aggarwal",
        "Alexei Borodin",
        "Victor Galitski"
      ],
      "abstract": "We introduce a family of generalized Rokhsar-Kivelson (RK) Hamiltonians, which are reverse-engineered to have an arbitrary edge-weighted superposition of dimer coverings as their exact ground state at the RK point. We then focus on a quantum dimer model on the triangular lattice, with doubly-periodic edge weights. For simplicity we consider a $2\\times1$ periodic model in which all weights are set to one except for a tunable horizontal edge weight labeled $\u03b1$. We analytically show that the model exhibits a continuous quantum phase transition at $\u03b1=3$, changing from a topological $\\mathbb{Z}_2$ quantum spin liquid ($\u03b1<3$) to a columnar ordered state ($\u03b1>3$). The dimer-dimer correlator decays exponentially on both sides of the transition with the correlation length $\u03be\\propto1/|\u03b1-3|$ and as a power-law at criticality. The vison correlator exhibits an exponential decay in the spin liquid phase, but becomes a constant in the ordered phase. We explain the constant vison correlator in terms of loops statistics of the double-dimer model. Using finite-size scaling of the vison correlator, we extract critical exponents consistent with the 2D Ising universality class.",
      "categories": [
        "cond-mat.str-el",
        "cond-mat.stat-mech",
        "math-ph",
        "quant-ph"
      ],
      "published": "2026-01-21T19:00:01+00:00",
      "url": "https://arxiv.org/pdf/2601.15377v1",
      "resource_uri": "arxiv://2601.15377v1"
    },
    {
      "id": "2512.04440v1",
      "title": "From informal markets to Limit Order Book dynamics: a mean field connection",
      "authors": [
        "Alvaro Navarro-Rubio",
        "Alejandro Lage-Castellanos"
      ],
      "abstract": "We propose a unified mean-field framework that bridges the dynamics of informal financial markets and formal markets governed by Limit Order Books (LOBs). Both settings are modeled as interacting particle systems on a 1D price lattice, with temporal evolution described by master equations that account for new entries, cancellations, and executions. The key insight is the introduction of a preferential interaction parameter $\u03a8$, which modulates the likelihood of transactions based on price compatibility: when $\u03a8=0$, interactions are random and uncoordinated, reproducing the structure of informal markets; as $\u03a8\\to \\infty$, only optimal (most mutually attractive) trades occur, recovering LOB-like dynamics. A grand-canonical interpretation is used to identify effective thermodynamic quantities -such as interaction energy and price-dependent chemical potentials -that underlie both systems. Most results are validated through numerical integration and simulations, although an analytical solution is shown to exist at least for the symmetric stationary case of the informal market.",
      "categories": [
        "cond-mat.stat-mech"
      ],
      "published": "2025-12-04T04:13:48+00:00",
      "url": "https://arxiv.org/pdf/2512.04440v1",
      "resource_uri": "arxiv://2512.04440v1"
    },
    {
      "id": "2512.15732v1",
      "title": "The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading",
      "authors": [
        "Yijia Chen"
      ],
      "abstract": "The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the \"Holy Grail\" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of \"Galaxy Empire,\" a hybrid framework coupling LSTM/Transformer-based perception with a genetic \"Time-is-Life\" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\\%$) and live performance (Capital Decay $>70\\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \\textit{Aleatoric Uncertainty} in low-entropy time-series, the \\textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.",
      "categories": [
        "q-fin.TR",
        "cs.LG",
        "cs.NE",
        "q-fin.CP"
      ],
      "published": "2025-12-05T19:30:26+00:00",
      "url": "https://arxiv.org/pdf/2512.15732v1",
      "resource_uri": "arxiv://2512.15732v1"
    },
    {
      "id": "2512.12250v1",
      "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting",
      "authors": [
        "Anna Perekhodko",
        "Robert \u015alepaczuk"
      ],
      "abstract": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "q-fin.PM"
      ],
      "published": "2025-12-13T09:21:43+00:00",
      "url": "https://arxiv.org/pdf/2512.12250v1",
      "resource_uri": "arxiv://2512.12250v1"
    },
    {
      "id": "2512.19251v1",
      "title": "Institutional Backing and Crypto Volatility: A Hybrid Framework for DeFi Stabilization",
      "authors": [
        "Ihlas Sovbetov"
      ],
      "abstract": "Decentralized finance (DeFi) lacks centralized oversight, often resulting in heightened volatility. In contrast, centralized finance (CeFi) offers a more stable environment with institutional safeguards. Institutional backing can play a stabilizing role in a hybrid structure (HyFi), enhancing transparency, governance, and market discipline. This study investigates whether HyFi-like cryptocurrencies, those backed by institutions, exhibit lower price risk than fully decentralized counterparts. Using daily data for 18 major cryptocurrencies from January 2020 to November 2024, we estimate panel EGLS models with fixed, random, and dynamic specifications. Results show that HyFi-like assets consistently experience lower price risk, with this effect intensifying during periods of elevated market volatility. The negative interaction between HyFi status and market-wide volatility confirms their stabilizing role. Conversely, greater decentralization is strongly associated with increased volatility, particularly during periods of market stress. Robustness checks using quantile regressions and pre-/post-Terra Luna subsamples reinforce these findings, with stronger effects observed in high-volatility quantiles and post-crisis conditions. These results highlight the importance of institutional architecture in enhancing the resilience of digital asset markets.",
      "categories": [
        "q-fin.CP",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "published": "2025-12-22T10:35:37+00:00",
      "url": "https://arxiv.org/pdf/2512.19251v1",
      "resource_uri": "arxiv://2512.19251v1"
    },
    {
      "id": "2512.23847v1",
      "title": "A Test of Lookahead Bias in LLM Forecasts",
      "authors": [
        "Zhenyu Gao",
        "Wenxi Jiang",
        "Yutong Yan"
      ],
      "abstract": "We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state-of-the-art pre-training data detection techniques, we estimate the likelihood that a given prompt appeared in an LLM's training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts.",
      "categories": [
        "q-fin.GN",
        "cs.LG",
        "q-fin.TR"
      ],
      "published": "2025-12-29T20:20:04+00:00",
      "url": "https://arxiv.org/pdf/2512.23847v1",
      "resource_uri": "arxiv://2512.23847v1"
    },
    {
      "id": "2512.13871v2",
      "title": "A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory",
      "authors": [
        "Shaun Sweeney",
        "Robert Shorten",
        "Mark O'Malley"
      ],
      "abstract": "This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.\n  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.\n  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.\n  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.\n  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.",
      "categories": [
        "eess.SY",
        "cs.HC",
        "cs.NI",
        "stat.AP"
      ],
      "published": "2025-12-15T19:59:58+00:00",
      "url": "https://arxiv.org/pdf/2512.13871v2",
      "resource_uri": "arxiv://2512.13871v2"
    },
    {
      "id": "2601.04959v1",
      "title": "Intraday Limit Order Price Change Transition Dynamics Across Market Capitalizations Through Markov Analysis",
      "authors": [
        "Salam Rabindrajit Luwang",
        "Kundan Mukhia",
        "Buddha Nath Sharma",
        "Md. Nurujjaman",
        "Anish Rai",
        "Filippo Petroni"
      ],
      "abstract": "Quantitative understanding of stochastic dynamics in limit order price changes is essential for execution strategy design. We analyze intraday transition dynamics of ask and bid orders across market capitalization tiers using high-frequency NASDAQ100 tick data. Employing a discrete-time Markov chain framework, we categorize consecutive price changes into nine states and estimate transition probability matrices (TPMs) for six intraday intervals across High ($\\mathtt{HMC}$), Medium ($\\mathtt{MMC}$), and Low ($\\mathtt{LMC}$) market cap stocks. Element-wise TPM comparison reveals systematic patterns: price inertia peaks during opening and closing hours, stabilizing midday. A capitalization gradient is observed: $\\mathtt{HMC}$ stocks exhibit the strongest inertia, while $\\mathtt{LMC}$ stocks show lower stability and wider spreads. Markov metrics, including spectral gap, entropy rate, and mean recurrence times, quantify these dynamics. Clustering analysis identifies three distinct temporal phases on the bid side -- Opening, Midday, and Closing, and four phases on the ask side by distinguishing Opening, Midday, Pre-Close, and Close. This indicates that sellers initiate end-of-day positioning earlier than buyers. Stationary distributions show limit order dynamics are dominated by neutral and mild price changes. Jensen-Shannon divergence confirms the closing hour as the most distinct phase, with capitalization modulating temporal contrasts and bid-ask asymmetry. These findings support capitalization-aware and time-adaptive execution algorithms.",
      "categories": [
        "q-fin.ST",
        "q-fin.TR",
        "stat.AP"
      ],
      "published": "2026-01-08T14:04:02+00:00",
      "url": "https://arxiv.org/pdf/2601.04959v1",
      "resource_uri": "arxiv://2601.04959v1"
    },
    {
      "id": "2601.05975v1",
      "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
      "authors": [
        "Kieran Wood",
        "Stephen J. Roberts",
        "Stefan Zohren"
      ],
      "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
      "categories": [
        "q-fin.TR",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-01-09T17:47:32+00:00",
      "url": "https://arxiv.org/pdf/2601.05975v1",
      "resource_uri": "arxiv://2601.05975v1"
    },
    {
      "id": "2512.04603v1",
      "title": "FX Market Making with Internal Liquidity",
      "authors": [
        "Alexander Barzykin",
        "Robert Boyce",
        "Eyal Neuman"
      ],
      "abstract": "As the FX markets continue to evolve, many institutions have started offering passive access to their internal liquidity pools. Market makers act as principal and have the opportunity to fill those orders as part of their risk management, or they may choose to adjust pricing to their external OTC franchise to facilitate the matching flow. It is, a priori, unclear how the strategies managing internal liquidity should depend on market condions, the market maker's risk appetite, and the placement algorithms deployed by participating clients. The market maker's actions in the presence of passive orders are relevant not only for their own objectives, but also for those liquidity providers who have certain expectations of the execution speed. In this work, we investigate the optimal multi-objective strategy of a market maker with an option to take liquidity on an internal exchange, and draw important qualitative insights for real-world trading.",
      "categories": [
        "q-fin.TR"
      ],
      "published": "2025-12-04T09:25:14+00:00",
      "url": "https://arxiv.org/pdf/2512.04603v1",
      "resource_uri": "arxiv://2512.04603v1"
    },
    {
      "id": "2601.14645v1",
      "title": "Paraxial diffusion-field retrieval. II. Fokker-Planck generalization of the transport-of-intensity equation",
      "authors": [
        "David M. Paganin",
        "Kaye S. Morgan"
      ],
      "abstract": "The transport-of-intensity equation (TIE) has been very widely employed for phase retrieval. In particular, the TIE is an elliptic second-order partial differential equation which may be solved for the phase of a coherent paraxial field such as a monochromatic scalar optical beam, given the intensity and longitudinal intensity derivative in a plane perpendicular to the optical axis. We show how the coherent flow associated with the TIE may be augmented by a diffusive flow associated with a scalar or tensor diffusion field. Such diffusive flow can arise via scattering from unresolved spatially random microstructure within an illuminated sample, the blurring effects of an extended chaotic source that illuminates the sample, the resolution-reducing effect of shot noise in detected intensity images of the sample, and the sharpening effect (negative diffusion) associated with scattering from sharp sample edges. Augmenting the TIE's modeling of coherent flow with a diffuse-flow channel leads to a Fokker-Planck extension to this equation. Two different TIE augmentations are obtained, using three different derivations. The inverse problems of phase retrieval and diffusion-field retrieval are then studied, for both defocus-based imaging and mask-based imaging (structured-illumination imaging). When symmetric overfocus and underfocus images are employed for the purposes of phase retrieval, the diffusive term drops out and our Fokker-Planck formalism implies that any ensuing TIE-based phase-retrieval method needs no modification in light of our formalism. However, the same focal-series dataset -- which is typically an infocus image, a weakly overfocused image, and a weakly underfocused image -- may also be employed to access the additional channel of information associated with the Fokker-Planck diffusion field. Our formalism is applicable to visible light, x-rays, electrons, and neutrons.",
      "categories": [
        "physics.optics",
        "math-ph"
      ],
      "published": "2026-01-21T04:38:42+00:00",
      "url": "https://arxiv.org/pdf/2601.14645v1",
      "resource_uri": "arxiv://2601.14645v1"
    },
    {
      "id": "2601.15686v1",
      "title": "Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing",
      "authors": [
        "Xinyu Wang",
        "Sicheng Lyu",
        "Yu Gu",
        "Jerry Huang",
        "Peng Lu",
        "Yufei Cui",
        "Xiao-Wen Chang"
      ],
      "abstract": "Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22T06:11:44+00:00",
      "url": "https://arxiv.org/pdf/2601.15686v1",
      "resource_uri": "arxiv://2601.15686v1"
    },
    {
      "id": "2601.15669v1",
      "title": "Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting",
      "authors": [
        "Jingjing Bai",
        "Yoshinobu Kawahara"
      ],
      "abstract": "Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22T05:51:56+00:00",
      "url": "https://arxiv.org/pdf/2601.15669v1",
      "resource_uri": "arxiv://2601.15669v1"
    },
    {
      "id": "2601.15625v1",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "authors": [
        "Zhiwei Zhang",
        "Fei Zhao",
        "Rui Wang",
        "Zezhong Wang",
        "Bin Liang",
        "Jiakang Wang",
        "Yao Hu",
        "Shaosheng Cao",
        "Kam-Fai Wong"
      ],
      "abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22T03:57:35+00:00",
      "url": "https://arxiv.org/pdf/2601.15625v1",
      "resource_uri": "arxiv://2601.15625v1"
    },
    {
      "id": "2601.15560v1",
      "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation",
      "authors": [
        "Sylvey Lin",
        "Eranki Vasistha"
      ],
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22T00:58:59+00:00",
      "url": "https://arxiv.org/pdf/2601.15560v1",
      "resource_uri": "arxiv://2601.15560v1"
    },
    {
      "id": "2601.15767v1",
      "title": "Recursive Flow: A Generative Framework for MIMO Channel Estimation",
      "authors": [
        "Zehua Jiang",
        "Fenghao Zhu",
        "Chongwen Huang",
        "Richeng Jin",
        "Zhaohui Yang",
        "Xiaoming Chen",
        "Zhaoyang Zhang",
        "M\u00e9rouane Debbah"
      ],
      "abstract": "Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22T08:58:48+00:00",
      "url": "https://arxiv.org/pdf/2601.15767v1",
      "resource_uri": "arxiv://2601.15767v1"
    },
    {
      "id": "2601.16118v1",
      "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware",
      "authors": [
        "Marco Ronzani",
        "Cristina Silvano"
      ],
      "abstract": "Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.",
      "categories": [
        "cs.AR",
        "cs.NE"
      ],
      "published": "2026-01-22T17:13:57+00:00",
      "url": "https://arxiv.org/pdf/2601.16118v1",
      "resource_uri": "arxiv://2601.16118v1"
    },
    {
      "id": "2601.16117v1",
      "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
      "authors": [
        "Abdul Hannan",
        "Daniele Falavigna",
        "Shah Nawaz",
        "Mubashir Noman",
        "Markus Schedl",
        "Alessio Brutti"
      ],
      "abstract": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
      "categories": [
        "cs.SD",
        "cs.CV"
      ],
      "published": "2026-01-22T17:11:44+00:00",
      "url": "https://arxiv.org/pdf/2601.16117v1",
      "resource_uri": "arxiv://2601.16117v1"
    },
    {
      "id": "2601.16038v1",
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "authors": [
        "Olga Bunkova",
        "Lorenzo Di Fruscia",
        "Sophia Rupprecht",
        "Artur M. Schweidtmann",
        "Marcel J. T. Reinders",
        "Jana M. Weber"
      ],
      "abstract": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22T15:11:02+00:00",
      "url": "https://arxiv.org/pdf/2601.16038v1",
      "resource_uri": "arxiv://2601.16038v1"
    },
    {
      "id": "2601.16036v1",
      "title": "Tri-Hybrid Beamforming Design for integrated Sensing and Communications",
      "authors": [
        "Tianyu Fang",
        "Mengyuan Ma",
        "Markku Juntti",
        "Nhan Thanh Nguyen"
      ],
      "abstract": "Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-01-22T15:09:38+00:00",
      "url": "https://arxiv.org/pdf/2601.16036v1",
      "resource_uri": "arxiv://2601.16036v1"
    },
    {
      "id": "2601.16025v1",
      "title": "EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery",
      "authors": [
        "Yajuan Xu",
        "Xixian Han",
        "Xiaolong Wan"
      ],
      "abstract": "Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22T14:52:36+00:00",
      "url": "https://arxiv.org/pdf/2601.16025v1",
      "resource_uri": "arxiv://2601.16025v1"
    },
    {
      "id": "2601.15973v1",
      "title": "Performance Scaling Laws for PD Array-based Receivers in IM/DD Optical Wireless Communication Systems",
      "authors": [
        "Aravindh Krishnamoorthy",
        "Robert Schober",
        "Harald Haas"
      ],
      "abstract": "We study the performance scaling laws for electrical-domain combining in photodetector (PD) array-based receivers employing intensity modulation and direct detection, taking into account the inherent square-law relationship between the optical and electrical received powers. The performance of PD array-based systems is compared, in terms of signal-to-noise ratio (SNR) and achievable rate, to that of a reference receiver employing a single PD. Analytical and numerical results show that PD arrays provide performance gains for sufficiently narrow beams and above an SNR threshold. Furthermore, increasing the number of PDs alone does not enhance performance, and joint optimization of beam pattern, transverse electromagnetic mode, received power, and PD positions is necessary. Our model and derived insights provide practical guidelines and highlight the trade-offs for the design of next-generation high-bandwidth PD array receivers.",
      "categories": [
        "eess.SP",
        "cs.IT"
      ],
      "published": "2026-01-22T13:53:48+00:00",
      "url": "https://arxiv.org/pdf/2601.15973v1",
      "resource_uri": "arxiv://2601.15973v1"
    },
    {
      "id": "2601.16126v1",
      "title": "Quantum Dimension Reduction of Hidden Markov Models",
      "authors": [
        "Rishi Sundar",
        "Thomas Elliott"
      ],
      "abstract": "Hidden Markov models (HMMs) are ubiquitous in time-series modelling, with applications ranging from chemical reaction modelling to speech recognition. These HMMs are often large, with high-dimensional memories. A recently-proposed application of quantum technologies is to execute quantum analogues of HMMs. Such quantum HMMs (QHMMs) are strictly more expressive than their classical counterparts, enabling the construction of more parsimonious models of stochastic processes. However, state-of-the-art techniques for QHMM compression, based on tensor networks, are only applicable for a restricted subset of HMMs, where the transitions are deterministic. In this work we introduce a pipeline by which \\emph{any} finite, ergodic HMM can be compressed in this manner, providing a route for effective quantum dimension reduction of general HMMs. We demonstrate the method on both a simple toy model, and on a speech-derived HMM trained from data, obtaining favourable memory--accuracy trade-offs compared to classical compression approaches.",
      "categories": [
        "quant-ph",
        "cond-mat.stat-mech"
      ],
      "published": "2026-01-22T17:27:52+00:00",
      "url": "https://arxiv.org/pdf/2601.16126v1",
      "resource_uri": "arxiv://2601.16126v1"
    },
    {
      "id": "2601.16011v1",
      "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications",
      "authors": [
        "Theodor Forgaard",
        "Jarle H. Reksten",
        "Anders U. Waldeland",
        "Valerio Marsocci",
        "Nicolas Long\u00e9p\u00e9",
        "Michael Kampffmeyer",
        "Arnt-B\u00f8rre Salberg"
      ],
      "abstract": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "published": "2026-01-22T14:38:00+00:00",
      "url": "https://arxiv.org/pdf/2601.16011v1",
      "resource_uri": "arxiv://2601.16011v1"
    }
  ]
}