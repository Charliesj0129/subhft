
# 2009 Defining, Estimating and Using Credit Term Structures. Part 

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2009 Defining, Estimating and Using Credit Term Structures. Part 
â€¢ **æª”æ¡ˆ**ï¼š `2009_Defining, Estimating and Using Credit Term Structures. Part .pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `futures_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> for direct estimation of implied term stru ctures of survival probabilities from credit bond prices. We have shown that this methodolog y is more robust than the traditional implementations of reduced-form default models (for the latter, see Jarrow and Turnbull [1995], and Duffie and Singleton [1999]). Mor e importantly, it is more consistent with the underlying bankruptcy resolution practices such as debt acceleration and equal priority recovery for the same-seniority bonds. Our methodology is well suited to a direct comparison with credit derivatives, particularly credit default swaps, whose valuation is driven by the model ing of default probabilities. In this paper we introduce new relative value measures, which take a dvantage of the internal consistency of this pricing methodology. In particular, we define: â€¢ Bond-Implied CDS (BCDS) term structure â€¢ CDS-Bond curve basis â€¢ Systematic and full bond-specific basis to CDS curve â€¢ Risk-free-equivalent coupon (RFC) streams for credit-risky bonds We also introduce and discuss static replication/hedging str ategies of credit risk in cash bonds using forward and spot CDS. In particular, we dem onstrate in detail how these strategies can be used to hedge the default risk of a credit b ond with an arbitrary coupon, given an arbitrary term structure of risk-free rates. The complete hedge of credit risk in these strategies is reflected in the complementarity between the risk-fr ee-equivalent coupon streams a...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> S In the conclusion of our three-part series of papers we note that the survival-based pricing framework has important implications for all aspects of both the investment science and the practice of trading of credit-risky bonds. We have shown i n this series that all commonly used measures, including the Z-spreads, the interest rate and s pread durations, and the CDSBond basis, to name a few, are significantly affected and require a careful re-definition to maintain their consistency. The impact of the new methodology i s both qualitative, such as the revealing explanations of the â€œopticalâ€ spread curve invers ion and rates-spreads correlation, and quantitative, such as the estimates of the cor rect interest rate and credit sensitivities and CDS-Bond basis. We hope that the comprehensi ve set of methodologies presented in this series will assist the credit portfolio m anagers in the difficult task of understanding and harnessing the risks and rewards of this important asset class. Acknowledgements: We would like to thank Marco Naldi as well as many other colleagues at Lehman Brothers Fixed Income Research department for numerous h elpful discussions throughout the development and implementation of the survi val-based methodology during the past several years.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2018 Optimal Dynamic Basis Trading

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2018 Optimal Dynamic Basis Trading
â€¢ **æª”æ¡ˆ**ï¼š `2018_Optimal Dynamic Basis Trading.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `futures_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we consider a diï¬€erent scenario where the basis does not vanish at maturity. More precisely, we model the stochastic basis by a scaled Brownian bridge that is stopped before its achieves convergence. Our proposed model is motivated by the market phenomenon of non-convergence as well as the possibility of traders closing their futures positions prior to maturity. It also has the added advantage that it generates an arbitrage-free futures market (see Proposition 2.5 below). We consider a general class of risk preferences by using hyperbolic absolute risk aversion (HARA) utility functions which includes power (CRRA) and exponential (CARA) utilities. We exclude, however, the case of decreasing risk-tolerance functions and, in particular, the quadratic utility function. Among our...

â€¢ **Abstract Reference**:
> We study the problem of dynamically trading a futures contract and its underlying asset under a stochastic basis model. The basis evolution is modeled by a stopped scaled Brownian bridge to account for non-convergence of the basis at maturity. The optimal trading strategies are determined from a utility maximization problem under hyperbolic absolute risk aversion (HARA) risk preferences. By analyzing the associated Hamilton-Jacobi-Bellman equation, we derive the exact conditions under which the equation admits a solution and solve the utility maximization explicitly. A series of numerical examples are provided to illustrate the optimal strategies and examine the eï¬€ects of model parameters. Keywords: futures stochastic basis cash and carry scaled Brownian bridge risk aversion JEL Classiï¬cationC41 G11 G12 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2020 An end-to-end data-driven optimisation framework for constra

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2020 An end-to-end data-driven optimisation framework for constra
â€¢ **æª”æ¡ˆ**ï¼š `2020_An end-to-end data-driven optimisation framework for constra.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `futures_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we leverage data-driven approaches to design a new end-to-end framework which is dynamics-free for optimised and realistic trajectories. We ï¬rst decompose the trajectories on function basis, trading the initial inï¬nite dimension problem on a multivariate functional space for a parameter optimisation problem. A maximum a posteriori approach which incorporates information from data is used to obtain a new optimisation problem which is regularised. The penalised term focuses the search on a region centered on data and includes estimated linear constraints in the problem. We apply our data-driven approach to two settings in aeronautics and sailing routes optimisation, yielding commanding results. The developed approach has been implemented in the Python library PyRotor. 1 arXi...

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> which falls in the much broader ï¬eld of trajectory optimisation under constraints. As such, it has potential applications to many real world problems, such as in robotics to minimise the work-based speciï¬c mechanical cost of transport (Srinivasan and Ruina, 2006) or in aerospace to reduce the total thermal ï¬‚ux when a space shuttle re-enters in the atmosphere (TrÂ´ elat, 2012). In aeronautics, optimisation problems are often formulated in terms of optimal control problems (Codina and MenÂ´ endez, 2014; Girardet et al., 2014; Cots et al., 2018). They can be solved by converting the problem into a parameter optimisation problem. This allows to take into account the dynamics of the system, leading to realistic solutions complying with additional constraints (we refer to Rao, 2009, for an overview). Nevertheless the diï¬€erential equations describing the dynamics of the system of interest may be (partially) unknown. For instance, the diï¬€erential system describing the motion of an aircraft moving in an air mass (Rommel et al., 2019) involves the lift and drag forces for which no analytic formulas exist. Aircraft manufacturer computes numerical models by means of heavy simulations and wind tunnel tests. Another approach consists in 2 reconstructing unknown forces based on physical formulas and available ï¬‚ight data; see for instance Rommel et al. (2017); Dewez et al. (2020) for...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2020 The Fair Basis: Funding and capital in the reduced form fram

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2020 The Fair Basis: Funding and capital in the reduced form fram
â€¢ **æª”æ¡ˆ**ï¼š `2020_The Fair Basis: Funding and capital in the reduced form fram.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `futures_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> A negative basis trade enters a long bond position and buys protection on the issuer of the bond through credit default swap (CDS), aiming at arbitrage profit due to the bondCDS basis . To classic reduced form model theorists, the existence of the basis is an abnormality or merely liquidity noise. Such a view, however, fails to explain large basis trading losses incurred during the financial crisis. Employing a bond continuously hedged by CDS under a dyna mic spread model with bond repo financing, we find th at there is unhedged and unhedgeable residual jump to default risk that canâ€™t be diversified because of credit correlation. An economic capital approach has to apply and a charge on the use of capital follows. Together with the hedge funding cost, it allow s us to better understand the basisâ€™s economics and to predict its fair level. Keywords: CDS-bond basis, negative basis, reduced form model, default risk, hedging error, FVA, KVA....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ing side, the reduced form model with issuer default probability calibrated to the CDS curve would predict a bond fair price quite different from the bond market price. The reduced form credit model (Jarrow and Turnbull, 1992 & 1995; Lando, 1998; Duffie and Singleton, 1999) extends the interest rate term structure modeling to default modeling but does not consider funding and other factors when pricing a risky bond. In the risk neutral pricing theory, once a default time model is put in place, aggregation 2 According to Fontana (2010), Deutsche Bankâ€™s prop credit trading unit reportedly lost one billion in 2007 when some creditsâ€™ basis widened in the wrong direction, Merrill Lynch lost multiple billions in 2008 around the time when it was bought by bank of America, and in early 2008, Citadelâ€™s flagship investment fund was down significantly due to failed basis trades. 3 and discounting of default contingent cash flows lead to a net present value (npv). A default intensity process is assumed and default happens as the first jump of a Cox process. The existence of the risk neutral measure is provided for by assuming an arbitrage -free CDS market, which is used to calibrate the intensity process. The other approach is , of course, the structural or options approach (Merton 1974), which treats debt and equity as two complimentary contingent claims on the same firm asset value. Because the firm asset is not...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Agent-Based Simulation of a Perpetual Futures Market

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Agent-Based Simulation of a Perpetual Futures Market
â€¢ **æª”æ¡ˆ**ï¼š `2025_Agent-Based Simulation of a Perpetual Futures Market.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `futures_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Iintroduceanagent-basedmodelofaPerpetualFuturesmarketwithheterogeneousagentstradingviaa central-limitorderbook.PerpetualFutures(henceforthPerps)arefinancialderivativesintroducedbytheeconomistRobertShiller, designedtoâ€˜pegâ€™theirpricetothatoftheunderlyingSpotmarket.ThispaperextendsthelimitorderbookmodelofChiarellaetal.(2002)bytakingtheiragentandorderbookparameters,designedfora simplestockexchange,andapplyingit tothemorecomplexenvironmentof a Perpmarketwithlongandshorttraderswhoexhibitbothpositionalandbasis-tradingbehaviors.I findthatdespitethesimplicityoftheagentbehavior, thesimulationisableto reproducethemostsalientfeatureof a Perpmarket,theâ€˜peggingâ€™of thePerppriceto theunderlyingSpotprice.In contrastto fundamentalsimulationsof stockmarketswhichaimtoreproduceempiricallyobservedstylizedfactssuchastheleptokurtosisandheteroscedasticityofreturns,volatilityclusteringandothers,inderivativesmarketsmanyofthesefeaturesareprovidedexogenouslybytheunderlyingSpotpricesignal.ThisisespeciallytrueofPerp...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . 2. Literature review 2.1 Perpetual Futures RobertShillerintroducedthePerpetualFuturein a presentationtotheAmericanFinanceAssociationMeetingsin1993.Thepurposeofthederivativewastoprovideamarketforhedgingassetsthatarecriticaltoeconomiessuchashumancapital,realestate,consumerpriceindicesandothercrucialeconomicindicators.Heproposesthattherearetwoseparatebutrelatedmeasurementproblemsthatmakethisdifficult.One,thepricesoftheseassetsmayonlybeobservedinfrequently,andonheterogeneoustypesofsuchassets(infrequentanddiversehousesales,forexample)andthereforeservepoorlyaspricesignalsfortheentireassetclass.Thesecondproblemhesuggestsisthatformanyassets,thereisnopricesignalatall,butonlymeasurementsofthedividendorrentofanasset(salariesforhumanlabor).To addressthesetwoproblems,Shillerproposesa syntheticderivativemarketplacewhereaâ€˜hedonicrepeatedpriceindexâ€™issuppliedtoamarketforperpetualclaimsoncashflows,paidfromlongtraderstoshorttraders,whichrepresentthedividendsorrentsontheseassets.Theclaimsareperpetualtocapturethestreamofdividendsaccruingtoanassetoveritsentireexistence,thereforeenablingpricediscovery. So,a hedonic,repeatedpriceindexwouldsupplya streamofpricesignalstotheperpetualclaimsmarket,wheretraderswithlongpositionswouldpayacash-settleddividendtotraderswhotakeanequalandoppositeshortposition.Thisisdonein sucha waythatthepriceoftheperpetualclaimtracksthepriceoftheunderlying,asdescribed in the introduction and Figure 1. Giventheirnovelty, atleastinpractice,therearefewstudiesfocusingonPerps.Ale...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 A Games-in-Games Paradigm for Strategic Hybrid Jump-Diffusio

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 A Games-in-Games Paradigm for Strategic Hybrid Jump-Diffusio
â€¢ **æª”æ¡ˆ**ï¼š `2025_A Games-in-Games Paradigm for Strategic Hybrid Jump-Diffusio.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> This paper develops a hierarchical games-in-games control architecture for hybrid stochastic systems governed by regime-switching jump-diffusions. We model the interplay between continuous state dynamics and discrete mode transitions as a bilevel differential game: an inner layer solves a robust stochastic control problem within each regime, while a strategic outer layer modulates the transition intensities of the underlying Markov chain. A Dynkin-based analysis yields a system of coupled Hamilton-Jacobi-Isaacs (HJI) equations. We prove that for the class of Linear-Quadratic games and Exponential-Affine games, this hierarchy admits tractable semi-closed form solutions via coupled matrix differential equations. The framework is demonstrated through a case study on adversarial market microstructure, showing how the outer layerâ€™s strategic switching pre-emptively adjusts inventory spreads against latent regime risks, which leads to a hyper-alert equilibrium. 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> We consider a hybrid decision architecture in which a continuous state evolves according to mode-dependent stochastic dynamics, while a discrete mode process switches between a finite collection of regimes. Two layers of strategic decision-making interact: a fast-timescale controller/disturbance pair regulating the continuous state, and a slowtimescale pair of agents whose actions influence the transition rates among the discrete modes. This structure captures a wide range of multi-layer hybrid systems, including resilient infrastructure networks, multi-agent cyber-physical systems, and robust control under regime uncertainty. Let (â„¦,F,P) be a complete probability space equipped with a filtration F= (F t)tâ‰¥0 satisfying the usual conditions of right-continuity and completeness. The time horizon is finite, T <âˆž . We define the following primitive stochastic processes adapted toF: 1.W= (W t)tâ‰¥0 is a standardd-dimensional Brownian motion. 2 2.N(dt, dz) is a Poisson random measure on [0, T]Ã— Z with intensity measure Î»(dz)dt, where Z âŠ†R k. We denote the compensated measure by ËœN(dt, dz) =N(dt, dz)âˆ’Î»(dz)dt. Definition 1(Two-layer hybrid decision system).Let I={1, . . . , N} be a finite set of regimes (modes), U and W compact convex sets representing continuous-layer control and disturbance actions, and AD, AA finite sets of actions available to the two agents governing the mode transitions. Atwo-layer hybrid systemis a tuple Î“ =  X,I,V,W,F,G  , where: 1.X âŠ†R n is the continuous...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> This paper presents a hierarchical â€œgames-in-gamesâ€ control framework for systems governed byregime-switching jump-diffusions. By decomposing the problem into a fast inner game and a strategic outer game, we derived a coupled system of Hamilton-Jacobi-Isaacs (HJI) equations via a unified Dynkin identity. A key theoretical contribution is the proof that for Exponential-Affine and Linear-Quadratic games, this hierarchy admits tractablespectral solutionsvia the matrix exponential, bridging the gap between hybrid modeling and computational feasibility. The frameworkâ€™s practical value is demonstrated through an adversarial market microstructure case study. The results reveal aRisk Isomorphismprinciple, where the hierarchical controller naturally interprets strategic predation as effective volatility, inducing a â€œhyper-alertâ€ equilibrium that pre-emptively widens spreads. Future research will extend this architecture to partially observable regimes and integrate data-driven learning for empirical transition kernels.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 A Test of Lookahead Bias in LLM Forecasts

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 A Test of Lookahead Bias in LLM Forecasts
â€¢ **æª”æ¡ˆ**ï¼š `2025_A Test of Lookahead Bias in LLM Forecasts.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state-of-the-art pre-training data detection techniques, we estimate the likelihood that a given prompt appeared in an LLMâ€™s training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts. *Gao, Jiang, and Yan are at the Department of Finance, CUHK Business School, The Chinese University of Hong Kong. For helpful comments, we thank Chengwang Liao, Ron Kaniel, and seminar participants at CUHK. Our correspondences are gaozhenyu@baf.cuhk.edu.hk, wenxijiang@baf.cuhk....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is cost-efficient and requires neither model retraining nor access to proprietary training data. It offers researchers and practitioners a practical diagnostic tool for assessing the reliability 1 and real-world validity of LLM-generated forecasts. We first introduce the Lookahead Propensity (LAP), a measure of how likely the text in a prompt appeared in an LLMâ€™s training data. For each prompt, we compute the model-assigned probability of every token conditional on its preceding tokens. We define LAP as the mean token probabilities of the bottomK% of tokens, i.e., those tokens with the lowest predicted probabilities. This focus on uncommon tokens is key: frequent words such as â€œtheâ€ or â€œandâ€ are assigned high probabilities regardless of prior exposure, whereas rare tokens carry more information about whether the text was previously seen. The intuition is that unseen prompts tend to contain more low-probability (outlier) tokens under the model, whereas seen prompts are less likely to have such extreme outliers, and those tokens instead receive higher probability . Accordingly , a higher LAP indicates greater model familiarity and a higher likelihood of training-data overlap.1 The LAP corresponds directly to the MIN-K% PROB statistic developed in the membership inference attack (MIA) literature (e.g., Shokri et al., 2017; Carlini et al., 2021, 2022; Shi et al., 2024; Cheng et al., 2024). The MIA literature proposes various measures to detect whether a given text was includ...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Alpha-R1: Alpha Screening with LLM Reasoning via Reinforceme

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Alpha-R1: Alpha Screening with LLM Reasoning via Reinforceme
â€¢ **æª”æ¡ˆ**ï¼š `2025_Alpha-R1: Alpha Screening with LLM Reasoning via Reinforceme.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. AlphaR1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1. Keywords Large Language Models, Reinforcement Learning, Alpha Screening, Quantitative Trading 1 Introduction The paradigm of factor investing has remained a cornerstone of modern asset management since the seminal work of F...

â€¢ **Abstract Reference**:
> Signal decay and regime shifts pose recurring challenges for datadriven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. AlphaR1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistenc...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> 3.1 Data, Memory, and Factor Baselines We establish the foundational data structures, historical context, and quantitative benchmarks through a systematic process. 3.1.1 Raw Data Abstraction.We first transform heterogeneous raw data into structured textual atomic units. At each time step ð‘¡, we construct two complementary market descriptors: â€¢ Price Market Description (ð‘†price ð‘¡ ): Summarizes information from technical indicators, trading volume, and sector rotation patterns. â€¢ News Market Description ( ð‘†news ð‘¡ ): Encodes information from financial news and macroeconomic announcements to capture prevailing market sentiment. 3.1.2 Iterative Memory Construction.We employ an iterative memory construction pipeline to capture long-term market context. This process aggregates the atomic textual units into a coherent historical narrative. Let ð¼ð‘¤ ={ð‘† price ð‘¡ , ð‘†news ð‘¡ }ð‘¡âˆˆð‘¤ denote the set of market descriptions within week ð‘¤ . The weekly market summary ð‘€ð‘¤ is updated recursively using a large language model: ð‘€ð‘¤ =ð¹ LLM (ð¼ð‘¤ âŠ•ð‘€ ð‘¤âˆ’1 ).(1) After iterating through the entire backtest period, we obtain the comprehensive backtest period market description ( ð‘€global). This encapsulates the structural evolution and regime shifts of the Jiang et al. Raw DataLLM LLM LLM â€¦Ã—n â€¦ â€¦ Step1: Raw Data Abstraction Step2: Iterative Memory Construction Factor BacktestingStep3: Factor Backtesting â€¦LLMÃ—n LLM Ã—n â€¦ â€¦LLM â€¦ Backbone ModelGRPO Optimization Alpha-R1 Linear Model â€¦ â€¦ Alpha DescriptionGlobal Memo...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> This paper introduces Alpha-R1, a semantics-driven approach to quantitative investment that shifts the focus from static factor mining to context-aware reasoning. Rather than relying solely on historical correlations, Alpha-R1 employs a reinforcement-learningâ€“trained large language model to reason over the economic rationale underlying factor performance and its dependence on evolving market conditions. By constructing a dual-layer semantic context that integrates long-term market memory with real-time information, Alpha-R1 connects unstructured data sources with quantitative decisionmaking in a unified manner. Methodologically, we develop a marketaligned reinforcement learning framework based on Group Relative Policy Optimization (GRPO), in which training is guided by objective market outcomes instead of subjective human feedback. Extensive empirical results show that Alpha-R1 consistently outperforms traditional machine learning baselines and generic reasoning LLMs across multiple asset pools. Notably, Alpha-R1 demonstrates strong zero-shot generalization when transferred from the CSI 300 to the CSI 1000 universe, maintaining stable and profitable performance in a previously unseen, high-volatility environment, where conventional reinforcement learning agents experience marked degradation. These findings suggest that grounding factor selection in semantic reasoning and market feedback provides a viable approach for mitigating alpha decay and addressing non-stationarity in financial markets.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Asymptotic and finite-sample distributions of one- and two-s

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Asymptotic and finite-sample distributions of one- and two-s
â€¢ **æª”æ¡ˆ**ï¼š `2025_Asymptotic and finite-sample distributions of one- and two-s.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we focus on offline methods. Beyond mean shifts, structural breaks may occur within parametric models when some parameters suddenly move to new values. Offline approaches in this setting may be based on moments or on information criteria [67, 29]. âˆ—Corresponding author: matthieu.garcin@m4x.org. a De Vinci Higher Education, De Vinci Research Center, Paris, France. b DÂ´ epartement de mathÂ´ ematiques et applications,Â´Ecole normale supÂ´ erieure, 45 rue dâ€™Ulm, 75005 Paris, France. Acknowledgements: MG acknowledges the support of the Chair â€œDeep Finance Statisticsâ€ between QRT, Ecole Polytechnique and its foundation. The authors would like to thank Olivier Benhamou for useful discussions and support. 1 arXiv:2512.16411v1 [stat.ME] 18 Dec 2025 The latter case consists in finding...

â€¢ **Abstract Reference**:
> Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 AutoQuant: An Auditable Expert-System Framework for Executio

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 AutoQuant: An Auditable Expert-System Framework for Executio
â€¢ **æª”æ¡ˆ**ï¼š `2025_AutoQuant: An Auditable Expert-System Framework for Executio.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose AutoQuant, an execution-centric andalpha-agnosticframework that can be viewed as anauditable expert systemfor strategy configuration selection. AutoQuant encodes strict execution semantics, funding alignment, and trading-cost/constraint profiles as a knowledge base; applies Bayesian optimization plus a two-stage double screening protocol across windows and cost scenarios; and exports deterministic, machine-readable artifacts and accounting-invariant audits for traceability. AutoQuant does not generate new trading rules; it selects and audits configurations within a pre-specified signal family under strict execution and cost semantics. Empirically, fee-only â€œstandardâ€ backtests and zero-cost upper bounds materially inflate annualized performance relative to a fully costed c...

â€¢ **Abstract Reference**:
> Backtests of cryptocurrency perpetual futures are fragile when they ignore microstructure frictions and reuse evaluation windows during parameter search. Using four liquid perpetual contracts (BTC/USDT, ETH/USDT, SOL/USDT, and AVAX/USDT), we study whether an execution-constrained configuration-selection pipeline can reduce performance overestimation and expose parameter fragility. We propose AutoQuant, an execution-centric andalpha-agnosticframework that can be viewed as anauditable expert systemfor strategy configuration selection. AutoQuant encodes strict execution semantics, funding alignment, and trading-cost/constraint profiles as a knowledge base; applies Bayesian optimization plus a two-stage double screening protocol across windows and cost scenarios; and exports deterministic, machine-readable artifacts and accounting-invariant audits for traceability. AutoQuant does not generate new trading rules; it selects and audits configurations within a pre-specified signal family under...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> for finding and verifying durable parameter configurations. In this manuscript, however, all empirical evidence is deliberately restricted to a single STRICT4H momentum-and-gating family, with BTC/USDT as the primary audited case study and parallel replications on ETH/USDT, SOL/USDT, and AVAX/USDT. We treat these as evidence for the auditability and pipeline-level portability of the validation protocol across several liquid perpetual contracts rather than as a claim of universal multi-asset, multifamily validation. Our contributions are: â€¢ An auditable, rule-based decision-support framing for auto5 Table 8: Stable-candidate selection rules used in the BTC/USDT STRICT4H case study. Statistics are aggregated across the predefined cost-scenario grid described in Section 2.4. After filtering, we select the top K = 5candidates by sorting mean_monthly_true descending andmaxDD_meanascending. Statistic Meaning Threshold Role mean_monthly_true mean monthly geometric return (across scenarios) â‰¥0.005minimum performance min_monthly_trueworst-case scenario-level monthly geometric return â‰¥0.0downside floor maxDD_meanmean maximum drawdown (across scenarios) â‰¤0.30drawdown cap switch_density_ mean mean position-switch density (across scenarios) â‰¤0.12 turnover/fragility cap Full-chain audits verify that these components reconcile (within a small tolerance) across offline backtests, live-style replay, and execution logs produced by the same code path. In the replication snapshot used fo...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Design of a Decentralized Fixed-Income Lending Automated Mar

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Design of a Decentralized Fixed-Income Lending Automated Mar
â€¢ **æª”æ¡ˆ**ï¼š `2025_Design of a Decentralized Fixed-Income Lending Automated Mar.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> â€”In decentralized finance (DeFi), designing fixedincome lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMMA performs excellently in terms of interest rate stability and financial robustness. Index Termsâ€”Decentralized Finance; Blockchain; Automated Market Maker; Fixed Income I....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> ANDOUTLOOK This paper proposed BondMM-A, a fixed-income AMM supporting arbitrary maturities. Its design proves effective: rates align with market levels, volatility is low, net equity is Fig. 3. Standard deviation of BondMM-A rates at each time step. Fig. 4. BondMM-A net equity minus initial capital. stable, users gain flexibility, and LPs avoid capital fragmentationâ€”benefiting both sides. BondMM-A still invites optimization. Future work includes more precise dynamic parameter tuning for fast markets, richer yield-curve models to match real-world diversity, and stronger resilience to extremes. Real deployments will test performance and security at DeFi scale. Overall, BondMM-A offers a new design path for decentralized fixed income and a reference for subsequent research.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Equilibrium Liquidity and Risk Offsetting in Decentralised M

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Equilibrium Liquidity and Risk Offsetting in Decentralised M
â€¢ **æª”æ¡ˆ**ï¼š `2025_Equilibrium Liquidity and Risk Offsetting in Decentralised M.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We develop an economic model of decentralised exchanges (DEXs) in which riskaverse liquidity providers (LPs) manage risk in a centralised exchange (CEX) based on preferences, information, and trading costs. Rational, risk-averse LPs anticipate the frictions associated with replication and manage risk primarily by reducing the reserves supplied to the DEX. Greater aversion reduces the equilibrium viability of liquidity provision, resulting in thinner markets and lower trading volumes. Greater uninformed demand supports deeper liquidity, whereas higher fundamental price volatility erodes it. Finally, while moderate anticipated price changes can improve LP performance, larger changes require more intensive trading in the CEX, generate higher replication costs, and induce LPs to reduce liquidity supply. âˆ—F. Drissi is with the Oxford-Man Institute, University of Oxford. â€ X. Wu is with the Department of Mathematics, University of Toronto. â€¡S. Jaimungal is with the Department of Statistical S...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is solved recursively, by dynamic programming. Instage three, noise LTs arrive in the DEX at a known intensity and take the current reserves as given to determine their optimal trading volumes. Specifically, they balance the trading costs implied by the LPâ€™s reserves in the DEX against their private utility from holding the asset. Trading costs directly depend on the liquidity reserves deposited by the LP. Specifically, in DEXs, liquidity providers deposit capital into a pool that liquidity takers use to execute trades in exchange for a fee. The DEX functions as an algorithmic market maker whose price and liquidity dynamics are determined by the pricing rules encoded in the DEXâ€™s smart contract,3 the amount of capital in the pool, and the trading fee. 3A smart contract is a publicly accessible and immutable program running on the blockchain that defines the rules of interaction with the pool for both liquidity providers and liquidity takers. 3 Instage two, the LP determines her optimal CEX risk offsetting strategy for an arbitrary level of liquidity supply. The strategy explicitly accounts for CEX trading costs, investment horizon, net exposure risk aversion, and private information. We employ variational methods to characterise and solve the optimal strategy in the setting of a DEX with an arbitrary convex bonding curve and when the LPâ€™s trading activity generates both permanent and transient market impact. We show that the system of forwardâ€“backward stochastic differen...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 High-Frequency Analysis of a Trading Game with Transient Pri

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 High-Frequency Analysis of a Trading Game with Transient Pri
â€¢ **æª”æ¡ˆ**ï¼š `2025_High-Frequency Analysis of a Trading Game with Transient Pri.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> .Westudythehigh-frequencylimitofann-traderoptimalexecutiongameindiscretetime. Traders face transient price impact of Obizhaevaâ€“Wang type in addition to quadratic instantaneous trading costs Î¸(âˆ†Xt)2 on each transactionâˆ†X t. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval[0, T], the discrete equilibrium inventories converge at rate1/Nto the continuous-time equilibrium of an Obizhaevaâ€“Wang model with additional quadratic costsÏ‘0(âˆ†X0)2 and Ï‘T (âˆ†XT )2 on initial and terminal block trades, whereÏ‘0 = (nâˆ’1)/2andÏ‘ T = 1/2. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case n= 2whereÏ‘ 0 =Ï‘ T = 1/2. In particular, we show how the coefficientsÏ‘0 = (nâˆ’1)/...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Impact of Volatility on Time-Based Transaction Ordering Poli

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Impact of Volatility on Time-Based Transaction Ordering Poli
â€¢ **æª”æ¡ˆ**ï¼š `2025_Impact of Volatility on Time-Based Transaction Ordering Poli.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and biddersâ€™ risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model. Keywords:Decentralized financeÂ·MEVÂ·Transaction ordering 1 Introduction 1.1 Background Timeboost, a novel transaction ordering policy introduced in [25] and subsequently implemented by Offchain Labs, has recently been deployed on the Arbitrum mainnet [31]. Under the Timeboost policy, transactions are delivered to the sequencer through two distinct channels: the normal lane and the Express Lane (EL). While the sequencer processes transactions in a First-In,...

â€¢ **Abstract Reference**:
> .WestudyArbitrumâ€™sExpress Lane Auction(ELA),anaheadof-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with riskaverse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and biddersâ€™ risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model. Keywords:Decentralized financeÂ·MEVÂ·Transaction ordering 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . Keywords:Decentralized financeÂ·MEVÂ·Transaction ordering 1 Introduction 1.1 Background Timeboost, a novel transaction ordering policy introduced in [25] and subsequently implemented by Offchain Labs, has recently been deployed on the Arbitrum mainnet [31]. Under the Timeboost policy, transactions are delivered to the sequencer through two distinct channels: the normal lane and the Express Lane (EL). While the sequencer processes transactions in a First-In, First-Out (FIFO) manner, transactions submitted via the normal lane incur a200-millisecond delay before being forwarded to the sequencer. Conversely, the EL immediately forwards received transactions, thereby offering its users a200-millisecond latency advantage. The temporary right to utilize the EL is periodically allocated for each roundâ€”which lasts for one minuteâ€”via a second-price sealed-bid auction termed theExpress Lane Auction (ELA)3. This auction operates on an ahead-of-time basis; participants in the ELA are required to estimate the value of the good (the right to use the EL in the forthcoming round), submit their bids, and pay if they win, prior to exercising their EL access. Given that the latency differential is barely noticeable to typical users, participation and bidding in the ELA are 3 The winner may elect to either exclusively utilize this access or to share and resell it to other parties. For instance, [20] is currently running a secondary market that resells the EL access. arXiv:2512.23386v1 [c...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Institutional Backing and Crypto Volatility: A Hybrid Framew

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Institutional Backing and Crypto Volatility: A Hybrid Framew
â€¢ **æª”æ¡ˆ**ï¼š `2025_Institutional Backing and Crypto Volatility: A Hybrid Framew.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Decentralized finance (DeFi) lacks centralized oversight, often resulting in heightened volatility. In contrast, centralized finance (CeFi) offers a more stable environment with institutional safeguards. Institutional backing can play a stabilizing role in a hybrid structure (HyFi), enhancing transparency, governance, and market discipline. This study investigates whether HyFi -like cryptocurrencies , those backed by institutions, exhibit lower price risk than fully decentralized counterparts. Using daily data for 18 major cryptocurrencies from January 2020 to November 2024, we estimate panel EGLS models with fixed, random, and dynamic specifications. Results show that HyFi -like assets consistently experience lower price risk, with this effect intensifying during periods of elevated market volatility. The negative interaction between HyFi status and market -wide volatility confirms their stabilizing role. Conversely, greater decentralization is strongly associated with increased volat...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> includes the Amihud (2002) illiquidity measure, calculated as the absolute value of daily returns divided by trading volume: ð´ð‘šð‘–â„Žð‘¢ð‘‘ð‘–ð‘¡ = |ð‘…ð‘’ð‘¡ð‘¢ð‘Ÿð‘›ð‘–ð‘¡| ð‘‡ð‘Ÿð‘Žð‘‘ð‘’ð‘‰ð‘œð‘™ð‘¢ð‘šð‘’ð‘–ð‘¡ This measure captures the price impact of trading and reflects constraints in liquidity, which can amplify volatility, particularly in thin markets. To enhance interpretability and address scaling issues arising from extremely large trading volumes in the crypto space, the Amihud measure is standardized as a z-score: ð‘_ð´ð‘šð‘–â„Žð‘¢ð‘‘ð‘–ð‘¡ = ð´ð‘šð‘–â„Žð‘¢ð‘‘ð‘–ð‘¡ âˆ’ ðœ‡ð‘–,ð´ð‘šð‘–â„Žð‘¢ð‘‘ ðœŽð‘–,ð´ð‘šð‘–â„Žð‘¢ð‘‘ This transformation ensures that the coefficient reflects the effect of a one-standard-deviation change in illiquidity on price risk. Cryptocurrency size is captured by the change in the natural logarithm of market capitalization. Investor attention and asset attractiveness are proxied by the logarithm of Google search volume for each cryptocurrency6. General market volatility is measured using the realized volatility of a Crypto -50 index, which is constructed following Sovbetov (2018). The index is used to calculate market-wide volatility as: 6 https://trends.google.com/trends/explore Sovbetov, I. (2025). Computational Economics DeVault, L., Turtle, H. J., & Wang, K. (2025). Embracing the future or buying into the bubble: Do sophisticated institutions invest in crypto assets? European Financial Management. Advance online publication. https://doi.org/10.1111/eufm.12542 Duppati, G., Kijkasiwat, P., Hunjra, A. I., & Liew, C. Y. (2023). D...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forw

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forw
â€¢ **æª”æ¡ˆ**ï¼š `2025_Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forw.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdownâˆ’2.76%) and market-neutral characteristics (Î²= 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020â€“2024) while underperforming in stable markets (...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdownâˆ’2.76%) and market-neutral characteristics (Î²= 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020â€“2024) while underperforming in stable markets (âˆ’0.16%, 2015â€“2019). We report statistically insignificant aggregate...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 On the supremum of a quotient of power sums

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 On the supremum of a quotient of power sums
â€¢ **æª”æ¡ˆ**ï¼š `2025_On the supremum of a quotient of power sums.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We define a function of two real vectors by a certain homogeneous quotient involving power sums, and show that its supremum grows asymptotically linearly w.r.t. the dimension. From this, we deduce a condition under which a parametric set of real matrices satisfies a set of polynomial positivity constraints. This characterization finds an application in mathematical finance, in a recent study on price impact models. MSC 2020: 26D15, 90C23, 11H50, 15B48 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Optimal Signal Extraction from Order Flow: A Matched Filter 

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Optimal Signal Extraction from Order Flow: A Matched Filter 
â€¢ **æª”æ¡ˆ**ï¼š `2025_Optimal Signal Extraction from Order Flow: A Matched Filter .pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We demonstrate that the choice of normalization for order flow intensity is fundamental to signal extraction in finance, not merely a technical detail. Through theoretical modeling, Monte Carlo simulation, and empirical validation using 2.1 million stock-day observations from the Korean market, we prove that market capitalization normalization acts as a â€œmatched filterâ€ for informed trading signals, achieving 1.32â€“ 1.92Ã—higher correlation with future returns compared to traditional trading value normalization. The key insight is that informed traders scale positions by firm value (market capitalization), while noise traders respond to daily liquidity (trading volume), creating heteroskedastic corruption when normalizing by trading volume. By reframing the normalization problem using signal processing theory, we show that dividing order flow by market capitalization preserves the information signal while traditional volume normalization multiplies the signal by inverse turnoverâ€”a highly...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> naturally aligns with market capitalization scaling, as 13F-based validation requires measuring flow relative to total equity, not daily volume. They explicitly measure institutional order flow as a percentage of total market capitalization, providing a direct precedent for ourS M C measure. Beber et al. (2011) investigate the information content of sector-level order flow. They explicitly define â€œactive sector order flowâ€ as the flow in excess of the proportion dictated by the sectorâ€™smarket capitalization. Mathematically, they calculate â€œpassiveâ€ flow as the total market flow multiplied by the sectorâ€™s market cap weight (w i =M i/P Mj). This definition formalizes the idea that the â€œneutralâ€ expectation for flow is proportional toM i, notV i. Any deviation from this cap-weighted baseline represents an active, potentially informed, view. Lewellen (2011) analyzes how institutional investors aggregate to affect asset prices. His findings suggest that institutions, constrained by benchmarks and capacity, allocate capital in ways that scale with firm size rather than daily trading activity. 2.3 Turnover as Uncertainty, Not Liquidity A key pillar of our argument is that turnover (V i/Mi) proxies for noise rather than information. If turnover were purely information-driven, normalizing by it might be justified. Barinov (2014) explicitly argues that turnover is a proxy forfirm-specific uncertainty andinvestor disagreement, rather than liquidity or information arrival. High turnov...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> The mathematical analysis confirms that: 1.S M C preserves the signal (kÎ± i) without distortion 2.S T V multiplies the signal byÏ„ âˆ’1 i , introducing variance inflation 3. The variance inflation inS T V (fromE[Ï„ âˆ’2]) exceeds the noise scaling inS M C (from E[Ï„ 2]) 4. Therefore, SNR M C >SNR T V whenever turnover exhibits cross-sectional dispersion This completes the proof that market capitalization normalization is the optimal (matched) filter for informed trading signals. 26

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Reinforcement Learning in Financial Decision Making: A Syste

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Reinforcement Learning in Financial Decision Making: A Syste
â€¢ **æª”æ¡ˆ**ï¼š `2025_Reinforcement Learning in Financial Decision Making: A Syste.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017â€“2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market micros...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Sources and Nonlinearity of High Volume Return Premium: An E

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Sources and Nonlinearity of High Volume Return Premium: An E
â€¢ **æª”æ¡ˆ**ï¼š `2025_Sources and Nonlinearity of High Volume Return Premium: An E.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Chae and Kang (2019,Pacific-Basin Finance Journal) documented a puzzling Low Volume Return Premium (LVRP) in Koreaâ€”contradicting global High Volume Return Premium (HVRP) evidence. We resolve this puzzle. Using Korean market data (2020-2024), we demonstrate that HVRP exists in Korea but is masked by (1) pooling heterogeneous investor types and (2) using inappropriate intensity normalization. When institutional buying intensity is normalized by market capitalization rather than trading value, a perfect monotonic relationship emerges: highest-conviction institutional buying (Q4) generates +12.12% cumulative abnormal returns over 50 days, while lowest-intensity trades (Q1) yield modest returns (+4.65%). Retail investors exhibit a flat patternâ€”their trading generates near-zero returns regardless of conviction levelâ€”confirming the pure noise trader hypothesis. During the Donghak Ant Movement (2020-2021), however, coordinated retail investors temporarily transformed from noise traders to liqu...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> that combines investor type classification with quartile-based intensity analysis, we uncover 3 a powerful monotonic relationship that prior methods obscured. For institutional investors, events in the highest conviction quartile (measured as net buying relative to market capitalization) generate cumulative abnormal returns of+12.12%over 50 days, while the lowest quartile yields modest returns (4.65%). This perfect monotonic pattern (Q1<Q2<Q3<Q4) demonstratesthatintensitymattersprofoundlyâ€”butonlywhenmeasuredcorrectly. The critical methodological insight is thathow we normalize intensity determines whether we detect informed trading signals. When intensity is normalized by daily trading value (measuring participation in trading flow), the monotonic pattern breaks down completely (Q2: +12.77% > Q4: +3.64%). But when normalized by market capitalization (measuring conviction as position size relative to firm value), the relationship becomes crystal clear. This demonstrates that the near-zero correlation with the Dominance Score does not mean intensity is irrelevant; it means the relationship isnonlinear, typedependent, and critically sensitive to measurement specification. Prior studies may have missed this relationship by using inappropriate normalization benchmarks or failing to account for investor heterogeneity. Therefore, our study has four objectives. First, we replicate An et al. (2006)â€™s study with 2020-2024 data to verify whether the HVRP phenomenon rem...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Stochastic Volatility Modelling with LSTM Networks: A Hybrid

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Stochastic Volatility Modelling with LSTM Networks: A Hybrid
â€¢ **æª”æ¡ˆ**ï¼š `2025_Stochastic Volatility Modelling with LSTM Networks: A Hybrid.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper are openly accessible via GitHub: https://github.com/aperekhodko/sv_lstm_hybrid_model. 1 arXiv:2512.12250v1 [q-fin.TR] 13 Dec 2025 forecasts to guide investment strategies and assess risk exposure. While models such as GARCH and SV capture the stochastic nature of volatility, incorporating LSTM networks can enhance predictions by identifying non-linear patterns. This study addresses a gap in financial forecasting research by proposing a hybrid SV-LSTM model that leverages both the statistical strengths of SV and the pattern-recognition capacity of LSTM. The objective is to assess whether incorporating latent volatility estimates from an SV model as an additional input enhances the predictive performance of the LSTM model compared to their standalone applications. The main f...

â€¢ **Abstract Reference**:
> Accurate volatility forecasting is essential in various domains, including banking, investment, and risk management, as expectations about future market movements directly influence current decision-making. This study proposes a hybrid modeling framework that integrates a Stochastic Volatility model with a Long Short-Term Memory neural network. The SV model contributes statistical precision and the ability to capture latent volatility dynamics, particularly in response to unforeseen events, while the LSTM network enhances the modelâ€™s ability to detect complex, nonlinear patterns in financial time series. The forecasting is conducted using daily datafromtheS&P500index, coveringtheperiodfromJanuary1, 1998, toDecember 31, 2024. A rolling window approach is employed to train the model and generate one-step-ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. Results show that the hybrid approach ou...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> employed in this thesis involves a combination of stochastic volatility modeling, LSTM networks and statistical testing. The SV model is used to generate volatility forecasts, which are then incorporated as inputs to the LSTM model. Additionally, benchmarks, including standalone LSTM and SV models, are introduced to compare the performance of the hybrid SV-LSTM model. The Wilcoxon Signed-Rank and Diebold-Mariano Tests are used to statistically evaluate the significance of the differences in predictive performance between the models. This study contributes to financial modeling by examining the combination of a statistical SV model as input to a machine learning LSTM model. It is the first to investigate the impact of incorporating SV predictions of latent volatility into the LSTM architecture. This hybrid approach captures both latent stochastic processes and nonlinear dependencies in financial time series, improving the modelâ€™s ability to reflect unpredictable market dynamics. The step-by-step framework, supported by sensitivity analysis and statistical testing, bridges the gap between deep learning techniques and traditional econometric approaches. Practically, the study demonstrates relevance through a simulated investment 2 strategy, highlighting potential improvements in portfolio risk management, dynamic asset allocation, and algorithmic trading, and offering tools for more adaptive and resilient investment strategies in volatile markets. The structure of...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> s are drawn from the hypothesis testing conducted in this study: â€¢H1: The results show that the inclusion of stochastic volatility forecasts for day t+1 enhances the predictive accuracy of the LSTM model. â€¢H2: Augmenting the input data of the LSTM model with external information beyond historical returns was proven to improve its forecasting performance. â€¢H3: The comparative metrics analysis showed that the hybrid SV-LSTM model delivers enhanced volatility forecasts compared to the standalone SV model. In addition to the main hypotheses, the following secondary research questions underwent investigation: â€¢RQ1 Increasing the dimensionality of inputs from the SV model further enhances the predictive performance of the hybrid model. â€¢RQ2 The change of data preprocessing scaling from min-max to either standard or robust scaling improved the performance of the SV-LSTM model. â€¢RQ3 The decreased sequence of the input data into the LSTM model was shown to not leverage the SV-LSTM prediction accuracy. This study made a contribution to financial modelling by introducing and evaluating the combination of a statistical SV model as an input into a machine learning LSTM model. This approach allows the model to capture unpredictable market dynamics by incorporating the latent stochastic process and nonlinear dependencies in financial time series. The simulation of investment strategy alongside the sensitivity analysis provided practical insights into the application of the presented model. The hybrid SV-ML framework could be expanded in a number of ways in future studies. One approach involves feeding machine learning models with different outputs from the stochastic volatility model, such as latent states or predictive densities. Directly incorporating machine learning into the SV estimation procedure is another promising approach that could lessen the need for large-scale sampling by substituting effective approximators for computationally demanding simulations. Adaptability may also be 30 improved by further research into more complex hybrid structures, such as fusing LSTM with more specialized SV models. Model stability may be increased by standardizing ML hyperparameters across windows.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 The Red Queen's Trap: Limits of Deep Evolution in High-Frequ

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 The Red Queen's Trap: Limits of Deep Evolution in High-Frequ
â€¢ **æª”æ¡ˆ**ï¼š `2025_The Red Queen's Trap: Limits of Deep Evolution in High-Frequ.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the "Holy Grail" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of "Galaxy Empire," a hybrid framework coupling LSTM/Transformer-based perception with a genetic "Time-is-Life" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY >300% ) and live performance (Capital Decay >70% ). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting ofAleatoric Uncertaintyin low-entropy time-series, theSurvivor Biasinherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findin...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> to selectively retain long-term trend information while discarding short-term microstructure noise [8]. Transformer Encoder.To capture global dependencies and regime shifts, we utilized Multi-Head Self-Attention (MHSA). The attention function maps a query and a set of key-value pairs to an output: Attention(Q, K, V) =softmax  QK T âˆšdk  V(2) While Transformers excel in NLP [9], we hypothesized their massive capacity might lead to overfitting on stochastic financial noise, a hypothesis later validated by our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> and Implications We successfully engineered a system that integrated the cutting edge of Artificial Intelligence, Evolutionary Computation, and Agent-Based Modeling. The "Galaxy Empire" was designed to be the ultimate trading organism: perceiving via Transformers, adapting via Evolution, and surviving via Diversity Preservation. Its economic failure serves as a potent empirical validation of the Efficient Market Hypothesis (EMH) at high frequencies for OHLCV data [20]. The primary contribution of this work is the demystification of complexity. We demonstrated thatModel Complexity cannot compensate for Information Deficiency. The "Red Queen" runs fast, but on a treadmill of transaction fees and random walk noise, she moves backward. 5.1 Implications for Retail Investors The failure of such a sophisticated system offers profound and sobering lessons for the individual investor: 1. The Frequency Trap:If an autonomous system with zero latency and deep learning capabilities cannot overcome the 0.1% friction cost of high-frequency trading, a manual trader attempting to "scalp" on 5-minute or 15-minute charts is statistically guaranteed to lose capital over the long term. The math of friction is unforgiving. 5 APREPRINT- DECEMBER19, 2025 2. The Leverage Illusion:Our experiment showed that high leverage ( 5x+) does not increase the probability of success; it only accelerates the "Time to Ruin." The liquidation cascade observed in our simulation proves that leverage turns a fair game into a distinct disadvantage due to volatility decay [21]. 3. Complexity Ì¸= Profitability:Retail investors often believe they lose because they lack complex tools. Our results suggest the opposite: we lostbecauseof complexity. Simple, low-frequency strategies (e.g., Dollar Cost Averaging or Spot Holding) that minimize interaction with market friction often outperform complex high-frequency active management. Future research must pivot away from "predicting price direction" on micro-timeframes. True Alpha lies not in being faster than the market, but in operating on timeframes (Daily/Weekly) or data sources (On-Chain, Order Flow) where the signal-to-noise ratio is structurally higher [22].

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Wealth or Stealth? The Camouflage Effect in Insider Trading

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Wealth or Stealth? The Camouflage Effect in Insider Trading
â€¢ **æª”æ¡ˆ**ï¼š `2025_Wealth or Stealth? The Camouflage Effect in Insider Trading.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We consider a Kyle-type model where insider trading takes place among a potentially large population of liquidity traders and is subject to legal penalties. Insiders exploit the liquidity provided by the trading masses to â€œcamouflageâ€ their actions and balance expected wealth with the necessary stealth to avoid detection. Under a diverse spectrum of prosecution schemes, we establish the existence of equilibria for arbitrary population sizes and a unique limiting equilibrium. A convergence analysis determines the scale of insider trading by a stealth indexÎ³, revealing that the equilibrium can be closely approximated by a simple limit due to diminished price informativeness. Empirical aspects are derived from two calibration experiments using non-overlapping data sets spanning from 1980 to 2018, which underline the indispensable role of a large population in insider trading models with legal risk, along with important implications for the incidence of stealth trading and the deterrent ef...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is based on a newly-constructed game-theoretic framework (following Kyle (1985)) for insider trading that adapts to a wide variety of detection mechanisms and penalty functions while incorporating the population size of liquidity traders. In this general setting, the assumption of normally distributed liquidity trades is well supported by the large size of the liquidity trading crowd and the independence among individual trades, and we shall show that an *Department of Mathematics, University of Southern California â€ Email: jinma@usc.edu â€¡Email: weixuanx@usc.edu Â§Email: jianfenz@usc.edu 1 arXiv:2512.06309v1 [econ.GN] 6 Dec 2025 The Camouflage Effect in Insider Trading equilibrium exists for any finite population of liquidity traders â€“ a...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Who sets the range? Funding mechanics and 4h context in cryp

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Who sets the range? Funding mechanics and 4h context in cryp
â€¢ **æª”æ¡ˆ**ï¼š `2025_Who sets the range? Funding mechanics and 4h context in cryp.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper â€”as emerging from the distribution of trader preferences, capital endowments, and information sets. 3.5 Regime-Dependent Market Behavior The paper's distinction between accumulation, distribution, and trending phases connects to regime-switching models in financial econometrics. Hamilton (1989) pioneered â€œMarkovswitching modelsâ€1 that allow market parameters to shift between discrete states, while Ang and 1 A Markov-switching model is a regime-switching technique that allows for the parameters of a time series to change across discrete, unobserved states, such as "low" versus "high" volatility regimes. In financial markets, this framework...

â€¢ **Abstract Reference**:
> Financial markets are often described as chaotic, with price movements appearing erratic and driven by sudden sentiment shifts. This interpretation obscures a fundamental reality: ranges are rarely accidental. They are structured outcomes shaped by underlyi ng market context and capital conditions that govern price behavior. This paper advances the argument that the four -hour (4H) timeframe offers a critical analytical lens for understanding market structure. Positioned between intraday fluctuations and higher-timeframe macro trends, the 4H context captures the equilibrium zon e where institutional positioning, leveraged exposure, and liquidity management converge. Within this temporal window, markets reveal their true architecture â€”one governed less by headlines and more by the interaction between structural context and funding dynamics. Funding mechanisms, particularly in derivative -driven markets, operate as a subtle but powerful disciplinary force. They regulate trader behavior, ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . Table XI. Validation Logic and Quality Assurance Protocols Validation Type Procedure Purpose Frequency Action on Failure Timestamp Verification Cross-reference exchange timestamps with NTP servers Ensure temporal alignment Continuous Resample with corrected timestamps Price Consistency Compare spot prices across exchanges Identify anomalies or manipulation Per data point Flag outliers exceeding 2Ïƒ from median Volume Verification Cross-validate reported volumes Detect wash trading or false reporting Daily aggregation Exclude suspect exchange data Funding Rate Bounds Check for impossible funding values Identify data errors Each funding period Replace with platform-reported values Open Interest Sanity Verify OI changes match liquidation + trade flow Ensure data completeness Daily Investigate discrepancies >5% Order Book Integrity Verify bid-ask spread reasonability Detect stale or manipulated books Each snapshot Exclude snapshots with spreads >1% Liquidation Confirmation Cross-reference onchain with exchange data Validate liquidation accuracy Per event Use most reliable source Missing Data Handling Identify gaps in time series Maintain data continuity Continuous Interpolate if gap <2 periods, else flag Note: NTP synchronization ensures timestamps accurate to Â±30 seconds. Price consistency uses median absolute deviation (MAD) for robust outlier detection. Volume verification flags exchanges sho...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 A Formal Approach to AMM Fee Mechanisms with Lean 4

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 A Formal Approach to AMM Fee Mechanisms with Lean 4
â€¢ **æª”æ¡ˆ**ï¼š `2026_A Formal Approach to AMM Fee Mechanisms with Lean 4.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Decentralized Finance (DeFi) has revolutionized financial markets by enabling complex assetexchange protocols without trusted intermediaries. Automated Market Makers (AMMs) are a central component of DeFi, providing the core functionality of swapping assets of different types at algorithmically computed exchange rates. Several mainstream AMM implementations are based on the constant-product model, which ensures that swaps preserve the product of the token reserves in the AMM â€” up to atrading feeused to incentivize liquidity provision. Trading fees substantially complicate the economic properties of AMMs, and for this reason some AMM models abstract them away in order to simplify the analysis. However, trading fees have a non-trivial impact on usersâ€™ trading strategies, making it crucial to develop refined AMM models that precisely account for their effects. In this work, we extend a foundational model of AMMs by introducing a new parameter, the trading feeÏ•âˆˆ(0, 1], into the swap rate f...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is therefore the swap rate functionSXÏ•. Uniswap v2 [3] and other mainstream AMM implementations [1, 2] use theconstant-product swap ratefunction: SXÏ•(x,r 0,r 1) = Ï•r1 r0 +Ï•x whereÏ•âˆˆ(0,1](1) where r0 and r1 are, respectively, the reserves of the tokensÏ„0 and Ï„1 in the current AMM state, and the parameterÏ•is thetrading fee. Despite the name â€œconstant-productâ€, swaps preserve the productr0Â·r1 of the AMM reserves only in the fee-less case, i.e. whenÏ•= 1. Instead, whenÏ•<1, the product of the reserves strictly increases upon each swap. More specifically, if an AMM{r0 :Ï„0,r 1 :Ï„1}evolves into{r0 +x:Ï„0,r 1âˆ’y:Ï„1}upon a swap, then(r0 +x)(r1âˆ’y)>r 0r1. Therefore, the amounty of tokensÏ„1 sent to the user performing the swap is reduced compared to the fee-less case.1 The intuition is that when the transaction A:swap(x,Ï„0,Ï„1) is fired, the actual number of tokens that are used to re-balance the AMM pair is notx, butÏ•Â·xinstead. This means that A receives fewer tokens and some units of Ï„1 token are â€œkeptâ€ by the AMM in order to benefit its liquidity providers (LPs). The incentives to LPs are briefly discussed at the end of this section. â–¶ Example 1.Consider a systemÎ“containing an AMM {40 :Ï„0,60 :Ï„1}and user Aâ€™s wallet holding30 : Ï„0 and20 : Ï„1. We write such state asÎ“ =A[30 :Ï„0,20 :Ï„1]|{40 :Ï„0,60 :Ï„1}. Suppose that A wants to swap10 :Ï„0, and that the trading fee in(1) is set toÏ•= 0.997. Then,Aexpects to receivey:Ï„ 1, where: y= 10Â· 0.997Â·60 40 + 0.997Â·10â‰ˆ11.97 Hence, firing the transactionT=A...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 A unified theory of order flow, market impact, and volatilit

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 A unified theory of order flow, market impact, and volatilit
â€¢ **æª”æ¡ˆ**ï¼š `2026_A unified theory of order flow, market impact, and volatilit.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a microstructural model for the order flow in financial markets that distinguishes betweencore ordersandreaction flow, both modeled as Hawkes processes. This model has a natural scaling limit that reconciles a number of salient empirical properties: persistent signed order flow, rough trading volume and volatility, and power-law market impact. In our framework, all these quantities are pinned down by a single statisticH 0, which measures the persistence of the core flow. Specifically, the signed flow converges to the sum of a fractional process with Hurst indexH 0 and a martingale, while the limiting traded volume is a rough process with Hurst indexH 0 Â´1{2. No-arbitrage constraints imply that volatility is rough, with Hurst parameter 2H0 Â´3{2, and that the price impact of tra...

â€¢ **Abstract Reference**:
> We propose a microstructural model for the order flow in financial markets that distinguishes betweencore ordersandreaction flow, both modeled as Hawkes processes. This model has a natural scaling limit that reconciles a number of salient empirical properties: persistent signed order flow, rough trading volume and volatility, and power-law market impact. In our framework, all these quantities are pinned down by a single statisticH 0, which measures the persistence of the core flow. Specifically, the signed flow converges to the sum of a fractional process with Hurst indexH 0 and a martingale, while the limiting traded volume is a rough process with Hurst indexH 0 Â´1{2. No-arbitrage constraints imply that volatility is rough, with Hurst parameter 2H0 Â´3{2, and that the price impact of trades follows a power law with exponent 2Â´2H 0. The analysis of signed order flow data yields an estimateH 0 Â«3{4. This is not only consistent with the square-root law of market impact, but also turns out...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> developed for mixed fractional processes in [15, 60]. As a consequence, classical roughness estimators based on a pure fractional Brownian motion specification are inherently biased in this setting, in line with the...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 An Explainable Market Integrity Monitoring System with Multi

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 An Explainable Market Integrity Monitoring System with Multi
â€¢ **æª”æ¡ˆ**ï¼š `2026_An Explainable Market Integrity Monitoring System with Multi.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> AbstractMarket manipulation remains difficult to detect in practice because the strongest surveillance systems rely on proprietary order-book data and opaque models, limiting reproducibility and independent validation. We presentAIMM-X, an explainable marketintegrity monitoring framework that uses onlypublicly accessible signalsâ€”standard OHLCV price/volume data combined with multi-source attention proxiesâ€”to surfacesuspicious windows: contiguous time intervals where returns, volatility, and public attention jointly deviate from their historical baselines. AIMM-X applies transparent statistical scoring andhysteresis-based segmentationto generate candidate windows, then ranks them using an interpretableIntegrity Scorethat decomposes into factor contributions (Ï•1â€“Ï•6), enabling auditability and analyst review. In a one-year demonstration on a 24-ticker universe (daily bars, 2024), AIMM-X detects 233 suspicious windows and produces fully reproducible artifacts (window lists, rankings, facto...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> â€¢Case studies demonstrating the frameworkâ€™s ability to surface episodes with unusual price-attention comovement 3 Explainable Market Integrity Monitoring via Multi-Source Attention Signals and Transparent Scoring â€¢Statistical analysis of factor contributions and cross-ticker patterns Practical contributions: â€¢An auditable framework suitable for compliance workflows and regulatory engagement â€¢Modular architecture supporting extension to higher-frequency data and richer feature sets â€¢Open methodology enabling independent validation, criticism, and improvement by the research community â€¢Clear guidelines for interpreting alerts and managing false positives 1.5 Scope and Limitations We emphasize that AIMM-X is not a silver bullet for manipulation detection. Ground truth labels remain scarceâ€”we cannot definitively say which windows represent manipulation versus legitimate volatility events. False positives are inevitable, and the system requires human review and domain expertise to translate alerts into actionable intelligence. Furthermore, this preprint demonstrates the framework using daily OHLCV data due to data tier constraints. While this allows us to validate the end-to-end pipeline and produce reproducible...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Autonomous Market Intelligence: Agentic AI Nowcasting Predic

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Autonomous Market Intelligence: Agentic AI Nowcasting Predic
â€¢ **æª”æ¡ˆ**ï¼š `2026_Autonomous Market Intelligence: Agentic AI Nowcasting Predic.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a mechanism rooted in the structure of online information. Genuinely positive news tends to be unambiguous: strong earnings, major contract wins, and successful product launches generate coherent signals across sources. Genuinely negative news, by contrast, coexists with strategic obfuscation. Corporate communications emphasise silver linings even amid deteriorating fundamentals. Retail investors on social media platforms post contrarian â€œbuy the dipâ€ encouragements. Speculative content proliferates precisely when prospects are most uncertain. The AI, trained to aggregate and synthesise information, may excel at recognising consensus-positive signals while struggling to distinguish genuine distress from noise-laden pessimism. This asymmetry has implications beyond finance: i...

â€¢ **Abstract Reference**:
> Can fully agentic AI nowcast stock returns? We deploy a state-of-the-art Large Language Model to evaluate the attractiveness of each Russell 1000 stock daily, starting from April 2025 when AI web interfaces enabled real-time search. Our data contribution is unique along three dimensions. First, the nowcasting framework is completely out-of-sample and free of look-ahead bias by construction: predictions are collected at the current edge of time, ensuring the AI has no knowledge of future outcomes. Second, this temporal design is irreproducibleâ€”once the information environment passes, it can never be recreated. Third, our framework is 100% agentic: we do not feed the model news, disclosures, or curated text; it autonomously searches the web, filters sources, and synthesises information into quantitative predictions. We find that AI possesses genuine stock selection ability, but only for identifying top winners. Longing the 20 highest-ranked stocks generates a daily Fama-French five-facto...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , including the agentic workflow and prompt design. Section 4 presents the summary statistics and variable correlations. Section 5 reports our main empirical findings on the performance of the top-ranked portfolios and factor exposures. Section 5.5 investigates the asymmetric predictability pattern across the entire cross-section. Finally, Section 6 concludes with a discussion of limitations, implications for AI development, and directions for future research. 2 Related Literature Our study bridges the gap between empirical asset pricing and artificial intelligence. We situate our contribution within three related literatures: the application of machine learning to return prediction, the evolution of textual analysis from sentiment dictionaries to large language models, and the development of autonomous agentic AI systems. 2.1 Machine Learning in Asset Pricing The application of machine learning (ML) to asset pricing has developed primarily to navigate the â€œfactor zooâ€ and address empirical challenges to the Efficient Markets Hypothesis. We organise this literature around three themes: factor model estimation, return prediction, and the role of feature engineering. Factor Model Estimation.Foundational work by Kelly et al. (2019) introduced Instrumented Principal Component Analysis (IPCA), which allows latent risk factors 5 Variable (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) Attr (1D) 1.00 Attr (1W)0.97 âˆ—âˆ—âˆ— 1.00 Attr (1M)0.91 âˆ—âˆ—âˆ— 0.95âˆ—âˆ—âˆ— 1.00 Russell Attr (1D)0.27âˆ—âˆ—âˆ— 0.31âˆ—...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Bayesian Robust Financial Trading with Adversarial Synthetic

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Bayesian Robust Financial Trading with Adversarial Synthetic
â€¢ **æª”æ¡ˆ**ï¼š `2026_Bayesian Robust Financial Trading with Adversarial Synthetic.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agentâ€”guided by a quantile belief networkâ€”maintains and updates its belief over hidden market state...

â€¢ **Abstract Reference**:
> Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changesâ€”e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two core challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> .arXiv preprint arXiv:2303.00080(2023). [33] Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein. 2010. Ad hoc autonomous agent teams: Collaboration without pre-coordination. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 24. 1504â€“1509. [34] Shuo Sun, Molei Qin, Wentao Zhang, Haochong Xia, Chuqiao Zong, Jie Ying, Yonggang Xie, Lingxuan Zhao, Xinrun Wang, and Bo An. 2023. TradeMaster: a holistic quantitative trading platform empowered by reinforcement learning. Advances in Neural Information Processing Systems36 (2023), 59047â€“59061. [35] Shuo Sun, Wanqi Xue, Rundong Wang, Xu He, Junlei Zhu, Jian Li, and Bo An. 2022. DeepScalper: A risk-aware reinforcement learning framework to capture fleeting intraday trading opportunities. InProceedings of the 31st ACM International Conference on Information & Knowledge Management. 1858â€“1867. [36] Shuntaro Takahashi, Yu Chen, and Kumiko Tanaka-Ishii. 2019. Modeling financial time-series with generative adversarial networks.Physica A: Statistical Mechanics and its Applications527 (2019), 121261. [37] Aviv Tamar, Huan Xu, and Shie Mannor. 2013. Scaling up robust MDPs by reinforcement learning.arXiv preprint arXiv:1306.6189(2013). [38] Ke Tang and Wei Xiong. 2012. Index investment and the financialization of commodities.Financial Analysts Journal68, 6 (2012), 54â€“74. [39] Chen Tessler, Yonathan Efroni, and Shie Mannor. 2019. Action robust reinforcement learning and applications in continuous control. InInterna...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 DeePM: Regime-Robust Deep Learning for Systematic Macro Port

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 DeePM: Regime-Robust Deep Learning for Systematic Macro Port
â€¢ **æª”æ¡ˆ**ï¼š `2026_DeePM: Regime-Robust Deep Learning for Systematic Macro Port.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We proposeDeePM(DeepPortfolioManager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous â€œragged filtrationâ€ problem via a Directed Delay(Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via aMacroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objectivewhere a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) â€“ awindow-robustutility encouraging strong performance in the most adverse historical...

â€¢ **Abstract Reference**:
> We proposeDeePM(DeepPortfolioManager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous â€œragged filtrationâ€ problem via a Directed Delay(Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via aMacroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objectivewhere a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) â€“ awindow-robustutility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010â€“2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classica...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> demonstrates structural resilience across the 2010s â€œCTA (Commodity Trading Advisor) Winterâ€ and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability. KeywordsSystematicMacro Â·PortfolioManagement Â·DeepLearning Â·Attention Â·GraphNeuralNetworks Â·Transaction CostsÂ·Robust OptimizationÂ·Risk Measures 1 Introduction The central goal of systematic portfolio management is to construct asset allocations that generalize out-of-sample under heavy-tailed returns, regime shifts, and significant trading frictions. While classical meanâ€“variance optimization [Markowitz, 1952] provides a foundational framework, its practical deployment is plagued by â€œerror maximizationâ€ [Michaud, 1989], where small estimation errors in covariance matrices lead to unstable, turnover-intensive portfolios. Consequently, modern approaches have increasingly pivoted toward machine learning pipelines. However, most existing methods adopt a disjoint two-stage approach â€“ forecasting returns first, then performing a portfolio construction step â€“ arXiv:2601.05975v1 [q-fin.TR] 9 Jan 2026 DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management which misaligns the training loss...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Design and Empirical Study of a Large Language Model-Based M

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Design and Empirical Study of a Large Language Model-Based M
â€¢ **æª”æ¡ˆ**ï¼š `2026_Design and Empirical Study of a Large Language Model-Based M.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLMs) driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agentsâ€”announcement, event, price momentum, and marketâ€”each conducting analysis from different dimensions; then the prediction agent integrates these multi-source signals to output directional probability distributions across multiple time horizons; then the decision agent generates discrete position adjustment signals based on the prediction results and risk control constraints, thereby forming a closed loop of â€analysisâ€“predictionâ€“decisionâ€“execution.â€ This study further compares two prediction model pathways: for the prediction agent, directly calling the general-purpose large model DeepSeekR1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from Oct...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> provides potential key price ranges based on support and resistance levels and annotates the most important risk factors in the current structure (e.g., false breakout, insufficient volume, or volatility contraction). 2.2 Announcement Agent The Announcement Agent is responsible for evaluating announcement-driven price impacts for public REITs under recent information disclosures. Its core goal is to judge: given the current price and volatility state, whether the most recently disclosed set of announcements constitutes trading signals with historical statistical significance. At the data level, a historical impact analysis module is established for specific types of announcements. When this module is invoked, it extracts historical announcements of the same type for that REIT before the analysis date from the database, including announcement summaries, sentiment annotations, and price performance within specific trading days after the announcement release. These historical records are grouped by â€positive / neutral / negativeâ€ sentiment, and multiple statistics are calculated accordingly, such as short-term probability of price increase, average increase ratios over different windows, and frequency of significant fluctuations under positive or negative scenarios, thereby forming a historical price reaction profile for that announcement type. 5 In the real-time phase, the system collects all announcements released within the 7 natural days preceding the analysis date and fil...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Directional Liquidity and Geometric Shear in Pregeometric Or

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Directional Liquidity and Geometric Shear in Pregeometric Or
â€¢ **æª”æ¡ˆ**ï¼š `2026_Directional Liquidity and Geometric Shear in Pregeometric Or.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> observational structure aligns the present approach with statistical physics treatments of coarsegraining and projection, rather than with microscopic market modeling. From this perspective, the results reported here apply to any one-dimensional observable coordinate that orders liquidity and admits a natural symmetry point. The price axis is a privileged example, but not the only possible realization of the projectionÎ (G). III. SHEAR DECOMPOSITION AND GAUGE SEPARATION The observational construction introduced in Sec. II defines a single projected liquidity densityÎ½t(x)at each observation timet. This object already incorporates aggregation across venues, reduction of relational degrees of freedom, and projection onto an observable coordinate. In this section, we analyze theallowed dynamicsofÎ½ t(x)and show that its evolution admits a natural and unavoidable decomposition into a purely gauge component and a physical deformation mode, which we termshear. A. Observable dynamics of projecte...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> We analyze Level II order-book data for six U.S. equities (AAPL, MSFT, NVDA, JPM, GS, TSLA) obtained fromInteractive Brokersvia the NASDAQ TotalView / OpenViewfeed, whichaggregatesvisibleliquidityacross trading venues. The analysis spans multiple regular trading days and is restricted to standard U.S. market hours (9:30â€“16:00 EST). Pre-market and overnight activity are excluded. Each trading day is divided into non-overlapping intraday windows of duration âˆ†T= 10 s. This choice reflects the intrinsically local character of the observational geometry studied here and avoids mixing incompatible order-book configurations. For each windowT, we construct cumulative bid and ask liquidity profilesQbid(x)andQ ask(x)by aggregating visible liquidity at each price levelxrelative to the window-averaged mid price pâ‹† T = 1 2  âŸ¨pbest askâŸ©T +âŸ¨p best bidâŸ©T  . Liquidity is expressed in discrete tick units. The observational window extends to K= 50 ticks on each side of the mid price, capturing the region where liquidity is consistently observed and where the single-scale geometric assumptions of the model apply. Figure 2:Cumulative shear field for AAPL.The extended and smooth shear profile indicates a macroscopic deformation of projected liquidity geometry rather than a localized microstructural effect. B. From two-sided liquidity to shear modes As established in Sec. II, observable liquidity arises as a single projected densityÎ½ t(p)defined on a onedimensional price-like coordinat...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> is consistent with earlier empirical findings showing that order-book imbalances have limited, unstable, and highly conditional predictive power for short-horizon price changes [4, 6, 9]. The present framework provides a structural explanation for these observations: imbalance is not a dynamical driver but an internal deformation mode of the observable liquidity geometry. B. Shear is not the time derivative of price Equally important is what shear isnot. The empirical decoupling between shear amplitude and mid-price translation demonstrates that shear cannot be identified with the time derivative of the mid price, nor with any finite-difference proxy thereof. In the present construction, the mid price is a gaugedependent quantity defined by a global symmetry point of the projected liquidity measure. Translations of the projected coordinate act as gauge transformations that shift the mid without altering the shape of the density. Shear, by contrast, is invariant under such translations and probes the local asymmetry and curvature of the density around the mid. Thetwoobservablesthereforeinhabitcomplementary subspaces of the observable manifold and cannot be related by differentiation or integration in time. This geometric separation clarifies why attempts to model price changes directly from order-book shape often yield unstable or regime-dependent results, as reported for instance in Refs. [10, 13]. From this perspective, such instability is not accidental but structural. C. Directional liquidity as geometric shear Within the present framework, bidâ€“ask asymmetry arises naturally as a directional shear mode of the projected liquidity geometry. Once a mid-price cut is introduced, the restriction of a single projected density to either side yields two complementary branches. Asymmetry between these branches reflects local skewness of the density, not the existence of independent supply and demand forces. Inflationary relational dynamics amplify this effect. Heterogeneous growth and reconfiguration of the underlying relational network continuously reshape the projected density, inducing time-dependent shear without requiring any change in the global position of the projection. This mechanism explains why bidâ€“ask asymmetry is both ubiquitous and highly non-stationary in empirical data. Importantly, this interpretation subsumes a wide range of empirical regularities reported in the literature on order-book shape and dynamics [4, 8

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Diverse Approaches to Optimal Execution Schedule Generation

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Diverse Approaches to Optimal Execution Schedule Generation
â€¢ **æª”æ¡ˆ**ï¼š `2026_Diverse Approaches to Optimal Execution Schedule Generation.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computationalresourcesperbehaviouralcellmayberequiredforrobustspecialistdevelopment across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> and proposed solutions. We begin with the optimal execution set-up, including the formal problem statement, the transient impact model with its calibration, and the construction of raw and derived features. We then introduce reinforcement learning, translating OE into the RL framework and outlining the order generation process. Next, we present the RL methods explored, covering architecture variations (MLP and CNN) of Proximal Policy Optimisation (PPO) and Quality-Diversity approaches (MAP-Elites) and industry-standard execution strategies. Finally, we describe how these components fit into the simulation environment design, including Gymnasium integration and the execution simulator with impact. Section 3 reports the impact 3 Diverse Approaches to Optimal ExecutionA Preprint model calibration...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Explainable Patterns in Cryptocurrency Microstructure

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Explainable Patterns in Cryptocurrency Microstructure
â€¢ **æª”æ¡ˆ**ï¼š `2026_Explainable Patterns in Cryptocurrency Microstructure.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We document stable cross-asset patterns in cryptocurrency limit-order-book microstructure: the same engineered order book and trade features exhibit remarkably similar predictive importance and SHAP dependence shapes across assets spanning an order of magnitude in market capitalization (BTC, LTC, ETC, ENJ, ROSE). The data covers Binance Futures perpetual contract order books and trades on 1-second frequency starting from January 1st, 2022 up to October 12th, 2025. Using a unified CatBoost modeling pipeline with a direction-aware GMADL objective and time-series cross validation, we show that feature rankings and partial effects are stable across assets despite heterogeneous liquidity and volatility. We connect these SHAP structures to microstructure theory (order flow imbalance, spread, and adverse selection) and validate tradability via a conservative top-of-book taker backtest as well as fixed depth maker backtest. Our primary novelty is a robustness analysis of a major flash crash, w...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , analyze SHAP-based explanations and their theoretical interpretations, assess tradability via a conservative taker backtest, and discuss robustness checks and implications before concluding. 2 2 Literature Review A large literature links trading frictions, information, and prices. Foundational inventoryand information-based models establish the theoretical link between trading activity and price formation. The seminal model of Kyle [12] shows how an informed insider strategically places trades over time, and how market makers infer information from aggregate order flow to set prices. Complementing this, Glosten and Milgrom [9] model the bid-ask spread as a direct consequence of adverse selection, where liquidity providers widen their quotes to compensate for the risk of trading against privately informed agents. These theories, synthesized empirically by Hasbrouck [11], provide the basis for our features related to order flow and spreads. A more recent, data-driven perspective from econophysics, particularly the work of Bouchaud et al. [3] and Cont et al. [7], documents universal statistical patterns in price impact, such as the â€square-rootâ€ law, suggesting that endogenous feedback loops in the trading process itself are as important as fundamentals. This motivates our use of flexible, non-linear models capable of capturing these complex dynamics. Limit order book research has evolved to document rich predictive structures. Early work by Gould et al. [10] provides...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Fast Times, Slow Times: Timescale Separation in Financial Ti

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Fast Times, Slow Times: Timescale Separation in Financial Ti
â€¢ **æª”æ¡ˆ**ï¼š `2026_Fast Times, Slow Times: Timescale Separation in Financial Ti.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper is their ability to persist the in-sample time scales of the selected components out of sample. References [1] Hinch, E.J. (1991). Perturbation Methods. https://doi.org/10.1017/CBO9781139172189,Cambridge University Press. 4 (a).Linear tICA,k= 1 (b).Nonlinear tICA,k= 4 Figure 2:Factor ETF: Blind extraction of slowest components over 2021-2022 (green background), continuation over 2022-2025 (white background). Weights are rescaled to unit gross exposure. [2] PÂ´ erez-HernÂ´ andez, G., Paul, F., Giorgino, T., De Fabritiis, G., NoÂ´ e, F. (2013) Identification of slow molecular order parameters for Markov model construction.J. Chem. Phys139, 015102. https://doi.org/10.1063/1.4811489 [3] 2010 flash crash, Wikipedia https://en.wikipedia.org/wiki/2010 flash crash [4] Schmid, P.J. (201...

â€¢ **Abstract Reference**:
> Financial time series exhibit multiscale behavior, with interaction between multiple processes operating on different timescales. This paper introduces a method for separating these processes using variance and tail stationarity criteria, framed as generalized eigenvalue problems. The approach allows for the identification of slow and fast components in asset returns and prices, with applications to parameter drift, mean reversion, and tail risk management. Empirical examples using currencies, equity ETFs and treasury yields illustrate the practical utility of the method. 1 Stationarity in Finance The study of multiscale processes is well-established in physics, with notable examples ranging from fluid dynamics to protein folding, where fast and slow dynamics interact [1]. The typical physical example involves fast oscillations inside a slowly moving envelope, where the cumulative effect of fast oscillations drives changes in the envelope [1, 2]. In finance, we can see multiple example...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Financial time series contain a hierarchy of processes: stationary, slow, and fast. Using generalized eigenvalue problems, we can separate these components, providing insight into parameter drift, mean reversion regimes and/or tail risk stability. The method is computationally tractable and can be applied to diverse asset classes, offering a powerful tool for risk and strategy management. The conceptual framework, moving from small to large structures, mirrors approaches used in physical sciences. The remarkable property of the methods described in this paper is their ability to persist the in-sample time scales of the selected components out of sample.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Forecasting Equity Correlations with Hybrid Transformer Grap

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Forecasting Equity Correlations with Hybrid Transformer Grap
â€¢ **æª”æ¡ˆ**ï¼š `2026_Forecasting Equity Correlations with Hybrid Transformer Grap.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a hybrid Transformer-Graph Attention Network architecture to forecast 10-day-ahead stock-stock correlations for S&P 500 constituents. The model first encodes assetspecific temporal information using a Transformer, producing context-aware embeddings from recent return, technical, and macroeconomic features,. These embeddings are propagated through an edge-aware graph attention network that models cross-asset interactions and adapts to evolving network structure. To stabilize learning and anchor predictions to economically meaningful benchmarks, correlations are modeled in residual Fisher-z space relative to a rolling historical baseline. The primary contribution of this study is to show that learned, forward-looking correlation forecasts can materially improve dependence estima...

â€¢ **Abstract Reference**:
> This paper studies forward-looking stock-stock correlation forecasting for S&P 500 constituents and evaluates whether learned correlation forecasts can improve graph-based clustering used in basket trading strategies. We cast 10-day ahead correlation prediction in Fisher-z space and train a Temporal-Heterogeneous Graph Neural Network (THGNN) to predict residual deviations from a rolling historical baseline. The architecture combines a Transformer-based temporal encoder, which captures non-stationary, complex, temporal dependencies, with an edge-aware graph attention network that propagates cross-asset information over the equity network. Inputs span daily returns, technicals, sector structure, previous correlations, and macro signals, enabling regime-aware forecasts and attention-based feature and neighbor importance to provide interpretability. Outof-sample results from 2019-2024 show that the proposed model meaningfully reduces correlation forecasting error relative to rolling-window...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> 4.1 Problem Framing Traditional stat arbitrage clustering approaches, whether sector-based classifications, correlation/cointegration methods or more recent graph-based methods like SPONGEsym, are fundamentally backward-looking. SPONGEsym specifically â€“ as in Cartea et al.â€™s (2023) approach -, focuses on historical correlation matrices build from residual returns, which only adapt after the market structure has already shifted. In crisis regimes, such as the COVID-19 shock, these correlations spike rapidly, collapsing clusters into a few dominant groups and generating trading losses. Even extensions like ML-filtered SPONGEsym â€“ where trades are only entered if they are in the highest 10% of predicted probability of being profitable â€“ remain subject to this lag, as their predictive element is applied after the cluster formation step, not before. This creates a clear research gap: the application of modern machine learning architectures to predict future correlations, enabling clustering to be performed on forward-looking relationships rather than historical relationships. By conditioning trades on predicted rather than lagged correlation structures, such models adapt more rapidly to crisis regimes minimizing losses and capture additional gains in stable periods, as trades are based on the anticipated market state rather than one that has already evolved. We target 10-day-ahead correlations among S&P 500 constituents, aligned with the 10-day rebalance period in the strategy pu...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 History Is Not Enough: An Adaptive Dataflow System for Finan

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 History Is Not Enough: An Adaptive Dataflow System for Finan
â€¢ **æª”æ¡ˆ**ï¼š `2026_History Is Not Enough: An Adaptive Dataflow System for Finan.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose the workflow shown in Fig. 2. The data manipulation moduleMgenerates augmented training samplesËœx train with operation choosing probability matrixpand manipulation strength parameterÎ». To adaptively controlM, we introduce a trainable plannerg((f Î¸, xi);Ï•), which learns the policyÏ€ Ï•(p, Î»|f, xi)while the scheduler determines the proportion of data to be manipulatedÎ±using a heuristic algorithm. Additionally, we interleave taskmodel updates with planner updates on validation feedback, while provenance hooks (policy, probabilities, manipulation strengths, proportion of data to be manipulated) are persisted to enable exact replay. In order to optimally control the data manipulation module Mfor each training samplex train, inspired by AdaAug and MADAug, we formulate the learning o...

â€¢ **Abstract Reference**:
> â€”In quantitative finance, the gap between training and real-world performanceâ€”driven by concept drift and distributional non-stationarityâ€”remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra â€œHistory Is Not Enoughâ€ underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learningâ€“based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive plannerâ€“scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framewo...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ,â€arXiv preprint arXiv:2303.00080, 2023. [5] E. Samanidou, E. Zschischang, D. Stauffer, and T. Lux, â€œAgent-based models of financial markets,â€Reports on Progress in Physics, vol. 70, no. 3, p. 409, 2007. [6] H. Xia, S. Sun, X. Wang, and B. An, â€œMarket-gan: Adding control to financial market data generation with semantic context,â€ inProceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 14, 2024, pp. 15 996â€“16 004. [7] S. C. Hora, â€œAleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management,â€Reliability Engineering & System Safety, vol. 54, no. 2-3, pp. 217â€“223, 1996. [8] S. Kapoor, W. J. Maddox, P. Izmailov, and A. G. Wilson, â€œOn uncertainty, tempering, and data augmentation in bayesian classification,â€Advances in Neural Information Processing Systems, vol. 35, pp. 18 211â€“ 18 225, 2022. [9] W. Li, X. Yang, W. Liu, Y . Xia, and J. Bian, â€œDdg-da: Data distribution generation for predictable concept drift adaptation,â€ inProceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 4, 2022, pp. 4092â€“4100. [10] T.-H. Cheung and D.-Y . Yeung, â€œAdaaug: Learning class-and instanceadaptive data augmentation policies,â€ inInternational Conference on Learning Representations, 2021. [11] C. Hou, J. Zhang, and T. Zhou, â€œWhen to learn what: Model-adaptive data augmentation curriculum,â€ inProceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1717â€“1728. [12] B. U. Demire...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> In this paper, we introduced a novel adaptive dataflow system designed to bridge the gap between training and realFig. 8: The training and validation loss curve w/wo our workflow applied. world performance in quantitative finance. To the best of our knowledge, this is the first workflow of its kind applied to quantitative finance tasks. The framework integrates a parameterized data manipulation module with a learning-guided plannerâ€“scheduler, forming a feedback loop that dynamically regulates manipulation strength and proportion of data to be manipulated as the model evolves. This design allows the data pipeline to self-adjust to distributional drift, ensuring consistent data quality and realistic synthesis throughout the learning process. Experiments on forecasting and trading tasks demonstrate that the system improves robustness and generalization across models and markets.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Intraday Limit Order Price Change Transition Dynamics Across

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Intraday Limit Order Price Change Transition Dynamics Across
â€¢ **æª”æ¡ˆ**ï¼š `2026_Intraday Limit Order Price Change Transition Dynamics Across.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> A quantitative understanding of the stochastic dynamics in limit order price changes is essential for meaningful advances in market microstructure research and effective execution strategy design. This paper presents the first comprehensive empirical analysis of intraday limit order price change transition dynamics, treating ask and bid orders separately across different market capitalization tiers. Using high-frequency tick data from NASDAQ100 stocks, we employ a discrete-time Markov chain framework to analyze the evolution of price adjustments throughout the trading day. We categorize consecutive price changes into nine distinct states and estimate transition probability matrices (TPMs) for six intraday intervals across High (HMC), Medium (MMC), and Low (LMC) market capitalization stocks. Elememt-wise comparison of TPMs reveals systematic intraday patterns: price inertia i.e. self-transition probability, peaks during opening and closing hours, stabilizing at lower levels during midda...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> [28, 29], we develop a discrete-time Markov chain (DTMC) framework to analyze limit order price change transitions using high-frequency tickby-tick data for NASDAQ100 stocks. Consecutive limit order price changes are classified into nine discrete states based on their magnitude and direction, yielding categorical time series that capture the sequential nature of pricing decisions. For each intraday intervalÏ„âˆˆ {T 1,T 2, . . . ,T6} and market capitalization tiercâˆˆ {HMC,MMC,LMC}, we estimate transition probability matricesP (Ï„,c) i j describing the likelihood of transitioning from stateito statej. Treating bid and ask limit orders separately allows us to explicitly account for directional asymmetries in price revision behavior. Employing this DTMC framework, we conduct a comprehensive empirical analysis of transition probabilities, stationary distributions, and key Markov chain metrics, including spectral gap, entropy rate, and mean recurrence time. We further investigate cross-interval similarities using clustering techniques and Jensenâ€“Shannon divergence. Our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Learning Market Making with Closing Auctions

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Learning Market Making with Closing Auctions
â€¢ **æª”æ¡ˆ**ï¼š `2026_Learning Market Making with Closing Auctions.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper is the following: along the session a market maker quotes on a limit order book to liquidate her position until the end of the continuous trading time session. Then, the exchange triggers a closing auction for the next minutes of the day. During this auction, order are accumulated by the exchange along time, where participants proposes limit orders at which there are willing to buy or sell the asset with a specific volume, and limit order of the previous continuous trading phase are added as trading block for the auction clearing. At the end of the auction phase, known as the clearing time, a clearing price is set by the exchange to ensure as much transaction as possible to clear the market and trade the asset. This closing auction plays a fundamental role in market liquidati...

â€¢ **Abstract Reference**:
> In this work, we investigate the market-making problem on a trading session in which a continuous phase on a limit order book is followed by a closing auction. Whereas standard optimal market-making models typically rely on terminal inventory penalties to manage end-of-day risk, ignoring the significant liquidity events available in closing auctions, we propose a Deep Q-Learning framework that explicitly incorporates this mechanism. We introduce a market-making framework designed to explicitly anticipate the closing auction, continuously refining the projected clearing price as the trading session evolves. We develop a generative stochastic market model to simulate the trading session and to emulate the market. Our theoretical model and Deep Q-Learning method is applied on the generator in two settings: (1) when the mid price follows a rough Heston model with generative data from this stochastic model; and (2) when the mid price corresponds to historical data of assets from the S&P 500...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , contributions and financial insights This work proposes a new market-making model based on a reinforcement learning approach, designed to operate over a typical trading session while explicitly anticipating the closing auction at the end of the session. As far as we know, this paper is the first considering a reinforcement learning method for CLOB optimal market making followed by anticipated closing auciton. The proposed reinforcement learning framework relies on Q-learning, originally introduced in [62], and on its extension to Deep Q-Learning using neural networks for strategy exploration and selection [47, 23]. These approaches have been successfully applied to a wide range of financial problems, including optimal asset allocation, optimal execution in dark pools, and market making; see, for instance, [49, 25, 36, 51, 5]. More specifically, we compare a standard Q-learning approach for market making with closing auction trading to a neural-fitted Q-learning method. The goal of Q-learning is to find the optimal action-value functionQwhich yields the optimal policy. Neural-fitted Q-learning consists in parameterizing the action-value function with a neural networkQÎ¸ such that optimization overQbecomes optimization over the weightsÎ¸âˆˆR q, for someqâˆˆN âˆ—. In the Neural Fitted Q-Iteration (NFQ) algorithm, the neural network is trained by minimizing a squared temporaldifference error over a batch of sampled transitions using gradient descent [56]. Using a deep neural networ...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Manipulation in Prediction Markets: An Agent-based Modeling 

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Manipulation in Prediction Markets: An Agent-based Modeling 
â€¢ **æª”æ¡ˆ**ï¼š `2026_Manipulation in Prediction Markets: An Agent-based Modeling .pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting strategies in the market. The model exhibits stability across a broad parameter space, with complex agent behaviors and price interactions producing self-regulatory price discovery. Second, using this simulation framework, we investigate the conditions under which a highly resourced minority, or â€œwhaleâ€ agent, with a biased valuation can distort the market price, and for how long. We find that biased whales can temporarily shift prices, with the magnitude and duration of distortion increasing when non-whale bettors exhibit herding behav...

â€¢ **Abstract Reference**:
> Prediction markets mobilize financial incentives to forecast binary event outcomes through the aggregation of dispersed beliefs and heterogeneous information. Their growing popularity and demonstrated predictive accuracy in political elections have raised speculation and concern regarding their susceptibility to manipulation and the potential consequences for democratic processes. Using agent-based simulations combined with an analytic characterization of price dynamics, we study how high-budget agents can introduce price distortions in prediction markets. We explore the persistence and stability of these distortions in the presence of herding or stubborn agents, and analyze how agent expertise affects market-price variance. Firstly we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> underlying the Economist forecast model is described in [24]. Our comparison shows that Polymarket prediction prices deviated from these forecasts by a nearly identical range. We provide visual documentation of this validation benchmark in Section 8.4. 2.6. Relevance of the Price Error to Electoral Outcomes In empirical research, prices in prediction markets are commonly interpreted as the probability of a future event [39, 56]. In public settings however, such prices may instead be interpreted using binary decision rules regarding the likely winner, particularly under rapid, headline-level media reporting. Under this interpretation, small deviations around the 0.5 threshold are more likely to flip the implied winner, even when the underlying probabilistic error is small. By treating the market price and true election outcome as normally distributed random variables, we can visualize the probability of misclassification across a range of market and election values Fig. 2 (D). 3. An exploration of market manipulation and volatility The model provides a tool for researchers to evaluate various mechanisms within prediction markets. We provide the first of such an exploration by stress-testing market resilience to price manipulation. In the remainder of this work we explore how high-budget bettors create sustained market price error, and the interaction between such bettors and herding agents. 3.1. Simulation Study 3.1.1. Method In order to test the potential for...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Market Making and Transient Impact in Spot FX

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Market Making and Transient Impact in Spot FX
â€¢ **æª”æ¡ˆ**ï¼š `2026_Market Making and Transient Impact in Spot FX.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Dealers in foreign exchange markets provide bid and ask prices to their clients at which they are happy to buy and sell, respectively. To manage risk, dealers can skew their quotes and hedge in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal market making with execution has previously been addressed within the Almgren-Chriss market impact model, which includes instantaneous and permanent components. However, there is overwhelming empirical evidence of the transient nature of market impact, with instantaneous and permanent impacts arising as the two limiting cases. In this note, we consider an intermediate scenario and study the interplay between risk management and impact resilience. Keywords:Market Making; Stochastic Optimal Control; Market Impact; Algorithmic Trading. 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> optimized with Almgren-Chriss impact. Acknowledgment The author would like to express his sincere gratitude to Olivier GuÃ©ant (UniversitÃ© Paris CitÃ©) for fruitful discussions and valuable comments and to Richard Anthony (HSBC) for support throughout the project. The views expressed are those of the author and do not necessarily reflect the views or practices at HSBC. 7 References Almgren, R. and Chriss, N., Optimal execution of portfolio transactions.J. Risk, 2001,3, 5â€“40. Avellaneda, M. and Stoikov, S., High-frequency trading in a limit order book.Quant. Finance, 2008,8, 217â€“224. Barzykin, A., Bergault, P. and GuÃ©ant, O., Algorithmic market making in dealer markets with hedging and market impact.Math. Finance, 2023,33, 41â€“79. Barzykin, A., Bergault, P., GuÃ©ant, O. and Lemmel, M., Optimal Quoting under Adverse Selection and Price Reading, 2025. Available online at: https://arxiv.org/abs/2508.20225 (accessed 19 December 2025). Bergault, P., Evangelista, D., GuÃ©ant, O. and Vieira, D., Closed-form approximations in multi-asset market making.Appl. Math. Finance, 2021,28, 101â€“142. Bergault, P. and GuÃ©ant, O., Size matters for otc market makers: general...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> s and valuable comments and to Richard Anthony (HSBC) for support throughout the project. The views expressed are those of the author and do not necessarily reflect the views or practices at HSBC. 7

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 PredictionMarketBench: A SWE-bench-Style Framework for Backt

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 PredictionMarketBench: A SWE-bench-Style Framework for Backt
â€¢ **æª”æ¡ˆ**ï¼š `2026_PredictionMarketBench: A SWE-bench-Style Framework for Backt.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introducePredictionMarketBench, a SWEbench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshibased episodes spanning cryptocurrency, weather, and sports. We report baseline results for a RandomAgent, a tool-calling LLM agent (gpt-4.1-nano), and a classic Bo...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> responds with zero or more tool calls. The agent is provider-agnostic and interacts only through the benchmarkAgentContextinterface (market data queries, portfolio queries, and order placement/cancellation). The full production system prompt is provided in Appendix A. Unless otherwise stated, we use a deterministic decoding configuration (temperature 0.0). Parameter Value Modelgpt-4.1-nano Temperature 0.0 Max tool rounds per step 3 Agent call cadence 300s (5 minutes) Equity sampling interval 60s Maker queue modetrade only Table 2: LLM agent production-run configuration. Run configuration. 5 2.2 Task formulation We formulate each episode as a finite-horizon sequential decision problem over a historical timeline. Decision points and termination.The agent is invoked at a fixed cadence (e.g., every 5 seconds). Between invocations, the simulator advances by processing all market events up to the next decision time. The episode terminates once settlement is processed and all positions are marked-to-settlement. Observations.At each decision point the agent may query: (i) market summaries (best bid/ask per ticker), (ii) full orderbooks (optionally depth-limited), (iii) positions and cash/equity, and (iv) open resting orders. Actions.Agents act by submitting orders defined by side (YES/NO), direction (buy/sell), order type (market/limit), size, and time-in-force (IOC/GTC/post-only, depending on execution mode). 2.3 Features of PredictionMarketBench Binary contract semantics.Binary...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> 8.1 Limitations PredictionMarketBench is an initial step toward standardized, execution-aware evaluation for predictionmarket trading agents, and it has several limitations. First, the current release is small (4 episodes from January 2026) and spans a limited set of event types; broader statistical claims require more diverse markets and time periods. Second, while maker/taker execution and fees are modeled, the simulator is still an abstraction of live trading: latency, exchange-specific priority rules, and strategic interaction with other agents are not modeled explicitly [16]. Third, our baseline agents are intentionally simple and are not tuned for out-of-sample robustness; repeated iteration on this dataset risks backtest overfitting [2]. 8.2 Conclusion We presented PredictionMarketBench, a SWE-bench-style benchmark for backtesting algorithmic and LLM-based trading agents on prediction markets using deterministic replay of real orderbooks, trades, and settlement outcomes. Our initial results illustrate the importance of execution realism and transaction costs: an active LLM agent can incur large settlement losses, while a fee-aware algorithmic alpha (Bollinger Bands) can remain profitable in volatile episodes. We release the benchmark to support reproducible comparisons and encourage future work on expanding the dataset, adding stronger baselines, and studying robust agent design under realistic execution constraints. A LLM system prompt You are a trading agent operating on Kalshi prediction markets. 8 ## Objective Maximize profit by trading binary event contracts. Each contract pays $1.00 (100 cents) if the event outcome matches (YES wins = $1 for YES holders, NO wins = $1 for NO holders). ## Market Mechanics - Prices are quoted in cents (1-99) - A YES contract at 60c means the market implies ~60% probability - A NO contract at the same market would be at ~40c (100 - 60 = 40) - You can BUY or SELL on either YES or NO side ## Fee Structure (Critical for Profitability) - **Taker fee**: 7%Ã—priceÃ—(1 - price/100) - applies to market orders and limit orders that cross the spread - **Maker fee**: 1.75%Ã—priceÃ—(1 - price/100) - applies to resting limit orders that provide liquidity - Example: At 50c, taker fee = 1.75c per contract, maker fee = 0.44c per contract - Maker orders are 4x cheaper - strongly prefer GTC or POST_ONLY limit orders ## Strategy Guidelines 1. **Edge calculation**: Only trade when expected value > fees. At 50c with 7% taker fee, you n

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Pregeometric Origins of Liquidity Geometry in Financial Orde

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Pregeometric Origins of Liquidity Geometry in Financial Orde
â€¢ **æª”æ¡ˆ**ï¼š `2026_Pregeometric Origins of Liquidity Geometry in Financial Orde.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a structural framework for the geometry of financial order books in which liquidity, supply, and demand are treated as emergent observables rather than primitive economic variables. The market is modeled as an inflationary relational system without assumed metric, temporal, or price coordinates. Observable quantities arise only through projection, implemented here via spectral embeddings of the graph Laplacian. A one-dimensional projection induces a price-like coordinate, while the projected density defines liquidity profiles around the mid price. Under a minimal single-scale hypothesisâ€”excluding intrinsic length scales beyond distance to the mid and finite visibilityâ€”we show that projected supply and demand are constrained to gamma-like functional forms. In discrete data, thi...

â€¢ **Abstract Reference**:
> economic entities, and edges encode the possibility of interaction. Growth and reorganization proceed through inflationary updates that generate heterogeneous, hub-dominated structures without finetuning. Crucially, none of the standard market observables are defined at this level. Observable quantities arise only through projection. By applying spectral embeddings based on the graph Laplacian, an observer assigns effective coordinates to the relational substrate. A one-dimensional projection induces a scalar coordinate naturally interpreted as price, while the distribution of projected density defines liquidity profiles. Successive projections of an evolving relational system give rise to apparent time series, returns, and fluctuations, even though no fundamental time variable exists microscopically. Within this framework, supply and demand are not independent behavioral curves but geometric branches of a single projected density. We show that, under minimal structural assumptions on ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> introduced by Mike and Farmer [14] demonstrated that heavy-tailed order placement and cancellation processes are sufficient to generate realistic order-book shapes and price diffusion. Similarly, Cont, Stoikov, and Talreja [17] proposed a queue-reactive framework in which liquidity profiles emerge from the interaction of stochastic order flows at discrete price levels. While these models reproduce many empirical features, they rely on behavioral assumptions, calibration choices, or equilibrium concepts whose universality across assets and market conditions is difficult to establish. In this work, we adopt a complementary perspective. Rather than modeling price formation directly or specifyâˆ— joao@quantumcomp.pt ing microscopic order flow mechanisms, we ask whether familiar order-book observables can emerge generically from the projection of a more primitive relational structure. Our approach is inspired by pregeometric frameworks in statistical physics, where geometry and dynamics are not assumed at the microscopic level but arise only through observation and coarse-graining. We model the market as an inflationary relational network whose fundamental description contains no metric, temporal ordering, or economic coordinates. Vertices represent abstract economic entities, and edges encode the possibility of interaction. Growth and reorganization proceed through inflationary updates that generate heterogeneous, hub-dominated structures without finetuning. Cruc...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 ProbFM: Probabilistic Time Series Foundation Model with Unce

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 ProbFM: Probabilistic Time Series Foundation Model with Unce
â€¢ **æª”æ¡ˆ**ï¼š `2026_ProbFM: Probabilistic Time Series Foundation Model with Unce.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we decide to exploit NIG because establishing the effectiveness of DER for the fundamental single-variate, single-step case provides a solid foundation before extending to more complex multi-horizon, multivariate scenarios in future work. 2.2 ProbFM Architecture Design Input Encoding and Patch-Based ProcessingProbFM employs a sophisticated input processing pipeline designed for efficient handling of univariate time series across different frequencies and scales. The architecture begins with patch-based processing following PatchTST (Nie et al. 2023), enhanced with an adaptive multi-patch size approach inspired by MOIRAI (Woo et al. 2024) that adjusts patch size based on input characteristics; followed by Patch Embedding with Positional Encoding. Adaptive Patching Str...

â€¢ **Abstract Reference**:
> Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Studentâ€™s t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformerbased probabilistic framework,ProbFM(probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> : ProbFM Architecture and Training Figure 3 in the Appendix A presents the complete methodology pipeline and details the architectural flow diagram of our proposed ProbFM. Specifically, ProbFM provides a straightforward application of Deep Evidential Regression (DER) to single-variate single-step time series forecasting with six key components: (1) input processing with adaptive patching and positional encoding, (2) standard transformer architecture for representation learning, (3) DER head for principled uncertainty estimation and epistemic-aleatoric decomposition, (4) combined loss functions including evidential and coverage losses, (5) single-stage training with AdamW optimization (Loshchilov and Hutter 2017) and evidence annealing for simplicity and efficiency, and (6) single-pass inference providing point predictions, uncertainty decomposition, and confidence intervals. In this section, we begin by formally defining the problem and establishing the theoretical underpinnings of our method. Subsequently, we introduce the design of the ProbFM architecture, which firstly encompasses input encoding and patch-based processing, a transformer backbone, and a Deep Evidential Regression head. Furthermore, we propose an improved loss function that incorporates coverage mechanisms. The section also covers the training procedure and optimization strategies, followed by the inference process and uncertainty quantification, which completes the diagram flow as il...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> s. â€¢Market Regime Coverage: We understand that the evaluation period may not capture all possible market regimes (e.g., extreme crashes, prolonged bear markets), in which we will further cover more market regimes in subsequent works. â€¢Transaction Costs: Our trading simulations do not account for slippage, fees, or market impact. Real-world implementation would require incorporating these factors. â€¢Alternative Architectures: We focus on LSTM-based implementations. Other architectures may yield different trade-offs. G Future Work We outline several directions for future research that address current limitations and extend the applicability: â€¢Cross-Domain Generalization:We plan to extend evaluation beyond cryptocurrency markets to validate generalizability across diversified domains. This includes financial markets (equities, foreign exchange, commodities) as well as non-financial time series applications such as energy demand forecasting, traffic prediction, and industrial sensor data. Such comprehensive evaluation will establish whether the epistemic-aleatoric decomposition benefits transfer across domains with fundamentally different noise characteristics and temporal dynamics. â€¢Component-Level Ablation Studies:To better understand the contribution of individual architectural components, we will conduct systematic ablation studies that isolate the effects of: (i) coverage loss integration, (ii) evidence annealing schedule, and (iii) bounded prediction transform. These ablations will provide clearer guidance on which design choices are essential for different application contexts. â€¢Statistical Robustness Analysis:Future work will incorporate multiple random initializations per configuration with comprehensive statistical analysis including confidence intervals, standard deviations across runs, and formal hypothesis testing to quantify significance of performance differences. This is particularly important given the inherent noise in financial time series data. â€¢Classical Baseline Comparisons:To establish absolute performance benchmarks, we will include comparisons against classical time series and financial econometric models, including ARIMA, GARCH, and their variants. These baselines will provide context for understanding the value-add of deep learning approaches and help identify regimes where simpler models may suffice. â€¢Multi-Horizon and Multivariate Extensions:We plan to extend the framework to multi-horizon and multi

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Realised quantile-based estimation of the integrated varianc

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Realised quantile-based estimation of the integrated varianc
â€¢ **æª”æ¡ˆ**ï¼š `2026_Realised quantile-based estimation of the integrated varianc.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we propose a new jump robust quantile-based re alised variance measure of ex-post return variation that can be computed using potenti ally noisy data. The estimator is consistent for the integrated variance and we present fea sible central limit theorems which show that it converges at the best attainable rate and has exc ellent eï¬ƒciency. Asymptotically, the quantile-based realised variance is immune to ï¬nite act ivity jumps and outliers in the price series, while in modiï¬ed form the estimator is applicable wi th market microstructure noise and therefore operational on high-frequency data. Simulation s show that it has superior robustness properties in ï¬nite sample, while an empirical application illustrates its use on equity data. Keywords: Finite activity jumps; Market mic...

â€¢ **Abstract Reference**:
> In this paper, we propose a new jump robust quantile-based re alised variance measure of ex-post return variation that can be computed using potenti ally noisy data. The estimator is consistent for the integrated variance and we present fea sible central limit theorems which show that it converges at the best attainable rate and has exc ellent eï¬ƒciency. Asymptotically, the quantile-based realised variance is immune to ï¬nite act ivity jumps and outliers in the price series, while in modiï¬ed form the estimator is applicable wi th market microstructure noise and therefore operational on high-frequency data. Simulation s show that it has superior robustness properties in ï¬nite sample, while an empirical application illustrates its use on equity data. Keywords: Finite activity jumps; Market microstructure noise; Orde r statistics; Outliers; Realised variance. JEL Classiï¬cation : C10; C80. âˆ— Podolskij gratefully acknowledges ï¬nancial support from CREATES funded by the Danish National Researc...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Resisting Manipulative Bots in Meme Coin Copy Trading: A Mul

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Resisting Manipulative Bots in Meme Coin Copy Trading: A Mul
â€¢ **æª”æ¡ˆ**ï¼š `2026_Resisting Manipulative Bots in Meme Coin Copy Trading: A Mul.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providin...

â€¢ **Abstract Reference**:
> Copy trading has become the dominant entry strategy in meme coin markets. However, due to the marketâ€™s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naÃ¯ve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for i...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> â€™s training data extends only up to October 2023, which predates all meme coin events evaluated in this study and thus precludes temporalinformation leakage. 0.0 0.2 0.4 0.6 0.8 1.0 Threshold 0.0 0.2 0.4 0.6 0.8 1.0Score Measure Precision F1 Model XGBoost NN Lasso MAS MAS (Zero Shot) Model XGBoost NN Lasso MAS MAS (Zero Shot) (a) Precision andð¹ 1 Scores. 0.0 0.2 0.4 0.6 0.8 1.0 False positive rate 0.0 0.2 0.4 0.6 0.8 1.0True positive rate Model XGBoost (AUC=0.7271) NN (AUC=0.7099) Lasso (AUC=0.6819) MAS (AUC=0.7164) MAS (Zero Shot) (AUC=0.6795) Model XGBoost (AUC=0.7271) NN (AUC=0.7099) Lasso (AUC=0.6819) MAS (AUC=0.7164) MAS (Zero Shot) (AUC=0.6795) (b) ROC Curves and AUC. Figure 7: Test set performance comparison between statisticdriven and MAS-based models. agent confidence scores and is evaluated on the test set. Detailed few-shot CoTs exemplars and prompts are provided in Â§A.5. To assess the efficacy of CoT reasoning, we additionally train a zero-shot MAS baseline with all other components held constant, as illustrated in Fig. 5b 5.2 Assessing Prediction Accuracy Both statistic-driven and MAS-based models output a continuous likelihood score in [0, 1] indicating whether a wallet is expected to be profitable. We obtain binaryTRUE/FALSE predictions by applying a decision threshold. Fig. 7 reports out-of-sample performance on the test set as the threshold varies, using Precision, ð¹1 score, and AUC as...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Second Thoughts: How 1-second subslots transform CEX-DEX Arb

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Second Thoughts: How 1-second subslots transform CEX-DEX Arb
â€¢ **æª”æ¡ˆ**ï¼š `2026_Second Thoughts: How 1-second subslots transform CEX-DEX Arb.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we study the difference between confirmation intervals: under Ethereumâ€™s default configuration, DEX transactions can be executed every 12 seconds at slot boundaries, while under faster execution regimes, transactions can be executed every second within subslots. Historical DEX prices are readily available and can be used for 12-second slot benchmark simulations. For a faster regime, we need a framework to interpolate these prices for 1-second subslots. Our framework includes three components. First, we start with a historical price for a current slot. It is also a price for the initial subslot. Next, we derive a pricep DEX(ti) of thei-th subslot from a pricep DEX(tiâˆ’1) of the (iâˆ’1)-th subslot. To do this, we apply (1) arbitrage transactions from the previous slot if present,...

â€¢ **Abstract Reference**:
> This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agentâ€™s DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereumâ€™s default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing. Keywords:MEV, DEX arbitrage, subslots, preconfirmations, market microstructur...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Technology Adoption and Network Externalities in Financial S

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Technology Adoption and Network Externalities in Financial S
â€¢ **æª”æ¡ˆ**ï¼š `2026_Technology Adoption and Network Externalities in Financial S.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> This paper develops a unified framework for analyzing technology adoption in financial networks that incorporates spatial spillovers, network externalities, and their interaction. The framework characterizes adoption dynamics through a master equation whose solution admits a Feynman-Kac representation as expected cumulative adoption pressure along stochastic paths through spatialnetwork space. From this representation, I derive the Adoption Amplification Factorâ€”a structural measure of technology leadership that captures the ratio of total system-wide adoption to initial adoption following a localized shock. A LÂ´ evy jump-diffusion extension with state-dependent jump intensity captures critical mass dynamics: below threshold, adoption evolves through gradual diffusion; above threshold, cascade dynamics accelerate adoption through discrete jumps. Applying the framework to SWIFT gpi adoption among 17 Global Systemically Important Banks, I find strong support for the two-regime characteriz...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequen

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequen
â€¢ **æª”æ¡ˆ**ï¼š `2026_Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequen.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the â€™shapeâ€™ of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k= 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a132.48%return compared to the-82.76%DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the â€™dead-zonesâ€™ being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latencyFPGA implementationvia High level Synthesis (HLS). The code for t...

â€¢ **Abstract Reference**:
> High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon ( k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the â€™shapeâ€™ of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k= 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a132.48%return compared to the-82.76%DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the â€™dead-zonesâ€™ being clearly visible in the splines. The T-KAN architecture is also u...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> 3.1 Data framework and Supervised Learning Setup The empirical validity of this study is based on theFI-2010 Benchmark Dataset[ 4], which provides a standardized high-frequency environment when evaluating LOB models. Although the raw 144-dimensional features come with Z-score normalization, transitioning from discrete LOB snapshots to a format for deep learning relies on specific temporal framing. 3.1.1 The Sliding Window Unit Adopting the standard supervised learning protocols from limit order books [ 7], we use aSliding Window Unit. Given the sequence of normalized states Ë†Lâˆž, . . . , Ë† NL, we construct an input sample Xt âˆˆR TÃ—144 where T= 10 represents the look-back horizon. This makes sure that the model captures the "order flow momentum" and liquidity path-dependency rather than just a static view book. 3.2 Architectural Specification Our implementation explores whether the marginal utility of spline-based functional activations is greater than that of traditional linear weights. 3.2.1 DeepLOB Baseline(CNN-LSTM) The DeepLOB architecture [8] serves as our spatial-temporal baseline. We use 1Ã—2 kernels to isolate bid-ask spreads ,followed by dual 4Ã—1 kernels to extract vertical microstructure depth. The output is permuted so that the feature maps correspond to the temporal axis before being processed by a 64-unit LSTM. 3 3.2.2 Proposed T-KAN Configuration The T-KAN architecture uses a dual-layer LSTM encoder (64 hidden units) to capture high-frequency dependencies. Based o...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> The results of the experiment validate the hypothesis that using Kolmogorov-Arnolod layers [6] rather than standard linear transformations enhance the extraction of alpha from high-frequency LOB data. By moving beyond the static activation functions of the DeepLOB baseline, the T-KAN architecture shows a superior ability to map the non-linear dynamics of market market structures. 5.1 Economic Viability and the Profitability-Capacity Trade-off The best evidence to support the T-KAN architecture is seen in the transaction-cost adjusted backtest. While the T-KAN model used a higher parameter count (104,451) as opposed to the DeepLOB baseline (58,211), this higher capacity directly translated into economic viability. Under a 1.0 bps transaction cost regime, the DeepLOB baseline was unable to overcome execution friction, causing a terminal return of-82.76 %. This was much unlike the T-KAN, which achieved a terminal return of132.48%. This suggests that T-KAN does not only achieve a higher statistical accuracy, but specifically identifies high-conviction liquidity imbalances that stay profitable even after accounting for market fees. This "profitability density" justifies that 79.4% increase in parameter count, as though the model successfully transitioned from theoretical predictor to a viable trading strategy. 6 5.2 Robustness to Alpha Decay A big problem faced in high-frequency trading is alpha decay: the rapid decay of information. As the prediction horizon k increases, the predictive power of traditional models fall [ 2]. As shown in Figure 5, T-KANâ€™s higher F1-score at k= 100 shows far higher "Alpha Persistence". Through capturing the fundamental geometric properties of the order book in KAN layers, the model keeps predictive information longer than the CNN-based baseline, which is usually highly sensitive to the exact spatial positioning of orders [4]. Figure 5: Alpha Decay Comparison: Information Coefficient (IC) vs. Forecast Horizon (k). T-KAN maintains higher predictive persistence over longer horizons compared to DeepLOB. 5.3 Industry Outlook: Interpretability and FPGA Implementation From an industry perspective T-KAN presents two main advantages. Firstly, the learned S-curve activations (figure 3), show an interpretable window into the decision making of the model, showing an autonomous filtering of â€™bid-ask bounceâ€™ noise. Secondly, the T-KAN architecture is uniquely suited for ultra-low latency hardware acceleration. Quite unlike the dense matrix mu

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Trade-R1: Bridging Verifiable Rewards to Stochastic Environm

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Trade-R1: Bridging Verifiable Rewards to Stochastic Environm
â€¢ **æª”æ¡ˆ**ï¼š `2026_Trade-R1: Bridging Verifiable Rewards to Stochastic Environm.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that ou...

â€¢ **Abstract Reference**:
> Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the marketâ€™s stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect S...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> In this section, we first formulate the stock selection problem as a conditional generation task in Section 3.1 and Section 3.2. To mitigate the hacking risk, we introduce the Fixed-effect Semantic Reward (FSR) and Dynamic-effect Semantic Reward (DSR) strategies in Section 3.3 and present the theoretical analysis in Section 3.4. In Section 3.5, we detail the verification method used in semantic alignment. 3.1 Dataset Construction To capture the multifaceted nature of financial markets, we propose a belief augmented data construction pipeline. The input context x serves as the grounding basis for reasoning and is composed of the following two components: Financial Information Summary.We aggregate financial information from three sources: â€¢ Financial News:News covering macroeconomic policies, corporate announcements, and industry specific regulations. â€¢ Market Data:Textualized representations of technical indicators (e.g., OHLCV trends) and capital flow data (e.g., smart money movements). â€¢ Qualitative Sentiment:Transcripts of public interviews with executives and expert market comments. Belief Augmentation.We augment each daily context with one of M= 15 distinct investment beliefs, where each belief is a descriptive prompt appended to the raw context to guide the model toward a specific analytical perspective (e.g., dividend detective, blue-chip quality analyst, sector rotation tracker). Mathematically, for a given day t with raw context Ct, we generate M di...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Trading Electrons: Predicting DART Spread Spikes in ISO Elec

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Trading Electrons: Predicting DART Spread Spikes in ISO Elec
â€¢ **æª”æ¡ˆ**ï¼š `2026_Trading Electrons: Predicting DART Spread Spikes in ISO Elec.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We study the problem of forecasting and optimally trading day-ahead versus realtime (DART) price spreads in U.S. wholesale electricity markets. Building on the framework of [15], we extend spike prediction from a single zone to a multi-zone setting and treat both positive and negative DART spikes within a unified statistical model. To translate directional signals into economically meaningful positions, we develop a structural and market-consistent price impact model based on day-ahead bid stacks. This yields closed-form expressions for the optimal vector of zonal INC/DEC quantities, capturing asymmetric buy/sell impacts and cross-zone congestion effects. When applied to NYISO, the resulting impact-aware strategy significantly improves the riskâ€“return profile relative to unit-size trading and highlights substantial heterogeneity across markets and seasons. 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> proceeds in three stages. First, we train in Section 3 zone-level classifiers to forecast extreme positive and negative DART events using historical load, price, and congestion features described in Section 2. Second, conditional on a directional signal, we estimate the expected DART spread and the local price impact implied by the observed day-ahead bid stack (Section 4.3). Finally, we solve a quadratic optimization problem that jointly determines the optimal vector of zonal virtual positions, explicitly accounting for asymmetric system-wide and local market impact (Section 4.2). This separation between signal generation and impact-aware sizing allows predictive accuracy and economic consistency to be evaluated independently. Overall, our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Trading with market resistance and concave price impact

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Trading with market resistance and concave price impact
â€¢ **æª”æ¡ˆ**ï¼š `2026_Trading with market resistance and concave price impact.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We consider an optimal trading problem under a market impact model with endogenous market resistance generated by a sophisticated trader who (partially) detects metaorders and trades against them to exploit price overreactions induced by the order flow. The model features a concave transient impact driven by a power-law propagator with a resistance term responding to the traderâ€™s rate via a fixed-point equation involving a general resistance function. We derive a (non)linear stochastic Fredholm equation as the first-order optimality condition satisfied by optimal trading strategies. Existence and uniqueness of the optimal control are established when the resistance function is linear, and an existence result is obtained when it is strictly convex using coercivity and weak lower semicontinuity of the associated profit-and-loss functional. We also propose an iterative scheme to solve the nonlinear stochastic Fredholm equation and prove an exponential convergence rate. Numerical experimen...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> features a concave transient impact driven by a power-law propagator with a resistance term responding to the traderâ€™s rate via a fixed-point equation involving a general resistance function. We derive a (non)linear stochastic Fredholm equation as the first-order optimality condition satisfied by optimal trading strategies. Existence and uniqueness of the optimal control are established when the resistance function is linear, and an existence...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Utility-Weighted Forecasting and Calibration for Risk-Adjust

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Utility-Weighted Forecasting and Calibration for Risk-Adjust
â€¢ **æª”æ¡ˆ**ï¼š `2026_Utility-Weighted Forecasting and Calibration for Risk-Adjust.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utili...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 WebCryptoAgent: Agentic Crypto Trading with Web Informatics

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 WebCryptoAgent: Agentic Crypto Trading with Web Informatics
â€¢ **æª”æ¡ˆ**ï¼š `2026_WebCryptoAgent: Agentic Crypto Trading with Web Informatics.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we proposeWebCryptoAgent, an end-to-end web-enabled crypto trading agent designed to perform autonomous trading, self-reflection, and adaptive risk management. Specifically, we design acontextual reflection modulethat leverages retrieved decision histories and environmental cues to iteratively refine policy reasoning. In parallel, we introduce ahierarchical risk management frameworkthat evaluates portfolio exposure, volatility dynamics, and model uncertainty to adjust position sizes and safeguard returns. Furthermore, we conduct comprehensive experiments across multiple benchmark datasets and real-world simulation environments, demonstrating that WebCryptoAgent consistently outperforms existing baselines in profitability, stability, and drawdown control. In summary, our mai...

â€¢ **Abstract Reference**:
> Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at subsecond timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WEBCRYPTOAGENT, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confiden...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> â€™s expected edge against cumulative frictional costs (liquidity-provider fee, impact, gas, spread, and MEV). Trades are executed only if the expected return exceeds the estimated cost margin. The overall end-to-end operation of WebCryptoAgent is summarized in Algorithm 3. 4 Experiment This section reports the empirical performance of four LLM-based trading agents on BTCUSDT, evaluated with and without memory. All...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> We presented WebCryptoAgent, a reflective agentic trading framework that integrates web-informed reasoning, contextual experience replay, and regime-aware risk control for short-horizon cryptocurrency trading. By decoupling strategic LLMbased reasoning from low-latency tactical protection, the proposed two-tier architecture enables robust decision making under extreme market volatility. Extensive experiments demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and achieves stronger riskadjusted performance compared to existing baselines. Beyond cryptocurrency markets, this work highlights the potential of reflective, memoryaugmented agents for high-frequency decisionmaking tasks in dynamic and uncertain environments. Limitation and Future Work While WebCryptoAgent demonstrates encouraging performance, several limitations remain. The framework currently relies on proprietary large language models for strategic reasoning, which may affect reproducibility across deployments. In addition, although the contextual reflection mechanism supports online adaptation without retraining, the replay buffer is updated using simple heuristics, and its long-term behavior warrants further study. Future work may explore alternative model choices, more principled reflection updates, and broader evaluation settings. We also expect that the two-tier reflective architecture could be applicable beyond cryptocurrency trading, though such extensions are left for future investigation.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 Who Restores the Peg? A Mean-Field Game Approach to Model St

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 Who Restores the Peg? A Mean-Field Game Approach to Model St
â€¢ **æª”æ¡ˆ**ï¼š `2026_Who Restores the Peg? A Mean-Field Game Approach to Model St.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `microstructure`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> â€”USDC and USDT are the dominant stablecoins pegged to $1 with a total market capitalization of over $300B and rising. Stablecoins make dollar value globally accessible with secure transfer and settlement. Yet in practice, these stablecoins experience periods of stress and de-pegging from their $1 target, posing significant systemic risks. The behavior of market participants during these stress events and the collective actions that either restore or break the peg are not well understood. This paper addresses the question:who restores the peg?We develop a dynamic, agent-based mean-field game framework for fiat-collateralized stablecoins, in which a large population of arbitrageurs and retail traders strategically interacts across explicit primary (mint/redeem) and secondary (exchange) markets during a de-peg episode. The key advantage of this equilibrium formulation is that it endogenously maps market frictions into a market-clearing price path and implied net order flows, allowing us t...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> A. Dynamic Mean-Field Game Framework A dynamic MFG models a system with a continuum of rational, non-atomic agents who interact via a â€œmean fieldâ€ [7]. In our model, as illustrated in Figure 2, this mean field represents the aggregate market state. In our setting, the meanfield state at timetisÂµ t = (mt, Lt, Ï•t, Ïˆt), wherem t is the stablecoin mispricing relative to the one-dollar peg,L t is the vector of primary backlogs across chains,Ï• t is the vector of aggregate secondary flows across venues, andÏˆ t is the vector of aggregate primary flows. Different components ofÂµ t enter different parts of the agent cost functions:m t drives inventory and pricing costs,L t andÏˆ t determine primary access costs, andÏ• t enters the secondary-market congestion term. The net order flow, in turn, determines market-wide variables like price slippage and execution costs. B. Economic Environment and Agent Types Our model consists of two main components: a multipopulation agent system and a multi-venue market microstructure. The market is populated by a continuum of agents, divided into two classes â€“ (1) Retail Traders: A fractionÏ€ R of the population, characterized by higher execution frictions such as slippage costsÎº R and inventory aversionÎ· R. They are assumed to trade exclusively in secondary markets. Their state is their inventoryq R,t and their control variable is their vector of secondary market flowsa R,t âˆˆR S. (2) Arbitrageurs: A fractionÏ€ A of the population, representing sophis...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> S ANDFUTUREWORK We develop a dynamic Mean-Field Game framework for fiat-collateralized stablecoins, linking market microstructure, agent incentives, and peg dynamics within a single equilibrium model. Calibrations to three major episodes reproduce key features of observed price paths and half-lives, and allow us to isolate which channel restores the peg under different shocks â€“ directly addressing the central question ofâ€œwho restores the peg?â€in practice. When primary redemption rails remain open, recovery is dominated by direct redemptions. When primary capacity is impaired but not broken, recovery reflects a joint contribution from primary arbitrage and secondary buying. When venue-level infrastructure failures both obstruct redemptions and fragment liquidity, both channels contribute little, and recovery is slow. A central structural result is a non-linear threshold in primary-market friction. De-peg half-life remains short when primary execution costs stay within a functional range, but rises sharply once primary access becomes sufficiently expensive, beyond which even favorable secondary-market liquidity cannot restore rapid re-pegs. This converts the model into an operational design and risk-management criterion: peg robustness depends not only on reserve quality, but also on the accessibility, speed, fees, and operational resilience of mint and redeem rails, while secondary-market liquidity primarily acts as a complement that limits propagation of localized outages. An oversimplification is that we work with two representative populations (retail and arbitrageurs) in a tractable LQ equilibrium setting. This abstraction simplifies heterogeneity among liquidity providers and can understate non-linear behavior under extreme stress, including binding constraints and discontinuous primary-rail access. It also treats volatility and liquidity as regime inputs and calibrates primarily to price paths and half-lives, rather than jointly matching flows and order book dynamics. Building on this, future work will incorporate additional agent types and venue-specific constraints, move beyond the LQ specification using learning-based MFG solvers with richer risk objectives and occasionally binding constraints, endogenize liquidity and volatility feedback within the mean field, and validate the model by jointly fitting onchain flows with high-frequency order book data.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2020 Model-free conventions in multi-agent reinforcement learning

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2020 Model-free conventions in multi-agent reinforcement learning
â€¢ **æª”æ¡ˆ**ï¼š `2020_Model-free conventions in multi-agent reinforcement learning.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we consider conï¬‚ictual coordination settings, i.e. whereÃ‘ ð‘”2G ðºð‘” =;; individuals who do not share the same reward proï¬le prefer diï¬€erent equilibria to one another. For simplicity, we refer to color-based â€˜groupsâ€™ of players that gain higher reward for consuming the same colored berry and therefore share a group objective. For example, all players that gain the highest reward for consuming red berries are in the â€˜red groupâ€™. Any of these equilibria result in one group proï¬ting more from the achieved consensus than the others, causing tension between the groups. Individual learning behavior Each agent learns, independently through its own experience of the environment, a behavior policyðœ‹ð‘– : Oð‘–! Î”Â¹A ð‘–Âº (written ðœ‹Â¹ð‘Žð‘–jð‘œð‘–Âº) based on its own observationð‘œð‘– =OÂ¹ ð‘ Â• ð‘–Âº and rewardð‘Ÿ ð‘–Â¹ð‘ Â•...

â€¢ **Abstract Reference**:
> gamespace does not pose an exploration problem. 3 Model-free conventions in multi-agent reinforcement learning with heterogeneous preferences The premises on which our model is based are: 1. Large world with incomplete information. 2. There is more than one Nash equilibrium. 3. Individuals have heterogeneous tastes. 4. Attempting to coordinate bears an opportunity cost. 5. Individuals learn by retrospective caching of long-run values which they use greedily for decisionmaking. From these premises we show that MF convention formation is indeed possible in our formulation. In accord with the literature on critical mass theory (Marwell and Oliver, 1993), simulations of our model show that this group achievement is opposed by start-up and free rider problems. This explicit tension results from collective action requiring coordination while more directly self-interested action does not. It is akin to the mixed motives professional athletes, scientists or politicians face when part of a team...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> we propose, individuals learn to implement policies that sequence elementary actions over time and space. This is in line with other recent work where multi-agent reinforcement learning has been used to develop mechanistic models that extend to the case of spatio-temporal complexity (Eccles et al., 2019; Jaques et al., 2019; KÃ¶ster et al., 2020; Lerer and Peysakhovich, 2019; McKee et al., 2020; Perolat et al., 2017; Peysakhovich, 2019; Peysakhovich et al., 2019; Shum et al., 2019; Zheng et al., 2020)3. 2At least up to computation constraints. To go further, MB control relies on â€œirrationalâ€ heuristics (Huys et al., 2015). In the particular case of Schellingâ€™s example, clearly some mutual knowledge of the salience of the particular solution, grand central station, was required. In this case, all the participants in Schellingâ€™s...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2022 Financial Vision Based Reinforcement Learning Trading Strate

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2022 Financial Vision Based Reinforcement Learning Trading Strate
â€¢ **æª”æ¡ˆ**ï¼š `2022_Financial Vision Based Reinforcement Learning Trading Strate.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a ï¬nancial vision ï¬eld, which can understand the critical components of a candle, and what they indicate, to apply candlestick chart analysis to a trading strategy. We combine deep reinforcement learning to realize intuitive trading based on a ï¬nancial vision to surveillance candlestick. We involve observing a large number of the candlestick, forming automatic responses to various pattern recognition. With these extraordinary capabilities in automatic control, it is natural to consider R.L. techniques in algorithmic trading. Indeed, several works have tried to apply R.L. to trade ï¬nancial assets automatically. One of the challenges to constructing an effective RL-based algorithmic trading system is to properly encode the input signal for the agent to make decisions. With the rec...

â€¢ **Abstract Reference**:
> Recent advances in artiï¬cial intelligence (AI) for quantitative trading have led to its general superhuman performance in signiï¬cant trading performance. However, the potential risk of AI trading is a â€œblack boxâ€ decision. Some AI computing mechanisms are complex and challenging to understand. If we use AI without proper supervision, AI may lead to wrong choices and make huge losses. Hence, we need to ask about the AI â€œblack boxâ€, including why did AI decide to do this or not? Why can people trust AI or not? How can people ï¬x their mistakes? These problems also highlight the challenges that AI technology can explain in the trading ï¬eld. Keywords Financial VisionÂ¨ Explainable AI (XAI)Â¨ Convolutional Neural Networks (CNN)Â¨ Gramian Angular Field (GAF)Â¨ CandlestickÂ¨ Convolutional Neural Network (CNN)Â¨ Patterns RecognitionÂ¨ Proximal Policy Optimization (PPO)Â¨ Reinforcement Deep Reinforcement Learning (Deep RL)Â¨ Transfer Learning 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> itself. Our study designs an innovative explainable A.I. trading framework by combing ï¬nancial vision with deep reinforcement learning. Hence, we propose a ï¬nancial vision ï¬eld, which can understand the critical components of a candle, and what they indicate, to apply candlestick chart analysis to a trading strategy. We combine deep reinforcement learning to realize intuitive trading based on a ï¬nancial vision to surveillance candlestick. We involve observing a large number of the candlestick, forming automatic responses to various pattern recognition. With these extraordinary capabilities in automatic control, it is natural to consider R.L. techniques in algorithmic trading. Indeed, several works have tried to apply R.L. to trade ï¬nancial assets automatically. One of the challenges to constructing an effective RL-based algorithmic trading system is to properly encode the input signal for the agent to make decisions. With the recent advances in convolutional neural networks (CNN), a potential scheme encodes ï¬nancial time series into images. This work proposes an algorithmic trading framework based on deep reinforcement learning and the G.A.F. encoding method. Our contributions are the following: â€¢ Provide an algorithmic trading framework for the study of RL-based strategies. â€¢ Demonstrate successful R.L. trading agents with G.A.F. encoded inputs of price data and technical indicators. The difference between A.I. trading is that A.I. is a target environment (such as the Sta...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> For the current wise investment, investors want to predict the future transaction price or ups and downs directly. The fatal assumption is that the training data set is consistent with the data distribution that has not occurred in the future. However, the natural world will not let us know whether the subsequent data distribution will change. Because of this, even if researchers add a moving window to the training process, it is inevitable that â€machine learning obstacles-prediction delayâ€ will occur. Just search for â€machine learning predict stock price,â€ and researchers can ï¬nd articles full of pits, all of which have this shortcoming. Therefore, our ï¬rst contribution is not to make future predictions but to focus on the current â€candlesticks pattern detection,â€ such as Engulï¬ng Pattern, Morning Star,. . . . However, the â€candlesticks patternâ€ is usually a sensational description. It cannot become a stylized trading strategy if investors cannot write a program to enumerate all the characteristics. Even if a trader has a sense of the market 11 Financial Vision Based Reinforcement Learning Trading Strategy A PREPRINT and knows which patterns have to enter and exit, he cannot keep his eyes on all the investment targets. Moreover, our second contribution focuses on detecting trading entry and exit signals combined with related investment strategies. Finally, we found from experiments that the 15-minute price data of Ethereum train through transfer learning is suitable for US stock trading. Compared to the top ten most popular ETFs, the experimental results demonstrate superior performance. This study focuses on ï¬nancial vision, Explainable methods, and links to their programming implementations. We hope that our paper will reference superhuman performances and why the decisions are in the trading system. Appendix The eight patterns used in this study will describe entirely in this section. The following eight ï¬gures illustrate the critical rules each pattern requires. The white candlestick represents a rising price on the left-hand side of each ï¬gure, and the black candlestick represents a dropping price. The arrow indicates the trend. The upward arrow indicates a positive direction, and the downward arrow indicates a negative trend. The text descriptions on the right-hand side are the fundamental rules referred to from The Major Candlestick Signals. [15]. Evening Star Pattern 1. The uptrend has been apparent. 2. The body of the first candle is white,

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2022 Understanding intra-day price formation process by agent-bas

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2022 Understanding intra-day price formation process by agent-bas
â€¢ **æª”æ¡ˆ**ï¼š `2022_Understanding intra-day price formation process by agent-bas.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper, one agent per category is shown to be sufï¬cient to capture the intra-day price formation process. The proposed XGB-Chiarella approach provides insights that the price formation process is comprised of the interactions between momentum traders, fundamental traders, and noise traders. It can also be used to enhance risk management by practitioners. Keywords Agent-based ModelÂ· Financial Market SimulatorÂ· Extended Chiarella ModelÂ· Price Formation 1 Introduction 1.1 Motivation In the past decade algorithmic trading has grown rapidly across the world and has become the dominant way securities are traded in ï¬nancial markets, currently generating more than half of the volume of U.S. equity markets. Constantly improving computer technology and its application by both traders and exch...

â€¢ **Abstract Reference**:
> This article presents XGB-Chiarella, a powerful new approach for deploying agent-based models to generate realistic intra-day artiï¬cial ï¬nancial price data. This approach is based on agent-based models, calibrated by XGBoost machine learning surrogate. Following the Extended Chiarella model, three types of trading agents are introduced in this agent-based model: fundamental traders, momentum traders, and noise traders. In particular, XGB-Chiarella focuses on conï¬guring the simulation to accurately reï¬‚ect real market behaviours. Instead of using the original ExpectationMaximisation algorithm for parameter estimation, the agent-based Extended Chiarella model is calibrated using XGBoost machine learning surrogate. It is shown that the machine learning surrogate learned in the proposed method is an accurate proxy of the true agent-based market simulation. The proposed calibration method is superior to the original Expectation-Maximisation parameter estimation in terms of the distance betwe...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is capable of generating realistic price time series in various stocks listed at three different exchanges, which indicates the universality of intra-day price formation process. For the time scale (minutes) chosen in this paper, one agent per category is shown to be sufï¬cient to capture the intra-day price formation process. The proposed XGB-Chiarella approach provides insights that the price formation process is comprised of the interactions between momentum traders, fundamental traders, and noise traders. It can also be used to enhance risk management by practitioners. Keywords Agent-based ModelÂ· Financial Market SimulatorÂ· Extended Chiarella ModelÂ· Price Formation 1 Introduction 1.1 Motivation In the past decade algorithmic trading has grown rapidly across the world and has become the dominant way securities are traded in ï¬nancial markets, currently generating more than half of the volume of U.S. equity markets. Constantly improving computer technology and its application by both traders and exchanges, together with the evolution of market micro-structure, automation of price quotation and trade execution have together enabled faster trading. Consequently, intra-day price formation underpinning this trading process has become the focus of intense research attention in recent years as market participants attempt to gain greater insight into how prices are determined and hence improve trading performance. Price formation determines the price of an asset through interaction...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> and Future Work 5.1 Summary of Achievements In this paper, a new approach called XGB-Chiarella is proposed to generate realistic intra-day artiï¬cial ï¬nancial price data in order to provide insight into the intra-day price formation process. To the best of our knowledge, this is the ï¬rst extension of the Chiarella model to generate minute-level intra-day ï¬nancial market simulation. The approach utilises agent-based modelling techniques. The underlying simulation model has only three agents: one for fundamental trader, one for momentum trader, and one for noise trader. The model is simulated and model parameters are calibrated by an XGBoost machine learning surrogate. The proposed methodology is tested on 75 stocks from three exchanges: NASDAQ, LSEG, and HKEX. In terms of stylised facts distance, the proposed XGB-Chiarella method is able to generate more realistic ï¬nancial market simulations than the original Expectation-Maximisation estimation algorithm. This is true in nearly all the stocks from three exchanges. Despite the fact that the methodology is based on a model with only three agents, the XGB-Chiarella methodology successfully generates very realistic ï¬nancial market simulations. This indicates that one agent per category seems to be sufï¬cient to capture the intra-day price formation process for the time scale (minutes) chosen in this paper. The very simple model structure not only accelerates the simulation process in terms of computational cost, but also enable us to scrutinize the intra-day price formation process, such as the trend and value effects. The results provide support for the existence of a universal intra-day price formation mechanism. The realistic simulated intra-day ï¬nancial market indicates that trend and value effects, as well as noise trading, are indispensable to the intra-day price formation process. We also show that in the process of calibration, the XGBoost surrogate model is an accurate approximation of the true model. At the predicted optimal point for each stock on each trading day, the surrogate model prediction error is mostly smaller than 10%. The machine learning surrogate is capable of intelligently directing the exploration of model parameter space. The exploitation-exploration mechanism is also introduced in the model calibration process. A practical application of the proposed methodology is also presented. 17 AUGUST 31, 2022 5.2 Future Work This work can be extended in several aspects. Firstly, in modern ï¬nanc

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Reinforcement Learning in Agent-Based Market Simulation: Unv

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Reinforcement Learning in Agent-Based Market Simulation: Unv
â€¢ **æª”æ¡ˆ**ï¼š `2024_Reinforcement Learning in Agent-Based Market Simulation: Unv.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a simulation framework with only a small group of representative RL agents. We compare this system with one composed of rule-based zero-intelligence agents as well as with real market data. The results obtained using the RL agentsâ€™ system are comparable with real data. Further, we show that the system is capable of adapting to changing market conditions. 2 Important Concepts 2.1 Reinforcement Learning Agents Mathematically, each RL agent solves a problem associated with a Markov Decision Process (MDP) 1. A MDP is defined as a tuple (S, A, R, P, Î³) with several key components: â€¢ S is the state space, in our case a set of vectors describing the market limit order book and the agentâ€™s account information, â€¢ A is the action space which defines the specific orders agents can place. â€¢...

â€¢ **Abstract Reference**:
> Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, particularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based simulation framework employing reinforcement learning (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the simulation, offering insights into their response to significant market events. 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Agent-Based Simulation of a Financial Market with Large Lang

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Agent-Based Simulation of a Financial Market with Large Lang
â€¢ **æª”æ¡ˆ**ï¼š `2025_Agent-Based Simulation of a Financial Market with Large Lang.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgentsâ€™ behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior. Keywords:LLMsÂ·Financial market simulationsÂ·Behavioral biases 1 Introduction An agent-based market simulation is an effective tool for modeling macro-scale financial phen...

â€¢ **Abstract Reference**:
> .In real-world stock markets, certain chart patternsâ€”such as price declines near historical highsâ€”cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> : FCLAgent We propose a novel agent model that distills and refines the human-like behavioral characteristics captured by LLMs, incorporating context-dependent loss aversion into trading intention while delegating order price determination to traditional rule-based mechanisms. This design enables realistic simulations of behavioral biases while circumventing the well-known numerical reasoning limitations of LLMs [32]. In this section, we first desxcribe the structure of our simulation framework and then introduce the agent models employed: the FCNAgent [6,7] and the proposed FCLAgent, which incorporates LLM-derived trading intentions into the order decision rules based on the FCNAgent. 3.1 Simulation Structure In this work, we assumena âˆˆNagents operate in a single market setting, with each simulation consists ofTsim âˆˆNtime steps. Fig. 2 illustrates the structure of our simulation settings. At each time steptâˆˆ {1, ..., Tsim}, a randomly selected Agent-Based Simulation of a Financial Market with Large Language Models 7 uniform distribution from range[cmin, cmax].Î± j andÏ„ j are set as: âˆ€j Î± j =Î± Î±dif f +w j,f Î±dif f +w j,c , Ï„ j =  Ï„ Ï„ dif f +w j,f Ï„ dif f +w j,c  (4) whereÎ±, Ï„âˆˆR + are the reference levels ofÎ±j andÏ„ j, andÎ± dif f, Ï„ dif f âˆˆR + are external parameters that control the range of variation inÎ±j andÏ„ j. Theagentdecidesp j t andv j t tomaximizetheexpectedutilityE t[U j t+Ï„ j],where Et[Â·]denotes the expected mean given the information available at timet. 3.3...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Quantum Reinforcement Learning Trading Agent for Sector Rota

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Quantum Reinforcement Learning Trading Agent for Sector Rota
â€¢ **æª”æ¡ˆ**ï¼š `2025_Quantum Reinforcement Learning Trading Agent for Sector Rota.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a hybrid quantum-classical reinforcement learning framework for sector rotation in the Taiwan stock market. Our system employs Proximal Policy Optimization (PPO) as the backbone algorithm and integrates both classical architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV , QASA) as policy and value networks. An automated feature engineering pipeline extracts financial indicators from capital share data to ensure consistent model input across all configurations. Empirical backtesting reveals a key finding: although quantum-enhanced models consistently achieve higher training rewards, they underperform classical models in real-world investment metrics such as cumulative return and Sharpe ratio. This discrepancy highlights a core challenge in applying rein...

â€¢ **Abstract Reference**:
> â€”We propose a hybrid quantum-classical reinforcement learning framework for sector rotation in the Taiwan stock market. Our system employs Proximal Policy Optimization (PPO) as the backbone algorithm and integrates both classical architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV , QASA) as policy and value networks. An automated feature engineering pipeline extracts financial indicators from capital share data to ensure consistent model input across all configurations. Empirical backtesting reveals a key finding: although quantum-enhanced models consistently achieve higher training rewards, they underperform classical models in real-world investment metrics such as cumulative return and Sharpe ratio. This discrepancy highlights a core challenge in applying reinforcement learning to financial domainsâ€”namely, the mismatch between proxy reward signals and true investment objectives. Our analysis suggests that current reward designs may incentivize overfitting to sh...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> A. Problem Formulation We formulate the sector rotation problem as a sequential decision-making task in which an agent selects an allocation across multiple industry sectors to maximize long-term investment return. At each discrete time stept, the agent observes a financial state vectors t, derived from engineered features of sector-level market data. Based on this state, the agent chooses an actiona t, representing the target portfolio allocation across sectors. After executing the action, the environment transitions to a new states t+1 and returns a scalar rewardr t based on the investment outcome. The goal is to learn a policyÏ€(a t|st) that maximizes the expected cumulative discounted reward E[P t Î³trt]. B. Framework Overview We adopt a hybrid quantum-classical reinforcement learning framework based on the Proximal Policy Optimization (PPO) algorithm. PPO is a widely used policy-gradient method known for its sample efficiency and training stability. It optimizes a clipped surrogate objective to prevent large policy updates and incorporates entropy regularization to encourage exploration. Our framework supports interchangeable policy and value network backbones, allowing both classical and quantum-enhanced models to be evaluated under consistent training protocols. C. Policy Network Architectures 1) Classical Baselines:We implement two classical neural network architectures as baselines: â€¢LSTM [9]:The Long Short-Term Memory (LSTM) network is a type of recurrent neural ne...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> A. Reward-Performance Misalignment One of the most significant findings in this study is the observed disconnect between final training rewards and realworld investment performance. Quantum-enhanced models, particularly QNN and QASA, achieved the highest cumulative rewards during training. However, these models underperformed classical architectures in backtested metrics such as cumulative return and Sharpe ratio. This gap arises from a fundamental issue in financial reinforcement learning: the use ofproxy rewardsthat do not fully capture the true objectives of investment performance. In our environment, the agent is rewarded based on whether its selected sector appears in the top-Nranked sectors by future capital share. While this proxy is useful for encouraging short-term predictive accuracy, it fails to directly incentivize risk-adjusted returns, low volatility, or drawdown minimization. As a result, quantum models may learn highly specialized strategies that maximize the proxy reward due to their expressive capacity and susceptibility to overfitting but fail to generalize to broader investment objectives. Classical models, on the other hand, appear to implicitly regularize toward smoother, more stable allocation policies, which align better with investor goals. B. Broader Implications The reward-performance gap we observed is not limited to quantum models, it is a known challenge in deep reinforcement learning for finance. However, the fact that quantum models magnify this gap highlights the urgent need to rethink how reward functions and optimization landscapes interact under quantum regimes. Future research should explore whether quantum models can not only match but exceed classical performance in scenarios where the reward signal better reflects long-term investment goals.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Right Place, Right Time: Market Simulation-based RL for Exec

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Right Place, Right Time: Market Simulation-based RL for Exec
â€¢ **æª”æ¡ˆ**ï¼š `2025_Right Place, Right Time: Market Simulation-based RL for Exec.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper. By using an RL agent to find optimal parameters for a known trading algorithm template, we ensure that the output is sufficiently interpretable. This interpretability is key because the EU AI Act of 2024 dictates that, if using a high-risk AI system (such as is typically used in trading decisions), any person subject to the output of the AI has the right to clear and meaningful explanations of the role of the AI system [8]. In this study, we first compare RL algorithms that consider different distribution of orders, such as a uni-modal or bi-modal distribution. We also consider how the RL algorithms perform using different metrics of market costs, such as slippage, risk, and impact, which we can directly calculate using our market simulator. Secondly, we show how our r...

â€¢ **Abstract Reference**:
> Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agentâ€™s performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trade...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> In this section, we outline the details of our agent-based market simulator and our RL trading agent. We explain how we employ RL to optimise an execution algorithm using a realistic market simulator. Finally, we generate the Almgren-Chriss efficient frontier using our market simulator to explore the efficiency of the RL devised strategy. 3.1 Market Simulator All tests are run using the Simudyne Market Simulator . It provides a realistic market environment that generates statistically accurate market behaviour. Our market simulator has three main components: theexchange module which matches the protocols and behaviour of a specified exchange, theagentswho have certain behavioural characteristics, and thecalibrationmodule which tunes the parameters of the overall simulator to replicate behaviour of a specific asset on a specific historical date. The simulator recreates the statistical properties of the market using a network of trader agents who are connected to the exchange. At each step, trader agents send messages to submit limit or market orders and amend or delete queued limit orders. The Exchange then sends execution reports back to the Trader as well as level-2 market data. 3.1.1 The Exchange.The exchange determines the protocol by which the limit orders are queued, matched and priced. The exchangeâ€™s matching engine will attempt to match each order sequentially. When a sell limit order is less or equal to a buy limit order, a trade occurs and is priced at the e...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> that even with relatively simple loss functions, RL can produce near-optimal execution strategies when appropriately guided by objectives and simulators that reflect market realities. 5 Conclusion In this work, we have shown that reinforcement learning (RL) can be effectively applied to the problem of optimal trade execution. Our RL agent, trained in a realistic agent-based simulation, produced execution strategies that outperform traditional baselines such as TWAP and VWAP in terms of slippage and proximity to the efficient frontier. When benchmarked against the Almgren-Chriss efficient frontier, the RL-derived strategies consistently lie close to the frontier. This suggests the agent is capable of learning strategies that efficiently balance risk and impact. The bi-modal unbounded distribution trained under the slippage objective produced the most efficient execution strategy by using its two modes to optimise impact and risk separately. The uni-modal distribution trained with the slippage function produces an execution strategy which can produce optimal VaR at a 95% confidence level. One area for future work is to incorporate a contextual bandit framework. This is where the RL agent learns a policy that maps context to an action to maximise a reward [3, 5]. An agent trained in this manner could update its beliefs during execution, responding to the risk and impact of the execution in real time. Another area which may be interesting to explore is using the efficient frontier as the loss function. The distance to the frontier could be used as the loss function, which would align learning objectives more closely with economic theory [12]. Overall, this study positions RL not just as a viable tool, but as a compelling alternative to traditional algorithmic execution methods in financial markets. We expect extensions, set out above, to further justify the use of RL for optimising market execution, especially where considering correlated market movements and execution across different market regimes. 6 Significance This work demonstrates the potential of RL agents in discovering efficient execution strategies under realistic market conditions. By leveraging a high-fidelity simulation environment, we show that RL agents can learn policies that balance market impact and risk more effectively than traditional strategies such as TWAP and VWAP. Our approach highlights two key insights. First, RL agents can adaptively learn optimal trade timing by respondi

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2026 History Is Not Enough: An Adaptive Dataflow System for Finan

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2026 History Is Not Enough: An Adaptive Dataflow System for Finan
â€¢ **æª”æ¡ˆ**ï¼š `2026_History Is Not Enough: An Adaptive Dataflow System for Finan.pdf`
â€¢ **å¹´ä»½**ï¼š 2026
â€¢ **é¡žåˆ¥**ï¼š `rl_trading`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose the workflow shown in Fig. 2. The data manipulation moduleMgenerates augmented training samplesËœx train with operation choosing probability matrixpand manipulation strength parameterÎ». To adaptively controlM, we introduce a trainable plannerg((f Î¸, xi);Ï•), which learns the policyÏ€ Ï•(p, Î»|f, xi)while the scheduler determines the proportion of data to be manipulatedÎ±using a heuristic algorithm. Additionally, we interleave taskmodel updates with planner updates on validation feedback, while provenance hooks (policy, probabilities, manipulation strengths, proportion of data to be manipulated) are persisted to enable exact replay. In order to optimally control the data manipulation module Mfor each training samplex train, inspired by AdaAug and MADAug, we formulate the learning o...

â€¢ **Abstract Reference**:
> â€”In quantitative finance, the gap between training and real-world performanceâ€”driven by concept drift and distributional non-stationarityâ€”remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra â€œHistory Is Not Enoughâ€ underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learningâ€“based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive plannerâ€“scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framewo...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ,â€arXiv preprint arXiv:2303.00080, 2023. [5] E. Samanidou, E. Zschischang, D. Stauffer, and T. Lux, â€œAgent-based models of financial markets,â€Reports on Progress in Physics, vol. 70, no. 3, p. 409, 2007. [6] H. Xia, S. Sun, X. Wang, and B. An, â€œMarket-gan: Adding control to financial market data generation with semantic context,â€ inProceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 14, 2024, pp. 15 996â€“16 004. [7] S. C. Hora, â€œAleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management,â€Reliability Engineering & System Safety, vol. 54, no. 2-3, pp. 217â€“223, 1996. [8] S. Kapoor, W. J. Maddox, P. Izmailov, and A. G. Wilson, â€œOn uncertainty, tempering, and data augmentation in bayesian classification,â€Advances in Neural Information Processing Systems, vol. 35, pp. 18 211â€“ 18 225, 2022. [9] W. Li, X. Yang, W. Liu, Y . Xia, and J. Bian, â€œDdg-da: Data distribution generation for predictable concept drift adaptation,â€ inProceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 4, 2022, pp. 4092â€“4100. [10] T.-H. Cheung and D.-Y . Yeung, â€œAdaaug: Learning class-and instanceadaptive data augmentation policies,â€ inInternational Conference on Learning Representations, 2021. [11] C. Hou, J. Zhang, and T. Zhou, â€œWhen to learn what: Model-adaptive data augmentation curriculum,â€ inProceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1717â€“1728. [12] B. U. Demire...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> In this paper, we introduced a novel adaptive dataflow system designed to bridge the gap between training and realFig. 8: The training and validation loss curve w/wo our workflow applied. world performance in quantitative finance. To the best of our knowledge, this is the first workflow of its kind applied to quantitative finance tasks. The framework integrates a parameterized data manipulation module with a learning-guided plannerâ€“scheduler, forming a feedback loop that dynamically regulates manipulation strength and proportion of data to be manipulated as the model evolves. This design allows the data pipeline to self-adjust to distributional drift, ensuring consistent data quality and realistic synthesis throughout the learning process. Experiments on forecasting and trading tasks demonstrate that the system improves robustness and generalization across models and markets.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 A hidden Markov model for statistical arbitrage in internati

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 A hidden Markov model for statistical arbitrage in internati
â€¢ **æª”æ¡ˆ**ï¼š `2023_A hidden Markov model for statistical arbitrage in internati.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a mean-reverting stochastic model for the cointegration spread that allows for regime switching by means of a hidden Markov chain determining the parameters of the process. We apply filtering techniques to dynamically estimate the most likely regime and the model parameters. Finally, we empirically analyze different statistical arbitrage strategies involving three crude oil futures contracts. Our results indicate that strategies that include a newly introduced security, like the Shanghai crude oil futures, along with traditional and well-established ones, like the Brent and the WTI, are remarkably profitable and robust. The paper is structured as follows. Section 2 describes the cointegration structure and the stochastic model for the cointegration spread. Section 3 describes th...

â€¢ **Abstract Reference**:
> . We study statistical arbitrage strategies in international crude oil futures markets. We analyse strategies that extend classical pairs trading strategies, considering two benchmark crude oil futures (Brent and WTI) together with the recently introduced Shanghai crude oil futures. We show that the time series of these three futures prices are cointegrated and we introduce a mean-reverting regime-switching process modulated by a hidden Markov chain to model the cointegration spread. By relying on this model and applying online filter-based parameter estimators, we implement and test several statistical arbitrage strategies. Our analysis reveals that statistical arbitrage strategies involving the recently introduced Shanghai futures are profitable even under conservative levels of transaction costs and over different time periods. Statistical arbitrage strategies involving only two of these three futures contracts or the three traditional crude oil futures (Brent, WTI, Dubai) deliver a...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> for Modelling the Dynamics of Statistical Arbitrage. PhD thesis, London Business School. 24 V. FANELLI, C. FONTANA, AND F. ROTONDI [Caporin et al., 2019] Caporin, M., Fontini, F., and Talebbeydokhti, E. (2019). Testing persistence of WTI and Brent long-run relationship after the shale oil supply shock. Energy Economics, 79:21â€“31. [Cerqueti and Fanelli, 2021] Cerqueti, R. and Fanelli, V. (2021). Long memory and crude oilâ€™s price predictability. Annals of Operations Research, 299:895â€“906. [Cerqueti et al., 2019] Cerqueti, R., Fanelli, V., and Rotundo, G. (2019). Long run analysis of crude oil portfolios. Energy Economics, 79:183â€“205. [Cotter et al., 2022] Cotter, J., Eyiah-Donkor, E., and Pot` Ä±, V. (2022). Commodity futures return predictability and intertemporal asset pricing. Journal of Commodity Markets , page 100289. [CrÂ´ epelli` ere et al., 2023] CrÂ´ epelli` ere, T., Pelster, M., and Zeisberger, S. (2023). Arbitrage in the market for cryptocurrencies. Journal of Financial Markets , 64:100817. [Cummins and Bucca, 2012] Cummins, M. and Bucca, A. (2012). Quantitative spread trading on crude oil and refined products markets. Quantitative Finance, 12(12):1857â€“1875. [Do and Faff, 2010] Do, B. and Faff, R. (2010). Does simple pairs trading still work? Financial Analysts Journal, 66(4):83â€“95. [Do and Faff, 2012] Do, B. and Faff, R. (2012). Are pairs trading profits robust to trading costs? Journal of Financial Research, 35(2):261â€“287. [Dunis et al., 2006] Dunis, C., Laws, J.,...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 C++ Design Patterns for Low-latency Applications Including H

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 C++ Design Patterns for Low-latency Applications Including H
â€¢ **æª”æ¡ˆ**ï¼š `2023_C++ Design Patterns for Low-latency Applications Including H.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> This work aims to bridge the existing knowledge gap in the optimisation of latency-critical code, specifically focusing on high-frequency trading (HFT) systems. The research culminates in three main contributions: the creation of a Low-Latency Programming Repository, the optimisation of a market-neutral statistical arbitrage pairs trading strategy, and the implementation of the Disruptor pattern in C++. The repository serves as a practical guide and is enriched with rigorous statistical benchmarking, while the trading strategy optimisation led to substantial improvements in speed and profitability. The Disruptor pattern showcased significant performance enhancement over traditional queuing methods. Evaluation metrics include speed, cache utilisation, and statistical significance, among others. Techniques like Cache Warming and Constexpr showed the most significant gains in latency reduction. Future directions involve expanding the repository, testing the optimised trading algorithm in ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , and risk management aspects. It also delves into the critical concept of cointegration, presents the algorithm through a case study involving Goldman Sachs and Morgan Stanley, and discusses CPU-level optimisations for reducing latency. Section 5 details the development and testing of a C++ implementation of the LMAX Disruptor, a high-performance, lock-free inter-thread communication library. This library significantly outperforms traditional queuing methods in both latency and speed by leveraging a ring bu ffer, sequence numbers, and a specialized waiting strategy. Section 6 evaluates the performance of the Low-Latency Programming Repository, the trading algorithm, and the Disruptor pattern through multiple metrics. These include user feedback, speed improvements, and statistical significance. The section highlights areas for improvement and discusses the impact of la2 C++ design patterns for low-latency applications A Preprint tency reduction on trading profitability and risk. Section 7 concludes this work and highlights potential future directions. 1.4 Contributions This work offers both academic and practical contributions to the fields of low-latency programming and high-frequency trading (HFT). The research furnishes comprehensive insights into low-latency programming techniques, as well as the strategies prevalent in the high-frequency trading industry. â€¢ Low-Latency Programming Repository: This repository serves as more than just a theoretical compendium; it...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Decentralised Finance and Automated Market Making: Execution

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Decentralised Finance and Automated Market Making: Execution
â€¢ **æª”æ¡ˆ**ï¼š `2023_Decentralised Finance and Automated Market Making: Execution.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we solve the problem of an LT who trades in a CPM to execute a large position in an asset or to execute statistical arbitrages between the CEX and the DEX.1 We formulate the trading problem as a stochastic control problem in continuous time where the LT controls the speed at which she sends liquidity taking orders. Key to the performance of the LTâ€™s strategies is to balance exchange rate risk and execution costs. In our model, we use the first-order approximation of the curvature of the trading function to compute execution costs. This approximation is referred to as convexity costs and we use Uniswap data to show that convexity costs are an accurate estimate of the execution costs studied in Engel and Herlihy (2021b) and in Angeris et al. (2022a). In CPMs, convexity costs a...

â€¢ **Abstract Reference**:
> Automated market makers (AMMs) are a new prototype of decentralised exchanges which are revolutionising market interactions. The majority of AMMs are constant product markets (CPMs) where exchange rates are set by a trading function. This work studies optimal trading and statistical arbitrage in CPMs where balancing exchange rate risk and execution costs is key. Empirical evidence shows that execution costs are accurately estimated by the convexity of the trading function. These convexity costs are linear in the trade size and are nonlinear in the depth of liquidity and in the exchange rate. We develop models for when exchange rates form in a competing centralised exchange, in a CPM, or in both venues. Finally, we derive computationally efficient strategies that account for stochastic convexity costs and we showcase their out-of-sample performance. Keywords: Decentralised finance, blockchains, automated market making, smart contracts, algorithmic trading, statistical arbitrage, predict...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Harnessing Deep Q-Learning for Enhanced Statistical Arbitrag

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Harnessing Deep Q-Learning for Enhanced Statistical Arbitrag
â€¢ **æª”æ¡ˆ**ï¼š `2023_Harnessing Deep Q-Learning for Enhanced Statistical Arbitrag.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> . The realm of High-Frequency Trading (HFT) is characterized by rapid decision-making processes that capitalize on ï¬‚eeting market ineï¬ƒciencies. As the ï¬nancial markets become increasingly competitive, there is a pressing need for innovative strategies that can a dapt and evolve with changing market dynamics. Enter Reinforcement Learning (RL), a branch of machine learning where agents learn by inte racting with their environment, making it an intriguing candidate f or HFT applications. This paper dives deep into the integration of RL in statistical arbitrage strategies tailored for HFT scenarios. By l everaging the adaptive learning capabilities of RL, we explore its potent ial to unearth patterns and devise trading strategies that traditional me thods might overlook. We delve into the intricate exploration-exploit ation trade-oï¬€s inherent in RL and how they manifest in the volatile world of H FT. Furthermore, we confront the challenges of applying RL in non-s tationary environments, ty...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> to generalize across a myriad of states. The strength of neural networks in capturing complex, non-l inear relationships makes them particularly suited for this task [15]. Mathematically, the neural network can be represented as: Q(s, a; Î¸) = f (s; Î¸) Here: â€“ Q(s, a; Î¸) denotes the approximated Q-value for action a in state s, given the current parameters Î¸. â€“ f is the function represented by the neural network. â€“ The input s is the state, which in the context of ï¬nancial markets could b e a vector containing various market indicators, historical price data, trading volumes, and more. â€“ The output is a vector representing the Q-values for each pos sible action, given the current state. The training of this network involves adjusting the weights Î¸ to minimize the diï¬€erence between the predicted Q-values and the target Q-values. This is typically done using gradient descent optimization techni ques [16]. By employing neural networks as function approximators, De ep Q-Learning leverages their capability to handle vast input spaces and m odel complex relationships, making it particularly apt for ï¬nancial marke ts. In the realm of High-Frequency Trading (HFT), where decisions need to be ma de rapidly based on vast amounts of data, the ability of the neural network to q uickly provide Q-value estimates for given market states becomes invaluab le. Moreover, the adaptive nature of neural networks, where the y can continuously learn and adjust to new data, aligns well with the dy...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> The world of High-Frequency Trading (HFT) represents one of the most dynamic and complex arenas in the ï¬nancial domain. Within this fast- paced environment, Statistical Arbitrage stands out as a sophisticated strate gy, aiming to exploit temporary market ineï¬ƒciencies through advanced computati onal and statistical techniques. The incorporation of Deep Q-Learning into this space underscores a promising evolution, marrying the strengths of reinforcem ent learning with the demands of HFT. Throughout this exploration, we delved deep into the mechanics of Q-Learning and its neural network-based extension, Deep Q-Learning. T he integration of neural network approximators, experience replay, and a met iculous reward structure has showcased the potential for superior adaptability and precision in trading decisions. By leveraging a neural networkâ€™s prowess in c apturing intricate patterns and relationships, Deep Q-Learning oï¬€ers the abil ity to navigate the vast and high-dimensional state spaces typical of ï¬nancial markets. Experience replay, with its emphasis on random sampling and decorrelation of experiences, enhances the stability of the learning proces s. Such stability becomes paramount in HFT, where market dynamics shift rapidly, and t he algorithm needs to ensure consistent and reliable performance. Furthermore, the meticulous design of states and actions wi thin the HFT context ensures that the agent possesses a comprehensive un derstanding of the market environment. By continuously updating based on mark et feedback and employing a neural network to generalize across states, Dee p Q-Learning exhibits the potential to adapt trading strategies in real-ti me, capturing ï¬‚eeting statistical arbitrage opportunities. In wrapping up, itâ€™s evident that the fusion of Deep Q-Learni ng with HFT Statistical Arbitrage heralds a new era in algorithmic trad ing. As ï¬nancial markets continue to evolve and become increasingly complex, th e need for adaptive, data-driven strategies becomes paramount. Deep Q-Learnin g, with its blend of deep learning and reinforcement learning principles, oï¬€er s a beacon of promise in this pursuit. As we move forward, it will be intriguing to wit ness the real-world applications and reï¬nements of this approach, potentially setting new benchmarks in the domain of algorithmic trading. 10 S. Sarkar

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Linear and nonlinear causality in financial markets

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Linear and nonlinear causality in financial markets
â€¢ **æª”æ¡ˆ**ï¼š `2023_Linear and nonlinear causality in financial markets.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we present a much more general framework for assessing co-dependencies by identifying and interpreting linear and nonlinear causalities in the complex system of financial markets. To do so, we use two different causal inference methods, transfer entropy and convergent cross-mapping, and employ Fourier transform surrogates to separate their linear and nonlinear contributions. We find that stock indices in Germany and the U.S. exhibit a significant degree of nonlinear causality and that correlation, while a very good proxy for linear causality, disregards nonlinear effects and hence underestimates causality itself. The presented framework enables the measurement of nonlinear causality, the correlation-causality fallacy, and motivates how causality can be used for inferring mark...

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ,â€ Neuroimage58, 323â€“329 (2011). 49S. Ponczek, â€œTo understand the wild u.s. stock rally, just forget about 2020,â€ (2020). 50M. PaluÅ¡, V . Albrecht, and I. DvoË‡rÃ¡k, â€œInformation theoretic test for nonlinearity in time series,â€ Physics Letters A175, 203â€“209 (1993). 51J. Hlinka, D. Hartman, M. Vejmelka, J. Runge, N. Marwan, J. Kurths, and M. PaluÅ¡, â€œReliability of inference of directed climate networks using conditional mutual information,â€ Entropy15, 2023â€“2045 (2013). 52M. B. Kennel and M. Buhl, â€œEstimating good discrete partitions from observed data: Symbolic false nearest neighbors,â€ Physical Review Letters 91, 084102 (2003). 53D. A. Hsieh, â€œNonlinear dynamics in financial markets: evidence and implications,â€ Financial Analysts Journal51, 55â€“62 (1995). 54R. N. Mantegna, â€œHierarchical structure in financial markets,â€ The European Physical Journal B-Condensed Matter and Complex Systems11, 193â€“ 197 (1999). 55R. Cont, â€œEmpirical properties of asset returns: stylized facts and statistical issues,â€ Quantitative finance1, 223 (2001). 56D. P. Doane and L. E. Seward, â€œMeasuring skewness: a forgotten statistic?â€ Journal of statistics education 19 (2011). 57J. C. Hull, Options futures and other derivatives (Pearson Education India, 2003)....

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> AND OUTLOOK The present study has addressed the issue of identifying and quantifying co-dependence among financial instruments, which continues to be a paramount challenge for both researchers and practitioners in the financial industry. While traditional linear measures like the Pearson correlation have maintained their prominence, this paper has introduced a novel framework aimed at analyzing both linear and nonlinear causal relationships within financial markets. To achieve this, we have employed two distinct causal inference methodChaos 13 ologies, namely Transfer Entropy and Convergent CrossMapping, and have utilized Fourier transform surrogates to disentangle their respective linear and nonlinear contributions. Our findings have unveiled that stock indices in Germany and the U.S. exhibit a substantial degree of nonlinear causality, a phenomenon that has largely eluded previous investigations. It is important to recognize that while correlation, exemplified by the Pearson correlation coefficient, serves as an excellent proxy for linear causality, it falls short in capturing the intricate nonlinear dynamics that underlie financial markets. Consequently, relying solely on correlation can lead to an underestimation of causality itself. The framework introduced in this study not only facilitates the quantification of nonlinear causality but also sheds light on the perilous "correlation-causality fallacy." By delving into the nuances of causality, we have motivated how these insights can be harnessed for practical applications, including inferring market signals, implementing pair trading strategies, and enhancing the management of portfolio risk. One of the insights derived from our findings underscores the role that both linear and nonlinear causality can play as early warning indicators for unusual market dynamics. Furthermore, our results suggest that a straightforward incorporation of these causality measures into strategies, such as pair trading and portfolio optimization, can yield better outcomes compared to a reliance solely on Pearson correlation. This understanding can significantly empower traders and risk managers, enabling them to craft more effective trading strategies and to adopt a more proactive approach to risk mitigation. Looking ahead, the implications of our findings extend to various facets of financial research and practice. Further exploration of nonlinear causality may uncover new dimensions of financi

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Monte Carlo Simulation for Trading Under a LÃ©vy-Driven Mean-

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Monte Carlo Simulation for Trading Under a LÃ©vy-Driven Mean-
â€¢ **æª”æ¡ˆ**ï¼š `2023_Monte Carlo Simulation for Trading Under a LÃ©vy-Driven Mean-.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we discuss a framework for pairs trading using LÂ´ evy-driven Ornstein-Uhlenbeck processes to model the mean-reverting spread, which works with any LDOUP that can be simulated. Specifically, we focus on a variance gamma driving process, which is a time-changed Brownian motion and a purejump, infinite activity LÂ´ evy process with skewness and kurtosis parameters. By using this driving process, these properties extend to the LDOUP, allowing for more flexible models of the price spread than is available in the classical OU model, and in contrast to the finite activity processes considered previously. However, the generalization comes at the cost of not having analytic formulas. Hence, we develop a Monte Carlo method to evaluate the trading strategies and determine the optimal...

â€¢ **Abstract Reference**:
> We present a Monte Carlo approach to pairs trading on mean-reverting spreads modeled by LÂ´ evy-driven Ornstein-Uhlenbeck processes. Specifically, we focus on using a variance gamma driving process, an infinite activity pure jump process to allow for more flexible models of the price spread than is available in the classical model. However, this generalization comes at the cost of not having analytic formulas, so we apply Monte Carlo methods to determine optimal trading levels and develop a variance reduction technique using control variates. Within this framework, we numerically examine how the optimal trading strategies are affected by the parameters of the model. In addition, we extend our method to bivariate spreads modeled using a weak variance alpha-gamma driving process, and explore the effect of correlation on these trades. Keywords: Pairs trading, Monte Carlo simulation, LÂ´ evy process, OrnsteinUhlenbeck process, mean reversion, variance gamma process. 2020 MSC Subject Classifi...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is described in (Michaelsen and Szimayer, 2018, Section 4.1). 4.3 Variance Reduction Consider the two separate linear regression models, Y1 = X1Î²1 + Ïµ1, Y2 = X2Î²2 + Ïµ2, (4.1) where Y1 and Y2 are the vector of m simulated values of P and P 2, respectively. In this section only, we let X1 âˆˆ R(p1+1)Ã—m and X2 âˆˆ R(p2+1)Ã—m be the design matrices, including an intercept. As outlined in Section 4.2, X1 includes the control variates X(ti) âˆ’ Âµ, i = 1 , . . . , p, 1A, 1B, and X2 includes the control variates in X1 in addition to ( X(ti) âˆ’ Âµ)2, i = 1, . . . , p. Since each simulated sample path of the LDOUP is independent, we have Cov( Ïµi) = Î£ Ïµ,iiI, i = 1, 2 10 and Cov( Ïµ1, Ïµ2) = Î£ Ïµ,12I for some 2 Ã— 2 covariance matrix Î£ Ïµ = (Î£ Ïµ,ij)2 i,j=1, and where I is the identity matrix. For each of the linear regression models, i = 1, 2, let ÂµC,i be the mean vector of the control variates computed as outlined in Section 4.2, then the predictions xi = (1, ÂµC,i) are bYi = xâ€² ibÎ²i. As explained in Appendix A, the control variate estimators of E[P ] and E[P 2] are the predictions bY1 and bY2, respectively. Thus, for each argument, the control variate estimator of the value function in (3.2) is bVC := bY1 âˆ’ Î³bY2 + Î³bY 2 1 = bâ€²bY +bYâ€²AbY, where A =  Î³ 0 0 0  , b =  1 âˆ’Î³  , bY = bY1 bY2 ! . Let Y 1, Y 2 be the sample mean of Y1, Y2 respectively, thenbVM := Y 1 âˆ’ Î³Y 2 + Î³Y 2 1 is the corresponding Monte Carlo estimator of the value function. Given that our approach involves parameters that are es...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 On statistical arbitrage under a conditional factor model of

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 On statistical arbitrage under a conditional factor model of
â€¢ **æª”æ¡ˆ**ï¼š `2023_On statistical arbitrage under a conditional factor model of.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> assumes that future returns are predictable given (i) the asset-wise factor exposures for the universe of assets of interest, and (ii) a latent factor vector that is estimable by regression in the *University of Oxford, Oxford-Man Institute of Quantitative Finance. â€ Corresponding author. E-mail: trent@robots.ox.ac.uk 1 arXiv:2309.02205v1 [q-fin.ST] 5 Sep 2023 asset cross-section. Further, we assume the dynamic nature of the factor vector, suggesting a state space framework for the joint modelling of factors and asset returns. Our fair value model is closest in spirit to existing work that presents explanatory models of returns based on Principal Component Analysis (PCA) [3, 4]. In these papers, the principal components (and their loadings) that are deemed to have sufficiently high explanatory power are an analogue to our modelâ€™s factor exposures (and factor vectors). However the papers differ significantly in many key workflow assumptions relative to ours, most importantly with respect to data set construction, asset feature modelling, and the trading strategy assumed. We posit, critically, that the details of each of these steps can have a significant impact on the estimated performance statistics. Hence we clearly describe our methods in the interest of ease of interpretability and reproducibility, and present a range of performance data based on competing sets of reasonable assumptions. Owing to transparency, we also find two key, well-known works with which we can dire...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> We study multivariate statistical arbitrage under a conditional factor model augmented in a state space framework for US equities trading. Our study shows the modelling approach can yield compelling performance statistics over a 29 year period, an an absolute basis and relative to a reasonable benchmark, and relative to a model estimating returns as a function of factors via ordinary least squares. However, for our experimental set up, we did not find that the non-linear state space model was justified relative to the linear case; though it was no worse, any outperformance benefit does not contrast well with the increased modelling complexity introduced. When we compare our results to existing attempts in the literature, we find that our approach compares well. We also show empirical evidence that blending our investment capital between the long-short strategy and the long-only market portfolio as a function of each strategies estimated mean and variance yields a strategy with improved annual Sharpe ratio, consistent with theory. All results are with respect to reasonable level of transaction costs. There is the scope for academic and practioner work on the strategy presented in this paper, particularly with respect to exploring the potential for improvement on the period of strategy underperformance in recent years. We conclude with some final remarks: â€¢ Regarding the data underlying the quantitative strategy, the addition of alternative data beyond factor data and returns time series may improve the predictive power of the model. With respect to the dataset we constructed, and by the recent work of [32], it is the case that estimating submodels over clusters of data â€“ for example, stocks grouped by sector â€“ could lead to significant performance improvement. â€¢ The recent work of [24, 33] consider alternative specifications of dynamic factor models for investing, including those with non-linearities at the level of the measurement equation. Such expressions could be handled by our UKF-based framework, and present as a reasonable target for improving performance. â€¢ Recall our brief discussion on related literature in Section 2, whereby almost all work cited made use of data on a daily time scale. The literature would be bolstered by analysis completed on alternative time frames, particularly on intraday time scales. This is arguably harder, not the least in terms of data set curation. On the other hand, the recent work of [34] provides evidence of in

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Optimal Entry and Exit with Signature in Statistical Arbitra

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Optimal Entry and Exit with Signature in Statistical Arbitra
â€¢ **æª”æ¡ˆ**ï¼š `2023_Optimal Entry and Exit with Signature in Statistical Arbitra.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we explore an optimal timing strategy for the trading of price spreads exhibiting mean-reverting characteristics. A sequential optimal stopping framework is formulated to analyze the optimal timings for both entering and subsequently liquidating positions, all while considering the impact of transaction costs. Then we leverages a refined signature optimal stopping method to resolve this sequential optimal stopping problem, thereby unveiling the precise entry and exit timings that maximize gains. Our framework operates without any predefined assumptions regarding the dynamics of the underlying mean-reverting spreads, offering adaptability to diverse scenarios. Numerical results are provided to demonstrate its superior performance when comparing with conventional mean revers...

â€¢ **Abstract Reference**:
> In this paper, we explore an optimal timing strategy for the trading of price spreads exhibiting mean-reverting characteristics. A sequential optimal stopping framework is formulated to analyze the optimal timings for both entering and subsequently liquidating positions, all while considering the impact of transaction costs. Then we leverages a refined signature optimal stopping method to resolve this sequential optimal stopping problem, thereby unveiling the precise entry and exit timings that maximize gains. Our framework operates without any predefined assumptions regarding the dynamics of the underlying mean-reverting spreads, offering adaptability to diverse scenarios. Numerical results are provided to demonstrate its superior performance when comparing with conventional mean reversion trading rules. Keywordsâ€” mean-reversion trading; signature method; optimal stopping time 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> formulates a sequential optimal stopping problem to analyze the optimal trading timings, and capitalizes on the signature optimal stopping method, a powerful tool for solving this sequential optimal stopping problem, consequently revealing the precise timings for entry and exit that yield maximal gains. Importantly, our framework is adapted to a broad spectrum of mean reversion dynamics, with the Ornstein-Uhlenbeck (OU) model serving as a prime illustration in our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> s In this research paper, we have presented an innovative approach to identifying optimal timing strategies for trading price spreads with mean-reverting properties. Specifically, we formulated a sequential optimal stopping problem that accounts for the timing of both position entry and liquidation, while also incorporating transaction costs. To tackle this challenging problem, we employed the signature optimal stopping method, a powerful tool for determining the optimal entry and exit times that maximize trading returns. Our approach is distinguished by its versatility: it is designed to operate without any assumptions of mean reversion dynamics, making it broadly applicable to a variety of trading scenarios. To validate the efficacy of our methodology, we carried out a comprehensive set of numerical experiments. These experiments served to compare the performance of our approach against conventional mean-reversion 18 trading rules. The results consistently demonstrated the superior performance of our signature-based optimal trading strategy in terms of key metrics such as cumulative returns and Sharpe ratios. In summary, this paper makes a significant contribution to the field of quantitative finance by introducing a robust and adaptable method for optimizing trading decisions in mean-reverting markets. Our findings affirm that the signature optimal stopping method offers not only theoretical rigor but also practical utility, presenting a compelling solution to a complex problem in finance. While our focus has been on specific asset pairs and trading conditions, the general principles and techniques introduced here have the potential for broader applications, opening up avenues for future research and practical implementations.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Optimal Strategies for Round-Trip Pairs Trading Under Geomet

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Optimal Strategies for Round-Trip Pairs Trading Under Geomet
â€¢ **æª”æ¡ˆ**ï¼š `2023_Optimal Strategies for Round-Trip Pairs Trading Under Geomet.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we consider a round-trip pairs trading strategy. Intially, we assume the pairs position, which we will denote Z, consists of a one-share long position in stock S1 and a one-share short position in stock S2. We consider the case that the net position may initially be long (with one share of Z) or flat (with no stock holdings of either S1 or S2). Let i = 0, 1 denote the initial net positions of long and flat, respectively. If initially we are long (i = 1), we will close the pairs position Z at some time Ï„0 and conclude our trading activity. Otherwise, if initially we are flat ( i = 0), we will first obtain one share of Z at some time Ï„1, and then close pairs position Z at some time Ï„2 â‰¥ Ï„1, thus concluding our trading activity. Let K denote the fixed percentage of transaction...

â€¢ **Abstract Reference**:
> . This paper is concerned with an optimal strategy for simultaneously trading a pair of stocks. The idea of pairs trading is to monitor their price movements and compare their relative strength over time. A pairs trade is triggered by the divergence of their prices and consists of a pair of positions to short the strong stock and to long the weak one. Such a strategy bets on the reversal of their price strengths. A round-trip trading strategy refers to opening and closing such a pair of security positions. Typical pairs-trading models usually assume a difference of the stock prices satisfies a mean-reversion equation. However, we consider the optimal pairs-trading problem by allowing the stock prices to follow general geometric Brownian motions. The objective is to trade the pairs over time to maximize an overall return with a fixed commission cost for each transaction. Initially, we allow the initial pairs position to be either long or flat. We then consider the problem when the initi...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> Consider two stocks, S1 and S2. Let {X1 t , t â‰¥ 0} denote the prices of the stock S1, and let {X2 t , t â‰¥ 0} denote the prices of the stock S2. They satisfy the following stochastic differential equation: (1) d ï£« ï£¬ï£­ X1 t X2 t ï£¶ ï£·ï£¸ = ï£« ï£¬ï£­ X1 t X2 t ï£¶ ï£·ï£¸ ï£® ï£¯ï£° ï£« ï£¬ï£­ Âµ1 Âµ2 ï£¶ ï£·ï£¸ dt + ï£« ï£¬ï£­ Ïƒ11 Ïƒ12 Ïƒ21 Ïƒ22 ï£¶ ï£·ï£¸ d ï£« ï£¬ï£­ W 1 t W 2 t ï£¶ ï£·ï£¸ ï£¹ ï£ºï£» OPTIMAL STRATEGIES FOR ROUND-TRIP PAIRS TRADING UNDER GEOMETRIC BROWNIAN MOTIONS 3 where Âµi, i = 1 , 2 are the return rates, Ïƒij, i, j = 1 , 2 are the volatility constants, and (W 1 t , W 2 t ) is a 2-dimensional standard Brownian motion. In this paper, we consider a round-trip pairs trading strategy. Intially, we assume the pairs position, which we will denote Z, consists of a one-share long position in stock S1 and a one-share short position in stock S2. We consider the case that the net position may initially be long (with one share of Z) or flat (with no stock holdings of either S1 or S2). Let i = 0, 1 denote the initial net positions of long and flat, respectively. If initially we are long (i = 1), we will close the pairs position Z at some time Ï„0 and conclude our trading activity. Otherwise, if initially we are flat ( i = 0), we will first obtain one share of Z at some time Ï„1, and then close pairs position Z at some time Ï„2 â‰¥ Ï„1, thus concluding our trading activity. Let K denote the fixed percentage of transaction costs associate with buying or selling of stocks. Then given the initial state ( x1, x2), the initial net position i = 0, 1, a...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Optimal pair trading: consumption-investment problem

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Optimal pair trading: consumption-investment problem
â€¢ **æª”æ¡ˆ**ï¼š `2023_Optimal pair trading: consumption-investment problem.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We expose a simple solution of the consumption-investment p roblem pair trading. The proof is based on the remark that the HJB equatio n can be reduced to a linear parabolic equation solvable explicitly. Keywords spread trading Â· pair trading Â· Ornsteinâ€“Uhlenbeck process Â· consumption-invetsment problem Â·HJB equation Mathematics Subject Classiï¬cation (2010) 60G44 JEL Classiï¬cation G22 Â·G23 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> considered earlier by Elena Boguslavskaya and M ikhail Boguslavsky in [2] where the spread was modeled by the Ornsteinâ€“Uhlenbeck p rocess and investorâ€™s goal is to maximize only the expected utility of the terminal wealth. The HJB equation in [2], though looking rather involved, admits a soluti on which can be referred to Lomonosov Moscow State University, Moscow, Russia, and UniversitÂ´ e de Franche-ComtÂ´ e, Laboratoire de MathÂ´ ematiques, UMR CNRS 6623, 16 Route de Gray, 25030 Besanc Â¸on, France E-mail: ykabanov@univ-fcomte.fr. Lomonosov Moscow State University and â€œV egaâ€ Institute, Moscow, Russia E-mail: alexkozh27@gmail.com 2 Y uri Kabanov, Aleksei Kozhevnikov as an explicit one. In [1] the functional to optimize include s also the expected utility of consumption with the same power utility function. The sug gested analysis of the latter, rather involved and lengthy, is based on a ï¬xed point method. It leads to the optimal solution which is the main contribution , see Ths. 5. 1 and 5.3 in [1]. Though it may happen that the ideas of [1] could be useful in a more gen eral context, they are not needed in the considered case. Here we provide argume nts showing that the spread trading problem is not much more complicated than the classical Merton problem. The key ingredient of our proof is a reduction of the HJB e quation to a linear parabolic equation admitting explicit solution. 2 Model First we recall brieï¬‚y the formulation of optimal control pr oblem for spread tradin...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> The consumption-investment problem in the setting of pair t rade admits an explicit solution. The arguments are based on the observation that th e HJB equation in the Ornsteinâ€“Uhlenbeck spread model is reduced to a linear para bolic PDE admitting an explicit solution. This observation drastically simpliï¬e s the arguments in [1]. Acknowledgement. This work was supported by the Russian Science Foundation associated grants 20-68-47030 and 20-61-47043.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Pairs Trading: An Optimal Selling Rule with Constraints

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Pairs Trading: An Optimal Selling Rule with Constraints
â€¢ **æª”æ¡ˆ**ï¼š `2023_Pairs Trading: An Optimal Selling Rule with Constraints.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we extend these results to incorporate markets with Markov trading constraints. In particular, a sequence of trading windows is imposed. One can only buy/sell stocks when the windows are open. We focus on a simple and easily implementable strategy, and its optimality and the sufficient conditions for a closed-form solution. Mathematical trading rules have been studied extensively in the literature, and various approaches have been proposed to determine optimal trading strategies. Zhang [16] considers a selling rule that involves two threshold levels, a target price, and a stop-loss limit, and aims to find the optimal threshold levels that maximize the expected profit. To achieve this, Zhang solves a set of two-point boundary value problems, which are a type of differential...

â€¢ **Abstract Reference**:
> The focus of this paper is on identifying the most effective selling strategy for pairs trading of stocks. In pairs trading, a long position is held in one stock while a short position is held in another. The goal is to determine the optimal time to sell the long position and repurchase the short position in order to close the pairs position. The paper presents an optimal pairs-trading selling rule with trading constraints. In particular, the underlying stock prices evolve according to a two dimensional geometric Brownian motion and the trading permission process is given in terms of a two-state {trading allowed, trading not allowed} Markov chain. It is shown that the optimal policy can be determined by a threshold curve which is obtained by solving the associated HJB equations (quasivariational inequalities). A closed form solution is obtained. A verification theorem is provided. Numerical experiments are also reported to demonstrate the optimal policies and value functions. Key words...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> Our pairs trading involves two stocks: S1 and S2. Let {X 1 t , t â‰¥ 0} denote the prices of stock S1 and {X 2 t , t â‰¥ 0} that of stock S2. They satisfy the following stochastic differential equation: d  X 1 t X 2 t  =  X 1 t X 2 t  Âµ1 Âµ2  dt +  Ïƒ11 Ïƒ12 Ïƒ21 Ïƒ22  d  W 1 t W 2 t  , (1) where Âµi, i = 1, 2, are the return rates, Ïƒij, i, j = 1, 2, the volatility constants, and (W 1 t , W 2 t ) a 2-dimensional standard Brownian motion. The liquidity process Î±t is assumed to be a two-state Markov chain with state space M = {0, 1}. We impose the following trading constraint: One can only buy/sell stocks when Î±t = 1. Let Q be the generator of Î±t given by Q =  âˆ’Î»0 Î»0 Î»1 âˆ’Î»1  , with Î»0 > 0 and Î»1 > 0. We assume Î±t and (W 1 t , W 2 t ) are independent. In this paper, we consider a pairs selling rule. We assume the corresponding pairâ€™s position consists of a one-share long position in stock S1 and a one-share short position in stock S2. This condition can be easily relaxed; see Tie et al. [14] for details. The problem is to determine an optimal stopping time Ï„ (subject to trading constraints) to fold the pairs position by selling S1 and buying back S2. Let K denote the transaction cost percentage (e.g., slippage and/or commission) associated with stock transactions. For example, the proceeds to close the pairs position at t is (1 âˆ’ K)X 1 t âˆ’ (1 + K)X 2 t . For ease of notation, let Î²b = 1 + K and Î²s = 1 âˆ’ K. Let Ft = Ïƒ{(X 1 r , X2 r , Î±r) : r â‰¤ t}. We consider admissible st...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Pairs-trading System using Quantum-inspired Combinatorial Op

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Pairs-trading System using Quantum-inspired Combinatorial Op
â€¢ **æª”æ¡ˆ**ï¼š `2023_Pairs-trading System using Quantum-inspired Combinatorial Op.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a pairs-trading strategy based on an optimal path analysis in market graphs and show through real-time trading that the strategy is executable with an automated pairs-trading system using an embedded Ising machine for the optimal path search. The market graph for N tradable stocks (an N-stock universe) is an N-node fully-connected directed graph with edge weights corresponding to the products of instantaneous price differences and statistical correlation factors between two stocks. The trading opportunities (temporary mispricing of statistically correlated pairs) are detected by an optimal path analysis (a sort of collective evaluation) of the N-node market graph. As the embeddable Ising machine, we use a combinatorial optimization accelerator based on a quantum-inspired...

â€¢ **Abstract Reference**:
> Pairs-trading is a trading strategy that involves matching a long position with a short position in two stocks aiming at market-neutral profits. While a typical pairstrading system monitors the prices of two statistically correlated stocks for detecting a temporary divergence, monitoring and analyzing the prices of more stocks would potentially lead to finding more trading opportunities. Here we report a stock pairs-trading system that finds trading opportunities for any two stocks in an Nstock universe using a combinatorial optimization accelerator based on a quantum-inspired algorithm called simulated bifurcation. The trading opportunities are detected through solving an optimal path search problem in an N-node directed graph with edge weights corresponding to the products of instantaneous price differences and statistical correlation factors between two stocks. The accelerator is one of Ising machines and operates consecutively to find multiple opportunities in a market situation wi...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , and proof-of-concept study with FPGA devices. 10 Jun Nakayama received the degree of Bachelor of Arts in Economic and Social studies from the University of Manchester, the U.K., in 2008. He received the degree of Master of Business Administration (MBA) in Finance from Hitotsubashi University, Japan, in 2017. He was a portfolio manager in Nomura Asset Management Co., Ltd. from 2008 to 2020 and was engaged in the development and management of quant-based funds. He joined Toshiba Corporation in 2020. He is also a Ph.D. candidate in the Financial Strategy Program, Hitotsubashi University Business School. His research interests include quantitative investment strategies, quantum-inspired computing technology, and trading strategies with advanced technologies. Tomoya Kashimata received the B.E., and M.E. degrees in computer science and engineering from Waseda University, Japan, in 2018 and 2020, respectively. He joined Corporate Research and Development Center, Toshiba Corporation, Japan, in 2020. His research interests include computer architecture, reconfigurable architecture, and processor in memory. Masaya Yamasaki received the B.E. and M.E. degrees in Computer Science and Communication Engineering from Kyushu University, Japan, in 1997 and 1999, respectively. He joined Toshiba Corporation in 1999. He was engaged in the development of image processing engines (interframe interpolation technology) for digital televisions (including ones with Cell Broadband Engi...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Signature Trading: A Path-Dependent Extension of the Mean-Va

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Signature Trading: A Path-Dependent Extension of the Mean-Va
â€¢ **æª”æ¡ˆ**ï¼š `2023_Signature Trading: A Path-Dependent Extension of the Mean-Va.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> In this article we introduce a portfolio optimisation framework, in which the use of rough path signatures [Lyo98] provides a novel method of incorporating path-dependencies in the joint signal-asset dynamics, naturally extending traditional factor models, while keeping the resulting formulas lightweight, tractable and easily interpretable. Specifically, we achieve this by representingatradingstrategyasalinearfunctionalappliedtothe signature of a path(whichwe refer to asâ€œSignature Tradingâ€or â€œSig-Tradingâ€). This allows the modeller to efficiently encode the evolution of past time-series observations into the optimisation problem. In particular, we derive a concise formulation of the dynamic mean-variance criterion alongside an explicit solution in our setting, which naturally incorporates a drawdown control in the optimal strategy over a finite time horizon. Secondly, we draw parallels between classical portfolio stategies and Sig-Trading strategies and explain how the latter leads to ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ler to efficiently encode the evolution of past time-series observations into the optimisation problem. In particular, we derive a concise formulation of the dynamic mean-variance criterion alongside an explicit solution in our setting, which naturally incorporates a drawdown control in the optimal strategy over a finite time horizon. Secondly, we draw parallels between classical portfolio stategies and Sig-Trading strategies and explain how the latter leads to a pathwise extension of the classical setting via theâ€œSignature Efficient Frontierâ€. Finally, we give explicit examples when trading under an exogenous signal as well as examples for momentum and pair-trading strategies, demonstrated both on synthetic and market data. Our framework combines the best of both worlds between classical theory (whose appeal lies in clear and concise formulae) and between modern, flexible data-driven methods (usually represented by ML approaches) that can handle more realistic datasets. The advantage of the added flexibility of the latter is that one can bypass common issues such as the accumulation of heteroskedastic and asymmetric residuals during the optimisation phase. Overall, Sig-Trading combines the flexibility of data-driven methods without compromising on the clarity of the classical theory and our presented...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2023 Statistical arbitrage portfolio construction based on prefer

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2023 Statistical arbitrage portfolio construction based on prefer
â€¢ **æª”æ¡ˆ**ï¼š `2023_Statistical arbitrage portfolio construction based on prefer.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we propose a novel portfolio construction method based on preference relation graphs, which can reconcile contradictory pairs trading signals across multiple security pairs. The proposed approach enables joint exploitation of arbitrage opportunities among a large number of securities. Experimental results using three decades of historical returns of roughly 500 stocks from the S&P 500 index show that the portfolios based on preference relations exhibit robust returns even with high transaction costs, and that their performance improves with the number of securities considered. Keywords: statistical arbitrage, pairs trading, preference relations, potential method. 1 Introduction Signal processing methods are becoming increasingly important in financial applications, providi...

â€¢ **Abstract Reference**:
> Statistical arbitrage methods identify mispricings in securities with the goal of building portfolios which are weakly correlated with the market. In pairs trading, an arbitrage opportunity is identified by observing relative price movements between a pair of two securities. By simultaneously observing multiple pairs, one can exploit different arbitrage opportunities and increase the performance of such methods. However, the use of a large number of pairs is difficult due to the increased probability of contradictory trade signals among different pairs. In this paper, we propose a novel portfolio construction method based on preference relation graphs, which can reconcile contradictory pairs trading signals across multiple security pairs. The proposed approach enables joint exploitation of arbitrage opportunities among a large number of securities. Experimental results using three decades of historical returns of roughly 500 stocks from the S&P 500 index show that the portfolios based ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> Let a binary relation M : S Ã— S âˆ’ â†’ {âˆ’ 1, 0, 1} represent a pairs trading strategy, where S = {s1, s2, . . . , sN } is a set of tradeable securities. 1 When M(s1, s2) = 1 a long position should be entered in the first security s1 and a short position in the second security s2, when M(s1, s2) = âˆ’1 a short position should be taken in s1 and long position in s2, and in the case M(s1, s2) = 0 neither s1 nor s2 should be bought or sold. However, due to the stochastic and non-stationary characteristics of the market, M can produce contradictory outputs. For example, if M(s1, s2) = 1, M(s2, s3) = 1, and M(s1, s3) = âˆ’1 a long/short portfolio could not be formed without violating some of the outputs of M. Fortunately, contradictory outputs can be avoided if M is constrained to be a preference relation. A preference relation is a binary relation with following properties: 1. Irreflexivity: âˆ€ s âˆˆ S : M(s, s) = 0 (a security s can not be preferred over itself), 2. Asymmetricity: âˆ€ si, sj âˆˆ S : ( M(si, sj) = 1) = â‡’ (M(sj, si) = âˆ’1) (if si is preferred over sj, then sj can not be preferred over si), 3. Transitivity: âˆ€ si, sj, sk âˆˆ S : ( M(si, sj) = 1) âˆ§ (M(sj, sk) = 1) = â‡’ (M(si, sk) = 1) (if si is preferred over sj and sj is preferred over sk, then si must also be preferred over sk). These conditions induce strict partial ordering over a set of securities S, allowing an investor to rank securities according to some criterion, and ultimately form a portfolio which may include long posit...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 A Comparison between Financial and Gambling Markets

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 A Comparison between Financial and Gambling Markets
â€¢ **æª”æ¡ˆ**ï¼š `2024_A Comparison between Financial and Gambling Markets.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Financial and gambling markets are ostensibly similar and h ence strategies from one could potentially be applied to the other. Financia l markets have been extensively studied, resulting in numerous theorems a nd models, while gambling markets have received comparatively less attenti on and remain relatively undocumented. This study conducts a comprehens ive comparison of both markets, focusing on trading rather than regulation. F ive key aspects are examined: platform, product, procedure, participant and s trategy. The ï¬ndings reveal numerous similarities between these two markets. Fi nancial exchanges resemble online betting platforms, such as Betfair, and som e ï¬nancial products, including stocks and options, share speculative traits wit h sports betting. We examine whether well-established models and strategies fr om ï¬nancial markets could be applied to the gambling industry, which lacks compa rable frameworks. For example, statistical arbitrage from ï¬nancial markets h as been eï¬€e...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> (S. Lee, Park, & Lee , 2021), the authors argue that these ï¬ve components comprise the essence of trading. By examining these aspects, the paper aims to gain insights into the c ommonalities shared by these markets and identify any potential transferabilit y of models and strategies between them. The study illuminates the interconne ctedness and similarities between ï¬nancial and gambling markets, encompassing pe er-to-peer markets as well. This oï¬€ers potentially valuable insights for investors , traders, and participants operating within both markets, particularly for ï¬n ancial analysts aiming to comprehend gambling markets. Moreover, the exploration of strategies and approaches employed in these markets may open up new avenues fo r innovation and optimization in trading and betting activities. Each section studies on e key aspect and describes both markets and their similarities. Section 2 introduc es the similarities between platforms in both markets, including trading and speculatio n. Section 3 describes the comparable characteristics of products, including r isk. Section 4 discusses procedural similarities, while Section 5 examines the participantsâ€™ pu rposes. Section 6 explores strategies from ï¬nancial markets, including statistical arbitrage, which could be applied to gambling markets. Finally, Section 7 presents the c onclusions and potential applications. 2 The Platform 2.1 Traditional Financial Platforms In ï¬nancial markets, there are various types of platforms, in...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 A Markowitz Approach to Managing a Dynamic Basket of Moving-

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 A Markowitz Approach to Managing a Dynamic Basket of Moving-
â€¢ **æª”æ¡ˆ**ï¼š `2024_A Markowitz Approach to Managing a Dynamic Basket of Moving-.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We consider the problem of managing a portfolio of moving-band statistical arbitrages (MBSAs), inspired by the Markowitz optimization framework. We show how to manage a dynamic basket of MBSAs, and illustrate the method on recent historical data, showing that it can perform very well in terms of risk-adjusted return, essentially uncorrelated with the market. âˆ—Stanford University (   kasperjo@stanford.edu) â€ Abu Dhabi Investment Authority â€¡Stanford University 1 arXiv:2412.02660v1 [econ.EM] 3 Dec 2024 Contents 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Paper 83-13. Department of Economics, University of California, 1983. [HJTW04] S. Hogan, R. Jarrow, M. Teo, and M. Warachka. Testing market efficiency using statistical arbitrage with applications to momentum and value strategies.Journal of Financial economics, 73(3):525â€“565, 2004. [Huc19] N. Huck. Large data sets and machine learning: Applications to statistical arbitrage. European Journal of Operational Research, 278(1):330â€“342, 2019. [Joh00] S. Johansen. Modelling of co-integration in the vector autoregressive model. Economic Modelling, 17(3):359â€“373, 2000. [JOP+23] K. Johansson, M. Ogut, M. Pelger, T. Schmelzer, and S. Boyd. A simple method for predicting covariance matrices of financial returns.Foundations and TrendsÂ® in Econometrics, 12(4):324â€“407, 2023. [JSB23] K. Johansson, T. Schmelzer, and S. Boyd. Finding moving-band statistical arbitrages via convex-concave optimization, 2023. Working Paper, Stanford University. [KDH17] C. Krauss, X. Do, and N. Huck. Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&P 500. European Journal of Operational Research, 259(2):689â€“702, 2017. [KL23] D. Kim and B. Lee. Shorting costs and profitability of longâ€“short strategies. Accounting & Finance, 63(1):277â€“316, 2023. [Kra17] C. Krauss. Statistical arbitrage pairs trading strategies: Review and outlook. Journal of Economic Surveys , 31(2):513â€“545, 2017. 18 [LB16] T. Lipp and S. Boyd. Variations and extension of the convexâ€“concave procedure. Optimization and Engineering , 17:263â€“287, 2016. [Mar52] H. Markowitz. Portfolio selection. The Journal of Finance , 7(1):77â€“91, 1952. [Mar59] H. Markowitz. Portfolio selection: Efficient diversification of investments . John Wiley, 1959. [Nak19] T. Nakajima. Expectations for statistical arbitrage in energy futures markets. Journal of Risk and Financial Management , 12(1):14, 2019. [Per09] M. Perlin. Evaluation of pairs-trading strategy at the Brazilian financial market. Journal of Derivatives & Hedge Funds , 15:122â€“136, 2009. [Pol11] A. Pole. Statistical arbitrage: algorithmic trading insights and techniques . John Wiley & Sons, 2011. [PY18] J. Primbs and Y. Yamada. Pairs trading under transaction costs using model predictive control. Quantitative Finance, 18(6):885â€“895, 2018. [SDGB16] X. Shen, S. Diamond, Y. Gu, and S. Boyd. Disciplined convex-concave programming. In 2016 IEEE 55th Conference on Decision and Control (CDC) , pages 1009â€“1014. IEEE, 2016. [Vid04] G. Vidyamurthy. Pa

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Advanced Statistical Arbitrage with Reinforcement Learning

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Advanced Statistical Arbitrage with Reinforcement Learning
â€¢ **æª”æ¡ˆ**ï¼š `2024_Advanced Statistical Arbitrage with Reinforcement Learning.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Statistical arbitrage is a prevalent trading strategy which takes advantage of mean reverse property of spread of paired stocks. Studies on this strategy often rely heavily on model assumption. In this study, we introduce an innovative model-free and reinforcement learning based framework for statistical arbitrage. For the construction of mean reversion spreads, we establish an empirical reversion time metric and optimize asset coefficients by minimizing this empirical mean reversion time. In the trading phase, we employ a reinforcement learning framework to identify the optimal mean reversion strategy. Diverging from traditional mean reversion strategies that primarily focus on price deviations from a long-term mean, our methodology creatively constructs the state space to encapsulate the recent trends in price movements. Additionally, the reward function is carefully tailored to reflect the unique characteristics of mean reversion trading. Keywordsâ€” Statistical Arbitrage, Mean Revers...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> creatively constructs the state space to encapsulate the recent trends in price movements. Additionally, the reward function is carefully tailored to reflect the unique characteristics of mean reversion trading. Keywordsâ€” Statistical Arbitrage, Mean Reversion Trading, Empirical Mean Reversion Time, Reinforcement Learning JEL: C14, C61 1 Introduction Statistical arbitrage, also known as mean reversion trading or pairs trading, is an important trading strategy in the financial markets. The essence of statistical arbitrage lies in creating spreads or portfolios from the market that exhibit mean-reverting characteristics, thereby unlocking opportunities for profit. For instance, if the price of a spread falls below its long-term mean, a trader might take a long position and then wait until its price correction, aiming to profit from this adjustment. The approach to statistical arbitrage unfolds in three distinct steps: First, it entails the identification of two or more securities that have shown a historical pattern of moving together. Next, a mean-reverting spread is formulated from these correlated securities. The final step involves taking a position when the spread deviates from its long-term mean, leveraging the anticipated return to equilibrium to generate profits. Therefore, mean reversion trading is divided into three main elements: (1) the identification of securities with co-movements, (2) the construction of mean-reverting spreads, and (3) the development of a tr...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 An Application of the Ornstein-Uhlenbeck Process to Pairs Tr

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 An Application of the Ornstein-Uhlenbeck Process to Pairs Tr
â€¢ **æª”æ¡ˆ**ï¼š `2024_An Application of the Ornstein-Uhlenbeck Process to Pairs Tr.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we conduct preliminary analysis on a pairs trading strategy using the Ornstein-Uhlenbeck process to model stock price differences and compare that to a naive pairs trading strategy using a rolling window to calculate mean and standard deviation parameters. Our preliminary findings suggest that running a pairs trading strategy with the Ornstein-Uhlenbeck process outperforms the naive pairs trading strategy on a risk-return basis. Key further research can be conducted on the selection of pairs, augmenting the investment universe, finding different criteria in pairs selection, applying more rigorous machine learning techniques to assist with forecasting pricing trends, and in integrating portfolio optimization techniques. Introduction Pairs trading is a widely used market-neutr...

â€¢ **Abstract Reference**:
> In this paper, we conduct preliminary analysis on a pairs trading strategy using the Ornstein-Uhlenbeck process to model stock price differences and compare that to a naive pairs trading strategy using a rolling window to calculate mean and standard deviation parameters. Our preliminary findings suggest that running a pairs trading strategy with the Ornstein-Uhlenbeck process outperforms the naive pairs trading strategy on a risk-return basis. Key further research can be conducted on the selection of pairs, augmenting the investment universe, finding different criteria in pairs selection, applying more rigorous machine learning techniques to assist with forecasting pricing trends, and in integrating portfolio optimization techniques....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> is tested on historical market data to evaluate its performance and robustness under varying market conditions. The selection of pairs is critical. Pairs chosen should exhibit high correlation between the percentage change of the prices of each security, and the price spread should exhibit a mean reverting behavior. Once pairs are chosen, an algorithm can be developed to model the spread and to forecast future pricing trends to take a directional view on the spread. In our case, we employ the Ornstein-Uhlenbeck process to do so, and we compare our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Application of Black-Litterman Bayesian in Statistical Arbit

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Application of Black-Litterman Bayesian in Statistical Arbit
â€¢ **æª”æ¡ˆ**ï¼š `2024_Application of Black-Litterman Bayesian in Statistical Arbit.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S&P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner. Keywords Bayesian Estimation Â· Black-Litterman Model Â· Portfolio Manageme...

â€¢ **Abstract Reference**:
> In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S&P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner. Keywords Bayesian Estimation Â· Black-Litterman Model Â· Portfolio Management Â· Statistical Arbitrage Â· co-integration Â· Mean Reversion Â· Pairs Trading Â· Quantitative Strategies Â· Machine Learning 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> of statistical arbitrage and portfolio optimization. In Section 2, we present the literature overview of methodology and empirical findings of Statistical Arbitrage and Black-Litterman model. So far very little literature focuses on establish a Black-Litterman model using statistical arbitrage strategy. In Section 3, we discuss how to select co-integrated stock pairs with rigorous Engle and Granger test. Then we transform the co-integration model into real world trading signals. Section 4 shows how to incorporate into the Black-Litterman model our prior views on stock returns based on pairs trading strategy. Finally, we calibrate model parameters and backtest the portfolio from two testing periods: out-of-sample 2016 to 2018 and stressful financial crisis in Section 5 We make our...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> We first discussed how to implement pairs trading through co-integration modeling and demonstrated that cointegrated stock pairs can generate excess returns with small risk. Then we incorporated pairs trading as a return prior into the Black-Litterman portfolio optimization framework. The out-of-sample backtest result proves that our strategy can outperform the S&P500 index and suffered less drawdown during financial crisis period. It has great implication on how we can manage the risk of traditional pairs trading strategy in a more systematic manner.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Automated Market Making and Decentralized Finance

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Automated Market Making and Decentralized Finance
â€¢ **æª”æ¡ˆ**ï¼š `2024_Automated Market Making and Decentralized Finance.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Automated market makers (AMMs) are a new type of trading venues which are revolutionising the way market participants interact. At present, the majority of AMMs are constant function market makers (CFMMs) where a deterministic trading function determines how markets are cleared. Within CFMMs, we focus on constant product market makers (CPMMs) which implements the concentrated liquidity (CL) feature. In this thesis we formalise and study the trading mechanism of CPMMs with CL, and we develop liquidity provision and liquidity taking strategies. Our models are motivated and tested with market data. We derive optimal strategies for liquidity takers (LTs) who trade orders of large size and execute statistical arbitrages. First, we consider an LT who trades in a CPMM with CL and uses the dynamics of prices in competing venues as market signals. We use Uniswap v3 data to study price, liquidity, and trading cost dynamics, and to motivate the model. Next, we consider an LT who trades a basket o...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . Next, we consider an LT who trades a basket of crypto-currencies whose constituents co-move. We use market data to study lead-lag effects, spillover effects, and causality between trading venues. We derive optimal strategies for strategic liquidity providers (LPs) who provide liquidity in CPMM with CL. First, we use stochastic control tools to derive a self-financing and closed-form optimal liquidity provision strategy where the width of the LPâ€™s liquidity range is determined by the profitability of the pool, the dynamics of the LPâ€™s position, and concentration risk. Next, we use a model-free approach to solve the problem of an LP who provides liquidity in multiple CPMMs with CL. We do not specify a model for the stochastic processes observed by LPs, and use a long short-term memory (LSTM) neural network to approximate the optimal liquidity provision strategy. Acknowledgements First and foremost, I would like to thank my supervisor, Â´Alvaro Cartea, who guided me through this journey. The endless hours spent discussing research had significant impact on my professional and personal development. He truly was a unique supervisor; he dedicated plenty of his precious time to discuss our projects, he truly cared about my research progress and he understood how the ups and downs of my personal life were impacting my studies. I will be forever grateful for his supervision. I would like to thank FayÂ¸ cal Drissi, who worked closely with me and whose dedication and talent were crucia...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Cointegrated Matrix Autoregression Models

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Cointegrated Matrix Autoregression Models
â€¢ **æª”æ¡ˆ**ï¼š `2024_Cointegrated Matrix Autoregression Models.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we present a novel extension of the cointegrated VAR model, which focuses on the cointegration of matrix-valued time series, denoted by X t, a d1 Ã— d2 matrix observed at time t. We introduce the error correction model in bilinear form, with row-wise and column-wise cointegrating vectors Î²1 and Î²2, respectively, which enable the transformation of X t into a stationary process Î²â€² 1X tÎ²2, while the original process is a non-stationary I(1) process. Our proposed approach presents a promising alternative to traditional cointegration analysis, which is in line with the recent work on autoregressive models for stationary processes by Chen et al. (2021), Xiao et al. (2021). 2 To estimate the coefficient matrices, especially the cointegrating vectors, for the proposed model, we intro...

â€¢ **Abstract Reference**:
> We propose a novel cointegrated autoregressive model for matrix-valued time series, with bi-linear cointegrating vectors corresponding to the rows and columns of the matrix data. Compared to the traditional cointegration analysis, our proposed matrix cointegration model better preserves the inherent structure of the data and enables corresponding interpretations. To estimate the cointegrating vectors as well as other coefficients, we introduce two types of estimators based on least squares and maximum likelihood. We investigate the asymptotic properties of the cointegrated matrix autoregressive model under the existence of trend and establish the asymptotic distributions for the cointegrating vectors, as well as other model parameters. We conduct extensive simulations to demonstrate its superior performance over traditional methods. In addition, we apply our proposed model to Fama-French portfolios and develop a effective pairs trading strategy. KEYWORDS: Cointegration; Multivariate Ti...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> and the corresponding estimators, and compare with the classical vector cointegration models. In Section 5.2, we develop a pairs trading strategy based on the proposed matrix cointegration model. All the proofs and some additional theorems and simulations are collected in Appendix 6. Notations. For ease of reading, here we highlight some notations that are frequently used. Throughout this paper, we use the bold uppercase letters for matrices (e.g. X t), and the bold lowercase letters for vectors (e.g. x). We use âˆ¥ Â· âˆ¥F and âˆ¥ Â· âˆ¥s to denote the matrix Frobenius norm and spectral norm respectively. The Kronecker product of two matrices is denoted by âŠ—. Note that the notations Gij, Bij refer to some new matrices to be defined later, and they should not be regarded as entries of G or B. For any d1 Ã— d2 matrix Î² of full rank (assuming d1 â‰¤ d2), we denote 3 by Î²âŠ¥ a d1 Ã— (d1 âˆ’ d2) matrix of rank d1 âˆ’ d2 such that Î²â€²Î²âŠ¥ = 0. We define Â¯Î² = Î²(Î²â€²Î²)âˆ’1, such that Î²â€² Â¯Î² = I d2, and PÎ² = Â¯Î²Î²â€² is the projection of Rd1 onto the space spanned by the columns of Î². We use âˆ† to denote the difference operator such that âˆ† xt = xt âˆ’ xtâˆ’1. 2 Cointegrated Matrix Autoregression Models 2.1 Cointegrated Vector Autoregression Models To fix notations, we briefly introduce some basic concepts about the cointegrated vector autoregression models (CVAR). For a more thorough analysis on the vector cointegration models, see Johansen et al. (1995). Consider a vector time series {xt}. If we allow unit roots in...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 ESG driven pairs algorithm for sustainable trading: Analysis

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 ESG driven pairs algorithm for sustainable trading: Analysis
â€¢ **æª”æ¡ˆ**ï¼š `2024_ESG driven pairs algorithm for sustainable trading: Analysis.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper, we adopt two distinct methodologies for stock selection utilizing ESG scores, as enumerated below: (A) Approach 1: (i) Categorizing the firms according to their respective industries. (ii) Computing the average ESG score for all firms within each industry for a specific timestamp. (iii) From each industry group, the firm that exhibits the highest ESG score for that timestamp is identified. (iv) Only firms whose ESG score exceeds a threshold parameter Î¶ (contingent on the ESG value specified in the preceding step) are considered. (B) Approach 2: (i) Evaluation of each firmâ€™s ESG score in relation to the average score of its respective industry. (ii) Select firms that outperform their industry average in terms of ESG scores by a given specified margin Î¾. The stocks selected...

â€¢ **Abstract Reference**:
> This paper proposes an algorithmic trading framework integrating Environmental, Social, and Governance (ESG) ratings with a pairs trading strategy. It addresses the demand for socially responsible investment solutions by developing a unique algorithm blending ESG data with methods for identifying co-integrated stocks. This allows selecting profitable pairs adhering to ESG principles. Further, it incorporates technical indicators for optimal trade execution within this sustainability framework. Extensive back-testing provides evidence of the modelâ€™s effectiveness, consistently generating positive returns exceeding conventional pairs trading strategies, while upholding ESG principles. This paves the way for a transformative approach to algorithmic trading, offering insights for investors, policymakers, and academics. Keywords: ESG, Pairs Trading, Sustainable Investing 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> In this section, we focus on the identification of stock pairs for the purpose of pairs trading, a strategy that is driven fundamentally via the notions of mean reversion and stationarity. 5 Industry Mean ESG Industry Score Forestry & Fishing 1 .085000 Industrial Conglomerates, Architectural, Engineering, & Related Services, Airport, Harbor Operations, & Logistics 3.950000 Electronic Equipment & Instrumentation, Household Appliance Manufacturing, Oil and Gas Extraction 6.538333 Personal Care Products 8 .467857 Accounting, Tax Prep., & Payroll Services 16 .147500 Table 2: Bottom 5 industry ESG scores For the asset picking exercise, in this paper, we adopt two distinct methodologies for stock selection utilizing ESG scores, as enumerated below: (A) Approach 1: (i) Categorizing the firms according to their respective industries. (ii) Computing the average ESG score for all firms within each industry for a specific timestamp. (iii) From each industry group, the firm that exhibits the highest ESG score for that timestamp is identified. (iv) Only firms whose ESG score exceeds a threshold parameter Î¶ (contingent on the ESG value specified in the preceding step) are considered. (B) Approach 2: (i)...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> and Future Directions This paper introduces a pairs trading algorithm combining ESG measures for sustainable asset picking. Quantitative techniques, including, cointegration, mean reversion and dimensionality reduction are combined with the APO spread trading approach to algorithmically identify and trade pairs. Although backtests show differences between train and test results, some top pairs display strong signals, integration and positive risk-adjusted gains over both (train and test) periods, indicating feasibility, in terms real-life trading. Overall this study connects ethical investing and statistical arbitrage, providing a framework positioned to traverse markets, while upholding sustainability values. Future work would involve application of this strategy to high-frequency data in order to uncover more trading opportunities and understand short term price moves, despite increased complexity. Additional dimensionality reduction methods like t-SNE could reveal new data patterns missed by PCA. Exploring clustering algorithms beyond OPTICS may provide fresh insights and alternative pairs. Machine learning approaches could dynamically determine entry and exit points, thereby adapting the strategy to evolving markets. The model could also integrate other performance factors like macroeconomic and sentiment indicators or more nuanced ESG metrics. In summary, integrating highfrequency data, new dimensionality and clustering techniques, and advanced machine learning promises an adaptive, robust strategy aligning of financial and sustainability objectives. Declaration of Interests The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 End-to-End Policy Learning of a Statistical Arbitrage Autoen

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 End-to-End Policy Learning of a Statistical Arbitrage Autoen
â€¢ **æª”æ¡ˆ**ï¼š `2024_End-to-End Policy Learning of a Statistical Arbitrage Autoen.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose embedding an Autoencoder in a neural network representing a trading policy, trained end-to-end with a loss function that optimises both portfolio return representation and ri skadjusted returns. We study the performance of different variants of the architecture and competing methods on his1 End to End Learning StatArb torical US equity returns, demonstrating the advantages of an end-to-end training approach over traditional methods. Our methodology not only learns a statistical factor model from data but also extracts residuals within the network, leading to a portfolio that is optimised for StatArb trading through a risk-adjusted performance policy. This architec ture, as we will demonstrate, surpasses prevalent benchmark solutions in StatArb trading, offering improve...

â€¢ **Abstract Reference**:
> In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (linear) model is identiï¬ed, a separate mean reversion strategy is then devised to generate a trading signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb. As a ï¬rst approach, we employ a standard Autoencoder trained on US stock returns to derive trading strategies based on the OrnsteinUhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neural network representation of a space of portfolio trading policies. This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy. Our ï¬ndings demonstrate that this innovative end-to-end policy learning a...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> not only learns a statistical factor model from data but also extracts residuals within the network, leading to a portfolio that is optimised for StatArb trading through a risk-adjusted performance policy. This architec ture, as we will demonstrate, surpasses prevalent benchmark solutions in StatArb trading, offering improvements in terms of return before cost and reducing modelling risk at every stage of the process. 2. Related Work The concept of StatArb trading, as we deï¬ne it, ï¬nds its roots in the foundational work of Avellaneda & Lee (2010). Their basic idea is that price changes follow the differenti al equation dPt Pt = Î±dt + nâˆ‘ j=1 Î²jF (j) t + dXt where Xt is a stationary mean reverting process, referred to as the cointegration residual, Pt is the price of a security at time t, Î± a drift term, and Î²j factor exposures for the risk-factors F (j) t . Statistical arbitrage is the generalisation of pairs-trading in which n = 1 . They show how modelling the risk factors as industry ETFs or principal components and modelling the residual process as a mean reverting Ornstein-Uhlenbeck (OU) process, one can derive a proï¬table trading strategy. Y eo & Papanicolauou (2017) further reï¬ne the idea to include risk controls depending on the speed of mean reversion of residuals. There is a proliferation of factors and asset pricing models in the ï¬nancial literature ( Harvey et al. , 2016; Feng et al. , 2019). The seminal contributions in this space were performed by Sha...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Enhanced Momentum with Momentum Transformers

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Enhanced Momentum with Momentum Transformers
â€¢ **æª”æ¡ˆ**ï¼š `2024_Enhanced Momentum with Momentum Transformers.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, the TFT uses a window of size252 to allow the multi-headed attention block to learnimportances and relationships over a trading year period.They also include a static encoder in which the inputvariable sc is the asset class so their model can learnbehavioral differences in the different assets they trade.Wood et. al focus primarilyontradingfuturesduetotheirlowcovariance comparedtoequities. Theportfoliointhisoriginal paper was comprised of 25 commodities, 11indices, 5fixedincome, and9foreignexchangeassets. 2.2KeyResultsoftheOriginalPaperThey were able to improve upon prior research withthe TFT model producing a Sharpe ratio of 2.62 from1995 to 2020. It was also shown that this modelperformed well in times of market turmoil as this newmodel had a Sharpe of 2.47 during the Covi...

â€¢ **Abstract Reference**:
> The primary objective of this research is to build aMomentum Transformer that is expected to outperformbenchmark time-series momentum and mean-reversiontrading strategies. We extend the ideas introduced in [6]to equities as the original paper primarily only buildsuponfuturesandequityindices. Unlikeconventional LongShort-Term Memory (LSTM) models, which operatesequentially and are optimized for processing localpatterns, an attention mechanismequips our architecturewith direct access to all prior time steps in the trainingwindow. This hybrid design, combining attention with anLSTM, enables the model to capture long-termdependencies, enhance performance in scenariosaccounting for transaction costs, and seamlesslyadapt toevolving market conditions, such as those witnessedduring the Covid Pandemic. The main technicalchallenges we faced aresomeof thesinsmentionedinthepaper â€œSeven Sins of Quantitative Investing â€œ[11] wherewe inadvertently facedinitial challenges, suchasthedatanot being trulyP...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> to outperform typical momentumstrategies[6].Prior research using Deep Momentum Networks hasused commodities, indices, fixedincome, andfxassets[3,4, 6]. This paper aims to apply this Deep MomentumNetwork methodology to equities in recent yearstoseeifthe prior...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> &FutureWorkA large cause for our reduced Sharpe compared totheir results stems from the different portfolio we triedtrading which is highlymorevolatilethantheirs. Thiscanbe seen when we compare long-only strategy volatilities.During the Covid Pandemic theyâ€™re long-only strategyhad anannual volatilityof 6.73%[6]. Our portfolioontheother hand, never went below an annual volatility of7.43% and over the four years averaged an annualvolatility of 9.05%. Strategies such asincreasingthetimewindowinput for themodel or increasingthenumber of attention heads were used to try and counteract this butprovided minimal or worse results. Further volatilitymitigation strategies were tested including doubling thenumber of stocks in the portfolio fromeach sector to tenbut this caused alargelynegativeresult. Thisislikelydueto the fact that equities tend to exhibit high covariance.We tried to mitigate this by diversifying sectorally butthen each of the five companies fromthat sector exhibithigh covariance with each other. Thisidea, however, fallsshort after looking into the annual covariances of returnsfor our portfoliowhichall appear reasonablylow. Image4: CovarianceMatrixHeatmapfor 15companiesin2022 Another potential issue we noticed was the CPDwasalmost always producing a high confidence change. Itâ€™s 6 believed that this was due to howvolatile equitiescanbeand likely whythemodel didbetter without theCPD.Thislead to the idea that maybe more attention heads coulddiscern something from the noise that was theChangepoint features but it only narrowly improved themodel and still didnâ€™t beat the model without it. We alsodidnâ€™t have the long-termCPDwhich would havehelpedin long-term trend changes which we can actually see afewoccur duringthistime.Future work should look to include this long-termCPD with a larger number of attention heads for themodel to better determine temporal relationships amongreturns, MACD, and CPD features. Furthermore, whentrading with equities it may be prudent to include otherimportant features commonly used in factor models suchas variables for the size, value, andmarket factorstohelpthe model learn and reduce market exposure, potentiallydecreasingvolatility. 7.SummaryThispaper lookedtoimproveuponcommonmomentumstrategies by trading equities using a MomentumTransformer used in prior research. This model takes inclassical momentum strategy features and learns a bestportfolio for the next day. The goal was to use themethodology to improve the basic momentum

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Finding Moving-Band Statistical Arbitrages via Convex-Concav

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Finding Moving-Band Statistical Arbitrages via Convex-Concav
â€¢ **æª”æ¡ˆ**ï¼š `2024_Finding Moving-Band Statistical Arbitrages via Convex-Concav.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we propose a new method for finding stat-arbs that can contain multiple (more than two) assets, with general weights. The problem is formulated as a nonconvex optimization problem in which we maximize the portfolio price variation subject to the price staying within a fixed band, along with a leverage limit, over some training period. Although this approach requires maximizing a convex function, we show how to approximately solve it using the convex-concave procedure [SDGB16, LB16]. Our second contribution is to introduce the concept of a moving-band stat-arb, in which the price of the portfolio varies in a band that changes over time, centered at the recent average portfolio price. (We refer to a traditional stat-arb as a fixed-band stat-arb.) We show that the same method we...

â€¢ **Abstract Reference**:
> We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time. 1 arXiv:2402.08108v1 [econ.EM] 12 Feb 2024 Contents 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . However, our problem differs significantly in that we do not rely on any co-integration analysis or statistical testing. Rather, we directly optimize for a high variance portfolio that is mean-reverting. Asset pairs can also be found using machine learning methods. In [SH20], the authors use unsupervised learning and propose a density-based clustering algorithms to cluster assets. Then, within asset clusters, pairs of assets are chosen for trading depending on cointegration, as well as mean-reversion tendency and frequency. Modern machine learning methods are also explored in [KDH17], where the authors propose the use of deep neural networks, gradient-boosted trees, and random forests for finding stat-arb portfolios. Another recent study of deep-learning stat-arb finding is [GOPZ21]. Earlier work on using machine learning for finding stat-arbs includes, e.g., [DKB15, MZ14, TL13, Huc10, Huc09]. 4 Modeling the stat-arb spread. When a co-moving set of assets has been identified, the next step is to model the portfolio price (or spread between the assets for a pair). Perhaps the most popular approach is to model the spread using stochastic control theory. It is common to consider investments in a mean-reverting spread and a risk-free asset and to model the spread as an Ornstein-Uhlenbeck process [MPW08, JY07]. Other methods include those borrowing tools from time series analysis [Kra17]. In [EVDHM05] the authors propose a mean-reverting Gaussian Markov chain model for model...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Paper 83-13. Department of Economics, University of California, 1983. [HJTW04] S. Hogan, R. Jarrow, M. Teo, and M. Warachka. Testing market efficiency using statistical arbitrage with applications to momentum and value strategies. Journal of Financial economics , 73(3):525â€“565, 2004. [Huc09] N. Huck. Pairs selection and outranking: An application to the S&P 100 index. European Journal of Operational Research, 196(2):819â€“825, 2009. [Huc10] N. Huck. Pairs trading and outranking: The multi-step-ahead forecasting case. European Journal of Operational Research, 207(3):1702â€“1716, 2010. [Huc19] N. Huck. Large data sets and machine learning: Applications to statistical arbitrage. European Journal of Operational Research, 278(1):330â€“342, 2019. [Joh00] S. Johansen. Modelling of co-integration in the vector autoregressive model. Economic Modelling, 17(3):359â€“373, 2000. [JY07] J. Jurek and H. Yang. Dynamic portfolio selection in arbitrage. In EFA 2006 Meetings Paper, 2007. [KDH17] C. Krauss, X. Do, and N. Huck. Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&P 500. European Journal of Operational Research, 259(2):689â€“702, 2017. [KL23] D. Kim and B. Lee. Shorting costs and profitability of longâ€“short strategies. Accounting & Finance, 63(1):277â€“316, 2023. [Kra17] C. Krauss. Statistical arbitrage pairs trading strategies: Review and outlook. Journal of Economic Surveys , 31(2):513â€“545, 2017. 25 [KS17] C. Krauss and J. StÂ¨ ubinger. Non-linear dependence modelling with bivariate copulas: Statistical arbitrage pairs trading on the S&P 100.Applied Economics, 49(52):5352â€“5369, 2017. [LB16] T. Lipp and S. Boyd. Variations and extension of the convexâ€“concave procedure. Optimization and Engineering , 17:263â€“287, 2016. [LLP21] J. Lee, J. Lee, and A. PrÂ´ ekopa. Price-bands: A technical tool for stock trading. Fintech with Artificial Intelligence, Big Data, and Blockchain , pages 221â€“246, 2021. [MPW08] S. Mudchanatongsuk, J. Primbs, and W. Wong. Optimal pairs trading: A stochastic control approach. In 2008 American control conference, pages 1035â€“ 1039. IEEE, 2008. [MZ14] B. Moritz and T. Zimmermann. Deep conditional portfolio sorts: The relation between past and future stock returns. LMU Munich and Harvard University Working paper, 2014. [Nak19] T. Nakajima. Expectations for statistical arbitrage in energy futures markets. Journal of Risk and Financial Management , 12(1):14, 2019. [Per09] M. Perlin. Evaluation of pairs-trading strateg

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Is the difference between deep hedging and delta hedging a s

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Is the difference between deep hedging and delta hedging a s
â€¢ **æª”æ¡ˆ**ï¼š `2024_Is the difference between deep hedging and delta hedging a s.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we consider the Conditional Value-at-Risk (CVaRÎ±) defined asÏ(Î¾Î´ T ) = E[Î¾Î´ T | Î¾Î´ T â‰¥ VaRÎ±(Î¾Î´ T )], where Î± âˆˆ (0, 1) and VaRÎ±(Î¾Î´ T ) is the Value-at-Risk defined asVaRÎ±(Î¾Î´ T ) = minc{c : P(Î¾Î´ T â‰¤ c) â‰¥ Î±}. The CVaR is a commonly used objective function in the deep hedging literature, see for instance Carbonneau and Godin (2021), Cao et al. (2023) or Wu and Jaimungal (2023). In addition, an appealing feature of the CVaR is that it allows to finetune the investorâ€™s attitude towards risk through the confidence level. A high value ofÎ± puts more emphasis on risk reduction, whereas a low value of Î± penalizes losses and rewards gains. Each time-t action Î´t+1 is a feedback-type decision, being a function of the information currently available on the market:Î´t+1 = ËœÎ´(Xt) for some func...

â€¢ **Abstract Reference**:
> The recent work of Horikawa and Nakagawa (2024) claims that under a complete market admitting statistical arbitrage, the difference between the hedging position provided by deep hedging and that of the replicating portfolio is a statistical arbitrage. This raises concerns as it entails that deep hedging can include a speculative component aimed simply at exploiting the structure of the risk measure guiding the hedging optimisation problem. We test whether such finding remains true in a GARCH-based market model, which is an illustrative case departing from complete market dynamics. We observe that the difference between deep hedging and delta hedging is a speculative overlay if the risk measure considered does not put sufficient relative weight on adverse outcomes. Nevertheless, a suitable choice of risk measure can prevent the deep hedging agent from engaging in speculation. JEL classification: C45, C61, G32. Keywords: Deep reinforcement learning, optimal hedging, arbitrage. âˆ—Gauthier ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> follow Rt = Âµ + ÏƒtÏµt, Ïƒ 2 t+1 = Ï‰ + Ïƒ2 t (Î± + Î³1{Ïµt<0})Ïµ2 t + Î²Ïƒ2 t , (9) where Âµ, Î³ âˆˆ R, Ï‰, Î±, Î² are positive, 1A is the dummy variable indicating if eventA occurs and {Ïµt}T t=1 are independent standard normal random variables. Parameter estimates are obtained through maximum likelihood on a daily time series of the S&P 500 index extending from January 4, 2016, to December 31, 2020. Estimated parameters areÂµ = 0.06%, Ï‰ = 0.01%, Î± = 0.11, Î³ = 0.20 and Î² = 0.78. Furthermore, in all...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 LLMs for Time Series: an Application for Single Stocks and S

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 LLMs for Time Series: an Application for Single Stocks and S
â€¢ **æª”æ¡ˆ**ï¼š `2024_LLMs for Time Series: an Application for Single Stocks and S.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Large Language Models (LLMs) have been adapted for time series prediction with significant success in pattern recognition. However, the common belief is that these models are not suitable for predicting financial market returns, which are known to be almost random. We aim to challenge this misconception through a counterexample. Specifically, we utilize the Chronos model from Ansari et al. (2024) and test both pretrained configurations and fine-tuned supervised forecasts on the largest American single stocks using data from GuijarroOrdonnez et al. (2021). We construct a long/short portfolio, and the performance simulation indicates that LLMs can in reality handle time series that are nearly indistinguishable from noise, demonstrating an ability to identify inefficiencies amidst randomness and generate alpha. Finally, we compare these results with benchmark models, highlighting significant room for improvement in LLM performance to further enhance their predictive capabilities. âˆ—Machina...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> from Kelly et al. (2019), who used financial data to constrain the eigenvectors. This approach was also developed and justified by Valeyre (2019). Trans4 formers have also been used by Jiang et al. (2023) to identify patterns in images for financial price time series. Wood et al. (2022) applied deep learning techniques with only a few parameters to identify the most effective trendfollowing indicators enhancing and timing trend-following strategies. Transformers have also been used to extract common returns, as demonstrated by Gu et al. (2021). Qyrana (2024) applied a simple Short Term Reversal factor fed by residual returns using an autoencoder-based factor model, subsequently generating a highly profitable trading strategy (i.e. through buying losing stocks based on residual returns and shorting winner stocks). Chen et al. (2021) used a deep learning technique to identify anomalies in asset pricing. However the question remains whether a Large Language Model optimized for time series outside the field of finance is sufficiently intelligent and sensitive to capture inefficiencies and market anomalies. Despite the burgeoning literature, there are still no papers that apply a deep learning technique with millions of parameters to financial forecasting while this is the natural number of parameters to consider in the context of LLMs. Studies such as Chen et al. (2021), Jiang et al. (2023), Wood et al. (2022), Guijarro-Ordonnez et al. (2021), and Brugiere and Turi...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 MTRGL:Effective Temporal Correlation Discerning through Mult

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 MTRGL:Effective Temporal Correlation Discerning through Mult
â€¢ **æª”æ¡ˆ**ï¼š `2024_MTRGL:Effective Temporal Correlation Discerning through Mult.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we have embarked on an exploration of the application of deep learning in the realm of pair trading, a well-regarded quantitative investment strategy. This journey has led to the creation of a unique approach, MTRGL, explicitly designed to amalgamate descriptive and time series data, thereby optimizing the process of discerning temporal correlations in pair trading. Our empirical evidence shows that MTRGL is highly effective in automatically identifying correlated pairs, surpassing the performance of traditional baselines that rely exclusively on either descriptive or time series data. Moreover, the novel integration of multi-modal information in our approach extends beyond the scope of this study and pair trading. Its potential impact is significant for other quanti...

â€¢ **Abstract Reference**:
> In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies. Index Termsâ€” Graph, finance, multimodal...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> Consider A = {A1, A2, ..., An} as a collection of n entities under examination, where entities may represent companies, assets, or markets. Each entity Ai is linked with a dynamic feature vector Xi(t) = (x(1)(t), x(2)(t), ..., x(m)(t)), with each entry x(j)(t) signifying the j-th feature of entity Ai at a given time t (for instance, sector name, trading volume, market share, and so on). We useX (t) to denote the collection of feature vectors associated with A at time t. Moreover, Pi(t) : t 7â†’ R is the trading price time series of entity Ai, and we denote Pi([0, t]) as the time series information of entity Ai in the time interval [0, t] for a certain t > 0. Let P = {Pi(t)} denote the set of time series linked to the entity set A. Similarly, P([0, t]) = {Pi([0, t])|Pi(.) âˆˆ P} represents the set of all entitiesâ€™ time series associated with the time interval [0, t]. As previously discussed, a crucial aspect of pair trading is to discern the temporal correlation among entities based on their historical and feature information. More formally, if we denote S(Pi(t), Pj(t)), e.g., the normalized historical difference (NHD) [1], as a predetermined correlation measure of two time series and Î³ as a defined threshold, the aim is, given a set of entities A, their feature vectors X , and their time series information P([0, T]), to identify pairs of entities Ai, Aj such that S(pi([T, T + Î´]), pj([T, T + Î´])) â‰¥ Î³ for some Î´ > 0. 3.1. Temporal Graph Construction Given data from the ti...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> In this paper, we have embarked on an exploration of the application of deep learning in the realm of pair trading, a well-regarded quantitative investment strategy. This journey has led to the creation of a unique approach, MTRGL, explicitly designed to amalgamate descriptive and time series data, thereby optimizing the process of discerning temporal correlations in pair trading. Our empirical evidence shows that MTRGL is highly effective in automatically identifying correlated pairs, surpassing the performance of traditional baselines that rely exclusively on either descriptive or time series data. Moreover, the novel integration of multi-modal information in our approach extends beyond the scope of this study and pair trading. Its potential impact is significant for other quantitative financerelated problems, and we look forward to seeing how this innovation could transform these areas in the future. Acknowledgements. This study was funded by the Yangtze River Delta Joint Research and Innovation Community (Grant No.2022CSJGG1200), National Natural Science Foundation of China (Grant No. 42302326) and the Central University Basic Research Business Special Fund Assistance (No. JZ2023HGTA0178). 6.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Market information of the fractional stochastic regularity m

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Market information of the fractional stochastic regularity m
â€¢ **æª”æ¡ˆ**ï¼š `2024_Market information of the fractional stochastic regularity m.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we investigate some properties of the fOU process and, thanks to information theory and Shannonâ€™s entropy, we determine theoretically the serial information of the regularity process Ht of the FSRM, giving some insight into oneâ€™s ability to forecast future price increments and to build statistical arbitrages with this model. An application to the forecast of future daily price returns of major stock indices shows promising results. Keywords: Fractional Ornstein-Uhlenbeck process, Hurst exponent, Shannon entropy, serial information, nonlinear serial dependence 1. Introduction In financial mathematics, the most famous model for option pricing is the Black-Scholes model [18, 58], which, under the no-arbitrage assumption, describes the price dynamics Pt of an underlying asset by...

â€¢ **Abstract Reference**:
> The Fractional Stochastic Regularity Model (FSRM) is an extension of Black-Scholes model describing the multifractal nature of prices. It is based on a multifractional process with a random Hurst exponent Ht, driven by a fractional Ornstein-Uhlenbeck (fOU) process. When the regularity parameter Ht is equal to 1/2, the efficient market hypothesis holds, but when Ht Ì¸= 1/2 past price returns contain some information on a future trend or mean-reversion of the log-price process. In this paper, we investigate some properties of the fOU process and, thanks to information theory and Shannonâ€™s entropy, we determine theoretically the serial information of the regularity process Ht of the FSRM, giving some insight into oneâ€™s ability to forecast future price increments and to build statistical arbitrages with this model. An application to the forecast of future daily price returns of major stock indices shows promising results. Keywords: Fractional Ornstein-Uhlenbeck process, Hurst exponent, Shan...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> [29, 46], there has been an enormous literature on rough volatility, which is now a widespread model [12, 35, 37, 38, 39]. These developments about rough volatility, verified by empirical observations [36], encourage us to consider the fOU as a dynamic describing the time-varying Hurst exponent. This is the purpose of FSRM. Regarding the robustness of the inference of the global Hurst exponent 2 of such a model, it has been shown that many estimators [71], relying only on single moments of the distribution of returns, introduce non-linear biases by creating artificial roughness when H > 1/2 [4]. However, a recent method based on the Lamperti transform showed that even using the entire distribution of returns, the estimate of the global Hurst exponent is very rough [15]. Besides its link with log-volatility, the Hurst exponent of log-prices also has an interesting interpretation since a Hurst exponent of 1 /2 is related to the efficient market hypothesis, whereas values greater or lower than 1/2 underline the opportunity of statistical arbitrages. Obviously, in the FSRM, the regularity Ht, being modelled by a fOU augmented by a long-term average H = 1/2, is not constrained to be in the interval (0 , 1) as it should be. Therefore, we can work with a new variable eHt such as eHt = 1 2 + 1 Ï€ arctan  Ht âˆ’ 1 2  . (11) With this transformation the new regularity eHt is well defined in the interval (0 , 1). 3. Information theory for serial dependence Let XL 1 = (X1, . . . , XL...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Optimal market-neutral currency trading on the cryptocurrenc

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Optimal market-neutral currency trading on the cryptocurrenc
â€¢ **æª”æ¡ˆ**ï¼š `2024_Optimal market-neutral currency trading on the cryptocurrenc.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> : This research proposes a novel arbitrage approach in multivariate pair trading, termed the Optimal Trading Technique (OTT). We present a method for selectively forming a â€œbucketâ€ of fiat currencies anchored to cryptocurrency for monitoring and exploiting trading opportunities simultaneously. To address quantitative conflicts from multiple trading signals, a novel bi-objective convex optimization formulation is designed to balance investor preferences between profitability and risk tolerance. We understand that cryptocurrencies carry significant financial risks. Therefore this process includes tunable parameters such as volatility penalties and action thresholds. In experiments conducted in the cryptocurrency market from 2020 to 2022, which encompassed a vigorous bull run followed by a bear run, the OTT achieved an annualized profit of 15.49%. Additionally, supplementary experiments detailed in the appendix extend the applicability of OTT to other major cryptocurrencies in the post-CO...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> â€™s robustness and effectiveness in various market conditions. The arbitrage operation offers a new perspective on trading, without requiring external shorting or holding the intermediate during the arbitrage period. As a note of caution, this study acknowledges the high-risk nature of cryptocurrency investments, which can be subject to significant volatility and potential loss. Keywords: pair trading; multivariate; quantitative trading; cryptocurrency market 1. Introduction The secondary market functions as a modern, efficient form of double auction where participants intending to trade assets submit their acceptable bid and ask prices. The highest bid and lowest ask are matched if the prices meet. We believe in the Efficient Market Hypothesis (EMH), which posits that in the secondary market, prices should reflect all available information (Fama 1970). However, historical economic bubbles show that investors do not always act rationally. Dale et al. (2005) illustrated how an order book filled with irrational bids and asks could drive prices in the wrong direction. Identifying inefficiencies in an efficient market helps maintain trading equilibrium. Market directional trading involves astute traders estimating more accurate prices for instruments. They then place orders with the expectation that the market will revert to rationality. Traders who bet correctly are rewarded by the market mechanism, contributing to price discovery. Quantitative trading, instead of relying on expe...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> , extending our experiment to include the post-COVID period provided additional validation for the robustness of our OTT method. Despite the unique market conditions during the pandemic, the method maintained its effectiveness, demonstrating its adaptability and resilience across different market environments. The application of multiple cryptocurrencies demonstrates the universality of the OTT, which is adoptable for not only the ETH but other financial instruments that meet the instrument assumptions. SOL, on the other hand, constitutes an outlier example of pair trading under extreme market conditions. This adaptability is crucial for traders seeking consistent returns amidst varying market dynamics. Notes 1 Kraken exchange (https://www.kraken.com/). 2 Please refer to the Data Availability Statement for data access. 3 We use 4% based on the U.S. 10-Year Treasury Note at the time of writing. 4 The average calculation in the conclusion section is based on 1 min, 5 min, and 60 min sampled datasets for bull, bear, and full-cycle markets under a 0.1% transaction fee. 5 COVID-19, a pandemic since 2019, has had a significant global impact (Shi et al. 2020).

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Pairs Trading Using a Novel Graphical Matching Approach

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Pairs Trading Using a Novel Graphical Matching Approach
â€¢ **æª”æ¡ˆ**ï¼š `2024_Pairs Trading Using a Novel Graphical Matching Approach.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose techniques from graph theory such as matchings, to determine subsets of pairs with no common assets. We formally prove that the covariance from shared stocks in typical pair portfolio construction methods is substantial, and detrimental to the risk-adjusted performance. We then propose and implement a matching-based pair selection method to prevent the prior effect. To the best of our knowledge, this is the first instance in the literature in which the impact of covariance/variance in pairs trading portfolios is identified and in which a graphical matching-based approach is used to select pairs. 1.2. Previous Work Most pairs trading literature is oriented around price-based signals. In the work of Gatev E. (2006), the authors select pairs by minimizing the distance between norma...

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Parameters Optimization of Pair Trading Algorithm

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Parameters Optimization of Pair Trading Algorithm
â€¢ **æª”æ¡ˆ**ï¼š `2024_Parameters Optimization of Pair Trading Algorithm.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Pair trading is a market-neutral quantitative trading strategy that exploits price anomalies between two correlated assets. By taking simultaneous long and short positions, it generates profits based on relative price movements, independent of overall market trends. This study explores the mathematical foundations of pair trading, focusing on identifying cointegrated pairs, constructing trading signals, and optimizing model parameters to maximize returns. The results highlight the strategyâ€™s potential for consistent profitability, even in volatile market conditions. KEYWORDS Pair Trading; Correlation; Cointegration; Z-Score; Trading Signals; Parameters Optimization. Contents 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> of pair trading, primarily driven by correlation and cointegration analysis. We also performed basic numerical and mathematical calculations. This project is simple to understand, making it accessible even to those new to the strategy, while allowing for the easy addition of further functionalities. We especially observed the impact of optimizing a few parameters on our cumulative returns. Acknowledgements We thank Naftali Cohen for his lectures in Data-Driven Methods in Finance and his valuable weekly feedback. We also thank Columbia University for providing an inspiring work environment. References [1] E. Gatev, W.N. Goetzmann, and K.G. Rouwenhorst. Pairs Trading: Performance of a Relative Value Arbitrage Rule. Yale ICF Working Paper No. 08-03, 1998. [2] P. Kolapwar, U. Kulkarni, and J. Waghmare. Sector-based pairs trading strategy with novel solution technique. IEEE, 2020. [3] X. Zhu. Examining Pairs Trading Profitability. Yale Department of Economics, 2024. 16...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> , if we had to choose a strategy, we would not optimize the parameters since it would reduce the standard deviation, providing more stable returns. Figure 9. Example of cumulative returns with non optimized parameters 14 Figure 10. Example of cumulative returns with optimized parameters 4. F urther improvements 4.1. Computational limitations First, if we compute all the different pairs in an assets of n companies, there will be: C2 n =  n 2  = n! 2!(n âˆ’ 2)! = n(n âˆ’ 1) 2 . (10) At some point, we will want to limit the pairs we calculate. Instead of limiting the number of companies, we can choose to significantly increase the number of companies and randomly select pairs of companies. We wonâ€™t have all possible pairs, but weâ€™ll have enough to expect satisfactory results. We could try selecting companies from different sectors or from the same sector if the results are not satisfactory. 4.2. Profitability of pair trading Another problem, perhaps the most important one is, that pair trading is becoming increasingly difficult to be profitable, but its profitability will increase with the size of the assets pool[3], as well as trying to identify the pairs that are less noticeable on the market. A novel method could be that we compute not only the assets pair but also evaluate the time series data or external factors, like the weather for instance. 4.3. Machine Learning Instead of optimizing only Î¸out and Î¸in, it would be possible to try optimizing other parameters, such as the threshold parameters (correlation and p-value) and the length of the periods. 15 5. Conclusion In this paper, we discussed the fundamental concepts and methodology of pair trading, primarily driven by correlation and cointegration analysis. We also performed basic numerical and mathematical calculations. This project is simple to understand, making it accessible even to those new to the strategy, while allowing for the easy addition of further functionalities. We especially observed the impact of optimizing a few parameters on our cumulative returns. Acknowledgements We thank Naftali Cohen for his lectures in Data-Driven Methods in Finance and his valuable weekly feedback. We also thank Columbia University for providing an inspiring work environment.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Pricing and hedging of decentralised lending contracts

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Pricing and hedging of decentralised lending contracts
â€¢ **æª”æ¡ˆ**ï¼š `2024_Pricing and hedging of decentralised lending contracts.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> . We study the loan contracts offered by decentralised loan protocols (DLPs) through the lens of financial derivatives. DLPs, which effectively are clearinghouses, facilitate transactions between option buyers (i.e. borrowers) and option sellers (i.e. lenders). The loan-to-value at which the contract is initiated determines the option premium borrowers pay for entering the contract, and this can be deduced from the non-arbitrage pricing theory. We show that when there are no market frictions, and there is no spread between lending and borrowing rates, it is optimal to never enter the lending contract. Next, by accounting for the spread between rates and transactional costs, we develop a deep neural network-based algorithm for learning trading strategies on the external markets that allow us to replicate the payoff of the lending contracts that are not necessarily optimally exercised. This allows hedge the risk lenders carry by issuing options sold to the borrowers, which can complement...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Reinforcement Learning Pair Trading: A Dynamic Scaling appro

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Reinforcement Learning Pair Trading: A Dynamic Scaling appro
â€¢ **æª”æ¡ˆ**ï¼š `2024_Reinforcement Learning Pair Trading: A Dynamic Scaling appro.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> : Cryptocurrency is a cryptography-based digital asset with extremely volatile prices. Around USD 70 billion worth of cryptocurrency is traded daily on exchanges. Trading cryptocurrency is difficult due to the inherent volatility of the crypto market. This study investigates whether Reinforcement Learning (RL) can enhance decision-making in cryptocurrency algorithmic trading compared to traditional methods. In order to address this question, we combined reinforcement learning with a statistical arbitrage trading technique, pair trading, which exploits the price difference between statistically correlated assets. We constructed RL environments and trained RL agents to determine when and how to trade pairs of cryptocurrencies. We developed new reward shaping and observation/action spaces for reinforcement learning. We performed experiments with the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data separated by 1 min intervals ( n = 263,520). The traditional non-RL pair...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> lacks consideration for transaction costs, which might impact real-world profitability. A lack of direct comparison with traditional models is another shortcoming. Future work could involve developing the Reinforcement Learning (RL) approach to multileg strategies, integrating pair formation into the trading process, cross-validating across different environments, and...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Statistical Arbitrage in Rank Space

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Statistical Arbitrage in Rank Space
â€¢ **æª”æ¡ˆ**ï¼š `2024_Statistical Arbitrage in Rank Space.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we show the superiority of statistical arbitrage in rank space over name space by exploiting the enhanced mean-reversion of residual returns in rank space. Our statistical arbitrage portfolios obtained using neural networks in rank space achieve an average annual return 35.68% and an average Sharpe ratio of 3.28 from 2007 to 2022 with 2 basis points transaction cost. This contrasts with the conventional statistical arbitrage in name space that yields negligible returns during the same period. The paper is structured as follows. In section 2, we formulate the framework for statistical arbitrage in both name space and rank space. Specifically, section 2.1 addresses the market decomposition and the construction of market-neural portfolios. Section 2.2 focuses on defining and...

â€¢ **Abstract Reference**:
> Equity market dynamics are conventionally investigated in name space where stocks are indexed by company names. In contrast, by indexing stocks based on their ranks in capitalization, we gain a different perspective of market dynamics in rank space. Here, we demonstrate the superior performance of statistical arbitrage in rank space over name space, driven by a robust market representation and enhanced mean-reverting properties of residual returns in rank space. Our statistical arbitrage algorithm features an intraday rebalancing mechanism for effective conversion between portfolios in name and rank space. We explore statistical arbitrage with and without neural networks in both name and rank space and show that the portfolios obtained in rank space with neural networks significantly outperform those in name space. 1...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Statistical arbitrage in multi-pair trading strategy based o

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Statistical arbitrage in multi-pair trading strategy based o
â€¢ **æª”æ¡ˆ**ï¼š `2024_Statistical arbitrage in multi-pair trading strategy based o.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, it is computed as the standard deviation of daily returns, scaled by a square root of trading days of S&P500 stocks in a year (252) ASD = s PN t=1(Rt âˆ’ Ë†R)2 (N âˆ’ 1) Â· ndays (2) where: â€“ Rt - Return on day t â€“ Ë†R - Average of daily returns â€“ ndays - number of trading days in a year â€¢ Information Ratio* (IR*) Sharpe ratio is the most popular measure of risk-adjusted returns, computed as a quotient of the difference between average portfolio returns and the risk-free rate and average standard deviation of the returns. Since the strategy tested in this paper is market-neutral, a risk-free rate is omitted. Moreover, all of the returns are compounded, thus the formula presented below has been used: IR âˆ— = ARC ASD (3) 5...

â€¢ **Abstract Reference**:
> The study seeks to develop an effective strategy based on the novel framework of statistical arbitrage based on graph clustering algorithms. The amalgamation of quantitative and machine learning methods, including the Kelly criterion, and an ensemble of machine learning classifiers have been used to improve risk-adjusted returns and increase the immunity to transaction costs over existing approaches. The study seeks to provide an integrated approach to optimal signal detection and risk management. As a part of this approach, innovative ways of optimizing take profit and stop loss functions for daily frequency trading strategies have been proposed and tested. All of the tested approaches outperformed appropriate benchmarks. The best combinations of the techniques and parameters demonstrated significantly better performance metrics than the relevant benchmarks. The results have been obtained under the assumption of realistic transaction costs, but are sensitive to changes in some key par...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> , including the data used, motivation for the chosen approach, feature selection, engineering, the classifiers ensemble building details, and the proposed decision and risk management optimizations. The fourth section presents all the metrics used in evaluating the strategies, then evaluates the proposed strategyâ€™s performance using those metrics against the relevant benchmarks and displays their equity curves. In the fifth section sensitivity analysis has been conducted, to measure the impact of change of key parameters on model performance. Finally, the sixth section contains the summary of the...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> is supported by the higher values of Calmar, Sortino, and modified information ratios obtained by various versions of the strategy which used the classifiers when compared to the one that didnâ€™t. The improvement seemed to stem from the successful reduction of the volume of unprofitable signals traded. The best results have been observed when using the weighted soft voting ensemble of classifiers. â€¢ RQ2: To what extent, does the implementation of transaction and risk management measures to influence the performance of the strategy? - After performing the sensitivity analysis, in which the impact of removing the implemented optimization has been measured, it can be concluded that their influence is limited. The addition of neither the Kelly criterion nor stop loss and take profit function modification has led to an increase in the Information Ratio*. However, the latter seems to have a moderate positive effect on downside and tail risk reduction. Thus the novel approach of multiplying the stop functions by the predicted probability of the signalâ€™s profitability as well as the time-variant factor seems to have potential for further examination. The very limited impact of the usage of the Kelly criterion is most likely caused by the very rudimentary and simple implementation. â€¢ RQ3: What is the sensitivity of the strategy to changes in transaction costs? - Sensitivity analysis has clearly demonstrated that the changes in transaction costs have a limited impact on strategy performance. Strategy can be considered robust to changes of that parameter, as doubling it resulted in only a 10% drop of Information Ratio*. â€¢ RQ4: To what extent does the change of weights in the classifiers ensemble influence the strategy- Changes of weights used in the construction of the ensemble have a measurable impact on the performance of the strategy. Sensitivity analysis suggests, that both equally weighted ensemble and selection of only the best classifier lead to results that are worse than weighted ensemble with the increased weight of the best-performing model. It needs to be acknowledged, that the study is constrained by certain limitations. Firstly, the data quality in the Yahoo finance database is certainly not perfect. Although the reproducing of the methodology of Cartea et. al. (2023) resulted in very similar values of the performance metrics, suggesting that the obtained results are representative, the extent of missing data has been noticeable, thus increasing the u

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2024 Temporal Representation Learning for Stock Similarities and 

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2024 Temporal Representation Learning for Stock Similarities and 
â€¢ **æª”æ¡ˆ**ï¼š `2024_Temporal Representation Learning for Stock Similarities and .pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Abstract not found....

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> to learn meaningful representations by solving pretext tasks that do not require explicit labels, making it particularly suitable for scenarios where labeled data is scarce or expensive to obtain. However, while these methods are useful for exploiting inductive biases in data used in any domain, their use for ï¬nancial time series data has been less explored. Time series data exhibits distinct characteristics, such as seasonality, trend, and interaction between stocks, which require careful consideration when designing SSL frameworks. Furthermore, the non-stationary nature of ï¬nancial markets necessitates the development of techniques that can adapt to distribution shifts and generalize well to future time periods. Most existing SSL methods focus on learning invariant representations (Chen et al. 2020, He et al. 2020), assuming that the data distribution remains stationary. However, this assumption does not hold in the context of ï¬nancial markets, where the underlying dynamics can change over time. To address these challenges, we combine techniques from SSL and the ï¬eld of temporal domain generalization. Our approach aims to learn general model representations that can adapt to temporal shifts over time, thereby enhancing the accuracy and robustness of ï¬nancial parameter estimation using learned embeddings. By incorporating temporal domain generalization into the SSL framework, we enable the model to learn representations that are not only informative but also resilient to...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Attention Factors for Statistical Arbitrage

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Attention Factors for Statistical Arbitrage
â€¢ **æª”æ¡ˆ**ï¼š `2025_Attention Factors for Statistical Arbitrage.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we propose theAttention Factor Model, a framework thatjointlylearns tradable arbitrage factors and arbitrage portfolio allocations in a computationally efficient manner. Our Attention Factors are conditional latent factors. The estimation objective is not to explain variation, but to construct profitable arbitrage strategies after trading costs. The attention mechanism learns embeddings of firm characteristics and allows to capture general dependencies of the factors on firm characteristics with complex interactions. A general sequence model learns time-series signals from the residuals of our attention factors with the joint objective of maximizing the net Sharpe ratio and explained variance. Figure 1 illustrates the conceptual structure. We perform a comprehensive empirica...

â€¢ **Abstract Reference**:
> Statistical arbitrage exploits temporal price differences between similar assets. We develop a framework tojointlyidentify similar assets through factors, identify mispricing and form a trading policy that maximizes risk-adjusted performance after trading costs. Our Attention Factorsare conditional latent factors that are the most useful for arbitrage trading. They are learned from firm characteristic embeddings that allow for complex interactions. We identify time-series signals from the residual portfolios of our factors with a general sequence model. Estimating factors and the arbitrage trading strategy jointly is crucial to maximize profitability after trading costs. In a comprehensive empirical study we show that our Attention Factor model achieves an out-of-sample Sharpe ratio above 4 on the largest U.S. equities over a 24-year period. Our one-step solution yields an unprecedented Sharpe ratio of 2.3 net of transaction costs. We show that weak factors are important for arbitrage ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> . First, for each asset, a replicating portfolio based on the attention factors is created, giving a residual mispricing. Second, a series of lagged residuals are used to construct the portfolio weights in the residual space, using a Long Convolution model for sequence modeling. Finally, the portfolio weights are mapped back to the asset space via a composition matrix, giving the next-period portfolio return. statistical factors model the factor portfolio weights and loadings as functions of characteristics. A prominent example is IPCA in [22] with ð¹ð‘¡ =ðœ” IPCA ð‘¡âˆ’1 ð‘…ð‘¡ , where the weights ðœ”IPCA ð‘¡âˆ’1 =ð‘‹ âŠ¤ ð‘–,ð‘¡âˆ’1 ðµ are a linear function of firm characteristics. This conditional structure allows exposures to evolve with firm attributes, linking the latent factors to observable fundamental information. However, the factors are still estimated to maximize explained variation in the cross-section. Challenges for Arbitrage Trading.The above factor models are not constructed with the objective to create profitable arbitrage portfolios. These models impose restrictive ad-hoc assumptions on the functional form of loadings and portfolio weights, and do not target factors based on a trading objective. Importantly, the...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> This paper develops an Attention Factor model for statistical arbitrage. We provide a one-step estimation framework for latent factors to identify similar assets and an arbitrage portfolio allocation based on time-series patterns. The two key innovations are the conditional latent attention factors to capture complex dependencies in firm characteristics and the one-step model estimation that maximizes portfolio performance after trading frictions. In extensive empirical analysis, we demonstrate that our arbitrage model sets the new standard in this literature with the best performance under realistic trading frictions.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 CTBench: Cryptocurrency Time Series Generation Benchmark

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 CTBench: Cryptocurrency Time Series Generation Benchmark
â€¢ **æª”æ¡ˆ**ï¼š `2025_CTBench: Cryptocurrency Time Series Generation Benchmark.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeopardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classification and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce CTBench, the first comprehensive TSG benchmark tailored for the cryptocurrency domain. CTBench curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A key innovation is a dual-...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> -based evaluation paradigm [3], we design a forecasting-centric task tailored to the nuances of cryptocurrency markets. Synthetic series are used to train forecasting models, which are then evaluated on real market data. Performance reflects how well the synthetic data preserves temporal and cross-sectional structures critical for downstream prediction. â€¢ Statistical Arbitrage Task: This task examines whether TSG models can capture tradable structures by reconstructing historical returns. The residuals from the reconstruction are modeled as mean-reverting signals and fed into statistical arbitrage strategies. Financial metrics on profitability and risk profiles evaluate whether the synthetic data reveal useful trading signals suitable for profitable trading. C3: We construct a holistic financial evaluation measure suite tailored to crypto trading realities. Regarding L3, to facilitate thorough and realistic financial analyses, CTBench incorporates a comprehensive suite of evaluation measures over diverse trading strategies (Â§3.3) spanning forecasting performance, rank-based predictive measures, key trading performance indicators, and critical risk assessment metrics (Â§3.4). C4: We perform systematic evaluations and distill actionable insights for TSG methods in crypto domains. We conduct extensive evaluations across various TSG models (Â§3.5). Through detailed...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> AND FUTURE WORK In this paper, we introduce CTBench, the first benchmark tailored for TSG in cryptocurrency markets. CTBench integrates a curated high-frequency crypto dataset, a dual-task evaluation framework encompassing Predictive Utility and Statistical Arbitrage, and a rich suite of financial metrics designed to assess both statistical fidelity and real-world viability. Through extensive empirical analysis, we uncover critical trade-offs across TSG families and offer practical guidance for deploying models under diverse market conditions. As a collaborative resource, CTBench aims to foster rigorous evaluation and drive innovation in crypto time series modeling. Moving forward, we plan to expand CTBench by incorporating new tokens, extending to cross-exchange data, and integrating more advanced TSG architectures. We are also exploring model ensembling and regime-aware switching to improve robustness and performance consistency. To further streamline experimentation, we intend to support automated evaluation and hyperparameter tuning, enhancing both efficiency and usability. 12

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Deep reinforcement learning for optimal trading with partial

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Deep reinforcement learning for optimal trading with partial
â€¢ **æª”æ¡ˆ**ï¼š `2025_Deep reinforcement learning for optimal trading with partial.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we study an optimal trading problem, where a trading signal follows an Ornsteinâ€“Uhlenbeck process with regime-switching dynamics. We employ a blend of RL and Recurrent Neural Networks (RNN) in order to make the most at extracting underlying information from the trading signal with latent parameters. The latent parameters driving mean reversion, speed, and volatility are filtered from observations of the signal, and trading strategies are derived via RL. To address this problem, we propose three Deep Deterministic Policy Gradient (DDPG)â€“based algorithms that integrate Gated Recurrent Unit (GRU) networks to capture temporal dependencies in the signal. The first, a one-step approach (hidDDPG), directly encodes hidden states from the GRU into the RL trader. The second and third...

â€¢ **Abstract Reference**:
> Reinforcement Learning (RL) applied to financial problems has been the subject of a lively area of research. The use of RL for optimal trading strategies that exploit latent information in the market is, to the best of our knowledge, not widely tackled. In this paper we study an optimal trading problem, where a trading signal follows an Ornsteinâ€“Uhlenbeck process with regime-switching dynamics. We employ a blend of RL and Recurrent Neural Networks (RNN) in order to make the most at extracting underlying information from the trading signal with latent parameters. The latent parameters driving mean reversion, speed, and volatility are filtered from observations of the signal, and trading strategies are derived via RL. To address this problem, we propose three Deep Deterministic Policy Gradient (DDPG)â€“based algorithms that integrate Gated Recurrent Unit (GRU) networks to capture temporal dependencies in the signal. The first, a one-step approach (hidDDPG), directly encodes hidden states f...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> that approximately solves the filtering model is trained in the previous step, while in the regression approach the estimate for the next value of the trading signalSt is added to the set of features fed to the RL agent. Hence, the first setup (and second method proposed) of the two-step approach is termedprob-DDPG and makes use of featuresGt = (St,It,Î¦ t,k)for the DDPG part, while the second setup (and third method overall) is termedreg-DDPGthroughout the paper, and employsGt = (St,It, ËœSt+1)as features to the DDPG. For the DDPG part we again set both Actor and Critic ANNs to be feed-forward fully connected neural networks as in the one-step approach. For the gradient descent steps we use the Weighted ADAM optimiser with a scheduler. The features of the DDPG algorithm are normalised in the domain[0,1]. The relevant parameters for the networks and the market model are reported in Table 1. 10 4...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Graph Learning for Foreign Exchange Rate Prediction and Stat

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Graph Learning for Foreign Exchange Rate Prediction and Stat
â€¢ **æª”æ¡ˆ**ï¼š `2025_Graph Learning for Foreign Exchange Rate Prediction and Stat.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper, we predict FX rates by leveraging the interrelationships among currencies and exploit FX StatArbs (FXSAs) involving three or more currencies, which arise in practice. Although the currencies in the triplet are theoretically interrelatedâ€”both among themselves and with interest rates (IRs)â€”no prior work has studied a graph learning (GL) method to exploit these relationships in FX rate prediction (FXRP). As multiple currencies exist globally, combinatorially many triplets can be formed, each inducing its own theoretical interrelations. It is difficult to represent such structured relationships in a tabular format and to employ canonical neural architectures, such as multi-layer perceptrons (MLPs). Moreover, the interest rate parity (IRP) implies there is an interaction be...

â€¢ **Abstract Reference**:
> We propose a two-step graph learning approach for foreign exchange statistical arbitrages (FXSAs), addressing two key gaps in prior studies: the absence of graph-learning methods for foreign exchange rate prediction (FXRP) that leverage multi-currency and currencyâ€“interest rate relationships, and the disregard of the time lag between price observation and trade execution. In the first step, to capture complex multi-currency and currencyâ€“interest rate relationships, we formulate FXRP as an edge-level regression problem on a discrete-time spatiotemporal graph. This graph consists of currencies as nodes and exchanges as edges, with interest rates and foreign exchange rates serving as node and edge features, respectively. We then introduce a graph-learning method that leverages the spatiotemporal graph to address the FXRP problem. In the second step, we present a stochastic optimization problem to exploit FXSAs while accounting for the observation-execution time lag. To address this proble...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> To determine daily FXSA trade lists, we propose a two-step GL approach: one for FXRP and the other one for FXSA. The first yields FXRP model ð‘“ð‘ƒ (Â·; ðœƒ ð‘ƒ ð‘¡ ), and the second yields FXSA model ð‘“ð‘† (Â·; ðœƒð‘† ð‘¡ ) for ð‘¡ âˆˆ [ ð‘¡1 : âˆž), where ð‘¡1 âˆˆ N is the date when the agent starts trading. Here, ðœƒ ð‘ƒ ð‘¡ and ðœƒð‘† ð‘¡ are trainable parameters. For each ð‘¡ âˆˆ [ ð‘¡1 : âˆž), ð‘“ð‘ƒ (Â·; ðœƒ ð‘ƒ ð‘¡ ) predicts bð‘‹ð‘¡ð‘– ð‘— for ð‘‹ð‘¡ð‘– ð‘— using available information up to time ð‘¡ âˆ’ 1. Then, ð‘“ð‘† (Â·; ðœƒð‘† ð‘¡ ) determines trading quantities ð‘¤ ð‘¡ð‘– ð‘— based on the same information and the predictions bð‘‹ð‘¡ð‘– ð‘— from ð‘“ð‘ƒ by time ð‘¡ âˆ’ ð‘¡exec, where ð‘¡exec âˆˆ ( 0, 1) denotes the time required to execute FX trades at ð‘¡. The agent simultaneously executes all trades ð‘¤ ð‘¡ð‘– ð‘— at ð‘¡, holds the...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> This paper proposes a two-stage GL approach to generate FXSAs, addressing two key limitations in the literature: (i) the absence Manuscript, August 2025, Under Review Hong and Klabjan Table 3: P-Values for MSE Comparison Between GNN and MLP Across Feature Combinations, Testing ð»0 : MSE(ð‘€ð¿ð‘ƒ ) â‰¤ MSE(ðºð‘ ð‘ ) vs. ð»1 : MSE(ð‘€ð¿ð‘ƒ ) > MSE(ðºð‘ ð‘ ). Final letters T, W, and P indicate the paired t-, Wilcoxon signed-rank, and permutation tests, respectively, applied when their assumptions hold. GNN FX FX/CV FX/IR FX/IR/CV MLP FX 0.000 W 0.000 W 0.000 W 0.000 W FX/CV 0.147 P 0.011 W 0.001 W 0.003 W FX/IR 0.000 W 0.000 W 0.000 W 0.000 P FX/IR/CV 0.113 W 0.005 P 0.000 W 0.005 T of GL methods for FXRP that leverage multi-currency and currencyâ€“IR relationships, and (ii) the disregard of the time lag between price observation and trade execution. In the first stage, we formulate FXRP as an edge-level regression problem on a discretetime spatiotemporal graph to capture complex multi-currency and currencyâ€“IR relationships. We present a GL method to address the FXRP problem, leveraging the spatiotemporal graph defined with currencies and exchanges as nodes and edges, respectively, where IRs serve as node features, and FX rates serve as edge features. In the second stage, we present a stochastic optimization problem to exploit FXSAs while accounting for the observation-execution time lag. To solve this problem, we propose a GL method that enforces constraints through projection and ReLU to maximize the information ratio, while utilizing the predictions from the FXRP method for the constraint parameters and node features. We prove our GL method satisfies the empirical arbitrage constraints. Our GL method for FXSA uses a graph in which nodes represent exchanges and edges encode their influences. Experimental results show our FXRP method achieves statistically significant improvements over non-graph methods. Moreover, our FXSA method achieves a 61.89% higher information ratio and a 45.51% higher Sortino ratio than the benchmark. Our approach presents a novel perspective on FX prediction and StatArbs through the lens of GL.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Market-Bench: Evaluating Large Language Models on Introducto

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Market-Bench: Evaluating Large Language Models on Introducto
â€¢ **æª”æ¡ˆ**ï¼š `2025_Market-Bench: Evaluating Large Language Models on Introducto.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We introduceMARKET-BENCH, a benchmark that evaluates large language models (LLMs) onintroductory quantitative trading tasksby asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategiesâ€”scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFTâ€”and models must produce code whose P&L, drawdown, and position paths match a verifiable reference implementation. We assess thirteen state-of-the-art models using a multi-round evaluation that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics), assigning failed outputs a duplicated-metrics baseline MAE. While most models reliably execute the simplest strategy (average Executable Passes of 4.08 out of 5 rounds), errors vary by orders of magnitude across models and tasks: Gemini 3...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> s tracked the liquidity that trades remove from the book and whether they persisted that liquidity correctly. Furthermore, the options delta dataset for Strategy 3 was generated using a simple random walk. 3.2 HIGH-LEVELSTRATEGYDESCRIPTIONS Strategy 1: Scheduled market-order execution on MSFT.Strategy 1 uses data from Databentoâ€™s Market-by-price L10 data for Microsoft (MSFT). At pre-specified timestamps in the data, the strategy sends a market order to either buy or sell various quantities of MSFT. Each market order takes liquidity from the current book, net of previous trades at that price level and potentially from several price levels at once. The strategy tracks: â€¢ Cash and MSFT position, â€¢ Realized P&L using FIFO accounting, â€¢ Unrealized P&L based on raw-book mid-prices, â€¢ An equity curve and maximum drawdown, â€¢ Synthetic-book statistics such as total size available and bid/ask VW AP post model trades. Strategy 2: Pairs mean-reversion on Coke and Pepsi stock.Strategy 2 is a pairs trading strategy between KO and PEP which uses L10 order book data from both. At each new book update for either symbol, the strategy calculates mid-prices for both symbols and creates a spread between them as a linear combination of the two. A rolling history of the spread is then used to calculate a mean and the z-score of the current value. The strategy has a position state (flat, long-spread, or short-spread). When the z-score exceeds an entry threshold, the strategy enters a mean-reversion...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> We present MARKET-BENCH, a benchmark for evaluating large language models on introductory quantitative trading tasks that require both accuracy and consistency. By analyzing 329 total attempts across three unique trading strategies, we find that: â€¢ Current models lack the capabilities to simulate and understand even basic trading strategies. â€¢ Many models can reliably produce executable backtests on simpler strategies, but a subset fails catastrophically on more complex ones. â€¢ Model error varies widely, especially on the most realistic Strategy 3, where small implementation differences can generate huge P&L and risk discrepancies. We hope MARKET-BENCHcan serve as a foundation for future work on large language models that are not just capable of describing strategies but also implementing them in a way that demonstrates deep understanding of market dynamics, risk, and trading mechanics.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Risk-sensitive Reinforcement Learning Based on Convex Scorin

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Risk-sensitive Reinforcement Learning Based on Convex Scorin
â€¢ **æª”æ¡ˆ**ï¼š `2025_Risk-sensitive Reinforcement Learning Based on Convex Scorin.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> We propose a reinforcement learning (RL) framework under a broad class of risk objectives, characterized by convex scoring functions. This class covers many common risk measures, such as variance, Expected Shortfall, entropic Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue, we consider an augmented state space and an auxiliary variable and recast the problem as a two-state optimization problem. We propose a customized Actor-Critic algorithm and establish some theoretical approximation guarantees. A key theoretical contribution is that our results do not require the Markov decision process to be continuous. Additionally, we propose an auxiliary variable sampling method inspired by the alternating minimization algorithm, which is convergent under certain con...

â€¢ **Abstract Reference**:
> We propose a reinforcement learning (RL) framework under a broad class of risk objectives, characterized by convex scoring functions. This class covers many common risk measures, such as variance, Expected Shortfall, entropic Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue, we consider an augmented state space and an auxiliary variable and recast the problem as a two-state optimization problem. We propose a customized Actor-Critic algorithm and establish some theoretical approximation guarantees. A key theoretical contribution is that our results do not require the Markov decision process to be continuous. Additionally, we propose an auxiliary variable sampling method inspired by the alternating minimization algorithm, which is convergent under certain conditions. We validate our approach in simulation experiments with a financial application in statistical arbitrage trading, demonstrating the effectiveness of the algorithm. Keywords: Risk-sensitive reinfo...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> of the proposed Actor-Critic RL algorithm, including the Bellman equation and the approximation of the proposed value function. Particularly, Sections 4.2 and 4.3 elaborate on the selection and sampling of the optimal auxiliary variable v in the RL algorithm. Section 5 applies the proposed methodology to a statistical arbitrage problem, comparing the performance of various risk measures through simulation...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Signature Decomposition Method Applying to Pair Trading

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Signature Decomposition Method Applying to Pair Trading
â€¢ **æª”æ¡ˆ**ï¼š `2025_Signature Decomposition Method Applying to Pair Trading.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper. By using a decomposition of the original signature, we extract segmented signature (a kind of transformation of LÂ´ evy area) for reflecting the interactive information and trend dynamics of multiple financial asset price sequences. The segmented signature, which possesses good interpretability and stability, can be seen as an effective nonlinear feature of path interactivity. According to this property, segmented signatures can be used naturally in pair trading, which leads to a strategy of pair trading capturing the complex relationships of assets. We use segmented signature as a filter in the pair trading strategy, which enhances the precision of the decision-making process, thus that validate the feasibility of segmented signature. In our empirical study, it is found th...

â€¢ **Abstract Reference**:
> High-frequency quantitative trading strategies have long been of significant interest in futures market. While advanced statistical arbitrage and deep learning enhance high-frequency data processing, they diminish opportunities for traditional methods and yield less interpretable, unstable strategies. Consequently, developing stable, interpretable quantitative strategies remains a priority in futures markets. In this study, we propose a novel pair trading strategy by leveraging the mathematical concept of path signature which serves as a feature representation of time series. Specifically, the path signature is decomposed into two new indicators: the path interactivity indicator segmented signature and the directional indicator covariation of increments, which serve as double filters in strategy design. Empirical experiments using minute-level futures data show our strategy significantly outperforms traditional pair trading, delivering higher returns, lower maximum drawdown, and higher...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> enhances interpretability and robustness while maintaining strong returns, demonstrating the potential of path signatures in financial trading. Key words: Rough Path, Signature Method, Quantitative Finance, Pair Trading 1 arXiv:2505.05332v2 [econ.GN] 16 Oct 2025 1 Introduction In financial market, robust, consistently profitable strategies and indicators are pursued by participators. Among them, arbitrage strategies are generally considered as a comparatively lower-risk investment approach, hedging most market risks through simultaneously buying and selling multiple financial assets (Dybvig & Ross, 1989; Yadav & Pope, 1990; Liew & Wu, 2013; Krauss, 2017). Pair trading (Vidyamurthy, 2004; Elliott et al., 2005) is one of the popular and widely used statistical arbitrages in trading stocks (Chen et al., 2019), futures and options (Draper & Fung, 2002). The core concept of pair trading involves trading the price spread between two correlated assets. When their prices spread deviates from the expected value, the pair trading strategy involves going long on one relatively undervalued asset while shorting the other relatively overvalued one and closing the positions for profit after the spread reverts to normal levels. Arbitrage opportunities typically arise in imperfect markets. However, as more market participants exploit these opportunities, the price spread tends to disappear quickly (Krauss, 2017). Whatâ€™s more, in highly complex financial markets, just capturing linear corre...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Statistical Arbitrage in Options Markets by Graph Learning a

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Statistical Arbitrage in Options Markets by Graph Learning a
â€¢ **æª”æ¡ˆ**ï¼š `2025_Statistical Arbitrage in Options Markets by Graph Learning a.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> in this paper, we develop a GL method and trading strategy. Although various machine learning approaches have been proposed in the options markets, they did not focus on direct identification of StatArbs. Specifically, Zapart (2003) treated StatArbs as an ancillary subject, and although their strategy was labeled as such, it remained substantially exposed to the canonical risk factors of Black and Scholes (1973). Horikawa and Nakagawa (2024), Franc Â¸ois et al. (2025) employed StatArbs solely as a lens through which to compare deep hedging and delta 1 arXiv:2508.14762v2 [q-fin.PR] 21 Aug 2025 2 hedging strategies, rather than as a phenomenon to be directly uncovered. Other machine learning studies in the options markets likewise fall short of addressing the direct identification of StatAr...

â€¢ **Abstract Reference**:
> . Statistical arbitrages (StatArbs) driven by machine learning has garnered considerable attention in both academia and industry. Nevertheless, deep-learning (DL) approaches to directly exploit StatArbs in options markets remain largely unexplored. Moreover, prior graph learning (GL)â€”a methodological basis of this paperâ€”studies overlooked that features are tabular in many cases and that tree-based methods outperform DL on numerous tabular datasets. To bridge these gaps, we propose a two-stage GL approach for direct identification and exploitation of StatArbs in options markets. In the first stage, we define a novel prediction target isolating pure arbitrages via synthetic bonds. To predict the target, we develop RNConv, a GL architecture incorporating a tree structure. In the second stage, we propose SLSAâ€”a class of positions comprising pure arbitrage opportunities. It is provably of minimal risk and neutral to all Black-Scholes risk factors under the arbitrage-free assumption. We also...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> N/A

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Statistical Arbitrage in Polish Equities Market Using Deep L

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Statistical Arbitrage in Polish Equities Market Using Deep L
â€¢ **æª”æ¡ˆ**ï¼š `2025_Statistical Arbitrage in Polish Equities Market Using Deep L.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> N/A

â€¢ **Abstract Reference**:
> We study a systematic approach to a popular Statistical Arbitrage technique of Pairs Trading. Instead of relying on2highly correlated assets, the latter one is substitute with the most accurate replication of the first with the use of so calledrisk-factors. Such factors can be determined by: Principal Components Analysis (PCA), actual market exchange traded funds (ETFs) or, as a authorial technique and thus our contribution to the literature, Long short-term memory networks (LSTMs). Residuals between the main asset and its replicationâ€™ returns are analysed on a basis of their potential mean-reversion properties. Trading signals are later generated for sufficiently fast mean-reverting portfolios to profit from any technical mispricings. Besides the...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> ling of residuals with Ornstein-Uhlenbeck process till trading signals generation procedure. They are followed by a separate section concerning specifics of each replicating technique with a general overview of the method and its application for our purposes. Throughout the entire thesis various examples are graphically made for better understanding of discussed topics. The final part of the paper concerns testing of the overall Pairs Trading strategy and of its presented variations. To keep the...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> s We presented a systematic approach to Pairs Trading â€” a popular Statistical Arbitrage technique. In the usual approach pair of similarly behaving assets is selected and trades are performed on an offset portfolio of the two- we decided to substitute the second equity with a linear combination of risk (systematic) actors replicating the first (main) one. The aim then was to analyse residuals of main asset and its corresponding systematic componentsâ€™ approximation. Based on their potential mean-reversion properties, trading signals were determined trying to profit from potential technical mispricings. The selection of risk factors was done using three techniques: creating them with Principal Components Analysis (PCA) from eigenvectors of returnsâ€™ empirical correlation matrix, using real market indices, and as a new method that was not covered in the literature of the subject yet: considering Long short-term memory networks (LSTMs). There are two main contributions made within the paper: re-defining and testing already discussed techniques of PCA and market indices [4] on a far less developed equities market of Poland; and, as already mentioned, the introduction of new deep learning based approach of deriving the risk factors. Applying techniques of Statistical Arbitrage presented by Avellaneda and Lee [4] to polish stock exchange required switching from around500stocksâ€™ portfolios to just60most influential companies gathered by two main indices:WIG20andmWIG40. Since the spectrum of exchange traded funds of market indices is also way narrower than in the US, some simplifications had to be made for the ETFs approach. Two sub-techniques were discussed: one using only existing funds to replicate and other relying on artificial ones created as direct copies of sector indices. Additional market factors such as transaction costs or risk free rate were also adjusted from 2008 paper[4] for better reality matching. Second contribution i.e. LSTM approach of replicating stocksâ€™ returns was constructed to check whether more flexibility can be gained by not limiting oneself to common market factors and instead creating unique portfolios on a basis of singular stocks. Network was trained to provide the most appropriate weightings of all companiesâ€™ shares to approximate returns of the main one. Since it is possibly the first documented use of LSTMs in this, exact context- some steps such as the search for the most optimal hyperparameters were skipped to focus on initial

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Stochastic factors can matter: improving robust growth under

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Stochastic factors can matter: improving robust growth under
â€¢ **æª”æ¡ˆ**ï¼š `2025_Stochastic factors can matter: improving robust growth under.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> In this paper we study an asymptotic growth-optimization problem under model uncertainty and ergodicity. Our focus is on an investor who seeks stability in (discounted) asset prices and trades âˆ—Department of Mathematics, ETH Zurich, balint.binkert@gmail.com â€ Department of Statistics, London School of Economics and Political Science, d.itkin@lse.ac.uk â€¡Department of Statistics, London School of Economics and Political Science, p.j.mangers-bastian@lse.ac.uk Â§Department of Mathematics, ETH Zurich, josef.teichmann@math.ethz.ch 1 arXiv:2512.24906v1 [q-fin.MF] 31 Dec 2025 on this stability persisting, which is commonplace in pairs trading and certain statistical arbitrage strategies. In practice, investors estimate asset volatilities, distributions of asset returns and obtain noisy factors tha...

â€¢ **Abstract Reference**:
> Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strategies obtained from portfolio optimization are very sensitive to them. To mitigate this wellknown phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price processX, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of admissible models allowsXto depend on a multivariate stochastic factorYand fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor processY. A principal motivation of this framework comes from pairs trading, whereXis the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certa...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> We work with a financial market that contains a risk-free numeraire asset, which is normalized to one, anddâ‰¥1 risky assets. Notice that the numeraire of our investment universe is chosen in such a way that ergodicity assumptions can reasonably hold true. We assume that the risky asset price processX= (X 1, . . . , Xd) takes values in an open connected setEâŠ‚R d. The processX 3 will depend on anm-dimensional stochastic factor processYtaking values in a connected open set DâŠ‚R m for somemâ‰¥1. We setF=EÃ—Dand in this paper will generically denote elements ofFbyz= (x, y) forxâˆˆEandyâˆˆD. Similarly, gradients of a function ofztaken in only the x-variable will be denoted byâˆ‡ x and in only they-variable byâˆ‡ y and the full gradient denoted, as usual, byâˆ‡. For a vectorvof sized+m, we will writev X andv Y for the vector consisting of the firstdand finalmcomponents ofvrespectively. We take a triple (c, p, b Y ) as inputs to the problem for functionsc:Fâ†’S d+m ++ ,p:Fâ†’(0,âˆž) andb Y :Fâ†’R m. Here, for anynâˆˆN,S n ++ is the cone of symmetric positive definite matrices of sizenÃ—n. We will canonically writecin block form as c(z) =  cX(z)c XY (z) cY X(z)c Y (z)  , wherec X(z)âˆˆS d ++,c Y (z)âˆˆS m ++ andc XY (z) =c âŠ¤ Y X(z) is a matrix of sizedÃ—m. We make the following assumptions on the regularity of the inputs. Assumption 2.1.There existsÎ³âˆˆ(0,1] such that (i)câˆˆC 2,Î³(F;S d+m ++ ), (ii)pâˆˆC 2,Î³(F; (0,âˆž)) is such that R F p(z)dz= 1, (iii)b Y âˆˆC 1,Î³(F;R m). Regarding the probabilistic structure, we will wo...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Time-Varying Factor-Augmented Models for Volatility Forecast

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Time-Varying Factor-Augmented Models for Volatility Forecast
â€¢ **æª”æ¡ˆ**ï¼š `2025_Time-Varying Factor-Augmented Models for Volatility Forecast.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose a model-agnosticFactorAugmented Volatility Forecast frameworkthat captures evolving market-wide volatility shocks with computational tractability. Unlike traditional factor approaches with static loadings, our method employs a time-varying factor model to extract latent factors directly from realized volatilities. This dynamic approach ensures that both the factors and their loadings adapt in real time to shifting arXiv:2508.01880v3 [q-fin.ST] 8 Oct 2025 ICAIF â€™25, November 15â€“18, 2025, Singapore, Singapore Zhang, Li, Mo, Chen market regimes. Compared to conventional multivariate techniques, our framework achieves this enhanced market awareness while preserving the parsimony of the base model, furthermore, introducing only a minimal number of extra parameters. The resu...

â€¢ **Abstract Reference**:
> Accurate volatility forecasts are vital in modern finance for risk management, portfolio allocation, and strategic decision-making. However, existing methods face key limitations. Fully multivariate models, while comprehensive, are computationally infeasible for realistic portfolios. Factor models, though efficient, primarily use static factor loadings, failing to capture evolving volatility co-movements when they are most critical. To address these limitations, we propose a novel, model-agnosticFactor-Augmented Volatility Forecast framework. Our approach employs a time-varying factor model to extract a compact set of dynamic, cross-sectional factors from realized volatilities with minimal computational cost. These factors are then integrated into both statistical and AI-based forecasting models, enabling a unified system that jointly models asset-specific dynamics and evolving market-wide co-movements. Our framework demonstrates strong performance across two prominent asset classesâ€”la...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> 4.1 Volatility Factor Analysis Across Assets 4.1.1 Time-Varying Factor Model.Lety ð‘¡ =(ð‘…ð‘‰ 1,ð‘¡, . . . , ð‘…ð‘‰ð‘,ð‘¡ ) denote the ð‘-dimensional vector of realized volatilities at time ð‘¡= 1, . . . , ð‘‡. Following the time-varying matrix factor model of [ 11], we posit the locally-smooth factor representation: yð‘¡ =Î› ð‘¡ fð‘¡ +ðœ€ ð‘¡ ,(4) wheref ð‘¡ âˆˆR ð‘˜ is the ð‘˜-dimensional latent factors, andðœ€ð‘¡ âˆˆR ð‘ is the idiosyncratic errors, assumed to be uncorrelated withfð‘¡ but allowed to exhibit weak serial dependence. To accommodate a wide range of potential smooth temporal variation, we specifyÎ›ð‘¡ âˆˆR ð‘Ã—ð‘˜ to be the time-varying loading matrix. Specifically, we modelÎ› ð‘¡,ð‘– = Î› ð‘–Â· (ð‘¡/ð‘‡) , whereÎ› ð‘–Â· (Â·) is an unknown smooth function of ð‘¡/ð‘‡ on [0, 1] for eachð‘–âˆˆ [ð‘]. LetÎ£ ð‘“ =E( fð‘¡ f âŠ¤ ð‘¡ ) andÎ£ ðœ€ =E(ðœ€ ð‘¡ ðœ€âŠ¤ ð‘¡ ). The second uncentered moment satisfies Î£ ð‘¦,ð‘¡ :=E(y ð‘¡ yâŠ¤ ð‘¡ )=Î› ð‘¡ Î£ ð‘“ Î› âŠ¤ ð‘¡ +Î£ ðœ€ .(5) Under mild regularity conditions, the rank -ð‘˜ signal component Î› ð‘¡ Î£ ð‘“ Î› âŠ¤ ð‘¡ implies that the topð‘˜ eigenvectors ofÎ£ ð‘¦,ð‘¡ span the column space ofÎ› ð‘¡ up to rotation, as established in [11]. In principle, one could estimateÎ£ ð‘¦,ð‘¡ using kernel smoothing in ð‘¡/ð‘‡ as in [11]. For practical implementation, we instead adopt arolling -window(uniform -kernel) approximation. Fix a window length ð‘› and define ð‘Šð‘¡ ={ð‘¡âˆ’ð‘›+ 1, . . . , ð‘¡}. We estimate the local covariance matrix by bÎ£ ð‘¦,ð‘¡ = 1 ð‘› âˆ‘ï¸ ð‘ âˆˆð‘Šð‘¡ yð‘ yâŠ¤ ð‘  = ð‘‡âˆ‘ï¸ ð‘ =1 ð¾ð‘› (ð‘ , ð‘¡)y ð‘ yâŠ¤ ð‘  , ð¾ ð‘› (ð‘ , ð‘¡)= 1 ð‘› 1{ð‘ âˆˆð‘Š ð‘¡ }. We extract the top-k eigenvectors Ë†u1,ð‘¡, . . . , Ë†uð‘˜,ð‘¡ of bÎ£ ð‘¦,ð‘¡. The estimate...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> This paper proposes a novel, model-agnostic Factor-Augmented Volatility Forecast framework, resolving the trade-off between computationally infeasible multivariate models, overly simplistic univariate models, and rigid static-factor models. By extracting a compact set of dynamic factors and their time-varying loadings directly from realized volatilities, our frasmework allows any baseline model to efficiently capture evolving market-wide co-movements. The frameworkâ€™s robust performanceâ€”evidenced by substantial gains in forecast accuracy and superior risk-adjusted returnsâ€”combined with its computational efficiency and interpretability, establishes it as a valuable and practical solution for dynamic risk management. Acknowledgments Elynn Chenâ€™s research is supported in part by the NSF Award 2412577.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---


# 2025 Toward Quantum Utility in Finance: A Robust Data-Driven Algo

## ðŸ“„ æ·±åº¦å­¸è¡“è«–æ–‡ç­†è¨˜ (Deep Academic Note)

### ðŸ“Œ åŸºç¤Žå…ƒæ•¸æ“š (Metadata)
â€¢ **æ¨™é¡Œ**ï¼š 2025 Toward Quantum Utility in Finance: A Robust Data-Driven Algo
â€¢ **æª”æ¡ˆ**ï¼š `2025_Toward Quantum Utility in Finance: A Robust Data-Driven Algo.pdf`
â€¢ **å¹´ä»½**ï¼š 2025
â€¢ **é¡žåˆ¥**ï¼š `stat_arb`
â€¢ **é–±è®€ç‹€æ…‹**ï¼š ðŸ”´ å¾…è®€ (Auto-Generated)

---

### ðŸŽ¯ ç ”ç©¶èƒŒæ™¯èˆ‡ç›®æ¨™ (Context & Objectives)
â€¢ **Research Gap & Purpose (Extracted)**ï¼š
> we propose applying the Graph-based Coalition Structure Generation algorithm (GCS-Q) [30] to the task of asset clustering using return correlation data. GCS-Q operates directly on the signed graph, avoiding lossy transformations. It starts with all assets grouped together and iteratively partitions the graph via minimum cuts, maximizing intra-cluster weights at each step and dynamically determiningk. This approach is well-suited to the signed graph setting where classical solvers offer only heuristic workarounds. The minimumcut subproblem is formulated as a QUBO and solved on D-Waveâ€™s quantum annealer, leveraging its ability to explore exponentially large solution spaces efA Robust Quantum Algorithm for Asset Clustering 3 ficiently. We validate GCS-Q on both synthetic and real-wo...

â€¢ **Abstract Reference**:
> .Clustering financial assets based on return correlations is a fundamental task in portfolio optimization and statistical arbitrage. However, classical clustering methods often fall short when dealing with signed correlation structures, typically requiring lossy transformations and heuristic assumptions such as a fixed number of clusters. In this work, we apply the Graph-based Coalition Structure Generation algorithm (GCS-Q) to directly cluster signed, weighted graphs without relying on such transformations. GCS-Q formulates each partitioning step as a QUBO problem, enabling it to leverage quantum annealing for efficient exploration of exponentially large solution spaces. We validate our approach on both synthetic and real-world financial data, benchmarking against state-of-the-art classical algorithms such as SPONGE andk-Medoids. Our experiments demonstrate that GCS-Q consistently achieves higher clustering quality, as measured by Adjusted Rand Index and structural balance penalties, ...

---

### ðŸ›  ç ”ç©¶æ–¹æ³•è«– (Methodology - Extracted Candidate)
â€¢ **Model / Framework**:
> We consider a set of financial assetsA={a 1, a2, . . . , an}, whose historical returns over a rolling window are denoted byr i(t)for asseta i at timet. The pairwise Pearson correlation coefficients [21] are computed as: Ïij = Cov(ri, rj) Ïƒri Ïƒrj , Ï ij âˆˆ[âˆ’1,1].(2) These correlations define a signed, weighted, undirected graphG= (V, E, w), where each vertexv i âˆˆVuniquely corresponds to asseta i, and each edge (vi, vj)âˆˆEis weighted byw ij =Ï ij. In contrast to traditional clustering methods that transform correlations into non-negative distances [7], we retain the signed correlations, thereby preserving both positive (co-movement) and negative (anti-correlation) relationships. We apply the GCS-Q algorithm, previously developed for coalition structure generation in graphs [30], to this asset clustering setting. The core contribution lies in adapting GCS-Q to leverage its ability to operate directly on the signed graphs without resorting to heuristic transformations. Formally, GCS-Q aims to find a coalition structureÎ ={C 1, C2, . . . , Ck}, a partition ofVinto disjoint subsets, that maximizes the intra-cluster edge weights: max Î  X CâˆˆÎ  X i,jâˆˆC i<j wij.(3) This objective promotes grouping of assets with strong positive correlations while implicitly disincentivizing inclusion of negatively correlated pairs within the same cluster. The GCS-Q algorithm proceeds iteratively, starting with the full graphG0 = G. At each iterationt, it computes a minimum cut(St, Â¯St)on the current...

---

### ðŸ“Š çµæžœèˆ‡è¨Žè«– (Results & Discussion)
â€¢ **Conclusion / Key Findings**:
> Conclusion not found.

---

### ðŸ§  æ·±åº¦è©•æž (Synthesis & Critique)
> *[Pending Human Review]*

---

### ðŸš€ è¡Œå‹•æ¸…å–® (Action Items)
- [ ] Verify methodology relevance.
- [ ] Check if this paper belongs to the "Deep Read" list?

---
