# History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis

Haochong Xia _[1,*]_, Yao Long Teng _[1,*]_, Regan Tan _[1]_, Molei Qin _[1]_, Xinrun Wang _[2]_, Bo An _[1,][†]_

1 _College of Computing and Data Science, Nanyang Technological University, Singapore_
2 _School of Computing and Information Systems, Singapore Management University, Singapore_


_{_ HAOCHONG001, yaolong001, rtan072, molei001 _}_ @e.ntu.edu.sg, xrwang@smu.edu.sg, boan@ntu.edu.sg



_**Abstract**_ **—In quantitative finance, the gap between training**
**and real-world performance—driven by concept drift and distri-**
**butional non-stationarity—remains a critical obstacle for building**
**reliable data-driven systems. Models trained on static historical**
**data often overfit, resulting in poor generalization in dynamic**
**markets. The mantra “History Is Not Enough” underscores the**
**need for adaptive data generation that learns to evolve with**
**the market rather than relying solely on past observations. We**
**present a drift-aware dataflow system that integrates machine**
**learning–based adaptive control into the data curation process.**
**The system couples a parameterized data manipulation module**
**comprising single-stock transformations, multi-stock mix-ups,**
**and curation operations, with an adaptive planner–scheduler that**
**employs gradient-based bi-level optimization to control the sys-**
**tem. This design unifies data augmentation, curriculum learning,**
**and data workflow management under a single differentiable**
**framework, enabling provenance-aware replay and continuous**
**data quality monitoring. Extensive experiments on forecast-**
**ing and reinforcement learning trading tasks demonstrate that**
**our framework enhances model robustness and improves risk-**
**adjusted returns. The system provides a generalizable approach**
**to adaptive data management and learning-guided workflow**
**automation for financial data.**
_**Index Terms**_ **—Adaptive dataflow, workflow automation, finan-**
**cial time-series, data augmentation,**


I. INTRODUCTION


Machine learning techniques have been widely applied
to quantitative finance research, encompassing tasks such
as trading [1] and forecasting [2]. The success of machine
learning in quantitative finance largely stems from its ability to leverage the vast amount of financial data generated
by markets [3]. However, the constantly changing dynamics
of the market present significant challenges. A fundamental
assumption in applying machine learning techniques is that
the data is independent and identically distributed (i.i.d.).
When this i.i.d. assumption does not hold true, machine
learning models tend to overfit the training data, which in
turn reduces their robustness when applied to unseen data.
The financial market, shaped by complex trader interactions,
naturally evolves, leading to concept drift— inconsistency in
the joint probability distribution between time _t_ and _t_ + _k_
as _Pt_ ( _X, Y_ ) _̸_ = _Pt_ + _k_ ( _X, Y_ ) _, k >_ 0, where _X_ is the feature


_∗_ Equal contribution.

_†_ Corresponding author.



and _Y_ is the target variable. For data-driven financial systems, such drift is not only a modeling challenge but also
a data management problem: pipelines trained on static data
lack mechanisms to adapt to distributional shifts over time.
Addressing this gap requires an adaptive dataflow capable of
managing evolving financial data streams.
Data manipulation techniques, such as data augmentation,
play a crucial role in enhancing the robustness and generalization of machine learning models by expanding the
training dataset to cover a broader range of potential market
conditions [4]. Agent-based models [5] and deep generative
models [6] require significant computational resources and
complex modeling. In contrast, data augmentation is simple and efficient. However, unlike in computer vision (CV),
data augmentation is not yet extensively integrated into the
quantitative finance pipeline, even though it can address both
aleatoric and epistemic uncertainties [7], [8]. One of the
key challenges is the absence of a commonly agreed-upon
benchmark for augmentation operations in time-series data,
particularly in the context of financial data. This gap exists
because augmentation can easily compromise the fidelity and
correlations inherent in financial data. To address this, we
design a parameterized data manipulation module that incorporates domain-specific constraints (e.g., K-line consistency,
cointegration, and temporal non-stationarity) through singlestock transformations, curation steps, multi-stock mix-ups, and
interpolation. This transforms augmentation from a heuristic
preprocessing step into a controllable and auditable dataflow
component that generates diverse yet faithful data.
Given the diversity in data, models, and tasks in quantitative finance, developing augmentations for varying use
cases requires domain expertise and meticulous fine-tuning,
making adaptation challenging. Existing adaptive approaches
to data handling and augmentation [9]–[11] address distributional bias in different ways: some rely solely on resampling
existing data without generating new samples, while others
employ augmentation strategies designed for fixed and stationary datasets. However, none of these methods continuously
adapt their transformation policies as data and model states
evolve. As a result, they are insufficient for non-stationary
financial environments where concept drift is persistent and
dynamic. From a data-system perspective, current pipelines
lack a controller that can automatically regulate transformation


operations based on feedback from downstream learning tasks.
To fill this gap, we introduce a learning-guided planner that
operates in the validation loop of the task model. This controller comprises an adaptive planner and a pacing scheduler
which monitors both model state and data quality to determine
optimal transformation probabilities and intensities. In effect, it
serves as an autonomous workflow manager that dynamically
adjusts data synthesis operations.
In the spirit of “History Is Not Enough,” we present a
drift-aware adaptive dataflow system that unifies financial data
synthesis, augmentation scheduling, and learning-based feedback control within a single workflow architecture. The system
integrates the parameterized data manipulation module with a
machine-learning-driven planner–scheduler that continuously
optimizes transformation policies based on validation performance and overfitting signals. Together, these components
constitute an adaptive data pipeline capable of improving data
quality, diversity, and downstream model robustness under
concept drift. To the best of our knowledge, this work provides
the first learning-guided dataflow architecture tailored for
financial time-series management. Our key contributions are
summarized as follows:

1) **Adaptive dataflow framework.** Motivated by empirical
evidence of strong concept drift in financial data, we design a unified _adaptive dataflow system_ that jointly _gen-_
_erates_, _curates_, and _schedules_ augmented data through a
machine learning–based planner–scheduler architecture.
2) **Financially grounded synthesis module.** We develop
a parameterized data manipulation module _M_ that embeds financial priors into the transformation operations
to ensure both realism and statistical diversity of the
synthesized data.
3) **Learning-guided augmentation control.** Our system
employs a bi-level optimization scheme where an adaptive planner and an overfitting-aware scheduler dynamically coordinate manipulation strength and proportion
of data to be manipulated according to model feedback.
This enables self-adjusting data synthesis and curriculum
pacing under concept drift, aligning data management
decisions with learning dynamics.
4) **Empirical and systemic effectiveness.** Experiments
on forecasting and reinforcement learning trading tasks
show consistent gains in risk-adjusted performance
while preserving data fidelity across various models and
two mainstream financial markets.


II. BACKGROUND AND RELATED WORKS


Data augmentation techniques are important; however, they
are less explored in time-series analysis. The method proposed by [12] is specifically designed for non-stationary timeseries. However, it requires the data to be periodic and is
specifically designed for contrastive learning. There is still a
lack of a benchmark for augmentation techniques for timeseries, especially financial time-series data. Data augmentation
techniques can be use-case specific, and finding the optimal
policies requires domain knowledge and fine-tuning. Fixed



policies often suffer from insufficient randomness. Conversely,
automated augmentation like AutoAugment [13] utilizes reinforcement learning to identify suitable augmentation policies,
but requires extensive computational resources and restricts
the randomness in its policies.
Curriculum learning initiates training with simple patterns
and progressively moves to more complex ones. Empirical
evidence has shown that curriculum learning [14] can enhance generalization. Recent works, such as AdaAug [10]
and MADAug [11], have introduced the idea of curriculum learning accompanied by data augmentation techniques.
AdaAug learns adaptive augmentation policies in a class and
instance dependent manner. On top of this, MADAug applies
a monotonic curriculum to introduce the augmented data to
the training process. These methods are designed to learn
robust image features. Consequently, there remains a lack of
workflows that can generate effective augmentation policies
tailored to the challenges of time-series data, particularly in
the financial domain.
Generating augmented data has been an important way to
expand historical data. Agent-based modeling [4], [5], [15]
generates the data stream with the actions of autonomous
agents. However, these models rely on the empirical behavior
models of agents in the market which are subjective and computationally expensive, suffering from complex parameter tuning and thus lack generalizability [16]–[18]. Deep generative
models generate data from an underlying distribution learned
from historical data. TimeGAN [19] combines an autoencoder
and GAN in a two-stage training to capture global and local
temporal patterns. Diffusion-TS [20] integrates seasonal-trend
decomposition into a transformer-based diffusion model with a
Fourier reconstruction loss, enabling interpretable multivariate
time-series generation. However, deep generative models need
complex training and have less explainable evaluation metrics,
complicating their integration into data management pipelines.


III. PRELIMINARIES


_A. Definitions_


**Financial Data and K-line Representation.** Financial data
are organized as sequences of _K-line_ (candlestick) tuples


_xt_ = [ _Ot, Ht, Lt, Ct, Vt_ ] _,_ (1)


where _Ot_, _Ht_, _Lt_, and _Ct_ denote the open, high, low, and
close prices within an interval, and _Vt_ is the traded volume.
A valid K-line must satisfy the consistency constraint


_Lt ≤_ min( _Ot, Ct_ ) _≤_ max( _Ot, Ct_ ) _≤_ _Ht,_ (2)


preserving market realism. These _K-line features_ encapsulate
short-term momentum, volatility, and asymmetry of price
movements. They serve as the core components of both the
forecasting input _X_ and the RL state _st_ . Our data-manipulation
module (Section IV) explicitly enforces these constraints during augmentation to ensure economic plausibility.

**Time Series Forecasting.** Let _X_ = _{xt ∈_ R _[d]_ _}_ _[T]_ _t_ =1 [denote a]
multivariate financial time series, where _xt_ collects _d_ market


(a) MCD (b) Weather (c) Electricity (d) ETT


(e) MCD (f) Weather (g) Electricity (h) ETT


Fig. 1: t-SNE visualizations to assess concept drift. Top row: t-SNE plot for _P_ ( _Y |X_ ), where the x-axis represents the feature
_X_ and the y-axis represents the daily return as the _Y_ . Bottom row: t-SNE plot for _P_ ( _X_ ). Orange dots mark the data in the
training set, while blue dots mark the data in the test set.



features at timestamp _t_ . Given a look-back window of length
_L_, a forecasting model

_fθ_ : R _[d][×][L]_ _→_ R _[d][′]_ (3)


predicts the next-period target ˆ _yt_ = _fθ_ ( _xt−L_ +1: _t_ ). The training
objective minimizes the expected loss


min _θ_ E( _xt,yt_ ) _∼D_ [ _ℓ_ ( _fθ_ ( _xt−L_ +1: _t_ ) _, yt_ ) ] _,_ (4)


where _ℓ_ ( _·_ ) is the mean squared error (MSE).
The prediction target _yt_ is defined as the _one-step close-to-_
_close return_ :
_yt_ = _[C][t]_ [+1] _[ −]_ _[C][t]_ _,_ (5)

_Ct_


where _Ct_ and _Ct−_ 1 denote the closing prices at time _t_ and
_t−_ 1, respectively. This formulation captures short-term price
dynamics and aligns with standard forecasting objectives in
quantitative finance tasks.


**Reinforcement Learning (RL) for Trading.** We formalize
the trading environment as a Markov decision process (MDP)


_M_ = ( _S, A, P, r, γ_ ) _,_


where:

_• State space S_ : _st_ = [ _xt−L_ +1: _t, pt_ ] concatenates recent
features and the current position _pt_ .

_• Action space A_ : discrete _{−_ 1 _,_ 0 _,_ 1 _}_ for _sell, hold, buy_ .

_• Transition probability function P_ ( _st_ +1 _|st, at_ ): follows
market evolution _xt_ +1 _∼_ _PX_ ( _·|xt_ ).

_• Reward function:_


_rt_ = _pt−_ 1 _rt_ [mkt] _−_ _c |_ ∆ _pt| ._ (6)

where _rt_ [mkt] = _Ct_ +1 _C−t_ _Ct_ is the market return and _c_ the

transaction-cost ratio.
The agent _πθ_ ( _at|st_ ) maximizes

_J_ ( _πθ_ ) = E _πθ_        - _[T]_        - _[ −]_ [1] _γ_ _[t]_ _rt_        - _,_ (7)


_t_ =0



with value function


_Q_ _[∗]_ ( _s, a_ ) = _r_ ( _s, a_ ) + _γ_ E _s′∼P_ [ max (8)
_a_ _[′][ Q][∗]_ [(] _[s][′][, a][′]_ [) ]] _[,]_

and optimal policy _π_ _[∗]_ ( _s_ ) = arg max _a Q_ _[∗]_ ( _s, a_ ). This setting
aligns with the RL trading experiments in Table III.


_B. Observation of Concept Drift in Financial Data_


We investigate concept drift in financial data and compare
it with time-series datasets: Weather [1], Electricity [2], and ETT

[21]. To examine the concept drift in _Pt_ ( _X, Y_ ) = _Pt_ ( _Y |X_ ) _×_
_Pt_ ( _X_ ), we visualize _Pt_ ( _Y |X_ ) and _P_ ( _X_ ) respectively. For
financial data, the _X_ comprises market prices and features,
while the _Y_ is the next day’s return of the close price. We use a
t-SNE [22] plot to mark training and testing data with different
colors. Similarly, we use another t-SNE plot to visualize
_Pt_ ( _X_ ) over time. As shown in Fig. 1, when compared to the
stock price of McDonald’s (MCD), the benchmark datasets
exhibit a more overlapping distribution for both _P_ ( _Y |X_ ) and
_P_ ( _X_ ). This indicates that the stock data demonstrate a more
evident concept drift compared to other benchmarks.


_C. Validation–Test Proximity_


While the t-SNE visualization qualitatively illustrates concept drift between training and test sets, our method assumes that the validation data provide a closer approximation to the near-future test distribution than the historical training data. To verify this assumption, we perform a
quantitative proximity analysis on both datasets using three
statistical distance metrics—Population Stability Index (PSI),
Kolmogorov–Smirnov (K–S) statistic, and Maximum Mean
Discrepancy (MMD)—computed for Train–Test and Validation–Test pairs. The metrics are given by:



PSI =



_k_




�( _pi −_ _qi_ ) ln - _pi_

_qi_

_i_ =1



_qi_




_,_ (9)



1https://www.bgc-jena.mpg.de/wetter/
2https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014


**Task Model Training**

**Planner Inference**


**Planner**
**Training**



Proportion _α_
Scheduler



_**Loss**_ Valid



1- _α_



Split Merge

_**Data**_ Train



Split



~

_**Data**_ Train


Task Model

_f_ _θ_



_α_



_α_



Data Manipulation

Module _M_



operation choosing

probability _p_



Planner _gΦ_



manipulation

strength _λ_


_**Loss**_ Valid



3



Update _Φ_



~~



_**Data**_ Train _**Data**_ Train 1 Update _θ_ _[']_



Data Manipulation



_**Data**_ Train



Module _M_



2 _**Data**_ Valid to _f_ _θ'_



Fig. 2: The workflow of training the planner and task model to learn a policy of controlling the data manipulation module
with the validation loss of the task model. The training step of the planner is marked with (1), (2), (3), and _fθ_ _[′]_ is a copy of
_fθ_ . The fire icon marks the flow where parameters are updated.



where _pi_ and _qi_ denote the proportions of observations falling
into bin _i_ for the baseline and target distributions, respectively,
and _k_ is the total number of bins.


_D_ KS = sup (10)
_x_ _[|][F]_ [1][(] _[x]_ [)] _[ −]_ _[F]_ [2][(] _[x]_ [)] _[|][,]_


where _F_ 1( _x_ ) and _F_ 2( _x_ ) are the empirical cumulative distribution functions (CDFs) of the two samples, and sup _x_ denotes
the maximum difference over all _x_ .


MMD [2] ( _F, U, V_ ) = E _u,u′∼PU_ [ _k_ ( _u, u_ _[′]_ )] + E _v,v′∼PV_ [ _k_ ( _v, v_ _[′]_ )]

_−_ 2 E _u∼PU_ _,v∼PV_ [ _k_ ( _u, v_ )] _,_ (11)


where _k_ ( _·, ·_ ) is a positive-definite kernel function, and _PU_ and
_PV_ denote the two probability distributions being compared.
In our experiments, we use the Radial Basis Function kernel.
A higher PSI or MMD, or a larger K–S statistic, indicates a
greater degree of distributional shift between the baseline and
target samples.
We combine all market features with the target as a
multi-variant time-series data. For the stocks dataset (daily,
2000–2024), we adopt a rolling-year protocol where each
fold incrementally extends the training horizon by one
year, splitting all samples from 2000 to (2010 + _k_ ) into
**Train/Validation/Test** subsets with ratios of 0.6/0.2/0.2. For
the crypto dataset (hourly, 2023-09-27 to 2025-09-26), we
apply a higher-frequency rolling protocol, expanding the
training horizon by one month per fold while maintaining
the same 0.6/0.2/0.2 chronological split. In each fold, PSI,
K–S, and MMD are evaluated between the Train–Test and
Validation–Test segments on feature distributions standardized



TABLE I: Average distributional distances between Train–Test
and Validation–Test sets across two datasets. Lower values
indicate that the validation distribution is closer to the test
distribution than the distribution.


**Train–Test** **Validation–Test**
**Dataset**
_PSI_ _K–S_ _MMD_ _PSI_ _K–S_ _MMD_


Stocks (2000–2024, daily) 12.62 0.7415 0.7528 **9.075** **0.6367** **0.6177**
Crypto (2023–2025, hourly) 4.396 0.2988 0.1841 **2.867** **0.2680** **0.1781**


using statistics computed from the training set. Across both
markets and temporal granularities, the results in Table I
consistently show that Dist(Val _,_ Test) _<_ Dist(Train _,_ Test),
confirming that the validation window statistically resembles
the near-future test distribution more closely than the historical
training data. This quantitative evidence supports the use of
validation feedback to guide our adaptive augmentation and
planner updates in Section IV-C.


_D. Formulation of the Adaptive Control Objective_

To mitigate the uncertainty caused by strong concept drift,
we manipulate the training data to enhance generalization.
Specifically, let _fθ_ ( _x_ ) denote the task model. To best estimate
the generalization ability of the model [10], the objective for
learning the task model is:


min _L_ val( _fθ, D_ valid) _,_ (12)


where _L_ val is the validation loss, and _D_ valid is the validation
dataset. Let _D_ train denote the training dataset of the task model.
Let _M_ denote the manipulation module, we have ˜ _x_ train _←_
_M_ ( _x_ train) for _x_ train _∈_ _D_ train. Our objective is to develop an
adaptive _M_ that is simple yet effective in addressing the poor
generalization caused by concept drift in financial data.


_E. Augmentation Operations_


We introduce the augmentation operations we will be using
in our data manipulation module _M_ .
**Single stock transformation operations.**


_•_ **Robustness Enhancement:** Introduces controlled variations suited for noisy financial data. _Jittering_ adds
noise to improve signal discrimination. _Scaling_ mitigates
volatility-induced magnitude bias, and _Magnitude Warp-_
_ing_ [23] applies non-linear distortions common in price
dynamics with cubic spline interpolation.

_•_ **Structural Variation:** _Permutation_ preserves local continuity but breaks strict sequencing, reflecting the partially
stochastic nature of market evolution.

_•_ **Decomposition and Recombination:** _STL Augmenta-_
_tion_ [24] decomposes series into trend, seasonality, and
residuals—components naturally present in financial time
series—and bootstraps residuals to better model regime
shifts and non-stationarity.

**Multi-stock mix-up operations.**


_•_ **Segment Replacement Operations:** Replace segments
of one stock with another, introducing local disruptions.
_Cut Mix_ replaces a portion of one stock with another.

_•_ **Weighted Average Operations:** Combine data using
weighted averages, creating smoother transitions between
points. _Linear Mix_ linearly combines two stocks.

_•_ **Frequency Domain Operations:** Combine underlying
frequency components to capture cyclical patterns. _Am-_
_plitude Mix_ mixes the amplitude of the Fourier transform
of two stocks which is essential for modeling cyclical
patterns, such as seasonal effects and market cycles.
Demirel and Holz (2024) combine significant frequencies
of two stocks by mixing their phases and magnitudes.


IV. METHOD


In this section, we first describe the overall workflow,
outlining how data is manipulated through the workflow. Next,
we introduce the parameterized data manipulation module _M_,
which implements financially grounded operations to enhance
data diversity while preserving realism and traceability. Finally, we detail the learning-guided controller, composed of
a curriculum-based planner and an overfitting-aware scheduler, which continuously regulates manipulation strength and
proportion of data to be manipulated based on validation
feedback. This controller closes the loop between data curation
and model learning, enabling automated quality assurance and
provenance tracking within a reproducible workflow.


_A. Overall Workflow_


To optimize the objective in equation (12) effectively with
the guidance of gradients, we propose the workflow shown in
Fig. 2. The data manipulation module _M_ generates augmented
training samples ˜ _x_ train with operation choosing probability
matrix _p_ and manipulation strength parameter _λ_ . To adaptively
control _M_, we introduce a trainable planner _g_ (( _fθ, xi_ ); _ϕ_ ),
which learns the policy _πϕ_ ( _p, λ|f, xi_ ) while the scheduler



determines the proportion of data to be manipulated _α_ using a heuristic algorithm. Additionally, we interleave taskmodel updates with planner updates on validation feedback,
while provenance hooks (policy, probabilities, manipulation
strengths, proportion of data to be manipulated) are persisted
to enable exact replay.
In order to optimally control the data manipulation module
_M_ for each training sample _x_ train, inspired by AdaAug and
MADAug, we formulate the learning of the adaptive augmentation curriculum as a bi-level optimization problem:


min _Lval_ ( _fθ, x_ valid) _, x_ valid _∈_ _D_ valid
_ϕ_



s.t. _θ_ = arg min _Ltrain_ ( _fθ,_ ˜ _x_ train)
_θ_

_x_ ˜train = _M_ ( _x_ train _, α, p, λ_ )



(13)



where the planner and the task model are trained alternately
with their respective objectives.


_B. Parameterized Data Manipulation Module_


Unlike simply aggregating existing augmentation operations
and applying them directly to financial data, the proposed
data manipulation module _M_ is designed as a **parameterized**
**synthesis module** specifically tailored to the statistical and
structural properties of financial time series. Each internal
operation functions as a low-level augmentation primitive,
while the _method_ lies in how these operations are integrated,
parameterized, and coordinated by the operation choosing
probability matrix _p_ and manipulation strength factor _λ_ to
preserve financial validity while introducing realistic diversity.
Rather than serving as a static augmentation toolkit, _M_
provides a controllable mechanism for generating diverse yet
high-fidelity financial data by accounting for temporal dependencies, cross-asset correlations, and market constraints such
as K-line consistency and non-stationarity. Given a training
sample _x_ train with the shape (Timestamps _,_ Stocks _,_ Features),
the manipulation is guided by _p_ and _λ_ as shown in Fig. 3.
For the _i_ -th operation among _n_ single-stock transformations
and the _j_ -th operation among _m_ multi-stock mix-up operations, we have [�] _i_ _[n]_ =1 - _mj_ =1 _[p][ij]_ [ = 1][ and] _[ λ][ij][ ∈]_ [[0] _[,]_ [ 1]][.]
The transformation and mix-up layers introduce controlled
diversity from different market perspectives, while the curation, normalization, and interpolation layers enforce economic
plausibility and maintain consistency with real-world financial
dynamics. Financial time series exhibit strong temporal dependence, cross-asset co-movement, and price-based constraints
(e.g., K-line consistency, non-stationarity, and heavy-tailed
noise), and our module explicitly encodes these properties
through four tightly coupled components, as illustrated in
Fig. 3. In our parameterized data manipulation module _M_,
each primitive is gated by financial integrity constraints and
normalization/denormalization checks, turning augmentation
into curated synthesis with auditable parameters.
**Transformation Layer.** We apply single-stock transformation
operations as introduced in Section III-E to the raw input
feature to manipulate each stock feature independently. The
operations are done with the data in its original value, as


**Data Manipulation Module Workflow**



Denormalization



Synthetic
Input Data



Strength λ
Probability

p



(Time, Stock, Feature) (Time, Stock, Feature) (Time, Stock, Feature) (Time, Stock, Feature)

**Influenced Dimensions**



Curation & Data
Transformation Mix-up Interpolation
Normalization



Fig. 3: The proposed data manipulation module. The manipulated dimension of data is marked with the respective color.



we will apply the following curation step in the original
space to ensure fidelity. These operations are target-invariant
to preserve the fidelity of financial data. Operations are parameterized with:


_• Jittering_ : Manipulation strength _λ_ controls the standard
deviation of the perturbation.

_• Scaling_ : _λ_ determines its amplitude.

_• Magnitude Warping_ [23]: _λ_ controls its warp intensity.

_• Permutation_ : _λ_ controls how many parts the sequence is
divided into before reordering.

_• STL Augmentation_ [24]: _λ_ controls its period.


**Algorithm 1** Mix-up Target Stock Sampling


1: **Input:** Source stock _a_ ; mix manipulation strength _λ ∈_

[0 _,_ 1]; cointegration _p_ -values **p** = _{paj}j_ ; candidate count
_k_

2: **Output:** Target stock index _b_
3: Exclude self: _paa ←_ ∅; define candidate set _C_ = _{j ̸_ = _a |_
_paj_ valid _}_ .
4: Set _β ←_ (1 _−_ _λ_ ) if _λ_ _≤_ 0 _._ 5 else _λ_ .
5: For each _j ∈C_, compute score



_Sj_ =




- _−_ _p_ _[β]_ _aj_ _[,]_ _λ_ _≤_ 0 _._ 5 (favor stronger cointegration)

_p_ [1] _aj_ _[/β][,]_ _λ>_ 0 _._ 5 (favor weaker cointegration)



6: Select top- _k_ indices _T_ by _Sj_ and apply a softmax over
_{Sj}j∈T_ to form probabilities _Qj_ .
7: Sample _b_ _∼_ Categorical( _Q_ ) and **return** _b_ .


**Curation and Normalization Layer.** After applying singlestock transformations, we curate the data to maintain financial
consistency by setting the highest price feature to “High” and
the lowest to “Low”. To conduct mix-up operations which
involve the exchange of data between stocks, it is crucial
to normalize the data. This is achieved using rolling-window
standard normalization applied to each feature of each stock.
For multiple layers of manipulation, the data is denormalized
when reintroduced to the workflow.
**Mix-up Layer.** We apply multi-stock mix-up operations as
introduced in Section III-E to the normalized data to mix each
source stock _a_ with target stock _b_, as described in Algorithm
1. We select a target stock _b_ by identifying the top _k_ most
correlated stocks with the source stock _a_ based on cointegration test p-values. To control the skewness of the operation



choosing probability, we apply a transformation to these pvalues, adjusted by a manipulation strength parameter _λ_ . For
_λ ≤_ 0 _._ 5, a power transformation compresses the p-values,
favoring more cointegrated stocks. For _λ >_ 0 _._ 5, an inverse
power transformation expands the p-values, increasing the
chance of selecting less cointegrated stocks. The transformed
_S_ are normalized to _Q_, and the target stock _b_ is sampled
from this distribution. The mix-up methods are target-variant
because they involve creating new samples by combining both
the features and targets of two different stocks. Operations are
parameterized with:


_• Cut Mix_ : _λ_ determines the area of the patch replaced
between two samples

_• Linear Mix_ : _λ_ controls the interpolation ratio between the
two inputs and their labels

_• Amplitude Mix_ : _λ_ it adjusts the relative amplitude or
energy contribution of each signal.

_• Demirel and Holz (2024)_ : _λ_ determines the blending ratio
between the two signals.


**Interpolation Compensation Layer.** While the curation module sets hard constraints to maintain the fidelity of the financial
data, we also apply an interpolation compensation to mitigate
potential extreme samples. We propose a mutual-informationaware mixing strategy termed _Binary Mix_ . Unlike standard
interpolation methods that blend two samples uniformly or
randomly, Binary Mix adaptively adjusts the interpolation ratio
based on the similarity between the original and augmented
data. Specifically, we compute the mutual information defined
in Equation 14, between the two samples to estimate how
semantically aligned they are, and reduce the mixing weight
accordingly. This ensures that less similar augmentations
contribute less to the final sample, preserving task-relevant
structure. The procedure is detailed in Algorithm 2 where the
factor _b_ mix is calculated with the mutual information. The less
mutual information the augmented data has with the original
data, the more it is compensated with the original data.



(14)

where _X_ and _Y_ are random variables with joint distribution
_p_ ( _x, y_ ) and marginals _p_ ( _x_ ) and _p_ ( _y_ ).




      - _fX,Y_ ( _x, y_ )
_fX,Y_ ( _x, y_ ) log
_X×Y_ _fX_ ( _x_ ) _fY_ ( _y_



��
MI( _X_ ; _Y_ ) =



_fX_ ( _x_ ) _fY_ ( _y_ )




d _x_ d _y,_


**Algorithm 2** Binary Mix: randomly selects segments from
either of the two stocks based on a binomial distribution.

1: **Input:** Original Data **x**, augmented data **y**, Factor _b_ max
2: **Output:** Compensated data **x** _[′]_

3: Randomly select feature _k_ for fast estimation

4: Calculate mutual information MI _xy_ and baseline MI _xx_ to
measure similarity and maximum similarity



5: Compute factor _b_ mix = _b_ max _−_ - MIMI _xxxy_




_b_ max



_′_
6: Compute compensated data **x** = _bmix_ **x** + (1 _−_ _bmix_ ) **y**
7: **return** compensated data **x** _[′]_ = _b_ mix **x** + (1 _−_ _b_ mix) **y**



**Algorithm 3** Proportion _α_ Scheduler


1: **Input:** Current early stopping counter _C_ es, Last early
stopping counter _C_ les, Epoch _E_, Threshold _τ_
2: **Output:** Proportion of data to be manipulated _α_
3: Let rate penalty _R_ penalty = 1 if _C_ es _> C_ les else 0 _._ 1.

4: Update _C_ les = _C_ es.
5: **return** _α_ = min(tanh( _[E]_ _τ_ [) + 0] _[.]_ [01] _[,]_ [ 1] _[.]_ [0)] _[ ×][ R]_ [penalty]


_C. Adaptive Curriculum_



Given the heterogeneity of financial data, learning objectives, and model architectures, designing a unified policy
to control the parameterized data manipulation module _M_
remains non-trivial. We introduce an _adaptive planner_ that
jointly observes model and data states to determine optimal
parameters, while an _overfitting-aware scheduler_ progressively
integrates augmented samples through a dynamic curriculum.
**Curriculum Planner.** We train a planner _gϕ_ to learn the
policy _πϕ_ ( _p, λ|f, xi_ ) to optimize the objective function in
equation (13). The state includes both the task model _fθ_
and the input sample _xi ∈_ _D_, ensuring that the curriculum
can be determined based on both the model and the data,
as supported by findings in [25]. To efficiently represent
the state, we use high-level representations of _fθ_ and _xi_ .
For the state of the model, we extract features from the
second-to-last fully connected layer inserted into the task
model. Comparably, for the state of the input sample _xi_, we
compute key metrics such as mean, volatility, momentum,
skewness, kurtosis, and trend. More concretely, we calculate:
Momentum = _X_ last _−X_ first _,_ where _X_ last and _X_ first are the values
of the feature at the last and first observations in the window,

1                   - _n_

respectively. Skewness = ( _n_ [1] _n_ ~~�~~ _ni_ =1 _i_ =1 [(] _[X]_ [(] _[X][i][−][i][−][X][X]_ [)][2][)][)][3] 3 _/_ 2 _[,]_ [ where] _[ X]_ _i_ [is]

the value of the feature at observation _i_, _X_ is the mean of
the feature values, and _n_ is the number of observations in

1                 - _n_

the window. Kurtosis = ( _n_ [1] _n_ ~~�~~ _nii_ =1=1 [(][(] _[X][X][i][i][−][−][X][X]_ [)][)][2][4][)] 2 _[−]_ [3] _[,]_ [ where] _[ X]_ _i_ [is]

the value of the feature at observation _i_, _X_ is the mean of
the feature values, and _n_ is the number of observations in

      - _n_
_i_ =1 [(] _[t][i][−][t]_ [)(] _[X][i][−][X]_ [)]

the window. Trend = ~~�~~ _n_ _,_ where _ti_ is the time

_i_ =1 [(] _[t][i][−][t]_ [)][2]
or index of observation _i_, _Xi_ is the value of the feature at
observation _i_, and _t_ and _X_ are the means of the time and
feature values within the window, respectively.
This ensures that the planner _gϕ_ can effectively learn a



1 - _n_
_n_ _i_ =1 [(] _[X][i][−][X]_ [)][3]



_n_ _i_ =1

_n_ [1] ~~�~~ _ni_ =1 [(] _[X][i][−][X]_ [)][2][)] 3 _/_ 2 _[,]_ [ where] _[ X]_ _i_ [is]



policy that adapts to the model and data, thereby optimizing
the curriculum more efficiently.
To address risk from inference uncertainty in financial tasks,
we propose a loss function inspired by the Sharpe ratio,
incorporating standard deviation to penalize volatility:


_L_ = _E_ ( _loss_ ) + _γ × σ_ ( _loss_ ) (15)


This formulation guides the model away from risky inferences
by penalizing. We set _γ_ = 0 _._ 05 for experiments.
**Over-fitting Aware Scheduler.** While the planner controls the
operation choosing probability _p_ and manipulation strength _λ_,
the proportion of data to be manipulated _α_ is determined by the
scheduler to provide a reasonable curriculum. In CV research,
it is common to apply augmentation to all training samples

[13], [26]. However, such a fixed application of augmentation
may not be suitable for our objective of addressing uncertainty
in financial markets. Therefore, we propose a dynamic data
manipulation strategy, as outlined in Algorithm 3. Curriculum
learning encourages the model to progress from simpler to
more challenging examples. As data augmentation increases
sample diversity and complexity, the poportion of augmented
data can be gradually raised during training. Hence, _α_ is
designed to increase over epochs, reflecting a soft curriculum
rather than a fixed rule. However, excessive manipulation
may adversely affect the learning process. Our objective is to
mitigate poor generalization caused by concept drift, which is
often indicated by overfitting—where the model performs well
on the training data but poorly on unseen validation data. To
prevent over-manipulation of the data, we monitor overfitting
during training by the validation loss: if the validation loss in
the current epoch is not lower than in the previous epoch by a
specified threshold, it suggests potential overfitting. When this
occurs, the penalty to regulate frequent manipulation _R_ penalty
is removed to introduce more diverse data.
By scheduling the proportion of data to be manipulated, we
balance generalization and robustness, enhancing the model’s
ability to handle market uncertainty without added complexity.


**Algorithm 4** Joint Training Scheme


1: **Input:** Training set _D_ train; Validation set _D_ valid; Update
frequency _freq_
2: **Output:** Planner _gϕ_ ; task model _fθ_
3: Initialize _θ_ 0, _ϕ_ 0
4: **while** max epoch not reached and no early stopping **do**
5: Get _α_ with Algo 3 and _p, λ_ = _g_ ( _fθ, x_ train)
6: Update _fθ_ (˜ _x_ train = _M_ ( _α, p, λ, x_ train); _L_ train)
7: **if** current step is divisible by _freq_ **then**
8: Copy _fθ_ as _fθ_ _[′]_, update it with _x_ [˜] ˜train with eq.(16)

9: Update _gϕ_ ( _x_ valid; _L_ valid( _fθ_ _[′]_ ))
10: **end if**
11: **end while**
12: **return** Trained planner _gϕ_ ; trained task model _fθ_


**Planner Training Scheme.** Given the complexity of this bilevel optimization problem, we adopt methods from [11], [27]



( [1]



1  - _n_
_n_ _i_ =1 [(] _[X][i][−][X]_ [)][4]
( _n_ [1] ~~�~~ _ni_ =1 [(] _[X][i][−][X]_ [)][2]



_n_ _i_ =1 _[i]_

_n_ [1] ~~�~~ _ni_ =1 [(] _[X][i][−][X]_ [)][2][)] 2 _[−]_ [3] _[,]_ [ where] _[ X]_ _i_ [is]



the value of the feature at observation _i_, _X_ is the mean of
the feature values, and _n_ is the number of observations in

      - _n_
_i_ =1 [(] _[t][i][−][t]_ [)(] _[X][i][−][X]_ [)]

the window. Trend = ~~�~~ _n_ _,_ where _ti_ is the time


to jointly train the planner _gϕ_ and the task model _fθ_ . The task
model is trained within the training loop, while the planner
is trained within the validation loop. This relies on a local
temporal smoothness assumption: when the data are partitioned chronologically, the validation segment is statistically
closer to the upcoming test segment than the earlier training
window as supported by Section III. This proximity supports
using validation risk as a practical surrogate for test risk,
thereby grounding the bi-level planner–scheduler optimization
in a measurable temporal relationship. The alternating update
scheme is outlined in Algorithm 4. The task model _fθ_ is first
updated. To stabilize the training process, the planner will then
be trained for every frequency _freq_ steps to learn the policy
_πϕ_ ( _p, λ|fθ, xi_ ), with the validation loss of _fθ_ _[′]_, which is a copy
of _fθ_ at the current step.
To learn the operation choosing probability of operation _p_,
we generate all the augmented data of the _n × m_ sets of the
operations with their manipulation strength _λij_ and sum all
the augmented data up with their weight in the probabilistic
_pij_, where



_n_



_i_ =1



_x_ ˜˜train =



_m_

- _pij × M_ (1 _,_ 1 _, λij, x_ train) _._ (16)


_j_ =1



_m_




Using a weighted sum instead of sampling with _pij_ accounts
for the effect of every augmentation operation combination,
thus leading to a better estimation when updating _ϕ_ .
To deal with non-differentiable operations, we use a
straight-through gradient estimator [28] to optimize the manipulation strength _λ_, where _[∂M]_ _∂λ_ [(] _[x][i]_ [)] = 1, with gradient:



_∂L_ [val]



_∂L_ [val]
_pij_

_∂fθ_

_i_

_∂L_ [val]
_pij_

_∂fθ_

_i_



_A. Experiment Setup_

_a) Datasets:_ We evaluate on two real-world financial
markets: (i) **Stocks (daily).** Price and technical indicators
for 27 stocks of the Dow Jones Industrial Average from
**2000-01-01** to **2024-01-01** ; (ii) **Crypto (hourly).** Price and
technical indicators for BTC, ETH, DOT, and LTC from
**2023-09-27** to **2025-09-26** . The datasets were divided into
training, validation, and test sets with ratios of 0 _._ 6, 0 _._ 2,
and 0 _._ 2. All normalization statistics, cointegration measures,
and other rolling or windowed computations were strictly
estimated within the training set to prevent any temporal
leakage. Experiments were run on a GeForce RTX 4090 GPU.


_b) Benchmark for Augmented Data:_ We compared our
augmented data with data generated by representative timeseries generative models, including TimeGAN, SigCWGAN

[29], RCWGAN [30], GMMN [31], CWGAN [32], RCGAN

[33] and Diffusion-TS.


_c) Benchmark for The Whole Workflow:_ In the experiments, the proposed method is compared with the following methods: (1) **Original** : Uses the original data without
any manipulation to the training scheme. (2) **RandAugment** :
Employs randomly chosen augmentations where _λ_ = 1 for
each augmentation and the proportion _α_ = 0 _._ 5 throughout the
training process. (3) **TrivialAugment** : This baseline employs
randomly chosen augmentations with a randomized _λ_ and
fixed proportion _α_ = 0 _._ 5. (4) **AdaAug** : This setup utilizes
the planner _gϕ_ for augmentations but maintains an _α_ = 0 _._ 5.
_d) Task Model:_ We evaluate our method using five
representative architectures across different model families:
GRU, LSTM, DLinear, TCN, and Transformer on forecasting
tasks, and DQN and PPO for reinforcement learning–based
trading tasks. To integrate any model into our method, the
downstream task model simply has to adopt a two-stage
architectural pattern that explicitly separates feature extraction
from prediction tasks. Unlike traditional models that employ a
mapping from input sequences to predictions, we implement a
modular design consisting of two distinct functions: the feature
extraction function _j_ ( _·_ ) that extracts learned representations
from the second-to-last layer of the network, and the prediction function _k_ ( _·_ ) that maps these extracted features to final
predictions where _k_ ( _j_ ( _·_ )) is the typical forward function. For
models that do not have at least 2 fully-connected layers, we
add them to facilitate this process. For the planner, we use
a Transformer. For all models, we set the learning rate to be
0.001, and the planner input dimensions to be 128. The batch
size for GRU and TCN at set at 128, LSTM and Transformer at
256, and DLinear at 1024. The patience for GRU, LSTM, TCN
and Transformer is set at 5 while DLinear at 8. The hidden
dimensions for the task model is set at 512 for GRU, LSTM,
DLinear and TCN, and at 256 for Transformer. The planner
layers are set at 2 for GRU, LSTM, TCN and Transformer,
and at 5 for DLinear, while the planner hidden dimensions are
set at 256 for GRU, LSTM, DLinear and TCN, and at 128 for
Transformer. The key hyperparameters are as follows:



=  _∂λ_



_∂fθ_



= 


_∂fθ_



_∂fθ_ _∂M_ ( _xi_ )
_∂M_ ( _xi_ ) _∂λ_

(17)
_∂fθ_
_∂M_ ( _xi_ ) _[.]_



The outer loop updates of planner _ϕt_ is given by:



_ϕt_ +1 = _ϕt −_ _β_ [1]

_n_ [val]



_n_ [val]

- _∇ϕL_ [val] _i_ [(ˆ] _[θ][t]_ [(] _[ϕ][t]_ [))] (18)

_i_ =1



where _β_ is the learning rate and _θ_ [ˆ] is the task model.


V. EXPERIMENTS


We aim to evaluate whether our adaptive dataflow system
effectively addresses three key challenges:


1) **Workflow effectiveness.** Does the proposed dataflow
system improve downstream task robustness and overall
data-processing efficiency compared to existing augmentation and generation pipelines?
2) **System adaptability.** Can the workflow generalize
across heterogeneous models and tasks?
3) **Data fidelity and curation quality.** Does the pipeline
generate diverse yet realistic financial time-series data
for downstream analysis?


TABLE II: Unified comparison over **Stocks** (upper block) and **Cryptos** (lower block) across five model families.


**GRU** **LSTM** **DLinear** **TCN** **Transformer**
**Method** _MSE_ _MAE_ _STD_ _MSE_ _MAE_ _STD_ _MSE_ _MAE_ _STD_ _MSE_ _MAE_ _STD_ _MSE_ _MAE_ _STD_


**Stocks** (exponents: MSE _×_ 10 _[−]_ [4], MAE _×_ 10 _[−]_ [2], STD _×_ 10 _[−]_ [3] )


Original 22.76 3.388 5.140 5.070 1.578 1.664 52.15 5.501 9.601 40.44 4.614 10.01 8.608 2.216 1.915
RandAug 16.74 2.864 3.994 4.646 1.495 1.613 662.7 19.86 116.7 58.74 4.925 22.12 8.264 2.160 1.892
TrivialAug 15.62 2.670 4.114 4.827 1.536 1.634 571.6 18.40 100.7 46.83 5.142 10.79 7.474 2.041 1.814
AdaAug 17.64 2.972 3.995 4.791 1.538 **1.567** 22.62 3.803 3.234 49.61 5.203 11.61 7.602 2.062 1.819
Ours **13.14** **2.496** **3.366** **4.253** **1.410** 1.571 **4.550** **1.485** **1.559** **34.87** **4.431** **7.864** **7.471** **2.046** **1.795**


**Cryptos** (exponents: MSE _×_ 10 _[−]_ [5], MAE _×_ 10 _[−]_ [3], STD _×_ 10 _[−]_ [3] )


Original 8.144 6.551 8.975 4.426 4.568 6.453 4.291 4.428 6.339 913.3 29.57 90.95 8.356 6.726 7.555
RandAug 8.315 6.442 9.042 4.348 4.509 6.385 4.275 4.412 6.324 1883 40.94 125.5 7.295 6.400 7.232
TrivialAug 8.268 6.571 8.967 4.361 4.518 6.401 4.273 4.411 6.322 855.2 32.93 83.30 11.41 7.318 8.809
AdaAug 12.54 8.142 11.16 4.460 4.583 6.476 4.278 4.422 6.329 332.3 32.43 56.38 7.877 6.484 7.395
Ours w/o mixup 7.552 6.231 8.437 4.402 4.511 6.422 4.252 4.403 6.278 469.3 29.47 66.37 7.732 6.207 7.113
Ours **6.916** **5.816** **8.157** **4.235** **4.324** **6.210** **4.138** **4.374** **6.122** **307.7** **31.87** **53.73** **4.941** **4.982** **6.661**




_•_ **Threshold** _τ_ : This has the same concept as the temperature parameter, and affects the proportion of data to be
manipulated _α_ . We set _τGRU_ to 14, _τLST M_, _τT CN_ and
_τT ransformer_ to 5, and _τDLinear_ to 30.

_•_ **Frequency** _freq_ : This controls the frequency in which
the planner updates. A smaller _freq_ value would increase
the number of updates, which increases the time taken for
training but helps the planner learn better (and vice versa).
We set _freqGRU_ to 15, _freqLST M_ and _freqT ransformer_
to 5, _freqDLinear_ to 8, and _freqT CN_ to 10.

_•_ **Planner training epoch start** : This controls when the
planner starts learning. A lower start would allow the
planner to update earlier, which improves the quality of
the synthetic time series but increase the time taken for
training. We set the start epoch to 10 for GRU, 5 for
LSTM, and 2 for DLinear, TCN and Transformer.

_e) Reinforcement Learning Environment:_ To evaluate the
transferability of the adaptive planner in downstream trading,
we implement a single-asset discrete-action trading environment. The environment simulates realistic trading dynamics
with transaction costs and evolving net value.
**Action space, position, and valuation.** We use an all-in/allout regime with discrete actions _at ∈{−_ 1 _,_ 0 _,_ 1 _}_ for _sell_, _hold_,
_buy_ . Let _st ≥_ 0 denote the number of shares held at _t_, cash _t_
the cash balance, _Pt_ the adjusted close, and _Vt_ = cash _t_ + _stPt_
the portfolio value. A proportional transaction cost _c_ = 10 _[−]_ [3]

applies to traded notional.
_Execution:_ If _at_ = 1 (enter long) and the account was in
cash, invest all money:

_st_ = [(1] _[ −]_ _[c]_ [)] _[ V][t][−]_ [1] _,_ cash _t_ = 0 _._

_Pt_

If _at_ = _−_ 1 (exit to cash) and the account was fully invested,
liquidate all shares:


_st_ = 0 _,_ cash _t_ = (1 _−_ _c_ ) _Vt−_ 1 _._


If _at_ = 0 (hold), or the action repeats the current regime
(already all-in or all-cash), no trade occurs:


_st_ = _st−_ 1 _,_ cash _t_ = cash _t−_ 1 _._



The portfolio then satisfies


_Vt_ = cash _t_ + _stPt,_


with costs incurred only on regime switches (buy/sell), i.e.,
when a trade occurs.
**RL agents.** We integrate this environment with two standard reinforcement learning algorithms: (i) Deep Q-Network
(DQN) [34], which learns a state–action value function
_Qθ_ ( _s, a_ ) via temporal-difference learning, and (ii) Proximal
Policy Optimization (PPO) [35], which optimizes a clipped
surrogate objective for the policy _πθ_ ( _a|s_ ). Both agents share
the same reward structure and hyper-parameters.


_B. Main Results_


_1) Forecasting:_ Five representative task models from different model families were used to test our method. GRU and
LSTM capture temporal dependencies, Transformers handle
long-range dependencies with self-attention, TCN employs
convolution for sequence modeling, and DLinear uses linear
decomposition. We train the models on a one-day close
return forecasting task with a 60-step lookback window and
evaluate their performance using MSE, MAE, and the standard
deviation of the loss over each timestep in the whole test
range to assess the prediction robustness as shown in Table II.
Results show that our method achieves the best performance,
significantly reducing MSE, MAE, and STD across all models. Additionally, we observe that different models respond
differently to augmented data. Stronger models such as GRU,
LSTM, and Transformer, which have smaller initial losses,
perform well with workflows like RandAug and TrivialAug.
Conversely, applying these workflows to weaker models such
as DLinear and TCN may have a detrimental effect, highlighting the need for an adaptive planner to ensure a model-agnostic
workflow. Furthermore, the performance improvement over
AdaAug demonstrates the efficacy of the scheduler.
_2) Transfer to Reinforcement Learning Trading:_ A key
quantitative trading task is utilizing reinforcement learning
(RL) for trading decisions [36], [37]. To evaluate the transferability of a learned policy, we conducted an experiment


TABLE III: Performance comparison of trading
results when our method is integrated with RL
methods.


MCD IBM INTC
Method
_TR↑_ _SR↑_ _TR↑_ _SR↑_ _TR↑_ _SR↑_


DQN 4.78 5.06 13.21 13.55 35.99 16.80
DQN + Ours 17.73 25.74 13.88 14.80 33.35 21.60
PPO 15.42 21.01 -3.62 -7.43 34.67 17.49
PPO + Ours 18.13 26.31 -2.80 -5.68 52.91 23.45


where the planner _gϕ_, trained on a one-day return forecasting
task with an LSTM task model, was applied to a single-stock
trading task using DQN [34] and PPO [35]. To ensure fair
comparisons without introducing additional data, all mix-up
operations were removed. The trained planner _gϕ_ was then
integrated into the RL single-stock training workflow. For the
scheduler, we used a simplified approach where _α_ = 0 was
set for the first 2 _×_ 10 [5] training steps, _α_ = 0 _._ 05 was applied
for the next 1 _×_ 10 [5] steps, and _α_ = 0 was again set for the
remaining steps to ensure convergence. For both DQN and
PPO, the embedding dimensions were set to 128, depth at 1,
initial amount at 1 _×_ 10 [4], transaction cost at 1 _×_ 10 _[−]_ [3] and
discount rate _γ_ at 0.99. For DQN, the policy learning rate is
set at 2 _._ 5 _×_ 10 _[−]_ [4], exploration fraction at 0.5, train frequency at
10, batch size at 128 and target network update frequency at
500. For PPO, the policy learning rate is set at 5 _×_ 10 _[−]_ [7], value
learning rate at 1 _×_ 10 _[−]_ [6], generalized advantage estimation _λ_
at 0.95, value function coefficient at 0.5, entropy coefficient
at 0.01 and target KL at 0.02. We use the _Total Return_ (TR)
to measure profitability, and the _Sharpe Ratio_ (SR) to assess
risk control ability. Formally, they are defined as:

TR = _[P][t][ −]_ _[P]_ [0] (19)

_P_ 0


where _Pt_ and _P_ 0 denote the final and initial portfolio values,
respectively.

SR = [E][[][return][]] (20)

_σ_ (return)


where E[return] denotes the expected return and _σ_ (return) represents the standard deviation of returns. As shown in Table III,
our method increased profit and reduced risk, demonstrating
the transferability. We conducted a case study of the DQN
results on INTC, where we obtained a slightly lower total
return (TR) but a much higher risk-adjusted return (SR) than
the baseline method, indicating superior risk control, as shown
in Fig. 4. From the trading decisions, we can see that our
method helps the DQN make more prudent actions, such as
selling holdings before a downtrend, thus avoiding risk. This
improved performance may result from encountering more
diverse market scenarios during the training stage, alleviating
the poor generalization caused by concept drift.


_C. Ablation Study_


To assess the contribution of each component in our
proposed data manipulation pipeline, we perform ablation



(a) DQN on INTC


(b) DQN + Ours on INTC


Fig. 4: Trading results where buy and sell actions are marked
with green and red labels.


experiments on the cryptocurrency dataset by systematically
disabling individual modules. Removing the _multi-stock mixup_
module leads to consistent increases in MSE, MAE, and
STD across all forecasting models, indicating that crossasset information is beneficial for learning more generalizable
temporal dynamics. Likewise, replacing our adaptive scheduler
with a fixed one—corresponding to the adapted AdaAug
baseline—results in degraded performance, highlighting the
importance of a learnable curriculum that adjusts augmentation
intensity over time. Furthermore, disabling both the scheduler
and planner (i.e., using randomly selected augmentations as
in TrivialAug and RandAug) yields further performance deterioration, particularly for TCN and Transformer architectures.
These findings suggest that augmentations must be dynamically and model-specifically controlled throughout training.
Overall, the results confirm that the combination of mixup,
adaptive scheduling, and a learned planner substantially enhances both the robustness and predictive accuracy of our
forecasting framework.


_D. Augmented Data Quality Evaluation_


To further ensure the safety of using augmented data, we
evaluate the quality of our augmented data from a financial
perspective both qualitatively and quantitatively.
_a) Addressing Concept Drift:_ To assess how our augmented data mitigates concept drift, we compare the training
and test distributions in Fig. 7. In (a), the original training set
clusters further from the test set, indicating potential drift. In
contrast, (b) demonstrates that the augmented training samples
cluster more closely to the test set, highlighting the ability of
our pipeline to address concept drift.


(a) Transformer


(b) LSTM


Fig. 5: Operation weights from planner for (a) Transformer
and (b) LSTM.


TABLE IV: Discriminative accuracy (Acc – 50%) _↓_ and
stylized-facts fidelity (ACF returns, ACF absolute returns,
leverage correlation) for various generative methods. A lower
discriminative score and smaller stylized-facts differences both
indicate better performance.


**Method** **Dis.** **ret** **abs ret** **Lev corr**


TimeGAN 48.2 0.0231 0.00987 0.0263
SIGCWGAN 48.3 0.0625 0.0113 0.0678
RCWGAN 48.5 0.0278 0.0142 0.00776
GMMN 49.6 0.0280 0.0555 0.146
CWGAN 48.5 0.0118 0.03177 0.0309
RCGAN 48.0 0.0210 0.0154 0.0251
Diffusion-TS 42.4 0.0361 0.026 0.022
**Ours** **14.1** **0.000133** **0.000478** **0.00224**


_b) Variable Controllability:_ To visualize how our data
manipulation parameters affect augmentation, we present tSNE plots in Fig. 6, showing that the module is tunable and
parameter changes lead to gradual, predictable shifts in the
synthetic data distribution, providing diverse and controlled
augmented data.
_c) Downstream Usability:_ We assess the suitability of the
operations for financial time series with a general experiment.
An LSTM was used to forecast the close returns of stocks in
DJI index and we use the classification accuracy of the return



TABLE V: Accuracy after applying operations in training.


**Operation** **Accuracy (%)**


None 50.72
Jittering 52.06
Scaling 51.54
Magnitude Warping 51.75
Permutation 51.95
STL Augment 51.75
Cut Mix 52.37
Linear Mix 51.85
Amplitude Mix 52.16

[12] Mix 51.65


direction as labels. The operations were applied to the entire
training set respectively to determine if the augmented data
was useful. The results in Table V indicate that all selected
operations improved the forecasting accuracy over the baseline
of using original historical data.
_d) Discriminative Score:_ Following TimeGAN, we evaluate the fidelity of augmented data using post-hoc RNN
classifiers to distinguish real from augmented data. An accuracy of 50% indicates indistinguishability, and fidelity is
measured as the classifier’s accuracy minus 50%. Although our
pipeline is not designed to be a generative model, we include
a comparison with deep generative models. This comparison
provides additional insight into the closeness of our augmented
data to the real distribution. As observed in Table IV, our
method achieves the lowest discriminative score, showing the
highest financial fidelity.
_e) Market Stylized Facts:_ Beyond classification-based
metrics, it is crucial to confirm that the augmented data adheres
to well-established stylized facts known to characterize real
financial markets.Concretely, we compute three key statistical
properties commonly observed in financial markets: the autocorrelation of returns, the autocorrelation of absolute returns,
and the leverage effect. They are defined as follows:


_ρr_ ( _k_ ) = [Cov][(] _[r][t][, r][t][−][k]_ [)] (21)

Var( _rt_ )


_ρ|r|_ ( _k_ ) = [Cov][(] _[|][r][t][|][,][ |][r][t][−][k][|]_ [)] (22)

Var( _|rt|_ )


_ρr,σ_ ( _k_ ) = Corr( _rt, σt_ + _k_ ) (23)


where _ρr_ ( _k_ ) and _ρ|r|_ ( _k_ ) denote the lag- _k_ autocorrelation of
returns and absolute returns, respectively, and _ρr,σ_ ( _k_ ) represents the leverage effect. The autocorrelation of returns helps
evaluate market efficiency, the autocorrelation of absolute
returns is central to risk modeling, and the leverage effect is
crucial for designing volatility models and assessing risk.
We evaluate how faithfully the augmented series captures
core market dynamics. As shown in Table IV, our augmented
data exhibits stylized facts that most closely align with those
found in true financial data, underscoring the practical relevance of our augmentation pipeline for financial tasks.


**Amplitude Mix** **Cut Mix** **Linear Mix** **[12] Mix**


Fig. 6: Comparison of data distribution as the parameters vary. Y-axis shows single-stock augmentations while X-axis shows
multi-stock mixups. Dots of different colors represent synthetic data from varying manipulation strength _λ_ .


(a) t-SNE of original data (b) t-SNE of augmented data


Fig. 7: t-SNE plots comparing original and augmented data.
The red box highlights that the augmented training data
(orange) becomes distributionally closer to test data (green).



_E. Additional Analyses_


_a) Operation Weights:_ We visualize the weights of the
probabilistic operations _p_ in Fig. 5a and Fig. 5b with our
provenance aware replay. As observed, _p_ evolves as the task
model undergoes training. The weights of these operations also
vary significantly between different models, indicating that a
planner is essential for a model-agnostic adaptive policy.
_b) Learning Curve:_ We conducted a qualitative analysis
to understand why our method enhances performance, as
shown in Fig. 8. Compared to the original workflow, our approach results in a lower validation loss, indicating successful
generalization and effectively addressing potential overfitting
caused by concept drift.


VI. CONCLUSION


In this paper, we introduced a novel adaptive dataflow
system designed to bridge the gap between training and real


Fig. 8: The training and validation loss curve w/wo our
workflow applied.


world performance in quantitative finance. To the best of our
knowledge, this is the first workflow of its kind applied to
quantitative finance tasks. The framework integrates a parameterized data manipulation module with a learning-guided
planner–scheduler, forming a feedback loop that dynamically
regulates manipulation strength and proportion of data to be
manipulated as the model evolves. This design allows the
data pipeline to self-adjust to distributional drift, ensuring
consistent data quality and realistic synthesis throughout the
learning process. Experiments on forecasting and trading tasks
demonstrate that the system improves robustness and generalization across models and markets.


REFERENCES


[1] S. Sun, M. Qin, W. Zhang, H. Xia, C. Zong, J. Ying, Y. Xie, L. Zhao,
X. Wang, and B. An, “Trademaster: A holistic quantitative trading
platform empowered by reinforcement learning,” _Advances in Neural_
_Information Processing Systems_, vol. 36, 2024.

[2] T. B. Shahi, A. Shrestha, A. Neupane, and W. Guo, “Stock price
forecasting with deep learning: A comparative study,” _Mathematics_,
vol. 8, no. 9, p. 1441, 2020.

[3] S. K. Sahu, A. Mokhade, and N. D. Bokde, “An overview of machine
learning, deep learning, and reinforcement learning-based techniques in
quantitative finance: Recent progress and challenges,” _Applied Sciences_,
vol. 13, no. 3, p. 1956, 2023.

[4] Z. Shi and J. Cartlidge, “Neural stochastic agent-based limit order book
simulation: A hybrid methodology,” _arXiv preprint arXiv:2303.00080_,
2023.

[5] E. Samanidou, E. Zschischang, D. Stauffer, and T. Lux, “Agent-based
models of financial markets,” _Reports on Progress in Physics_, vol. 70,
no. 3, p. 409, 2007.

[6] H. Xia, S. Sun, X. Wang, and B. An, “Market-gan: Adding control to
financial market data generation with semantic context,” in _Proceedings_
_of the AAAI Conference on Artificial Intelligence_, vol. 38, no. 14, 2024,
pp. 15 996–16 004.

[7] S. C. Hora, “Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management,” _Reliability_
_Engineering & System Safety_, vol. 54, no. 2-3, pp. 217–223, 1996.

[8] S. Kapoor, W. J. Maddox, P. Izmailov, and A. G. Wilson, “On uncertainty, tempering, and data augmentation in bayesian classification,” _Ad-_
_vances in Neural Information Processing Systems_, vol. 35, pp. 18 211–
18 225, 2022.

[9] W. Li, X. Yang, W. Liu, Y. Xia, and J. Bian, “Ddg-da: Data distribution
generation for predictable concept drift adaptation,” in _Proceedings of_
_the AAAI Conference on Artificial Intelligence_, vol. 36, no. 4, 2022, pp.
4092–4100.

[10] T.-H. Cheung and D.-Y. Yeung, “Adaaug: Learning class-and instanceadaptive data augmentation policies,” in _International Conference on_
_Learning Representations_, 2021.

[11] C. Hou, J. Zhang, and T. Zhou, “When to learn what: Model-adaptive
data augmentation curriculum,” in _Proceedings of the IEEE/CVF Inter-_
_national Conference on Computer Vision_, 2023, pp. 1717–1728.

[12] B. U. Demirel and C. Holz, “Finding order in chaos: A novel data
augmentation method for time series in contrastive learning,” _Advances_
_in Neural Information Processing Systems_, vol. 36, 2024.

[13] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaugment: Learning augmentation policies from data,” _arXiv preprint_
_arXiv:1805.09501_, 2018.

[14] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum
learning,” in _Proceedings of the 26th Annual International Conference_
_on Machine Learning_, 2009, pp. 41–48.

[15] W. Zhang, L. Zhao, H. Xia, S. Sun, J. Sun, M. Qin, X. Li, Y. Zhao,
Y. Zhao, X. Cai _et al._, “Finagent: A multimodal foundation agent for
financial trading: Tool-augmented, diversified, and generalist,” _arXiv_
_preprint arXiv:2402.18485_, 2024.

[16] M. D. Gould, M. A. Porter, S. Williams, M. McDonald, D. J. Fenn, and
S. D. Howison, “Limit order books,” 2013.

[17] T. Preis, S. Golke, W. Paul, and J. J. Schneider, “Statistical analysis of
financial returns for a multiagent order book model of asset trading,”
_Phys. Rev. E_, vol. 76, p. 016108, Jul 2007. [Online]. Available:
https://link.aps.org/doi/10.1103/PhysRevE.76.016108

[18] S. Vyetrenko, D. Byrd, N. Petosa, M. Mahfouz, D. Dervovic, M. Veloso,
and T. Balch, “Get real: Realism metrics for robust limit order book
market simulations,” in _Proceedings of the First ACM International_
_Conference on AI in Finance_, 2020, pp. 1–8.

[19] J. Yoon, D. Jarrett, and M. Van der Schaar, “Time-series generative
adversarial networks,” _Advances in Neural Information Processing Sys-_
_tems_, vol. 32, 2019.

[20] X. Yuan and Y. Qiao, “Diffusion-ts: Interpretable diffusion for general
time series generation,” _arXiv preprint arXiv:2403.01742_, 2024.

[21] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,
“Informer: Beyond efficient transformer for long sequence time-series
forecasting,” in _Proceedings of the AAAI Conference on Artificial_
_Intelligence_, vol. 35, no. 12, 2021, pp. 11 106–11 115.

[22] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” _Journal_
_of Machine Learning Research_, vol. 9, no. 11, 2008.




[23] T. T. Um, F. M. Pfister, D. Pichler, S. Endo, M. Lang, S. Hirche,
U. Fietzek, and D. Kuli´c, “Data augmentation of wearable sensor data
for parkinson’s disease monitoring using convolutional neural networks,”
in _Proceedings of the 19th ACM international conference on multimodal_
_interaction_, 2017, pp. 216–220.

[24] R. B. Cleveland, W. S. Cleveland, J. E. McRae, I. Terpenning _et al._,
“Stl: A seasonal-trend decomposition,” _J. off. Stat_, vol. 6, no. 1, pp.
3–73, 1990.

[25] S. Saxena, O. Tuzel, and D. DeCoste, “Data parameters: A new family of
parameters for learning a differentiable curriculum,” _Advances in Neural_
_Information Processing Systems_, vol. 32, 2019.

[26] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment:
Practical automated data augmentation with a reduced search space,”
in _Proceedings of the IEEE/CVF conference on computer vision and_
_pattern recognition workshops_, 2020, pp. 702–703.

[27] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng,
“Meta-weight-net: Learning an explicit mapping for sample weighting,”
_Advances in Neural Information Processing Systems_, vol. 32, 2019.

[28] Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, and Y. Yang,
“Differentiable automatic data augmentation,” in _Computer Vision–_
_ECCV 2020: 16th European Conference_, 2020, pp. 580–595.

[29] H. Ni, L. Szpruch, M. Wiese, S. Liao, and B. Xiao, “Conditional sig-wasserstein gans for time series generation,” _arXiv preprint_
_arXiv:2006.05421_, 2020.

[30] Y.-L. He, X.-Y. Li, J.-H. Ma, S. Lu, and Q.-X. Zhu, “A novel virtual
sample generation method based on a modified conditional wasserstein
gan to address the small sample size problem in soft sensing,” _Journal_
_of Process Control_, vol. 113, pp. 18–28, 2022.

[31] Y. Li, K. Swersky, and R. Zemel, “Generative moment matching
networks,” 2015.

[32] Y. Yu, B. Tang, R. Lin, S. Han, T. Tang, and M. Chen, “Cwgan: Conditional wasserstein generative adversarial nets for fault data generation,”
in _2019 IEEE International Conference on Robotics and Biomimetics_
_(ROBIO)_ . IEEE, 2019, pp. 2713–2718.

[33] C. Esteban, S. L. Hyland, and G. R¨atsch, “Real-valued (medical)
time series generation with recurrent conditional gans,” _arXiv preprint_
_arXiv:1706.02633_, 2017.

[34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
_et al._, “Human-level control through deep reinforcement learning,”
_Nature_, vol. 518, no. 7540, pp. 529–533, 2015.

[35] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” _arXiv preprint arXiv:1707.06347_,
2017.

[36] M. Qin, S. Sun, W. Zhang, H. Xia, X. Wang, and B. An, “Earnhft:
Efficient hierarchical reinforcement learning for high frequency trading,”
in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 38,
no. 13, 2024, pp. 14 669–14 676.

[37] C. Zong, C. Wang, M. Qin, L. Feng, X. Wang, and B. An, “Macrohft: Memory augmented context-aware reinforcement learning on high
frequency trading,” _arXiv preprint arXiv:2406.14537_, 2024.


