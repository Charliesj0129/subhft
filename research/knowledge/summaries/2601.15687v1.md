**FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation**


[KHUSRAV BADALOV, Neouly Co., Ltd., Republic of Korea](HTTPS://ORCID.ORG/0009-0009-6906-919X)

[YOUNG YOON, Hongik University, Republic of Korea](HTTPS://ORCID.ORG/0000-0002-5249-2823)


Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing


event-driven rules across heterogeneous services. A TAP applet links a _trigger_ to an _action_ and must bind trigger outputs ( _ingredients_ )


to action inputs ( _fields_ ) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often


yields non-executable applets that still require manual configuration. We study the _function-level configuration_ problem: generating


complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture


for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over


schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger

action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis,


trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and


agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both


trigger and action _functions_ must match the ground truth. For comparison with service-level baselines, we map functions to their


parent services and evaluate at the service level. FARM reaches 79% joint accuracy and improves over TARGE by 21 percentage points.


FARM also generates ingredient-to-field bindings, producing executable automation configurations.


CCS Concepts: â€¢ **Computing methodologies** â†’ _Rule learning_ ; **Knowledge representation and reasoning** ; **Supervised learning** ;


**Multi-agent planning** ; â€¢ **Information systems** â†’ **Similarity measures** .


Additional Key Words and Phrases: Trigger-Action Programming, Natural Language Processing, Internet of Things (IoT), Web of


Things (WoT), Multi-Agentic AI, Retrieval Augmentation Generation (RAG)


**ACM Reference Format:**


Khusrav Badalov and Young Yoon. 2026. FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation. 1, 1


[(January 2026), 34 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)


**1** **Introduction**


Trigger-Action Programming (TAP) is a declarative programming paradigm that allows users to define event-driven


automation rules. It is the essential foundation behind systems such as IFTTT (If This Then That), Zapier, and similar


platforms including Microsoft Power Automate and Apple Shortcuts. Ur et al. [45] demonstrated the practicality of TAP


in smart home contexts, while Rahmati et al. [38] provided a comparative analysis of IFTTT and Zapier. TAP has become


a mainstream interface for end-user automation across Web of Things (WoT) ecosystems, connecting both IoT devices


and web applications; Mi et al. [34] found that over 400 services spanning both categories are integrated on IFTTT


alone. Ur et al. [44] showed that real-world TAP rule corpora exhibit substantial diversity in services, rule structures,


and user-written natural language descriptions, making TAP a challenging target for robust intent understanding and


program synthesis.


Historically, TAP interaction was associated with composing simple IF-THEN rules by manually selecting a trigger


and an action and combining them into a mashup [15, 38]. Recently, LLM-driven assistants and agentic systems have


[Authorsâ€™ Contact Information: Khusrav Badalov, Neouly Co., Ltd., Seoul, Republic of Korea, khusravvvb99@gmail.com; Young Yoon, Hongik University,](https://orcid.org/0009-0009-6906-919X)

Computer Engineering, Seoul, Republic of Korea, young.yoon@hongik.ac.kr.


2026. Manuscript submitted to ACM


Manuscript submitted to ACM 1


2 Badalov and Yoon


shifted this workflow toward natural language specifications, where a model is expected to (i) infer user intent, (ii)


select services, and (iii) produce executable configurations [22, 42]. Figure 1 illustrates this paradigm shift, contrasting


traditional manual configuration with agentic AI-driven mashup generation. However, in practice, AI-based TAP


builders frequently fail on underspecified or ambiguous requests: users often phrase the same intent in multiple ways,


provide incomplete context (e.g., location, device identity, thresholds), or implicitly encode preferences (e.g., comfort vs.


energy saving) [21, 27]. In such cases, current systems may generate multiple inconsistent trigger-action variants, select


incorrect parameters, or produce brittle applets that are syntactically valid but semantically misaligned with user goals.



Requires manual paramter definition, complex data mapping, and
technical knowledge, often leading to errors.



Uses natural language understanding and autonomous agents to
interpret, plan, and execute complex tasks



Fig. 1. An example of an TAP applet interface for manual configuration and our proposal for an automated applet generation.


**1.1** **Motivation**


Consider a common scenario: Alice asks an AI assistant, â€œNotify me when itâ€™s going to rain tomorrow.â€ Although


seemingly straightforward, existing TAP platforms frequently mishandle such requests: alerts may trigger for the


wrong location, apply an unintended threshold, or fail due to missing implicit parameters. This example highlights a


central challenge in contemporary TAP systems: the widening semantic gap between informal user language and the


structured, parameter-rich function-level invocations required for reliable automation. We use the term _function-level_


to distinguish from service-level identification: while a service (e.g., â€œGoogle Sheetsâ€) may expose multiple triggers


and actions, a function refers to a specific trigger (e.g., â€œNew row added to spreadsheetâ€) or action (e.g., â€œUpdate cell in


spreadsheetâ€), each with its own input parameters, output data fields, and schema constraints. Prior work also shows


that users struggle to debug such failures, because the root cause may lie in hidden parameters, platform semantics, or


multi-rule interactions Zhang et al. [63].


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 3


Recent advances in TAP generation have made significant strides. Yusuf et al.[61] demonstrated that transformer


seq2seq models can generate TAPs with improved service and field accuracy from natural language descriptions. Liu et


al. [28] learning-based approaches, including attention and latent alignment formulations, framed TAP inference as


service/action prediction under weak supervision . However, these state-of-the-art works largely focus on predicting


the correct services and fields, and do not explicitly ensure that generated configurations are aligned with the userâ€™s


higher-level intent or preferences.


Conversely, Wu et al. [55] pioneered implicit intention prediction through multi-view representation learning in their


MvTAP framework, showing that users often create rules driven by latent goals (e.g., â€œenergy savingâ€ when turning


off devices). Yet MvTAP addresses intention classification rather than the end-to-end generation of executable applet


configurations with function-level parameters. In parallel, Hsu et al. [13] showed that even seemingly correct TAP


rules can yield undesirable outcomes due to cross-rule interactions and hidden automation chains, reinforcing that


correctness must be defined at the level of executable behavior, not only service accuracy.


This dichotomy between generation accuracy and intention understanding represents a critical gap. A system can


generate syntactically valid TAPs while still missing the userâ€™s actual intent. On the other hand, a system can correctly


infer, for instance, â€œenergy savingâ€, without being able to translate that goal into concrete trigger conditions and action


parameters. In addition, the move toward agentic LLM tool use introduces a further constraint: systems must reliably


decide _when_ to call tools, _which_ tools to call, and _how_ to parameterize calls, under uncertainty in the user request Schick


et al.[40].


To clearly illustrate these challenges, we performed a systematic study on IFTTT and identified recurrent failure


patterns that neither intent-only nor generation-only approaches fully address:


  - **Understanding the Core of Services:** Services vary widely in functional richness. Popular Web services such


as Google Drive, X (formerly Twitter), and Spotify expose many triggers and actions, while WoT device services


such as smart thermostats or light sensors are often sparse due to hardware constraints. This induces severe


imbalance in training data and increases the risk of overfitting to richly represented services [44].


  - **Intention-Structure Mismatch:** Classification-based approaches can identify latent intent, and generation

based approaches can produce valid structures. However, aligning intent with structural choices (trigger condi

tions, action parameters, and field values) remains unreliable [55, 61].


  - **Context-Dependent Field Selection:** Field names alone are insufficient. The same services may require different


threshold values or options depending on intent (e.g., comfort-focused vs. energy-saving automations), and


current generators are weak at selecting values that reflect user goals [63].


  - **Cross-Functional Reasoning:** Similar intents can couple disparate services (e.g., weather plus messaging,


occupancy plus HVAC). Existing approaches often treat service selection as independent, missing intent-driven


cross-service dependencies [55].


These observations motivate our core insight: effective TAP configuration requires simultaneously understanding


user intentions and generating accurate technical implementations. Traditional approaches, whether intent-centric or


generation-centric, address only one side of this dual requirement.


Through experimentation and analysis of 16k+ real IFTTT applets with enriched metadata, we find that neither


intention understanding nor structure generation alone suffices. The remaining challenge is architectural: bridging intent


comprehension with executable configuration generation. This motivates our multi-agent approach that decomposes


the problem and coordinates complementary capabilities.


Manuscript submitted to ACM


4 Badalov and Yoon


**1.2** **Research Questions**


Our investigation was guided by the following research questions:


(1) **How can we handle severe service-level imbalance across TAP domains, where some platforms expose**


**dozens of triggers and actions while others provide only a few?**


The TAP ecosystem exhibits extreme distributional skew: popular services like Google Drive, X (formerly Twitter),


or Spotify have rich function-level coverage, while niche WoT devices such as Philips Hue or Nest Thermostat


may expose only one or two triggers. This imbalance leads to underrepresentation of sparse function-level


metadata in learned representations, causing models to favor well-represented services regardless of user intent


[44]. We investigate whether training separate encoders for triggers and actionsâ€”combined with a layer freezing


strategy that preserves pretrained semantic knowledgeâ€”can achieve reliable retrieval across both common and


rare services.


(2) **How can a multi-agent architecture decompose the TAP generation task to produce complete, exe-**


**cutable applet configurations rather than just service name predictions?**


Prior approaches predict only which services to use, leaving users to manually configure fields and parameters.


We investigate whether separating the problem into specialized agentsâ€”one for understanding trigger events,


another for reasoning about action requirements, and a third for verifying compatibilityâ€”can generate complete


configurations including field bindings that map trigger outputs to action inputs.


**1.3** **Contributions**


To summarize, this work makes the following contributions:


  - We introduce **FARM** [1] (Field-Aware Resolution Model), a two-stage framework that combines contrastive-trained


dual encoders for high-recall candidate retrieval with multi-agent LLM-based selection for precise configuration


generation.


  - We propose a **layer freezing strategy** for domain-specific encoder fine-tuning that preserves pretrained semantic


knowledge while adapting to trigger-action retrieval, achieving over 90% recall at rank 5 for both triggers and


actions.


  - We design a **four-agent selection pipeline** where specialized agents (Intent Analyzer, Trigger Selector, Action


Selector, Verifier) coordinate to generate complete function-level configurations with field bindingsâ€”not just


service names. These configurations include trigger input parameters, action required fields, and ingredient-to

field mappings that specify how trigger outputs populate action inputs.


  - We demonstrate that FARM achieves **79% joint accuracy** on clear queries, outperforming prior approaches by


+21 points over TARGE [5], while additionally generating executable applet configurations that prior methods


do not produce.


  - We evaluate FARM across three test conditionsâ€”clear queries (Gold data), ambiguous queries (Noisy data), and


rare function-level queries (One-shot data) â€”demonstrating robust performance even on challenging inputs


where existing methods struggle.


[1Code: https://github.com/DinaHongik/FARM. Dataset available upon request: young.yoon@hongik.ac.kr](https://github.com/DinaHongik/FARM)


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 5


**2** **Background and Related Work**


Many existence methods have been conducted in the area of trigger-action programming (TAP), and we divided most


of the relevant research into two groups: 1) Rule synthesis and generation 2) Rule searching and classification.


Early efforts on automatic trigger-action program (TAP) composition treated the problem as a multi-class classification


task mapping natural language (NL) descriptions to pre-defined trigger-action functions. Quirk et al. [36] first framed


TAP generation as classification by training a binary logistic regression classifier for each possible "if-then" rule


component. Their system extracted linguistic features (unigrams, bigrams, character trigrams) from the NL description


and predicted which trigger or action functions should appear in the resulting recipe. While effective, this approach


required one classifier per candidate function, which did not scale as new IoT services and functions emerged. Subsequent


classification-based approaches improved efficiency and accuracy. Yoon et al. [59] has been proposed a CRF-based


learning method that identifies relevant trigger services and predicts trigger-action pairs for user requests. This approach


combined information retrieval with parallel learning engines, outperforming earlier single-model classifiers in both


accuracy and training time. Deep learning further enhanced classification methods: the Latent Attention Model (LAM)


by Liu et al. [28] trained separate neural classifiers for triggers and actions however introduced a "latent" attention


mechanism (LAM) to weight words in the description by importance. LAM achieved state-of-the-art accuracy at the


time, yet still treated trigger and action prediction independently, failing to capture their inter dependencies. In general,


pure classification approaches are fast and straightforward but tend to struggle when user descriptions are implicit or


vague, since they ignore the joint semantics of triggers and actions. To address the limitations of disjoint classification,


researchers turned to generative or semantic parsing techniques that model TAP creation as a structured prediction or


sequence-to-sequence generation problem. Beltagy and Quirk [1] recast TAP synthesis as constructing a parse tree


(sequence of rule components) rather than independent classifications. Their system predicted one component of the


recipe at a time, conditioned on the NL input and previously generated components, using an ensemble of logistic


regression and a multilayer perceptron. This structured approach outperformed the earlier purely binary classification


by Quirk et al. [36] and mitigated the scalability issue, although it still relied on bag-of-words text features and thus


ignored deeper semantic context. More advanced generative models leverage modern neural sequence learning. Yao et


al. [58] developed an interactive semantic parsing framework where a conversational agent learns to synthesize if-then


rules through dialogue, employing hierarchical reinforcement learning to decide on sub-goals and rule components. This


interactive approach allowed the agent to ask clarifying questions and handle complex recipes, but training it required


costly simulation of user interactions. In contrast, one-shot generation models aim to produce the rule in a single pass.


Yusuf et al. [61] introduced RecipeGen, a Transformer-based sequence-to-sequence model that directly "translates" a NL


description into a trigger-action script. By framing TAP creation as language generation rather than classification, their


approach could learn the implicit relationships between trigger and action phrases (e.g., understanding that "photo


tagged with me on Facebook" implies a Facebook file URL trigger and a Dropbox upload action). To boost accuracy,


RecipeGen was warm-started with a pre-trained encoder, adapting it to the IFTTT domain vocabulary. This generative


model significantly outperformed the LAM classifier on multiple real-world TAP datasets, achieving higher recall and


BLEU scores for correct trigger-action predictions. The success of such sequence-learning methods demonstrates that


modeling the joint structure of TAPs can handle unclear or complex user requests better than independent classifiers.


However, generative models must ensure the validity of produced rules (respecting available services/fields), often


addressed through constrained decoding or post-validation.


Manuscript submitted to ACM


6 Badalov and Yoon


A third line of work uses hybrid approaches that combine data-driven learning with domain knowledge or that recast


TAP creation as a recommendation problem. Instead of parsing free-form descriptions, these methods often assume


partial user input (such as chosen trigger or a high-level goal) and leverage existing rule repositories or ontologies to


suggest the rest of the rule. Corno et al. [7] developed RecRules, a recommendation system that represents IoT channels


and functions in a semantic graph and uses collaborative filtering on past user-created rules. Given a userâ€™s current


context or usage history, RecRules performs semantic reasoning over the graph (using manually crafted ontological


rules) and then suggests likely trigger-action combinations based on similarity to what other users have done. This


knowledge-driven system improved the relevance of suggestions but required maintaining an expert-defined semantic


model of the domain.


To incorporate explicit user intentions in automation, Corno et al. [6] proposed TAPrec, which introduced an


ontology (EUPont) of goal-oriented activities to guide rule creation. In TAPrec, when a user selects a trigger, the system


infers the userâ€™s high-level goal (e.g. "personalize room lighting") by mapping the trigger to an OWL class in the


ontology, and then recommends an action that fulfills that goal. This approach effectively casts action selection as a


goal-conditioned classification task, constrained by expert knowledge of typical IoT goals. They later extended this


idea with a conversational assistant called HeyTAP [8] which asks the user questions in natural language, extracts an


abstract intention from the dialogue, and matches it to predefined goal classes to recommend an appropriate if-then


rule. These ontology-backed systems emphasize semantic constraints (ensuring the recommended trigger and action


logically serve the same goal), but they come with the overhead of manually constructing and updating the ontology and


labeling rules with intention metadata. Recent research strives to reduce this manual effort by using learning on graph

structured representations. Huang et al. [16] introduced TAP-AHGNN, an attention-based heterogeneous graph neural


network that learns embeddings for TAP components (devices, triggers, actions) and uses them to recommend services.


TAP-AHGNN integrates a knowledge graph of the IoT domain with GNNs, automatically capturing relationships (e.g.


which devices or services are often used together) and predicting the next action given a trigger context. This hybrid


model leverages structured knowledge and data-driven patterns, outperforming purely collaborative or content-based


recommenders by learning complex cross-service associations (while still requiring an initial knowledge graph schema).


Another emerging trend is applying large language models: King et al. [22] present Sasha, a smart-home assistant that


feeds ambiguous voice commands and home context into an LLM to generate possible automation rules. Sasha showed


the viability of general-purpose LLMs for TAP recommendation, but still needed each training rule to be labeled with


an intention (goal) to guide the model.


**2.1** **Comparison and Limitations of Previous Works**


_2.1.1_ _Comparison._ The evolution from classification-based to generative and hybrid approaches demonstrates a steady


increase in the capability of TAP automation systems. Early classification methods (e.g., Quirk et al. [36], Yoon et


al. [59], Kuang et al. [24]) were effective for known triggerâ€“action pairs but faced challenges in scalability and capturing


semantic nuances. Generative approaches (e.g., Yusuf et al. [61], Yao et al. [58], Liu et al. [29], Liu et al. [30]), ranging


from structured predictors to neural sequence-to-sequence models, introduced greater flexibility by jointly modeling


triggers and actions while using context from natural language descriptions. These methods achieved improved accuracy


on complex user requests and unseen combinations.


_2.1.2_ _Limitations of Previous Work._ While the methods mentioned above improve the efficiency and accuracy of


user-created TAP rules, to the best of our knowledge, none of the existing TAP research explains how a userâ€™s natural


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 7


language query can directly specify which function-level parameters and fields are required, nor how these can be


used to automatically mash up two independent functionâ€”trigger and actionâ€”into a complete applet. As illustrated in


Fig. 2, IFTTT applets operate across _credential boundaries_ (marked with Ã—) that isolate each serviceâ€™s authentication


contextâ€”the trigger serviceâ€™s OAuth credentials never reach the action service, and vice versa. This security model


means that only validated data (ingredients) flows between stages, requiring any automated system to understand both


the schema validation requirements and the field-level mappings needed to bridge these isolated services.


Trigger components Action components Optional filter layer Validation/System layer


Fig. 2. IFTTT applet execution pipeline with credential boundaries.


Beyond this configuration gap, existing approaches suffer from limited expressivity in handling complex automation


scenarios. The simple one-to-one trigger-action mapping requires an "impedance match" between trigger outputs


and action inputs at a shared abstraction level. When deeper customization is neededâ€”such as combining data from


multiple sources or applying conditional transformationsâ€”users must write external "glue code" or resort to traditional


programming methods. This limitation prevents platforms from supporting real-world scenarios involving multiple


conditions, sequential triggers, or stateful computations, ultimately restricting the practical utility of automated TAP


generation systems. Furthermore, current generation methods produce rules in isolation without considering inter-rule


interactions or security implications. When multiple TAP rules are simultaneously enabled, complex system behaviors


emerge that are difficult to diagnose. Wang et al. [49] demonstrated that 66% of synthetic IFTTT deployments exhibit


potential inter-rule vulnerabilities, including action conflicts and infinite actuation loops. Existing TAP synthesis


approaches focus solely on functional correctness for individual rules, lacking mechanisms to verify compatibility with


a userâ€™s existing rule set or to ensure that generated configurations do not introduce unintended security risks. Table 1


provides a comprehensive comparison of existing approaches.


Manuscript submitted to ACM


8 Badalov and Yoon


Table 1. Comparison of TAP Automation Approaches: Functional and Non-Functional Aspects


**Functional Capabilities** **Non-Functional Aspects**
**Authors** **Method** **Input** **Output**

**Compos.** **Iterative** **Service** **Reported** **Dataset** **Data**
**Reasoning** **Refine** **Extend.** **Accuracy** **Scale** **Require.**


Quirk et al. [36] Binary NL query Executable Ã— Ã— Ã— Channel: 50% 114K recipes High
Classification AST (independent) (retrain) Function: 37% 160 channels (labeled pairs)


Yoon et al. [59] CRF-based NL query Service Ã— Ã— Ã— MA: 51.6% 270K pages Medium-High
Learning names (independent) (retrain) NE: 32.7% 1K-9K train (seq. labels)


Liu et al. [28] Neural NL query Service Ã— Ã— Ã— 87.5% 68K train High
(LAM) Attention names (independent) (retrain) (joint pred.) 584 test (labeled pairs)


Beltagy & Quirk [1] Structured NL query Executable Partial Ã— Ã— Tree: 42.6% 77.5K train High
Prediction JSON (sequential) (1-shot) (retrain) Channel: 54% 5.2K dev, 4.3K test (labeled deriv.)


Yusuf et al. [61] Transformer NL query Executable Implicit Ã— Ã— BLEU: 59.1% 45K-120K High
(RecipeGen) Seq2Seq Output (decoder) (1-shot) (retrain) SR@1: 50.6% merged sets (labeled pairs)
MRR@3: 0.575


Yao et al. [58] Hierarchical Dialogue Executable âœ“ âœ“ Ã— Sim: 89.4% 291K recipes High
RL JSON (dialogue) (retrain) Human: 63.4% + sim. dialogue (recipes + sim.)


Corno et al. [6] Ontology Partial Service âœ“ Ã— Manual N/A Ur et al. scale Medium
(TAPrec) Reasoning selection names (goal-based) update (recommends) + EUPont ont. (rules + ont.)


Huang et al. [16] GNN Context Service Partial Ã— Partial HR@10: 0.937 9,884 recipes Medium
(TAP-AHGNN) Embeddings names (graph) NDCG@10: 0.952 1,249 triggers (graph struct.)
MRR@10: 0.967 748 actions


Wu et al. [55] Multi-view Multi-view User Ã— Ã— Ã— Micro-F1: 0.783 N/A Medium
(MvTAP) Learning (U/D/K) intentions (independent) (retrain) Macro-F1: 0.751 N/A (multi-label)


Cimino et al. [5] Cross-view NL query Executable âœ“ Ã— Partial EM: 61% (gold) 34.8K rules Medium-High
(TARGE) Contrastive (user intent) rules (CRG: trigger- (1-shot, (clustering, EM: 41% (noisy) 468 T-channels (rules for
Learning + (channel + action top-K no full EM: 47% (1-shot) 446 A-channels contrastive
LLM + functionality) conditioned) available) retrain) MRR@3: 0.65/0.45/0.50 1.3K T-func learning +
Perplexity 948 A-func LLM LoRA)
Ranking


**FARM** **Contrastive** **NL query** **Executable** âœ“ âœ“ âœ“ **R@1: T72%/A79%, R@5: 92%** **16.5K applets** **Medium**
**(Ours)** **Learning +** **JSON** **(explicit)** **(fallback)** **(index)** **MRR@5: 0.81, MRR@3: 0.84** **1.7K trigger functions** **(12.6K pairs**
**RAG + Multi-Agentic** **JM: 81%/62%/70%** **1.3K action functions** **for encoder)**
**AI** **Faith: 0.44-0.48, Topic: 0.80-0.82**


**Legend:** âœ“= Has capability; Ã—= Does not have capability; N/A = Method does not produce executable outputs by design (recommendation systems).
**Data Requirements:** FARM requires 16.5K applets (12.6K training pairs) for contrastive encoder fine-tuning and 3,011 Function-level schema
descriptions for indexing (Applets). TARGE requires 34.8K rules for cross-view contrastive learning and LLM fine-tuning via LoRA. Other supervised
methods require 45K-291K labeled (query â†’ trigger + action) pairs or complex annotations.
**FARM Metrics:** R@1 = Stage 1 retrieval (T=Trigger encoder, A=Action encoder reported separately); R@5 = Stage 1 retrieval (both encoders); MRR@5 =
Stage 1 retrieval Mean Reciprocal Rank (macro-avg over trigger/action); MRR@3 = End-to-end Selection MRR; JM = Joint Accuracy on
Gold/Noisy/One-shot sets; Faith = Faithfulness; Topic = Topic Adherence (Stage 2 quality).
**Compositional Reasoning:** Explicit verification that trigger outputs match action inputs (FARM, TAPrec); Implicit through decoder architecture
(RecipeGen); Partial through goal-based reasoning (TAPrec, TAP-AHGNN).


**Service Extensibility:** Whether new services can be added without retraining the entire model. FARM supports this through simple index updates.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 9


**3** **Approach**


In this section, we will introduce our purposed method, which comprises the following 2 stages which is illustrated in


Figure 3:


**3.1** **System Architecture Overview**


Stage 1 uses contrastive-trained dual encoders for high-recall candidate retrieval. Stage 2 applies multi-agent selection


with LLM-based verification to produce executable bindings. This leverages complementary strengths: neural speed


and recall, plus LLM precision and schema-awareness.


Formally, given a user query _ğ‘_, Stage 1 retrieves candidate sets:


T _ğ‘˜_ = top- _ğ‘˜_ (sim(E _ğ‘‡_ ( _ğ‘_ ) _,_ E _ğ‘‡_ ( _ğ‘¡ğ‘–_ ))) (1)
_ğ‘¡ğ‘–_ âˆˆT



A _ğ‘˜_ = top- _ğ‘˜_
_ğ‘_ _ğ‘—_ âˆˆA



ï¿½sim(E _ğ´_ ( _ğ‘_ ) _,_ E _ğ´_ ( _ğ‘_ _ğ‘—_ ))ï¿½ (2)



where E _ğ‘‡_ and E _ğ´_ are the trigger and action encoders respectively, T and A are the full function catalogs, and sim(Â· _,_ Â·)

denotes cosine similarity. Stage 2 then selects the optimal pair ( _ğ‘¡_ [âˆ—] _,ğ‘_ [âˆ—] ) âˆˆT _ğ‘˜_ Ã— A _ğ‘˜_ through multi-agent selection and


generates the binding function _ğ›½_ : F _ğ‘_ â†’I _ğ‘¡_ âˆªS, mapping action input fields to trigger output ingredients or static


values.


**3.2** **Stage 1: Contrastive Dual-Encoder Retrieval**


The retrieval stage addresses the scalability challenge by reducing the search space from _ğ‘‚_ (|T | Ã— |A|) to _ğ‘‚_ ( _ğ‘˜_ [2] ) where


_ğ‘˜_ â‰ª|T | _,_ |A|. We train separate encoders for triggers and actions because the same query requires identifying different


semantic aspects: the _event_ to detect (trigger) versus the _action_ to perform.


_3.2.1_ _Schema-Enriched Text Representation._ Unlike prior work that embeds only service names, we encode complete


function schemas to enable field-level retrieval. Each function _ğ‘¥_ âˆˆT âˆªA is represented as:


text( _ğ‘¥_ ) = [channel] [category] name _._ desc || schema( _ğ‘¥_ ) (3)


where || denotes concatenation and schema( _ğ‘¥_ ) encodes the functionâ€™s data interface:


**Trigger Schema** (data provider):


        schema( _ğ‘¡_ ) = â€œProvides: â€ (name _ğ‘–,_ type _ğ‘–_ ) (4)


_ğ‘–_ âˆˆI _ğ‘¡_


**Action Schema** (data consumer):


       schema( _ğ‘_ ) = â€œRequires: â€ (name _ğ‘“_ _,_ required _ğ‘“_ ) (5)


_ğ‘“_ âˆˆF _ğ‘_


where I _ğ‘¡_ denotes the ingredient set of trigger _ğ‘¡_, F _ğ‘_ denotes the field set of action _ğ‘_, and [ï¿½] represents formatted

concatenation. This representation enables the model to learn that, for example, â€œlog sensor dataâ€ should retrieve


actions requiring data fields, not just services with â€œlogâ€ in the name.


_Contrastive Learning with InfoNCE._ We train encoders using the InfoNCE objective [46], which learns to discriminate


positive query-function pairs from in-batch negatives:


Manuscript submitted to ACM


10 Badalov and Yoon







exp( _ğ‘ _ ( _ğ‘,ğ‘‘_ [+] )/ _ğœ_ )
log ~~ï¿½~~ _ğµ_
_ğ‘–_ =1 [exp][(] _[ğ‘ ]_ [(] _[ğ‘,ğ‘‘][ğ‘–]_ [)/] _[ğœ]_ [)]







LInfoNCE = âˆ’E( _ğ‘,ğ‘‘_ + )âˆ¼D



(6)



where _ğ‘ _ ( _ğ‘,ğ‘‘_ ) = cos(E( _ğ‘_ ) _,_ E( _ğ‘‘_ )) is the cosine similarity between query and document embeddings, _ğœ_ is the temperature

hyperparameter, _ğµ_ is the batch size, and _ğ‘‘_ [+] is the positive (ground-truth) function for query _ğ‘_ . The denominator sums


over all _ğµ_ documents in the batch, treating co-occurring samples as hard negatives.


The temperature _ğœ_ = 0 _._ 05 creates a sharp softmax distribution that strongly penalizes confusion between similar


candidates. With batch size _ğµ_ = 16, each training step provides 15 hard negatives sampled from the same domain,


encouraging fine-grained discrimination.


_3.2.2_ _Semantic Preservation via Layer Freezing._ A critical challenge in domain-specific training is catastrophic forget

ting [9, 23, 26, 62]: the model loses pretrained semantic knowledge while adapting to the target task. We observed that


full training caused the model to memorize exact service names while forgetting semantic equivalences (e.g., â€logâ€ â‰ˆ


â€recordâ€ â‰ˆ â€œadd rowâ€).

We address this through selective layer freezing. Let _ğœƒ_ = { _ğœƒ_ [(][0][)] _,ğœƒ_ [(][1][)] _, ...,ğœƒ_ [(] _[ğ¿]_ [)] } denote the parameters of an _ğ¿_ -layer


transformer. We partition layers into frozen and trainable sets:


T0 (0.85)
T1 (0.65)
T2 (0.58)
T3 (0.85)
T4 (0.65)


A0 (0.79)
A1 (0.55)
A2 (0.47)
A3 (0.68)
A4 (0.47)


Fig. 3. FARM two-stage architecture. **Stage 1** : Dual contrastive encoders retrieve top- _ğ‘˜_ trigger and action candidates; cross-score
matrix ranks _ğ‘˜_ Ã— _ğ‘˜_ pairs by compatibility scores (70% ingredient-field coverage, 30% retrieval quality). **Stage 2** : Field-aware selection
pipeline performs deep ingredient-to-field analysisâ€”Intent Analyzer decomposes query intent, Trigger Selector identifies optimal
trigger _ğ‘¡_ [âˆ—] and extracts its ingredient schema _ğ¼ğ‘¡_ (available output fields), Action Selector evaluates field compatibility and generates
bindings _ğ›½_ mapping ingredients to action input fields, and Verifier scores binding completeness and semantic coherence. Fallback
retries with next candidate pair when verification score _ğ‘ _ _< ğœƒ_ .


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 11


Fig. 4. Contrastive training pipeline for dual encoders. **(a)** Training data construction: query-function pairs extracted from IFTTT
dataset. **(b)** Dual encoder architecture with layer freezingâ€”trainable layers (orange, 18%) adapt to domain-specific patterns while
frozen layers (blue-gray, 82%) preserve pretrained semantics. **(c)** In-batch negative sampling: diagonal entries are positive pairs,
off-diagonal entries serve as hard negatives. **(d)** InfoNCE objective in embedding space: queries are pulled toward positive documents
and pushed away from negatives.


_ğœƒ_ frozen = { _ğœƒ_ [(][0][)] _, ...,ğœƒ_ [(] _[â„“]_ [)] } _,_ _ğœƒ_ train = { _ğœƒ_ [(] _[â„“]_ [+][1][)] _, ...,ğœƒ_ [(] _[ğ¿]_ [)] } (7)


During training, gradients are computed only for _ğœƒ_ train:


_ğœƒ_ train [(] _[ğ‘¡]_ [+][1][)] [=] _[ ğœƒ]_ train [(] _[ğ‘¡]_ [)] [âˆ’] _[ğœ‚]_ [âˆ‡] _[ğœƒ]_ train [L][InfoNCE] (8)


For our final transformer encoder, we have chosen EmbeddingGemma [39] with 24 layers. We froze layers 0-11


(including the embedding layer), resulting in 82% of the parameters (252M of 307M) frozen. This preserves the pretrained


modelâ€™s semantic generalization in lower layers while allowing upper layers to learn domain-specific retrieval patterns.


**Theoretical Motivation.** Research on transformer representations [19, 31, 43] shows that lower layers encode


lexical and syntactic features while upper layers encode task-specific semantics. By freezing lower layers, we preserve


the modelâ€™s ability to recognize semantic equivalences (â€œdarkness detectedâ€ â‰ˆ â€œlow light sensedâ€) while training upper


layers for trigger-action discrimination.


_3.2.3_ _Training Configuration._ We train separate encoders for triggers and actions with identical architectures but


independent weights. Table 2 summarizes the key hyperparameters. We use a low temperature ( _ğœ_ = 0 _._ 05) to create sharp


softmax distributions that strongly penalize confusion between similar candidates. With batch size 16, each training


step provides 15 hard negatives sampled from the same domain, encouraging fine-grained discrimination between


semantically similar functions.


_3.2.4_ _Trigger-Action Programming Formulation._ Trigger Action Programming TAP enables end users to create automa

tion rules in the form IF trigger THEN action, following the paradigm of Ur et al. [44]. Formally, given a user query _ğ‘_


expressed in natural language, the system must:


(1) **Select** a trigger _ğ‘¡_ [âˆ—] âˆˆT from the trigger catalog


Manuscript submitted to ACM


12 Badalov and Yoon


**Parameter** **Value**


Base Model Pretrained Text Encoder
Frozen Layers 0â€“11 (+ embeddings)
Loss Function InfoNCE
Temperature _ğœ_ 0.05
Batch Size 16
Learning Rate 2 Ã— 10 [âˆ’][5]

Epochs 3
Training Pairs 12,652


Table 2. Encoder training configuration.


(2) **Select** an action _ğ‘_ [âˆ—] âˆˆA from the action catalog


(3) **Generate** bindings _ğ›½_ : F _ğ‘_ â†’I _ğ‘¡_ âˆªS mapping action input fields to trigger output ingredients or static values


Each trigger _ğ‘¡_ exposes a set of _ingredients_ I _ğ‘¡_ = { _ğ‘–_ 1 _,ğ‘–_ 2 _, ..._ }â€”typed output fields produced when the trigger fires (e.g.,

Subject, Body for an SMS trigger). Each action _ğ‘_ requires a set of _fields_ F _ğ‘_ = { _ğ‘“_ 1 _, ğ‘“_ 2 _, ..._ }â€”typed input parameters needed


for execution (e.g., To, Subject for an email action). The binding function _ğ›½_ creates the data flow that makes applets


executable.


_3.2.5_ _Why Retrieval Alone is Insufficient._ While retrieval-augmented generation (RAG) has achieved success in many


NLP tasks [18, 25], several factors make pure retrieval insufficient for TAP:


**Selection Ambiguity.** High recall ensures correct functions appear in the candidate set, but does not identify _which_


candidate is correct. With high retrieval recall (Section 4.4.1 shows R@5 _>_ 92% for both encoders), selecting the correct


pair from _ğ‘˜_ Ã— _ğ‘˜_ combinationsâ€”25 candidate pairs when _ğ‘˜_ = 5 (Fig. 14)â€”requires reasoning beyond embedding similarity.


Naive selection that simply takes the rank-1 result from each encoder evaluates only 1 pair and achieves 58% joint


accuracy, whereas multi-agent search over all 25 pairs achieves 81% (Section 4.6).


**Schema Compatibility.** Retrieval operates on text similarity, not schema compatibility. A trigger providing


[temperature, humidity] may rank highly for â€œweather loggingâ€ but be incompatible with an action requiring


[image_url, caption]. Understanding data flow between functions requires schema-level reasoning.


**Binding Generation.** Even with correct trigger-action pairs, generating executable applets requires mapping trigger


ingredients to action fieldsâ€”determining that StockName should fill the row_content field. This semantic alignment


task is beyond retrievalâ€™s capability.


**Ambiguity Resolution.** User queries often admit multiple valid interpretations. â€œTurn on lights when I leaveâ€ could


use location triggers (GPS-based) or calendar triggers (schedule-based); an LLM can reason about context and user


intent to select appropriately.


These limitations motivate our two-stage architecture: retrieval for efficient candidate generation, followed by


multi-agent selection for reasoning and binding.


_3.2.6_ _Design Consideration: Why Not LoRA?._ Low-Rank Adaptation (LoRA) [14] has become the dominant parameter

efficient fine-tuning method, achieving impressive results across many tasks by learning low-rank update matrices

Î” _ğ‘Š_ = _ğµğ´_ where _ğµ_ âˆˆ R _[ğ‘‘]_ [Ã—] _[ğ‘Ÿ]_ and _ğ´_ âˆˆ R _[ğ‘Ÿ]_ [Ã—] _[ğ‘‘]_ with rank _ğ‘Ÿ_ â‰ª _ğ‘‘_ . Extensions like QLoRA [10] further reduce memory


requirements through quantization.


However, for contrastive retrieval training, we found layer freezing superior to LoRA for three reasons:


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 13


Fig. 5. Stage 2 multi-agent selection components. (a) Agent structure showing input query, LLM processing with retrieved candidates
_ğ¶ğ‘˜_, and structured JSON output. (b) State-based pipeline where Intent Analyzer, Trigger Selector, Action Selector, and Verifier
communicate through shared state. (c) Ingredient-to-field binding illustrating how trigger outputs (Subject, Body) dynamically
populate action inputs, with static user-provided values for required fields.


**Semantic Preservation.** LoRA modifies all layers simultaneously through low-rank updates, which can subtly alter


the pretrained semantic space. Layer freezing ensures that frozen layers retain pretrained representations, which we


validate empirically in Section 4.6.


**Representation Stability.** Contrastive learning requires stable representations for effective in-batch negative


sampling. LoRAâ€™s distributed updates across all layers can destabilize the embedding space during training, whereas


layer freezing maintains a stable foundation in lower layers while adapting upper layers.


**Computational Simplicity.** Layer freezing requires no additional adapter modules, reducing implementation


complexity and inference overhead. The frozen/trainable partition is straightforward: gradients simply are not computed


for frozen parameters.


Recent work by Biderman et al. [2] has shown that LoRA can underperform full fine tuning on certain tasks, and


our domain, contrastive retrieval with small datasets, appears to be one where the inductive bias of layer freezing that


preserves lower layer semantics is particularly beneficial.


_3.2.7_ _State-Based Agent Communication._ Unlike message-passing multi-agent systems where agents exchange discrete


messages, our agents communicate through a _shared state object_ that accumulates information as execution proceeds.


This design pattern, common in dataflow architectures, provides several advantages: (1) any downstream agent can


access any upstream decision, (2) state can be serialized for debugging and replay, and (3) conditional routing decisions


can inspect any state field.


Table 3 defines the key state fields and their producers/consumers.


Manuscript submitted to ACM


14 Badalov and Yoon


**State Field** **Producer** **Consumer**


query _ğ‘_ Input All agents
trigger_candidates T _ğ‘˜_ Stage 1 Trigger Selector
action_candidates A _ğ‘˜_ Stage 1 Action Selector
search_intents ( _ğ‘ğ‘‡_ _,ğ‘ğ´_ ) Intent Analyzer Selectors
selected_trigger ( _ğ‘¡_ [âˆ—] _, ğ¼ğ‘¡_ ) Trigger Selector Action Selector
bindings ( _ğ‘_ [âˆ—] _, ğ›½_ ) Action Selector Verifier
verifier_score _ğ‘ _ Verifier Routing logic
llm_overrode_rag Selectors Fallback logic


Table 3. State schema for inter-agent communication. Each agent reads upstream fields and writes to designated output fields.


_3.2.8_ _Intent Analyzer Agent._ The Analyzer Agent initiates Stage 2 by analyzing the user query using chain-of-thought


(CoT) reasoning [54]. Given query _ğ‘_, the agent performs:


**Intent Decomposition.** The analyzer identifies two distinct intents embedded in the query: (1) the trigger event to


detect, and (2) the action to execute. For example, given â€œChange the light to green if stock price rises,â€ the analyzer


extracts:


   - Trigger intent: â€œstock price risesâ€ (financial market event)


  - Action intent: â€œchange light to greenâ€ (smart home control)


**Search Query Generation.** The analyzer generates optimized search queries ( _ğ‘ğ‘‡_ _,ğ‘ğ´_ ) to clarify trigger and action


intents. These queries may expand abbreviations (â€œtempâ€ â†’ â€œtemperatureâ€), resolve ambiguities, or reformulate


colloquial expressions into function-compatible terminology.


**Reasoning Trace.** Following the CoT paradigm [50, 53], the agent produces an explicit THINKING section followed


by a Intent Analyze:


THINKING: 1. User wants to detect stock price changes (TRIGGER).


2. User wants to control smart light color (ACTION).


3. Need financial function for trigger, IoT function for action.


PLAN: Search for stock/price triggers and light/color actions.


_3.2.9_ _Trigger Selector Agent._ The Trigger Selector applies a two-stage selection process combining neural retrieval


with LLM re-ranking, following the retrieve-then-rerank paradigm established in modern information retrieval [35].


**Candidate Pair Ranking.** Rather than selecting trigger and action independently, we pre-compute compatibility


scores for all _ğ‘˜_ Ã— _ğ‘˜_ candidate pairs:

score( _ğ‘¡ğ‘–,ğ‘_ _ğ‘—_ ) = 0 _._ 7 Â· coverage( _ğ‘¡ğ‘–,ğ‘_ _ğ‘—_ ) + 0 _._ 3 Â· [sim][(] _[ğ‘,ğ‘¡][ğ‘–]_ [) +][ sim][(] _[ğ‘,ğ‘]_ _[ğ‘—]_ [)] (9)

2


where coverage( _ğ‘¡ğ‘–,ğ‘_ _ğ‘—_ ) measures ingredient-to-field compatibility (defined below in Equation 11) and the retrieval


similarity term provides ranking based on Stage 1 encoder quality. This weighted combination prioritizes schema


compatibility (70%) while using retrieval confidence (30%) as a tie-breaker. Pairs are sorted by this score to create a


priority queue enabling best-first search through the candidate space.


**Agreement-Based Selection.** The agent presents top- _ğ‘˜_ trigger candidates to the LLM with full function descriptions.


The LLM selects its preferred match _ğ‘¡ğ¿ğ¿ğ‘€_ with reasoning. To prevent hallucination from degrading retrieval quality, we


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 15


employ an _agreement-based_ mechanism:



_ğ‘¡ğ¿ğ¿ğ‘€_ if _[ğ‘ ][ğ‘…ğ´ğº]_ [(] _[ğ‘¡][ğ¿ğ¿ğ‘€]_ [)] â‰¥ _ğœğ‘‡_

_ğ‘ ğ‘…ğ´ğº_ ( _ğ‘¡_ 0)


_ğ‘¡_ 0 otherwise



_ğ‘¡_ [âˆ—] =



ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³



(10)



where _ğ‘¡_ 0 is RAGâ€™s top candidate, _ğ‘ ğ‘…ğ´ğº_ (Â·) denotes retrieval similarity, and _ğœğ‘‡_ = 0 _._ 95 is the trigger agreement threshold.

This ensures LLM can only override RAG when its choice has comparable retrieval confidence, balancing reasoning


capability with retrieval reliability.


**Ingredient Extraction.** After selection, the agent extracts available ingredients _ğ¼ğ‘¡_ = {( _ğ‘›ğ‘ğ‘šğ‘’,ğ‘¡ğ‘¦ğ‘ğ‘’,ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’_ )} from


the triggerâ€™s function schema. Ingredients represent data fields emitted when the trigger fires (e.g., StockName, Price,


PercentageChange).


_3.2.10_ _Action Selector Agent._ The Action Selector receives the selected trigger ( _ğ‘¡_ [âˆ—] _, ğ¼ğ‘¡_ ) and evaluates action candidates


for schema compatibility.


**Cross-Scoring.** The agent computes a coverage score measuring how well trigger ingredients can satisfy action


requirements:

_ğ‘_ : âˆƒ _ğ‘–_ âˆˆ _ğ¼ğ‘¡_ _,_ match( _ğ‘–, ğ‘“_ )}|
coverage( _ğ‘¡,ğ‘_ ) = [|{] _[ğ‘“]_ [âˆˆF] _[ ğ‘Ÿğ‘’ğ‘]_ (11)

|F _ğ‘_ ~~_[ğ‘Ÿğ‘’ğ‘]_~~ |

where F _ğ‘_ _[ğ‘Ÿğ‘’ğ‘]_ denotes required action fields. The match( _ğ‘–, ğ‘“_ ) function checks compatibility using:


  - **Direct matching** : temperature â†’ temperature


  - **Substring matching** : stock_name â†’ name


  - **Semantic mapping** : message â‰ˆ body â‰ˆ content


**LLM Re-ranking.** Similar to trigger selection, the LLM re-ranks action candidates with full context about the


selected trigger and its ingredients. The agreement threshold _ğœğ´_ = 0 _._ 80 is lower than triggers, as action selection benefits


more from LLM reasoning about functional intent.

**Binding Generation.** Upon selecting action _ğ‘_ [âˆ—], the agent generates bindings _ğ›½_ mapping each action field to a data


source:


_ğ›½_ = {( _ğ‘“,_ src _,_ val) : _ğ‘“_ âˆˆF _ğ‘_ } (12)


where src âˆˆ{ingredient _,_ static}. Ingredient bindings use trigger data (e.g., StockName â†’ row_content); static


bindings use placeholder values for fields without matching ingredients.


_3.2.11_ _Verifier Agent._ The Verifier implements the LLM-as-Judge pattern [64] to evaluate the complete applet configu
ration ( _ğ‘¡_ [âˆ—] _,ğ‘_ [âˆ—] _, ğ›½_ ):


  - **Binding Quality** : Are bindings semantically appropriate?


  - **Completeness** : Are all required action fields bound?


  - **Executability** : Can this configuration execute without runtime errors?


The Verifier produces a quality score _ğ‘ _ âˆˆ[0 _,_ 1] and natural language critique. A rule-based fallback handles LLM


parsing failures by checking trigger/action presence, binding count, and required field coverage.


_3.2.12_ _Quality-Gated Fallback._ When the verifier score falls below threshold _ğœƒğ‘£_ = 0 _._ 5, the system triggers fallback


logic:


Manuscript submitted to ACM


16 Badalov and Yoon


(1) **LLM Fallback** : If the LLM overrode RAGâ€™s selection (llm_overrode_rag = true), retry with RAGâ€™s original top


choice. This catches cases where LLM reasoning was incorrect.


(2) **Pair Iteration** : If still failing, advance to the next candidate pair from the priority queue and restart from Trigger


Selector.


The system attempts up to _ğ‘˜_ [2] pairs before declaring failure. In our evaluation (Section 4.4.2), most queries succeed


within two attempts due to Stage 1â€™s high recall.


_3.2.13_ _Execution Example._ We illustrate the complete pipeline with query: â€œChange light to green if stock price rises.â€


**Step 1: Analyzer.** Analyzes query and decomposes: _ğ‘ğ‘‡_ = â€œstock price rises,â€ _ğ‘ğ´_ = â€œchange light color.â€ Produces


reasoning trace explaining the decomposition.


**Step 2: Trigger Selector.** Receives candidates T5 from RAG. Top results: _ğ‘‡_ 0 = â€œPrice rises aboveâ€ (0.753), _ğ‘‡_ 1 =

â€œTodayâ€™s price rises by percentageâ€ (0.744). LLM selects _ğ‘‡_ 1 with reasoning about percentage-based detection being more

appropriate. Agreement ratio 0 _._ 744/0 _._ 753 = 0 _._ 99 â‰¥ 0 _._ 95, so LLM override accepted. Extracts _ğ¼ğ‘¡_ = [StockName, Price,


PercentageChange, CheckTime].


**Step 3: Action Selector.** Evaluates A5 against _ğ¼ğ‘¡_ . Computes coverage scores. Selects _ğ´_ 0 = â€œTurn on / change light

modeâ€ (0.627). Generates bindings: _ğ›½_ = [(color, static, â€œgreenâ€), (light, static, â€œLiving roomâ€)].

**Step 4: Verifier.** Evaluates ( _ğ‘¡_ [âˆ—] _,ğ‘_ [âˆ—] _, ğ›½_ ). Confirms all required fields bound. Assigns score _ğ‘ _ = 0 _._ 85 â‰¥ 0 _._ 5. Output


accepted.


_3.2.14_ _Chain-of-Thought Prompting._ All agents employ chain-of-thought (CoT) prompting [4] to produce interpretable


reasoning traces. Each prompt instructs the LLM to:


(1) Analyze user intent explicitly before making decisions


(2) Evaluate each candidateâ€™s fitness with stated criteria


(3) Check schema compatibility between data sources and targets


(4) Justify the final selection with concrete reasoning


This structured deliberation improves decision accuracy through explicit reasoning and provides interpretability for


system debugging and error analysis.


**3.3** **Embedding Model Selection**


The choice of base encoder significantly impacts retrieval quality. We identify three key selection criteria:


**Semantic Understanding.** The encoder should be trained on diverse corpora to provide robust semantic represen

tations that generalize to WoT service descriptions. Models trained primarily on narrow domains may struggle with the


mixed technical vocabulary common in function documentation.


**Efficiency.** The model should balance performance with computational tractability for domain-specific training.


Larger models often offer diminishing returns, as pretrained general knowledge matters less than task-specific adaptation.


**Embedding Dimensionality.** The embedding dimension should balance expressiveness with retrieval efficiency,


enabling fast approximate nearest neighbor search over large function catalogs.


For our implementation, we select Gemma Embedding [39] ( 307M parameters, 768-dimensional embeddings), which


satisfies these criteria while remaining efficient for contrastive training.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 17


**4** **Experiments**


We evaluate FARM through comprehensive experiments designed to assess both stages of our architecture. Our


evaluation addresses three key questions: (1) Does contrastive training with layer freezing improve retrieval quality


while preserving semantic generalization? (2) Does multi-agent selection improve end-to-end accuracy over retrieval


alone? (3) How does FARM compare to existing trigger-action programming approaches?


**4.1** **Function-Level TAP Dataset**


Our dataset comprises 1,724 trigger functions and 1,287 action functions from the IFTTT platform, spanning 19


categories including Smart Home, Finance, Health, Developer Tools, Social Media, and others. This creates a search


space of 1 _,_ 724 Ã— 1 _,_ 287 = 2 _,_ 218 _,_ 788 possible trigger-action pairs. Critically, our dataset operates at the _function level_


with complete schema specifications, which differs fundamentally from traditional service-level identification tasks.


_4.1.1_ _Service-Level vs. Function-Level Granularity._ To understand this distinction, consider the difference between


identifying a service and selecting a specific trigger or action:


  - **Service-Level (Service):** Identify the service providing functionality (e.g., â€œThe New York Timesâ€, â€œGoogle


Sheetsâ€, â€œEvernoteâ€)


  - **function-Level (Trigger/Action):** Select the specific trigger or action within that service (e.g., â€œNew article


from searchâ€, â€œAdd row to spreadsheetâ€, â€œAppend to noteâ€)


A single service may expose dozens of distinct trigger and action functions. For instance, the New York Times service


provides multiple trigger functions (â€œNew article from searchâ€, â€œNew popular articleâ€, â€œNew article in sectionâ€, etc.), each


with different functionality, parameters, and data outputs. Service-level identification only determines _which service_ to


use. Functions-level selection determines _which specific trigger or action_ to configure and execute.


_4.1.2_ _Schema Structure and Data Interface Specifications._ Each trigger or action function in our dataset includes


complete schema specifications that define its data interface. Table 4 shows a concrete example from our dataset,


illustrating the full schema structure for a trigger-action pair.:


**Trigger Schema:**


  - service_name: Function identifier (e.g., â€œNew article from searchâ€)


  - category: Domain category (e.g., â€œNews & informationâ€)


  - description: Natural language explanation of trigger behavior


  - Trigger fields: Input parameters required to configure the trigger (e.g., â€œSearch forâ€ with type String)


  - Ingredients: Output data fields produced when the trigger fires, with:


**â€“** Slug: Programmatic identifier (e.g., Title, ArticleUrl)


**â€“** Type: Data type (String, Date, Number, Boolean, etc.)


**â€“** Filter code: Access path used by the IFTTT filter code environment


**â€“** Example: Representative value showing expected format


**Action Schema:**


  - service_name: Function identifier (e.g., â€œAppend to noteâ€)


  - category: Domain category (e.g., â€œPopular servicesâ€)


  - description: Natural language explanation of action behavior


Manuscript submitted to ACM


18 Badalov and Yoon


**Trigger Function: â€œNew article from searchâ€** **Action Function: â€œAppend to noteâ€**


**Category:** News & information **Category:** Popular services



**Description:** This Trigger fires every time a new article that is
published by The New York Times matches a search query you
specify.



**Description:** This Action will append to a note as determined
by its title and notebook.



**Trigger Fields:** **Action Fields:**

 - Search for (String, required)  - Title (String, required)

                                  - Body (String, required)

                                  - Notebook (String, optional)

                                    - Tags (String, optional)


**Ingredients (Outputs):** **Data Flow Mapping:**

 - Title (String) Ingredients â†’ Fields

 - Author (String)

 - Blurb (String) Example binding:

 - ArticleUrl (String)  - Ingredient Title â†’ Field Title

 - ImageUrl (String)  - Ingredient Blurb â†’ Field Body

 - Source (String)  - Ingredient Keywords â†’ Field Tags

 - Section (String)

 - Keywords (String)

 - PublishedDate (Date with time)


Table 4. Example function-Level Schema from Dataset. The trigger provides 9 typed ingredients (Title, Author, Blurb, ArticleUrl,
ImageUrl, Source, Section, Keywords, PublishedDate); the action requires 4 fields (Title, Body, Notebook, Tags), with Title and
Body marked as required. This schema information enables reasoning about data flow compatibility and automatic generation of
ingredient-to-field bindings.


  - Action fields: Input parameters required to execute the action, with:


**â€“** Label: Human-readable field name (e.g., â€œTitleâ€, â€œBodyâ€)


**â€“** Slug: Programmatic identifier (e.g., title, body)


**â€“** Required: Boolean indicating if field must be provided


**â€“** Helper text: Usage instructions or constraints


**â€“** Filter code method: Method signature used by the IFTTT filter code environment to set the field value


_4.1.3_ _Why Function-Level Schemas Enable Executable Applets._ The distinction between service-level and function-level


is not merely semantic. It fundamentally changes the task complexity and system requirements.


**Service-Level Limitation:** Prior work treats TAP as a classification problem over service names. A system that


outputs â€œThe New York Timesâ€ and â€œEvernoteâ€ provides no actionable information. Which specific trigger should be


used. Which specific action should be selected. How to configure the parameters. Which ingredient values should fill


which action fields. The output requires extensive manual configuration before execution.


**Function-Level Advantage:** Our datasetâ€™s function-level schemas enable fully automated applet generation. By


including:


  - **Function specifications** : The exact trigger and action functions to select (â€œNew article from searchâ€, â€œAppend


to noteâ€)


  - **Type information** : Data type constraints for validation (String, Date, Boolean)


  - **Required field markers** : Which parameters must be provided versus optional


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 19


  - **Ingredient definitions** : Available data outputs from triggers


  - **Field requirements** : Expected inputs for actions


The system can reason about _schema compatibility_ (Does the trigger produce data the action needs?), generate


_ingredient-to-field bindings_ (Which trigger outputs map to which action inputs?), and produce _executable configurations_


ready for deployment without human intervention.


For example, given the query â€œSend New York Times articles about recipes to Evernoteâ€, a service-level system


outputs service names, while our function-level system produces:


   - Trigger: â€œNew article from searchâ€ with field Search for = â€œrecipeâ€


  - Action: â€œAppend to noteâ€ with bindings:


**â€“** Title â† Ingredient Title


**â€“** Body â† Ingredient Blurb


**â€“** Tags â† Ingredient Keywords


This complete specification is executable within the IFTTT execution model without further configuration. The


function-level granularity with schema information is what makes our task and dataset fundamentally different from


prior service identification approaches.


We evaluate FARM on the IFTTT platform, a widely-used trigger-action programming service for WoT (Web of


Things) automation. The service catalog comprises:


  - **Triggers** : 1,724 distinct functions across 19 categories (Smart Home, Finance, Health, Developer Tools, Social


Media, etc.)


  - **Actions** : 1,287 distinct functions across 19 categories


  - **Search Space** : 1 _,_ 724 Ã— 1 _,_ 287 = 2 _,_ 218 _,_ 788 possible trigger-action function pairs


Each function entry contains structured schema information: function name, category, natural language description,


and data interface specifications (ingredients for triggers, required fields for actions). Following IFTTTâ€™s internal


terminology, the dataset labels specific functions as service_name (e.g., â€œNew article from searchâ€, â€œAdd row to


spreadsheetâ€), which we evaluate at the function-level rather than the platform-level (e.g., â€œThe New York Timesâ€,


â€œGoogle Sheetsâ€).


_Terminology._ IFTTT refers to individual trigger/action functions as service_name (e.g., â€œAdd row to spreadsheetâ€).


We refer to these as _functions_ throughout. We reserve _platform service_ for the parent application (e.g., â€œGoogle Sheetsâ€).


Unless stated otherwise (e.g., Table 5), all Stage 1 and Stage 2 metrics are computed at the function level.


**4.2** **Experimental Setup**


_4.2.1_ _Evaluation Sets._ Following the evaluation protocol established by Cimino et al. [5], we construct three evaluation


sets to test different aspects of system robustness:


  - **Gold Set** : Clear, well-formed automation requests with unambiguous intent (e.g., â€œWhen darkness detected, log


to spreadsheetâ€)


  - **Noisy Set** : Vague or ambiguous descriptions requiring inference (e.g., â€œtrack my fitness stuffâ€)


  - **One-Shot Set** : Queries involving rare or specialized functions with limited training exposure


_4.2.2_ _Model Configuration._ **Stage 1 - Contrastive Encoders:**


Manuscript submitted to ACM


20 Badalov and Yoon


  - **Base Model** : EmbeddingGemma [39] ( 307M parameters, 768-dimensional embeddings)


  - **Architecture** : 24 transformer layers, 12 attention heads, 2,048 token context


  - **Layer Freezing** : Layers 0â€“11 frozen, layers 12â€“23 trainable (18% of total parameters trainable)

  - **Training** : InfoNCE loss [46], _ğœ_ = 0 _._ 05, batch size 16, 3 epochs, learning rate 2 Ã— 10 [âˆ’][5]


**Stage 2 - Multi-Agent Selection:**


  - **LLM** : IBM Granite 4.0 Small [17] via local inference


  - **Temperature** : 0.0 (deterministic outputs)


  - **Note** : The multi-agent system operates through _prompting only_ â€”no additional training is performed on the LLM.


As Brown et al. [3] demonstrated, large-scale language models can achieve strong task performance without


fine-tuning, and Wei et al. [52] showed that emergent abilities arise at sufficient scale through prompting alone.


Agents use carefully designed system prompts with chain-of-thought reasoning [53] to elicit step-by-step analysis


before producing structured outputs.


**4.3** **Evaluation Metrics**


We employ metrics from established benchmarks to enable fair comparison with prior work.


_4.3.1_ _Stage 1: Retrieval Metrics._ We evaluate retrieval effectiveness using Recall at cutoff _ğ¾_ (Recall@K) and Mean


Reciprocal Rank at cutoff _ğ¾_ (MRR@K), two standard ranking metrics in information retrieval [32, 47].


**Recall@K** measures retrieval coverage: what fraction of queries have the correct answer in the top- _ğ¾_ results? Given


a query set _ğ‘„_ :



Recall@K = [1]

| _ğ‘„_ |



âˆ‘ï¸ - 
I âˆƒ _ğ‘‘_ âˆˆD _ğ‘_ _[ğ¾]_ s.t. _ğ‘‘_ âˆˆR _ğ‘_ _,_ (13)

_ğ‘_ âˆˆ _ğ‘„_



where D _ğ‘_ _[ğ¾]_ [denotes the top] _[ ğ¾]_ [retrieved documents for query] _[ ğ‘]_ [,][ R] _[ğ‘]_ [is the set of relevant documents for] _[ ğ‘]_ [, and][ I][(Â·)][ is the]


indicator function. Practically, R@1 asks: â€œIs the correct answer ranked first?â€ while R@5 asks: â€œIs the correct answer


somewhere in the top 5?â€ We report R@1 and R@5 to evaluate both precision (top result quality) and recall (candidate


set coverage for Stage 2).


**Joint Recall@K** extends this to dual-encoder retrieval, measuring when _both_ components succeed simultaneously:



Joint Recall@K = [1]

| _ğ‘„_ |



âˆ‘ï¸ - 
I _ğ‘¡_ true âˆˆD _ğ‘_ _[ğ¾]_ [(] _[ğ‘‡]_ [) âˆ§] _[ğ‘]_ [true] [âˆˆD] _ğ‘_ _[ğ¾]_ [(] _[ğ´]_ [)] _,_ (14)

_ğ‘_ âˆˆ _ğ‘„_



where D _ğ‘_ _[ğ¾]_ [(] _[ğ‘‡]_ [)][ and][ D] _ğ‘_ _[ğ¾]_ [(] _[ğ´]_ [)][ denote the top] _[ ğ¾]_ [triggers and actions retrieved for query] _[ ğ‘]_ [. For example, Joint R@5 = 85%]


means that for 85% of queries, the correct trigger appears in the trigger encoderâ€™s top-5 _and_ the correct action appears


in the action encoderâ€™s top-5. Critically, Joint R@5 establishes the _theoretical ceiling_ for Stage 2 performance: Stage 2


selection cannot choose a correct pair that Stage 1 did not retrieve (Fig. 14).


**Mean Reciprocal Rank (MRR@K)** measures ranking quality by considering _where_ the correct answer appears


within the top- _ğ¾_ results:



rank1 _ğ‘_ if rank _ğ‘_ â‰¤ _ğ¾,_ (15)

0 if rank _ğ‘_ _> ğ¾,_



MRR@K = [1]

| _ğ‘„_ |



âˆ‘ï¸


_ğ‘_ âˆˆ _ğ‘„_



ï£±ï£´ï£´ï£²

ï£´ï£´ï£³



where rank _ğ‘_ is the rank position of the first relevant document for query _ğ‘_ . MRR@K gives partial credit based on


position: rank 1 receives score 1.0, rank 2 receives 0.5, rank 5 receives 0.2, while anything beyond rank _ğ¾_ receives 0. The


cutoff _ğ¾_ defines the evaluation scope and we use two values: (1) **MRR@5** for Stage 1 retrieval evaluation, measuring


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 21


how well individual encoders rank trigger and action candidates (macro-averaged over both encoders, reported in


Fig. 7); (2) **MRR@3** for function-level prediction, applied _twice_ â€”first to evaluate FARMâ€™s own trigger-action pair


selection performance, then to compare FARM against prior work (LAM [28], RecipeGen++ [60], TARGE [5]) using the


same metric for fair comparison (Table 5). MRR@3 follows established evaluation protocols in the TAP literature for


function-level prediction tasks.


_4.3.2_ _Stage 2: Selection and Generation Metrics._ We define end-to-end evaluation metrics that measure system perfor

mance on complete applet generation:


**Goal Accuracy** measures task completion with partial credit, evaluating whether the system achieves the userâ€™s


automation intent:



1 _._ 0 if _ğ‘¡_ pred = _ğ‘¡_ true âˆ§ _ğ‘_ pred = _ğ‘_ true


0 _._ 5 if ( _ğ‘¡_ pred = _ğ‘¡_ true) âŠ•( _ğ‘_ pred = _ğ‘_ true)


0 _._ 0 otherwise



Goal Accuracy( _ğ‘_ ) =



ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³



(16)



where âŠ• denotes exclusive OR (exactly one match). Practically, Goal Accuracy = 1.0 means the applet is fully correct, 0.5


means partial success (user achieves either trigger monitoring or action execution, but not the complete automation),


and 0.0 means complete failure. We consider a query successful when Goal Accuracy â‰¥ 0 _._ 5, used to compute the Success


Rate metric.


**Joint Accuracy** measures service-pair correctness, requiring both trigger and action to be correctly selected:


Joint Accuracy( _ğ‘_ ) = I( _ğ‘¡_ pred = _ğ‘¡_ true âˆ§ _ğ‘_ pred = _ğ‘_ true) _,_


where _ğ‘¡_ pred and _ğ‘_ pred are the predicted trigger and action service names, _ğ‘¡_ true and _ğ‘_ true are the ground truth services,

and I(Â·) is the indicator function. Joint Accuracy returns 1 only when both services are correct, and 0 otherwise. It is


computed from the _full system_ (Stage 1 + Stage 2 multi-agent selection), allowing Stage 2 LLM reasoning to refine Stage


1 candidates. Unlike Goal Accuracy which provides partial credit (0.5 for one service correct), Joint Accuracy is binary


(0/1), making it stricter and comparable to metrics used in prior work (LAM, RecipeGen++, TARGE).


**Trigger/Action Accuracy** measures individual component correctness:



Trigger Accuracy = [1]

| _ğ‘„_ |



âˆ‘ï¸

I( _ğ‘¡_ pred = _ğ‘¡_ true) _,_ (17)
_ğ‘_ âˆˆ _ğ‘„_



and similarly for Action Accuracy with _ğ‘_ pred and _ğ‘_ true. These metrics isolate which component (trigger selection vs.

action selection) causes failures, informing system debugging and improvement.


**Success Rate** measures task-level success with partial credit, defined as the fraction of queries where the system


achieves at least partial automation:



Success Rate = [1]

| _ğ‘„_ |



âˆ‘ï¸

I(Goal Accuracy( _ğ‘_ ) â‰¥ 0 _._ 5) _._ (18)

_ğ‘_ âˆˆ _ğ‘„_



Since Goal Accuracy( _ğ‘_ ) âˆˆ{0 _._ 0 _,_ 0 _._ 5 _,_ 1 _._ 0} and Joint Accuracy( _ğ‘_ ) âˆˆ{0 _,_ 1}, the following ordering holds pointwise and


therefore also in expectation:


Success Rate â‰¥ E[Goal Accuracy] â‰¥ E[Joint Accuracy] _._ (19)


Practically, Success Rate counts queries where at least one service (trigger or action) is correctly identified. Success


Rate _<_ 100% indicates failures in service identification or system crashes.


Manuscript submitted to ACM


22 Badalov and Yoon


We adopt generation quality metrics from the RAGAS evaluation framework [11], following the functional definitions


provided in the official documentation [37].


**Faithfulness** measures whether generated applet configurations are grounded in retrieved function schemas,


preventing hallucination:

[able in retrieved schemas][|]
Faithfulness = [|][Claims verif] _._ (20)

|Total claims in generated applet|


A â€œclaimâ€ is an assertion about service capabilities, field names, or data types in the generated applet. For example,


if the system generates a binding trigger.ingredient_name â†’ action.field_name, Faithfulness checks whether


both ingredient_name exists in the trigger schema and field_name exists in the action schema. Faithfulness = 1.0


means all generated content is verifiable from retrieved documentation; lower scores indicate hallucinated field names


or capabilities not present in the actual function specifications.


**Topic Adherence** evaluates whether selected services match the userâ€™s intended automation domain:

Topic Adherence = [|][Applets matching reference automation domain][|] _._ (21)

|Total applets|


Practically, if a userâ€™s query mentions â€œfitness tracking,â€ Topic Adherence checks whether the selected trigger and


action belong to health/fitness services (e.g., Fitbit, Google Fit, MyFitnessPal) rather than unrelated domains (e.g., social


media, smart home). This metric ensures the system respects user intent boundaries and does not inappropriately cross


automation contexts.


**4.4** **Experiment results**


_4.4.1_ _Stage 1: Contrastive Learning Results._ We first evaluate the retrieval stage in isolation to understand the impact of


contrastive training and layer freezing. Figure 6 illustrates the training progression over three epochs. Figure 7 presents


retrieval performance comparing pretrained baseline against contrastive-trained encoders on the Gold evaluation set.


Layer freezing with contrastive training achieves substantial improvements across all retrieval metrics. Trigger


encoder: R@1 improves from 30% to 72% (+140%), R@5 from 59% to 92% (+56%), and MRR@5 from 42% to 79% (+88%).


Action encoder: R@1 improves from 39% to 79% (+103%), R@5 from 57% to 92% (+61%), and MRR@5 from 48% to


83% (+73%). Both encoders achieve _>_ 92% R@5, ensuring the target trigger/action almost always appears in the top-5


candidates for Stage 2 selection.


Critically, these individual R@5 scores correspond to a measured Joint R@5 of 85%, meaning both the correct trigger


_and_ correct action appear in their respective top-5 results for 85% of queries. This Joint R@5 establishes the theoretical


upper bound for Stage 2 performanceâ€”our multi-agent selection can only choose from pairs that Stage 1 successfully


retrieved.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 23

































Fig. 6. Contrastive training curves showing loss convergence and R@1 improvement. Both encoders converge within 3 epochs, with
the majority of improvement in the first epoch.



























Fig. 7. Stage 1 retrieval quality. Contrastive training with layer freezing significantly improves trigger encoder (R@1: 30%â†’72%,
R@5: 59%â†’92%, MRR@5: 42%â†’79%) and action encoder (R@1: 39%â†’79%, R@5: 57%â†’92%, MRR@5: 48%â†’83%).


_4.4.2_ _Stage 2: Multi-Agent Selection Results._ We evaluate the complete two-stage system, measuring end-to-end selection


accuracy and generation quality across all three evaluation sets (Gold, Noisy, and One-Shot). Figure 8 presents selection


accuracy across all three evaluation sets.


**Gold Set:** Clear queries enable high-confidence selection. The gap between individual accuracy (89-90%) and joint


accuracy (81%) reflects the compounding difficulty of selecting both services correctly.


**Noisy Set:** Ambiguous queries challenge the system, particularly for action selection (72%). Vague descriptions


require significant inference.


**One-Shot Set:** The system maintains reasonable performance on rare functions, demonstrating robust generalization


from contrastive learning.


Manuscript submitted to ACM


24 Badalov and Yoon

























Fig. 8. Stage 2 multi-agent selection performance across evaluation sets. The system achieves 81% joint accuracy on Gold, with
graceful degradation on challenging inputs.


_4.4.3_ _Generation Quality._ Figure 9 presents RAGAS [11] quality metrics for generated applet configurations.















Fig. 9. Generation quality metrics across evaluation sets. Faithfulness (0.44â€“0.48) measures grounding in retrieved schemas; Topic
Adherence (0.80â€“0.82) measures alignment with userâ€™s automation domain.


High topic adherence across all sets indicates that selected services consistently align with the userâ€™s stated automation


domain. Moderate faithfulness scores reflect the challenging nature of generating precise field bindings that require


inference beyond explicit context.


_4.4.4_ _Comprehensive Multi-Dataset Evaluation._ Figure 10 presents a comprehensive breakdown of system performance


across all three evaluation sets, revealing both overall effectiveness and robustness characteristics.


**Gold Set Results:** Clear, well-formed queries enable high performance across all metrics. The system achieves 97%


success rate (at least partial automation achieved), 89% goal accuracy (average function-level performance with partial


credit), and 81% joint accuracy (both trigger and action functions correctly selected). Individual component accuracy


reaches 88% for triggers and 90% for actions, with the gap to joint accuracy (81%) reflecting the compounding difficulty


of perfect service-pair selection.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 25





































































Fig. 10. Multi-agentic evaluation across test sets. **(a)** Overall performance: Gold achieves 97% success rate (at least one service
correct) and 89% goal accuracy (average with partial credit); Noisy shows 8pp degradation in success rate; One-Shot maintains 94%
success. The ordering Success Rate _>_ Goal Accuracy reflects metric definitions: Success Rate = binary threshold (goal_accuracy â‰¥
0.5), Goal Accuracy = arithmetic mean. **(b)** Service selection: Trigger accuracy (88%/79%/81%), Action accuracy (90%/72%/83%), Joint
accuracy (81%/62%/70%) for Gold/Noisy/One-Shot respectively. **(c)** Quality metrics: Topic adherence remains stable (82%) across
sets; Faithfulness moderate (44â€“48%). **(d)** Performance degradation: Noisy queries show largest drops in joint accuracy (19pp) while
One-Shot maintains resilience (11pp drop).


**Noisy Set Results:** Vague or ambiguous descriptions present significant challenges. Success rate drops to 89% (8pp


degradation from Gold), goal accuracy to 75.5% (13.5pp drop), and joint accuracy to 62% (19pp drop). The ordering


Success Rate (89%) _>_ Goal Accuracy (75.5%) _>_ Joint Accuracy (62%) reflects the metric hierarchy: Success Rate requires


only partial correctness (one service), Goal Accuracy averages partial credit, while Joint Accuracy requires both services


correct. Action selection proves particularly challenging (72% accuracy vs 90% on Gold), as ambiguous queries require


substantial inference to identify appropriate actions. Topic adherence remains stable at 79.7%, indicating robust domain


alignment despite query ambiguity.


**One-Shot Set Results:** Queries involving rare functions demonstrate robust generalization from contrastive learning.


The system maintains 94% success rate (3pp drop from Gold) and 82% goal accuracy (7pp drop), with 70% joint accuracy


(11pp drop). Both trigger (81%) and action (83%) selection remain competitive, showing that layer freezing preserves


sufficient pretrained semantic knowledge to handle low-resource scenarios.


**Quality Metrics Analysis:** Faithfulness scores range from 44% to 48% across all sets, reflecting the challenging


nature of schema-grounded binding generation that requires inference beyond explicit context. Topic adherence remains


Manuscript submitted to ACM


26 Badalov and Yoon


consistently high (80â€“82%), indicating that the multi-agent system reliably selects services within the userâ€™s intended


automation domain across all test conditions.


**4.5** **Comparison with Related Work**


We compare FARM against three established baselines for trigger-action programming: Liu et al. [28], Yusuf et al. [60],


and Cimino et al. [5]. Table 5 presents quantitative results on a service-level prediction task across all three evaluation


sets. Although FARM operates at function-level granularity with full schema specifications, for this table we map


each predicted function to its parent platform service to enable fair comparison with baselines. To ensure a controlled


comparison, we train and evaluate each baseline on the same service-level splits derived from our dataset.


**Test Gold** **Test Noisy** **Test One-shot**
**Method** Joint Acc MRR@3 Joint Acc MRR@3 Joint Acc MRR@3


LAM [28] 0.24 0.27 0.23 0.26 0.12 0.19
RecipeGen++ [60] 0.31 0.35 0.29 0.34 0.17 0.24
TARGE [5] 0.58 0.63 0.39 0.44 0.45 0.49


**FARM (Ours)** **0.79** **0.84** **0.62** **0.69** **0.70** **0.77**


Table 5. Comparison with prior trigger-action programming approaches on a **service-level** benchmark. We derive service-level labels
by mapping each trigger/action _function_ in our dataset to its parent platform service. We re-ran LAM, RecipeGen++, and TARGE
using the authorsâ€™ released code on this service-level dataset, using identical train/validation/test splits across all methods. FARM
operates at function granularity, but for this table we map FARMâ€™s predicted functions to their parent services to enable direct
comparison. **Joint Accuracy** is computed on the top-1 predicted trigger-service and action-service pair. **MRR@3** is computed as the
mean reciprocal rank of the ground-truth trigger-action _service pair_ within the top-3 ranked pairs produced by each method.


**4.6** **Ablation Studies**


We conduct ablation studies to understand the contribution of each system component.


_4.6.1_ _Layer Freezing Ablation._ Figure 11 compares full training against layer freezing on retrieval accuracy.


Layer freezing preserves the pretrained encoderâ€™s understanding in frozen layers (0â€“11) while training upper layers


(12â€“23) for domain-specific patterns.


_4.6.2_ _LoRA vs. Layer Freezing._ We compare our layer freezing approach against Low-Rank Adaptation (LoRA) [14], the


dominant parameter-efficient fine-tuning method. Figure 12 presents results across retrieval accuracy and parameter


efficiency.


Layer freezing achieves 85% Joint R@5 with 18% trainable parameters, outperforming full fine-tuning (83% Joint


R@5, 100% params) and all LoRA variants. LoRAâ€™s low-rank constraint limits its ability to adapt to the TAP domain,


with increasing rank ( _ğ‘Ÿ_ =8â†’32) providing only marginal improvements. Layer freezing provides the best trade-off:


highest retrieval performance with moderate parameter cost.


_4.6.3_ _Component Ablation._ **Component Contributions:**


(1) **Contrastive Training (+49 pts Joint R@1):** Improves retrieval top-1 dual success from near-random (7%) to


viable (56%).


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 27



























Fig. 11. Layer freezing ablation using R@5 metric (consistent with Figure 12). Layer freezing achieves slightly better joint R@5
accuracy (85%) with 92%/92% for trigger/action compared to full fine-tuning (83% joint) with 93%/89% for trigger/action, while using
only 18% of trainable parameters. Note: R@5 is used here instead of R@1 to match the LoRA ablation evaluation protocol, which uses
_ğ‘˜_ =5 for retrieval evaluation.





























Fig. 12. LoRA vs. Layer Freezing ablation. (a) Joint R@5 accuracy by adaptation method. Layer freezing achieves the highest accuracy
(85%) while LoRA variants underperform (69â€“73%). (b) Efficiency vs. accuracy trade-off showing layer freezing provides the best
balance of parameter efficiency (18%) and retrieval performance.


(2) **Layer Freezing (+2 pts Joint R@1):** Improves retrieval top-1 dual success from 56% to 58% while using only


18% trainable parameters.


(3) **Multi-Agent Selection (+23 pts Joint Accuracy):** This substantial improvement (58% â†’ 81%) stems from


expanding the search space: naive selection evaluates only 1 pair (rank-1 from each encoder), while multi-agent

selection evaluates up to _ğ‘˜_ [2] = 25 pairs through cross-compatibility scoring (Fig. 13, 14).


Manuscript submitted to ACM


28 Badalov and Yoon



















Fig. 13. Component ablation showing incremental contributions.





















Fig. 14. Stage 2 improvement mechanism. (a) Multi-agent selection achieves 81% joint accuracy, a +23 point improvement over naive
rank-1 selection (58%), approaching the 85% retrieval ceiling (Joint R@5). (b) The improvement stems from evaluating 25 trigger-action
pairs (5Ã—5) with LLM-based cross-compatibility scoring, compared to naive selection which evaluates only 1 pair (rank-1 from each
encoder).


_4.6.4_ _LLM Selection Ablation._ We evaluate the impact of different large language models on the multi-agent selection


pipeline. Table 6 presents generation quality metrics for various LLMs suitable for tool calling and agentic workflows.


IBM Granite 4.0 achieves the highest scores on both metrics, with +9â€“17 points improvement in Faithfulness and +6â€“


14 points improvement in Topic Adherence compared to alternatives. This performance advantage stems from Graniteâ€™s


optimization for enterprise agentic workflows with native tool calling support, enabling more reliable structured JSON


output generation and better instruction following for our agent prompting strategy.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 29


**LLM** **Faithfulness** **Topic Adherence**


LLaMA 3 70B [33] 0.38 0.74
Qwen2 72B [57] 0.41 0.76
Mistral Large [20] 0.36 0.71
DeepSeek-Coder [12] 0.33 0.68


**IBM Granite 4.0 Small** [17] **0.50** **0.82**


Table 6. LLM Ablation for Multi-Agent Selection. Faithfulness measures factual grounding in retrieved schemas; Topic Adherence
measures alignment with automation domains. Higher is better. Note: These metrics represent best-case LLM performance on the
Gold set; end-to-end system faithfulness across all datasets (Gold/Noisy/One-Shot) ranges from 0.44â€“0.48 as shown in Fig. 9.


_4.6.5_ _Base Encoder Selection._ To evaluate the impact of base encoder choice on our contrastive learning framework, we


compare performance across four state-of-the-art embedding models with diverse architectures and training objectives.


We apply our layer freezing strategy (freezing layers 0â€“11, training 12â€“23) to EmbeddingGemma and compare against


alternative models to assess both absolute performance and the generalizability of our approach.


EmbeddingGemma with layer freezing achieves the best performance across all datasets and configurations (58%


Gold, 50% Noisy, 54% One-Shot Joint R@1; Trigger R@5 = 92%, Action R@5 = 92% on Gold), validating our ablation


findings and demonstrating consistent robustness to dataset quality variations. Different base encoders respond


differently to training strategies: ModernBERT models benefit from full training (e.g., ModernBERT-large on Gold


improves from 50% to 52%), whereas BGE and E5 models achieve higher performance under layer freezing, similar to


EmbeddingGemma. This indicates that layer freezing is particularly effective for encoders with strong multilingual


pretraining (EmbeddingGemma, BGE, E5), while architectures optimized for long-context processing (ModernBERT)


benefit from full parameter updates. Performance degradation from Gold to Noisy datasets follows similar trends across


models (7â€“10 percentage point drops), indicating that dataset quality affects all architectures comparably. Notably, even


under their preferred training strategies, no alternative model surpasses EmbeddingGemma with layer freezing on


any dataset split, confirming both the effectiveness of our approach and the importance of base encoder selection for


trigger-action retrieval (Table 7).


**Layer Freezing (18%)** **Full Training (100%)**


_Joint R@1_ _Joint R@1_


**Base Model** **Params** **Gold** **Noisy** **One-Shot** **R@5** **Gold** **Noisy** **One-Shot** **R@5**


**EmbeddingGemma** [39] 307M **0.58** **0.50** **0.54** **0.92** **0.56** 0.48 **0.52** **0.91**
ModernBERT-base [51] 149M 0.45 0.38 0.42 0.85 0.47 0.40 0.44 0.87
ModernBERT-large [51] 395M 0.50 0.42 0.48 0.89 0.52 0.44 0.50 **0.91**
BGE-base-en-v1.5 [56] 109M 0.48 0.40 0.45 0.88 0.46 0.38 0.43 0.86
E5-large-v2 [48] 335M 0.52 0.44 0.49 0.90 0.50 **0.49** 0.47 0.89


Table 7. Base encoder comparison.


Manuscript submitted to ACM


30 Badalov and Yoon


**4.7** **Efficiency Analysis**


We analyze FARMâ€™s computational efficiency across retrieval, selection, and training phases. Table 8 summarizes


the latency and resource requirements for each system component. All experiments conducted on NVIDIA Tesla


V100-SXM2-32GB GPUs (32GB memory).


Table 8. System Efficiency Metrics


**Metric** **Value**


_Stage 1 (Retrieval)_


Encoding Latency âˆ¼50ms per query


Vector Search (Qdrant) âˆ¼5ms for top-5


_Stage 2 (Selection)_


LLM Calls per Query 2â€“4


Average Total Latency âˆ¼3s (local inference)


LLM Configuration 3Ã— V100 GPUs


Fallback Rate _<_ 5%


_Training_


Encoder Training Time âˆ¼20 minutes


Training Hardware Single V100 GPU


Training Data Size 12,652 pairs


The two-stage architecture provides computational efficiency: Stage 1 reduces the search space from 2.2M candidate

pairs to _ğ‘˜_ [2] = 25 combinations (with _ğ‘˜_ = 5), enabling Stage 2â€™s LLM reasoning to focus on high-quality candidates. This


design is necessary because naive selection (taking rank-1 from each encoder, evaluating only 1 pair) achieves just 58%


joint accuracy, whereas evaluating the full _ğ‘˜_ Ã— _ğ‘˜_ search space with multi-agent reasoning achieves 81%. The system


uses _ğ‘˜_ = 5 throughout, matching the R@5 evaluation metrics reported in ablation studies.


**4.8** **Discussion**


_4.8.1_ _Why Two Stages?_ Our two-stage architecture addresses a fundamental trade-off between efficiency and accuracy.


Stage 1 provides fast (âˆ¼55ms) retrieval that reduces the search space from 2.2 million possible pairs to a manageable


candidate set. However, _retrieval ranking alone is insufficient for accurate selection_ .


To understand why, consider the selection problem: given top- _ğ‘˜_ candidates from each encoder, we must identify


the correct trigger-action pair. A naive approach would take the rank-1 result from each encoder (evaluating only


1 pair), but this achieves just 58% joint accuracy because it requires _both_ encoders to rank their correct items first


simultaneously. Individual R@1 scores (Trigger: 72%, Action: 79%) compound to approximately 58% joint success, and


when either encoder ranks the correct item at position 2â€“5, the naive approach fails.


Stage 2 addresses this limitation by evaluating the cross-product of candidates: with _ğ‘˜_ = 5, the system considers up

to _ğ‘˜_ [2] = 25 trigger-action pairs (Fig. 14b). Multi-agent selection uses LLM reasoning to score pairs based on schema


compatibility, ingredient-to-field matching, and intent alignmentâ€”factors invisible to embedding-based retrieval. This


expanded search achieves 81% joint accuracy (Fig. 14a), approaching the retrieval ceiling of 85% (Joint R@5). The 4-point


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 31


gap between achieved (81%) and ceiling (85%) reflects LLM reasoning errors on queries where correct pairs exist in the


candidate set but are incorrectly scored.


Put quantitatively: Stage 2 succeeds on 95% of retrievable queries (81% / 85%), demonstrating that the multi-agent


search effectively exploits Stage 1â€™s high-recall candidate sets. Neither stage alone could achieve this: Stage 1 lacks


reasoning capabilities, while Stage 2 requires the computational efficiency of Stage 1 to focus LLM evaluation on


high-quality candidates.


_4.8.2_ _Prompt-Based Multi-Agent Design._ Stage 2 operates through _prompting only_ â€”no task-specific fine-tuning. This


provides flexibility (agents modified via prompts), interpretability (explicit reasoning traces), and generalization (pre

trained LLM knowledge handles edge cases). We mitigate potential output inconsistency through structured schemas


and agreement-based override mechanisms.


_4.8.3_ _Limitations._


(1) **Retrieval Ceiling:** At _ğ‘˜_ =5, individual recall reaches 92% and Joint R@5 reaches 85%. Stage 2 cannot recover


if the correct service is not retrieved in Stage 1â€™s top-5, establishing an 85% theoretical upper bound on joint


accuracy.


(2) **Complex Queries:** Multi-intent queries (e.g., â€œturn off lights AND lock doorâ€) require decomposition not yet


implemented.


(3) **Faithfulness:** Moderate scores (0.44â€“0.48) indicate room for improvement in explicit schema grounding.


(4) **LLM Dependency:** Stage 2 quality depends on the underlying LLM capability; smaller models may underperform.


Several promising directions emerge from this work:


(1) **Execution Validation:** Developing automated evaluation of ingredient-to-field binding correctness through


integration with live IFTTT API endpoints. This would enable measuring end-to-end applet execution success


rates and identifying failure modes in binding generation.


(2) **Query Decomposition:** Implementing multi-intent query parsing to handle complex automation requests


requiring multiple trigger-action pairs.


(3) **Reinforcement Learning from Human Feedback:** Training a cross-encoder reranker using Group Relative


Policy Optimization (GRPO) [41], which eliminates the need for a critic model by estimating baselines from


group scores. This would enable learning nuanced scoring functions from user acceptance/rejection signals


while remaining memory-efficient. GRPOâ€™s compatibility with existing LLM infrastructure makes it particularly


suitable for improving Stage 2 selection quality through iterative refinement.


(4) **Cross-Platform Generalization:** Extending evaluation to other TAP platforms (Zapier, Make, Home Assistant)


to validate approach generalizability across different service ecosystems and schema formats.


(5) **User Studies:** Conducting human evaluation to assess practical utility and user satisfaction with generated


applets in real-world scenarios, including longitudinal studies of applet usage patterns.


**Acknowledgment**


This research was supported by the Ministry of Science and ICT (MSIT), Korea, under the Information Technology


Research Center (ITRC) program supervised by the Institute for Information and Communications Technology Planning


and Evaluation (IITP) (IITP-2025-RS-2023-00259099, 35%), by the National Research Foundation of Korea (NRF) grant


funded by the Korea government (MSIT) (RS-2023-00240211, 35%), and by the Culture, Sports and Tourism R&D Program


Manuscript submitted to ACM


32 Badalov and Yoon


through the Korea Creative Content Agency, funded by the Ministry of Culture, Sports and Tourism in 2024, titled


â€œGlobal Expert Personnel Training in MR Space Computing-based Technologiesâ€ (RS-2024-00399186, 30%).


**References**


[1] I. Beltagy, K. Lo, and E. H. Hovy. 2016. Improved Semantic Parsers for If-Then Statements. In _Proceedings of the 58th Annual Meeting of the Association_

_for Computational Linguistics_ [. Association for Computational Linguistics, Online, 1â€“10. doi:10.18653/v1/P16-1069](https://doi.org/10.18653/v1/P16-1069)

[2] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy

Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. 2024. LoRA Learns Less and Forgets Less. _arXiv preprint arXiv:2405.09673_ (2024).

[https://arxiv.org/abs/2405.09673](https://arxiv.org/abs/2405.09673)

[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. 2020. Language Models are Few-Shot Learners. In _Advances in Neural Information_

_Processing Systems (NeurIPS)_, Vol. 33. 1877â€“1901.

[4] Wei-Lin Chiang, Liang Zheng, Ying Sheng, Soheil Zhuang, Ziqi Wu, Yu Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, and Eric P. Xing. 2023. Chatbot

Arena: An Open Platform for Evaluating LLMs by Human and AI Judges. _arXiv preprint arXiv:2305.14387_ (2023).

[5] Gaetano Cimino, Vincenzo Deufemia, and Mattia Limone. 2025. IoT Rule Generation With Cross-View Contrastive Learning and Perplexity-Based

Ranking. _IEEE Internet of Things Journal_ 12, 17 (2025), 35828â€“35847.

[6] Fulvio Corno, L. De Russis, and A. Monge Roffarello. 2020. TapRec: Supporting the Composition of Trigger-Action Rules Through Dynamic [Ë™]

Recommendations. In _Proceedings of the 25th International Conference on Intelligent User Interfaces_ . 579â€“588.

[7] Fulvio Corno, L. De Russis, and A. Monge Roffarello. 2019. RecRules: Recommending If-Then Rules for End-User Development. _ACM Transactions_

_on Intelligent Systems and Technology_ 10, 5 (2019), 1â€“27.

[8] FCorno, L. De Russis, and A. Monge Roffarello. 2021. From Usersâ€™ Intentions to If-Then Rules in the Internet of Things. [Ë™] _ACM Transactions on_

_Information Systems_ 39, 4 (2021), 1â€“33.

[9] Matias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A Continual

Learning Survey: Defying Forgetting in Classification Tasks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2021).

[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. _arXiv preprint_

_arXiv:2305.14314_ [(2023). https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)

[11] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2024. RAGAS: Automated Evaluation of Retrieval Augmented Generation. In

_Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations_ . Association for

[Computational Linguistics, St. Julians, Malta, 150â€“158. doi:10.18653/v1/2024.eacl-demo.16](https://doi.org/10.18653/v1/2024.eacl-demo.16)

[12] Daya Guo et al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming. _arXiv_ [(2024). https://arxiv.org/abs/2401.14196](https://arxiv.org/abs/2401.14196)

[13] Kai-Hsiang Hsu, Yu-Hsi Chiang, and Hsu-Chun Hsiao. 2019. SafeChain: Securing Trigger-Action Programming from Attack Chains. _IEEE Transactions_

_on Information Forensics and Security_ [14, 10 (2019), 2607â€“2622. doi:10.1109/TIFS.2019.2907062](https://doi.org/10.1109/TIFS.2019.2907062)

[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of

Large Language Models. _arXiv preprint arXiv:2106.09685_ [(2022). https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)

[15] Justin Huang and Maya Cakmak. 2015. Supporting Mental Model Accuracy in Trigger-Action Programming. In _Proceedings of the 2015 ACM_

_International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp)_ [. ACM, 215â€“225. doi:10.1145/2750858.2805830](https://doi.org/10.1145/2750858.2805830)

[16] Zijun Huang and Jiangfeng Li. 2023. TAP-AHGNN: An Attention-Based Heterogeneous Graph Neural Network for Service Recommendation on

Trigger-Action Programming Platform. In _Proceedings of the 2023 International Conference on Data Science and Information Technology_ . Springer,

[157â€“170. doi:10.1007/978-981-99-4752-2_12](https://doi.org/10.1007/978-981-99-4752-2_12)

[[17] IBM. 2025. IBM Granite 4.0: Hyper-Efficient, High-Performance Hybrid Models for Enterprise. https://www.ibm.com//new/announcements/ibm-](https://www.ibm.com//new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models)

[granite-4-0-hyper-efficient-high-performance-hybrid-models. Official IBM announcement page.](https://www.ibm.com//new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models)

[18] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,

and Edouard Grave. 2022. Atlas: Few-shot Learning with Retrieval Augmented Language Models. _arXiv preprint arXiv:2208.03299_ [(2022). https:](https://arxiv.org/abs/2208.03299)

[//arxiv.org/abs/2208.03299](https://arxiv.org/abs/2208.03299)

[19] Ganesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah. 2019. What Does BERT Learn About the Structure of Language?. In _Proceedings of the 57th_

_Annual Meeting of the Association for Computational Linguistics_ . 3651â€“3657.

[20] Albert Jiang et al. 2023. Mistral 7B. _arXiv_ [(2023). https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825)

[21] Seonghyeon Kim et al. 2025. Leveraging LLMs for Efficient and Personalized Smart Home Automation. _arXiv preprint arXiv:2601.04680_ (2025).

Demonstrates LLM failures on underspecified IoT instructions.

[[22] E. King. 2023. Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models. arXiv:2305.09802 [cs.HC] https://arxiv.org/](https://arxiv.org/abs/2305.09802)

[abs/2305.09802](https://arxiv.org/abs/2305.09802)

[23] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho,

Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_

114, 13 (2017), 3521â€“3526.


Manuscript submitted to ACM


FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation 33


[24] Zhejun Kuang, Yusheng Zhu, Dawen Sun, Jian Zhao, Yongheng Xing, Feng Wang, and Lei Sun. 2025. SAFE-TAP: Semantic-aware and fused

embedding for TAP rule security detection. _Neurocomputing_ [656 (2025), 131529. doi:10.1016/j.neucom.2025.131529](https://doi.org/10.1016/j.neucom.2025.131529)

[25] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim

RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In _Advances in Neural_

_Information Processing Systems_ [, Vol. 33. https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)

[26] Zhizhong Li and Derek Hoiem. 2016. Learning without Forgetting. In _Proceedings of the European Conference on Computer Vision (ECCV)_ . 614â€“629.

[27] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah Smith, and Yejin Choi. 2023. Weâ€™re

Afraid Language Models Arenâ€™t Modeling Ambiguity. (2023), 790â€“807.

[28] Chang Liu, Xinyun Chen, Eui Chul Shin, Minghui Chen, and Dawn Song. 2016. Latent Attention for If-Then Program Synthesis. In _Advances in_

_Neural Information Processing Systems_, Vol. 29.

[29] Liwei Liu, Wei Chen, Lu Liu, Kangkang Zhang, Jun Wei, and Yan Yang. 2021. TAGen: Generating Trigger-Action Rules for Smart Homes

by Mining Event Traces. In _Proceedings of the 19th International Conference on Service-Oriented Computing (ICSOC 2021)_ . Springer, 652â€“662.

[doi:10.1007/978-3-030-91431-8_41](https://doi.org/10.1007/978-3-030-91431-8_41)

[30] Liwei Liu, Wei Chen, Tao Wang, Wei Wang, Guoquan Wu, and Jun Wei. 2023. Generating Scenario-Centric TAP Rules for Smart Homes by

Mining Historical Event Logs. In _2023 IEEE International Conference on Web Services (ICWS)_ . IEEE, Conference Location: Chicago, IL, USA.

[doi:10.1109/ICWS60048.2023.00015](https://doi.org/10.1109/ICWS60048.2023.00015)

[31] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A. Smith. 2019. Linguistic Knowledge and Transferability of Contextual

Representations. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human_

_Language Technologies_ . 1073â€“1094.

[32] Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze. 2008. _Introduction to Information Retrieval_ . Cambridge University Press.

[33] Meta AI. 2024. The LLaMA 3 Model Family. _arXiv_ [(2024). https://arxiv.org/abs/2404.09437](https://arxiv.org/abs/2404.09437)

[34] Xianghang Mi, Feng Qian, Ying Zhang, and XiaoFeng Wang. 2017. An Empirical Characterization of IFTTT: Ecosystem, Usage, and Performance. In

_Proceedings of the 2017 Internet Measurement Conference (IMC)_ [. ACM, 398â€“404. doi:10.1145/3131365.3131369](https://doi.org/10.1145/3131365.3131369)

[35] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. _arXiv preprint arXiv:1901.04085_ [(2019). https://arxiv.org/abs/1901.04085](https://arxiv.org/abs/1901.04085)

[36] Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to Code: Learning Semantic Parsers for If-Then Recipes. In _Proceedings of the_

_53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing_

_(Volume 1: Long Papers)_ . 878â€“888.

[[37] RAGAS Contributors. 2024. RAGAS Documentation: Metrics for Retrieval-Augmented Generation. https://docs.ragas.io/en/stable/. Accessed:](https://docs.ragas.io/en/stable/)

2025-12-19.

[38] Amir Rahmati, Earlence Fernandes, Jaeyeon Jung, and Atul Prakash. 2017. IFTTT vs. Zapier: A Comparative Study of Trigger-Action Programming

Frameworks. _arXiv preprint arXiv:1709.02788_ (2017).

[39] Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou,

Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi

Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divyashree

Sreepathihalli, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca,

Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo HernÃ¡ndez Ãbrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen,

Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie

Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren,

Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, GaÃ«l Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon,

Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Qin Yin, Yunhsuan Sung, Raphael Hoffmann, Tris

Warkentin, Armand Joulin, Tom Duerig, and Mojtaba Seyedhosseini. 2025. EmbeddingGemma: Powerful and Lightweight Text Representations.

_arXiv preprint arXiv:2509.20354_ [(2025). https://arxiv.org/abs/2509.20354](https://arxiv.org/abs/2509.20354)

[40] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

Toolformer: Language Models Can Teach Themselves to Use Tools. In _Advances in Neural Information Processing Systems (NeurIPS)_ . Proceedings

publication.

[41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024.

DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. _arXiv preprint arXiv:2402.03300_ (2024).

[42] Yingtian Shi et al. 2024. Bridging the Gap Between Natural User Expression with Complex Automation Programming in Smart Homes. _arXiv_

_preprint arXiv:2408.12687_ (2024).

[43] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In _Proceedings of the 57th Annual Meeting of the_

_Association for Computational Linguistics_ . 4593â€“4601.

[44] Blase Ur, Melwyn Pak Yong Ho, Stephen Brawner, Jiyun Lee, Sarah Mennicken, Noah Picard, Diane Schulze, and Michael L. Littman. 2016. Trigger
Action Programming in the Wild: An Analysis of 200,000 IFTTT Recipes. In _Proceedings of the 2016 CHI Conference on Human Factors in Computing_

_Systems_ [. Association for Computing Machinery, New York, NY, USA. doi:10.1145/2858036.2858556](https://doi.org/10.1145/2858036.2858556)

[45] Blase Ur, Elyse McManus, Melwyn Pak Yong Ho, and Michael L. Littman. 2014. Practical Trigger-Action Programming in the Smart Home.

In _Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ . Association for Computing Machinery, New York, NY, USA.


Manuscript submitted to ACM


34 Badalov and Yoon


[doi:10.1145/2556288.2557420](https://doi.org/10.1145/2556288.2557420)

[46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. In _arXiv preprint_

_arXiv:1807.03748_ .

[47] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In _Proceedings of the Eighth Text Retrieval Conference (TREC-8)_ .

[48] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by

Weakly-Supervised Contrastive Pre-training. _arXiv preprint arXiv:2212.03533_ (2022).

[49] Qi Wang, Pubali Datta, Wei Yang, Si Liu, Adam Bates, and Carl A. Gunter. 2019. Charting the Attack Surface of Trigger-Action IoT Platforms. In

_Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)_ . ACM, 1439â€“1453.

[50] Xuezhi Wang and Denny Zhou. 2024. Chain-of-Thought Reasoning Without Prompting. _arXiv preprint arXiv:2402.10200_ (2024).

[arXiv:2402.10200 [cs.CL] doi:10.48550/arXiv.2402.10200](https://arxiv.org/abs/2402.10200)

[51] Benjamin Warner, Antoine Chaffin, Benjamin ClaviÃ©, Orion Weller, Oskar HallstrÃ¶m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak,

Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder

for Fast, Memory Efficient, and Long Context Finetuning and Inference. _arXiv preprint arXiv:2412.13663_ (2024).

[52] Jason Wei, Yi Tay, Rishi Bommasani, et al. 2022. Emergent Abilities of Large Language Models. _Transactions on Machine Learning Research (TMLR)_

(2022).

[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-Thought

Prompting Elicits Reasoning in Large Language Models. _arXiv preprint arXiv:2201.11903_ [(2022). arXiv:2201.11903 [cs.CL] doi:10.48550/arXiv.2201.11903](https://arxiv.org/abs/2201.11903)

[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought

Prompting Elicits Reasoning in Large Language Models. In _Advances in Neural Information Processing Systems_, Vol. 35. Curran Associates, Inc.,

24824â€“24837.

[55] G. Wu et al. 2025. User Intention Prediction for Trigger-Action Programming via Multi-view Representation Learning. _Expert Systems with_

_Applications_ (2025). Article information available via ScienceDirect.

[56] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. _arXiv_

_preprint arXiv:2309.07597_ (2023).

[57] An Yang et al. 2024. Qwen2 Technical Report. _arXiv_ [(2024). https://arxiv.org/abs/2407.10671](https://arxiv.org/abs/2407.10671)

[58] Zijun Yao, Jiangfeng Li, Huijuan Zhang, Chenxi Zhang, and Gang Yu. 2019. Interactive User Interface Design for Trigger-Action Programming. In

_Proceedings of the International Conference on Smart Internet of Things (SmartIoT)_ .

[59] Y. Yoon. 2016. Performance analysis of CRF-based learning for processing WoT application requests expressed in natural language. _SpringerPlus_ 5

[(2016), 1324. doi:10.1186/s40064-016-3012-9](https://doi.org/10.1186/s40064-016-3012-9)

[60] I. N. B. Yusuf, D. B. A. Jamal, Lingxiao Jiang, and David Lo. 2022. RecipeGen++: An Automated Trigger-Action Programs Generator. In _Proceedings_

_of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering_ . 1672â€“1676.

[61] I. N. B. Yusuf, L. Jiang, and D. Lo. 2022. Accurate Generation of Trigger-Action Programs with Domain-Adapted Sequence-to-Sequence Learning. In

_Proceedings of the 30th International Conference on Program Comprehension (ICPC)_ . 99â€“110.

[62] Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual Learning Through Synaptic Intelligence. In _Proceedings of the 34th International_

_Conference on Machine Learning (ICML)_ . 3987â€“3995.

[63] Lefan Zhang, Cyrus Zhou, Hannah Yu, Blase Ur, and Michael L. Littman. 2023. Helping Users Debug Trigger-Action Programs. _Proceedings of the_

_ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies_ [(2023). doi:10.1145/3569506](https://doi.org/10.1145/3569506)

[64] Liang Zheng, Wei-Lin Chiang, Ying Sheng, Soheil Zhuang, Ziqi Wu, Yu Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, et al. 2023. Judging

LLMs by LLMs: A Benchmarking Framework for Evaluating Large Language Models. _arXiv preprint arXiv:2306.05685_ (2023).


Manuscript submitted to ACM


