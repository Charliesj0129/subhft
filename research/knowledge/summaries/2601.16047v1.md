# **Enhanced Representation-Based Sampling for** **the Efficient Generation of Datasets for** **Machine-Learned Interatomic Potentials**

#### Moritz R. Schäfer and Johannes Kästner [∗]

_Institute for Theoretical Chemistry, University of Stuttgart, Pfaffenwaldring 55, 70569_


_Stuttgart, Germany_


E-mail: kaestner@theochem.uni-stuttgart.de


1


**Abstract**


In this work, we present Enhanced Representation-Based Sampling (ERBS), a novel


enhanced sampling method designed to generate structurally diverse training datasets


for machine-learned interatomic potentials. ERBS automatically identifies collective


variables by dimensionality reduction of atomic descriptors and applies a bias potential


inspired by the On-the-Fly Probability Enhanced Sampling framework. We highlight


the ability of Gaussian moment descriptors to capture collective molecular motions


and explore the impact of biasing parameters using alanine dipeptide as a benchmark


system. We show that free energy surfaces can be reconstructed with high fidelity us

ing only short biased trajectories as training data. Further, we apply the method to


the iterative construction of a liquid water dataset and compare the quality of simu

lated self-diffusion coefficients for models trained with molecular dynamics and ERBS


data. Further, we active-learn models for liquid water with and without enhanced sam

pling and compare the quality of simulated self-diffusion coefficients. The self-diffusion


coefficients closely match those simulated with a reference model at a significantly re

duced dataset size. Finally, we compare the sampling behaviour of enhanced sampling


methods by benchmarking the mean squared displacements of BMIM [+] BF4 [–] trajecto

ries simulated with uncertainty-driven dynamics and ERBS and find that the latter


significantly increases the exploration of configurational space.


2


### **1 Introduction**

Machine-learned interatomic potentials (MLIPs) have proven themselves to be a suitable


approach to studying the dynamics of atomistic systems over the last couple of years. [1,2] By


training a machine learning model on the results of ab initio calculations, it is possible to


perform molecular dynamics simulations with nearly the accuracy of the reference meth

ods and cost that scales linearly with system size. Following the seminal work by Behler


and Parinello [3,4] and Bartok and Csanyi, [5–7] numerous model architectures have been pro

posed. These include a broad range of approaches, from descriptor-based kernel models [8,9]


to equivariant message passing models [10,11] and many others. [12–14] While a significant focus


by the community has been put into developing new architectures, all data-driven models


are only as good as the data they were trained on. The quality of training data is becoming


increasingly critical with the emerging interest in atomistic foundation models. [15,16] Unlike


potentials trained for a single system, these models aim to maximize transferability and ro

bustness across domains. [17] However, current observations suggest inconsistent performance


away from equilibrium and in the treatment of soft modes, [18] likely caused by a heavy re

liance on relaxation trajectories as training data. Consequently, the development of methods


capable of generating structurally diverse datasets is of utmost importance for the continued


advancement of general-purpose atomistic models.


Historically, constructing datasets for MLIPs was a costly and labour-intensive process,


as reference data was generated using ab initio molecular dynamics. [1,8] Sampling a configu

ration space with this approach results in highly correlated samples, each requiring costly


calculations. With the emergence of active learning approaches, [19] researchers have started to


use preliminary MLIPs for the sampling of candidate configurations, for example, by molec

ular dynamics. Thus, it became possible to reduce the usage of costly quantum chemical


reference methods only on the most informative candidates, resulting in more compact and


more informative datasets.


A key advancement was the introduction of uncertainty estimation methods to the realm


3


of MLIPs. With techniques such as model ensembling, [19–21] Gaussian processes, [22,23] or op

timal experimental design, [24] it became possible to estimate the error of model predictions.


Using these uncertainty estimates, MLIP-driven simulations can be terminated when uncer

tainty is high, and the most uncertain configurations can be selected for recalculation using


ab initio methods, an approach called uncertainty-driven dynamics (UDD). The model is


then retrained on the updated dataset. This iterative process, often called active learning


or learning-on-the-fly, [23,25] gradually extends datasets with informative structures, resulting


in more compact datasets that cover large parts of the relevant configurational space.


Dataset creation was further improved by the use of enhanced sampling methods, which


increase structural diversity in the collected configurations. The initial work by Herr et al. [26]


utilized the root mean square deviation (RMSD) between the current configuration and a


series of previous ones as a collective variable. A metadynamics-like bias potential [27] was


applied to promote the exploration of diverse regions within configurational space. More


recent approaches have been tailored more specifically for MLIPs. Yoo et al. [28] have used the


descriptor of high-dimensional neural network potentials as the collective variable instead of


the RMSD.


Uncertainty-driven dynamics [29] and hyperactive learning [25,30,31] use the model’s uncer

tainty to bias the system towards regions where the model is less confident. Contour explo

ration [32,33] evolves the system along constant potential energy contours. The position updates


are only limited by the local curvature of the PES thereby allowing for significantly larger


position updates than MD, without relying on uncertainty estimates like uncertainty-driven


dynamics.


Each of these provides a significant improvement in sampling efficiency over unbiased


MD at a constant temperature. However, these methods also have certain limitations. The


approach by Yoo et al., [28] which performs metadynamics in descriptor space, constructs per

atom bias potentials, requiring a large number of descriptor comparisons and potentially


leading to significant computational overhead. Uncertainty-based methods, while effective


4


in driving exploration toward regions of high model error, do not explicitly account for the


separation in timescales of different degrees of freedom. Intermolecular forces, such as those


determining dynamical observables in liquids, are significantly smaller than intramolecular


ones. Thus, if the target quantity is small and underestimated, a calibrated uncertainty


estimate will also be small, and uncertainty-based methods will not significantly enhance


sampling along slow degrees of freedom. This highlights a core conceptual difference be

tween sampling objectives: increasing epistemic uncertainty versus increasing input diversity.


Uncertainty-driven approaches are reactive; they rely on the model effectively identifying its


own knowledge gaps, leaving them vulnerable to poor calibration or noise. Conversely, in

put diversity-driven approaches aim to maximize the volume of explored descriptor space


independent of model error. By forcing the system to populate underrepresented regions of


the descriptor manifold, they ensure robust generalization and comprehensive phase space


coverage.


In this work, we introduce Enhanced Representation-Based Sampling (ERBS), a novel


enhanced sampling method for efficiently generating training data for MLIPs. Starting from


the mean descriptor of the system, we extract a small set of collective variables (CVs) via


principal component analysis (PCA). [34] Using these CVs, we construct a bias potential based


on the recently introduced OPES-Explore framework. [35] The combination of these CVs and


bias potential allows for a rapid exploration of configurational space by following trajectories


that sample preferentially along the _k_ maximum variance components of the MLIP features.


This work is structured as follows. We begin with a description of the ERBS method.


Its usefulness in creating a static, non-active learned dataset is explored for the alanine


dipeptide system. Here, a screening of the bias parameters is performed, and the resulting


configurational space coverage is analysed. For some sets of these parameters, the generated


trajectory is used as training data for MLIPs. We first cross-validate the prediction metrics of


models using both biased and unbiased validation datasets. Models trained on low- and high

temperature MD and ERBS trajectories are then used to compute the free energy surface


5


(FES) of the dihedral angles in alanine dipeptide. We find that the ERBS-trained models


achieve lower errors with respect to the true FES compared to the MD-trained models, with


the low-temperature MD model not producing a stable trajectory at all.


As a demonstration of ERBS use in an active learning setting, we turn to liquid water.


Two active learning workflows are set up, one using unbiased MD and one using ERBS biasing


for sampling candidate configurations. At each iteration of the workflow, the prediction


metrics on the water dataset by Cheng et al. [36] are calculated, and the diffusion coefficients


are simulated. We find that the prediction error on the literature test set decreases and


diffusion coefficients converge to the value obtained from a model trained on the dataset by


Cheng et al. significantly faster for the ERBS run.


Finally, we compare the sampling behaviour of ERBS with that of UDD for the viscous


room temperature ionic liquid 1-butyl-3-methylimidazolium tetrafluoroborate (BMIM [+] BF4 - ).


Across a wide range of parameters, ERBS increases the mean squared displacement of up to


4 times compared to MD and 2 times compared to the best UDD result, indicating enhanced


exploration of configurational space.

### **2 Methods**

#### **2.1 Gaussian Moment Neural Network**


Our enhanced-sampling approach descriptor agnostic. However, here we base it on the


Gaussian-Moment Neural Network (GMNN) [37,38] approach as it offers very fast training and


inference times, while still achieving good prediction accuracy. Thus, GMNN is discussed


here.


Given an atomic configuration _S_ consisting of Cartesian coordinates **R** and atomic num

bers _Z_, potentials used in molecular dynamics map from _S_ to a potential energy _E_ . Most


MLIPs utilize an atomic energy decomposition to predict energies for each local atomic


environment. [1,3]


6


_E_ ( _S,_ _**θ**_ ) =



_N_ atoms


_Ei_ ( _**G**_ _i,_ _**θ**_ ) (1)

_i_



Restricting the range of interactions can be motivated by the short-sightedness of elec

tronic matter, [39] and the resulting linear scaling with system size has significantly contributed


to these models’ scalability. The GMNN model consists of a descriptor, which constructs an


invariant representation of each atom, and neural networks for predicting atomic energies.


First, the pairwise distances between a central atom i and its neighbors j are expanded in


a radial basis, using Gaussian [12] or Bessel functions. [40] Embedding parameters _βZi,Zj_ _,n′,n_ are


used to form linear combinations of the _n_ _[′]_ original basis functions dependent on the atomic


numbers of the central and neighboring atoms, _Zi_ and _Zj_ . This results in a contracted radial


channel n. Finally, angular information is captured by Cartesian moments, i.e., polynomials


of the unit distance vectors **ˆ** _**r**_ _ij_ up to some rotation order _L_ .


      Ψ _i,L,n_ = _RZi,Zj_ _,n_ ( _rij, βZi,Zj_ _,n′,n_ ) _**r**_ **ˆ** _[⊗]_ _ij_ _[L]_ (2)


_j_ = _i_


The invariant descriptor _**G**_ is obtained from fully contracting the equivariant features


Ψ _i,L,n_ according to


_Gi,n_ 1 _,n_ 2 = (Ψ _i,_ 1 _,n_ 1) _a_ (Ψ _i,_ 1 _,n_ 2) _a_
... (3)


_Gi,n_ 1 _,n_ 2 _,n_ 3 = (Ψ _i,_ 1 _,n_ 1) _a_ (Ψ _i,_ 3 _,n_ 2) _a,b,c_ (Ψ _i,_ 2 _,n_ 3) _b,c._


Atomic energies are predicted from neural networks as _Ei_ = NN( _**G**_ _i_ ) and adjusted by


7


element-specific scaling and shifting parameters, _σZi_ and _µZi_ .


_Ei_ = _σZi ·_ NN( _**G**_ _i_ ) + _µZi_ (4)


Finally, the atomic energies _Ei_ are summed up as in Equation (1). Forces are calculated as


the gradient of the total energy using automatic differentiation. All learnable parameters


of the model are optimized using stochastic gradient-based optimization. The loss function


minimized during training contains terms for energy and force errors:



_L_ ( _**θ**_ ) =



_N_ train�



_k_ =1



_λE||Ek_ [ref] _[−]_ _[E]_ [(] _[S][k][,]_ _**[ θ]**_ [)] _[||]_ 2 [2]







+ _λF_



_N_ atoms [(] _[k]_ [)]



_i_



1

_||_ _**F**_ [ref] _i,k_ _[−]_ _**[F]**_ _[ i]_ [(] _[S][k][,]_ _**[ θ]**_ [)] _[||]_ 2 [2]
3 _N_ atoms [(] _[k]_ [)]



_._ (5)



Here, _λE_ and _λF_ denote hyperparameters for weighting the energy and force loss contribu

tions, respectively.

#### **2.2 Active Learning**


In cases where no previously existing dataset can be used for training an MLIP, a new one


has to be created from scratch. However, ab initio molecular dynamics simulations are an


expensive way to create training data as subsequent time steps are highly correlated. Con

sequently, a common approach is to start from a handful of samples, train an initial model,


and use it to sample new candidate structures. A crucial step during sampling simulations is


to terminate the trajectories when the model predictions become too inaccurate. Estimating


the error in model predictions is known as uncertainty quantification and is an active area


of research. [20,41–44]


In the present work, we use shallow ensembles recently proposed by Kellner and Ceri

otti. [45] A shallow ensemble shares the weights for all but the last linear layer. Instead of


predicting a single atomic energy as in Equation (4), the model instead predicts _N_ ens values,


8


and the uncertainty of a prediction can be estimated from the sample standard deviation of



the ensemble.



_N_ ens


( _x_ [(] _[m]_ [)] _−_ _x_ ¯) [2] (6)

_m_



_σx_ =




~~�~~

~~�~~


- [1]

_N_ ens



The mean of the ensemble predictions is used to drive the dynamics.


A crucial aspect is the calibration of the predicted uncertainty, i.e., how well predicted


uncertainty and true error correlate. By training a shallow ensemble on a probabilistic loss


function, like the negative log likelihood (NLL), miscalibrated uncertainty estimates are


directly penalized during training.



NLL = [1]

2



�( _x −_ _x_ ref)2 
+ log 2 _πσ_ [2] (7)
_σ_ [2]



Once a sampling simulation has completed or was terminated after exceeding an uncer

tainty threshold, the trajectory represents a pool of candidate data and new datapoints can


be selected from it. Given a pool _D_ pool = _S_ 1 _, ..., Sn_, batch active learning methods select a


subset of the structures _D_ batch _⊂D_ pool that maximizes an acquisition function _a_, [46] which


may depend on the model parameters:


_D_ batch = arg max _a_ ( _{S_ 1 _, ..., Sb},_ _**θ**_ ) (8)
_{S_ 1 _,...,Sb}⊂D_ pool


Throughout this work, we use greedy maximum-distance selection with a last-layer gra

dient feature map: [47]


_ϕ_ ll( _S_ ) = _∇θ_ ll _E_ ( _S, θ_ ) (9)


_S_ = arg max min (10)
_S∈D_ pool _/D_ batch _S_ _[′]_ _∈D_ pool _∪D_ batch _[||][ϕ]_ [ll][(] _[S]_ [)] _[ −]_ _[ϕ]_ [ll][(] _[S][′]_ [)] _[||]_ [2]


The feature map _ϕ_ ll( _S_ ) serves to compare the similarity of two structures in terms of the


model’s last layer weight gradient. Equation (10) is applied iteratively to select structurally


9


diverse points in feature space, a selection algorithm also known as farthest point sampling. [48]


More details on the selection methods can be found in the original work by Zaverkin et al. [47]


a) b)



Iteration _n_


Stable


0 20 40
Duration / ps



Iteration 1

20

Uncertainty

15 Energy


10


5

Unstable

0

0 20 40
Duration / ps



Seed Data


c)



Model
Training


DFT Sampling


Data
Selection


Structure



Descriptors Enhanced Exploration



s'


N


Collective

Variables

s



MD



Enhanced


s



P



Figure 1: (a) Illustration of an active learning cycle: Starting from seed data, models are
iteratively trained and used to sample candidate configurations. From these candidates, the
most informative ones are selected for DFT calculations. (b) Sampling stability improves and
predicted uncertainties decrease over active learning iterations. (c) Workflow for constructing
the ERBS potential: high-dimensional atomic descriptors **s** _[′]_ are aggregated and reduced to
collective variables **s** via dimensionality reduction. The bias potential flattens the sampled
distribution _P_ .


Figure 1a) depicts the typical steps involved in an active learning loop. As training


iterations proceed, model uncertainty decreases, and the quality and stability of simulated


trajectories increase (Figure 1b). Viewed differently, it takes increasingly longer for the model


to encounter informative new configurations during a sampling simulation. As a result, the


sampling trajectories need to become progressively longer.


In order to increase the diversity of sampled structures, enhanced sampling methods


for the generation of MLIP training data have been developed. One such approach, UDD,


builds directly on the model’s uncertainty estimate and uses it to steer the dynamics towards


more uncertain regions. In UDD, a bias potential is constructed to encourage exploration of


regions with high uncertainty: [29]



_E_ UDD( _σE_ [2] [) =] _[ A]_ �exp - _−_ _σE_ [2]
_N_ ens _N_ atoms _B_ [2]


10




- 
_−_ 1 (11)


Here, _A_ and _B_ are empirically determined parameters determining the strength of the bias


and its gradient. The MD is then propagated by the sum of the MLIP prediction and _E_ UDD.


The methods described so far and the GMNN architecture are implemented in `apax`, [49] which


is used for all model trainings and MD simulations in this work.

#### **2.3 Enhanced Representation Based Sampling**


Improving configurational space sampling for MLIP data generation can be framed as ex

ploring diverse inputs to the model. To an MLIP like GMNN, the descriptor represents


the model’s input and provides a general-purpose set of collective variables for identifying


undersampled regions in configuration space. For the present method, we use the average


descriptor vector of the entire system.



1
**s** _[′]_ =
_N_ atoms



_N_ atoms


_**G**_ _i_ (12)

_i_



Constructing a global descriptor ensures differentiability and computational efficiency


and has found various applications in dataset analysis and data selection. [50–52] Here, we


use a variant of the descriptor in Equation (3) without element-dependent parameters, _**β**_ .


However, including them or using other model features would also be possible. Over the


course of a simulation, system-averaged descriptors are collected at fixed time intervals and


used as reference descriptors to compare the similarity between the current configuration


and past ones.


The descriptor is high-dimensional, and its entries are correlated with each other. As a


result, sampling the descriptor space directly is challenging due to the curse of dimension

ality. A reduced set of CVs is obtained from principal component analysis (PCA), [34] which


identifies the most relevant collective motions in descriptor space. PCA is a common dimen

sionality reduction method that has also seen various applications in CV-based enhanced


sampling. [53,54] As usual for PCA, the data matrix is first centred using the per-feature mean


11


_**µ**_ .


**ˆS** = **S** _[′]_ _−_ **1** _N_ ref _**µ**_ [T] (13)


Here, the data matrix **S** _[′]_ consists of stacked reference descriptors **s** _[′]_, and **S** **[ˆ]** represents its


centered version. The principal components **V** are obtained from a singular value decom

position of **S** **[ˆ]** . By truncating **V** to the first _k_ columns, we obtain a projection matrix, **V** [(] _[k]_ [)],


used to reduce the dimensionality of the descriptor. During sampling simulations, the CVs


are computed via the feature dimensionality reduction function _ϕ_ .


_ϕ_ ( **s** _[′]_ ) = ( **s** _[′]_ _−_ _**µ**_ ) **V** [(] _[k]_ [)] = **s** (14)


Although PCA is used throughout all experiments, we emphasize that other choices of


_ϕ_, such as autoencoders, [55] are possible. Based on these CVs, a bias potential is constructed


similarly to the “explore” variant of On-the-Fly Probability Enhanced Sampling (OPES). [35,56]


While OPES estimates the unbiased probability distribution, OPES-explore estimates the


well-tempered one. Since the well-tempered distribution is smoothed out, fewer kernels are


required to estimate it. [57] The probability density of the CV space is modelled on-the-fly by


depositing Gaussian kernels, _K_, at fixed intervals during a molecular dynamics simulation


centered on the current averaged descriptor **s** _j_ .



1   
_−_ [1]
det ( **Σ** )(2 _π_ ) _[k]_ [exp] 2



1
_K_ ( **s** _,_ **s** _j_ ) = ~~�~~




[1] - (15)

2 [(] **[s]** _[ −]_ **[s]** _[j]_ [)][T] **[Σ]** _[−]_ [1][(] **[s]** _[ −]_ **[s]** _[j]_ [)]



We restrict ourselves to isotropic covariances **Σ** for computational efficiency. As we


implement the kernel compression algorithm from Invernizzi and Parrinello, [56] these may


become diagonal over the course of a simulation.


The overall well-tempered probability density _p_ [WT] _n_ ( **s** ) is, thus, modelled by the average


12


over all kernels _K_ ( **s** _,_ **s** _j_ ).



1
_p_ [WT] _n_ ( **s** ) = _N_ ref



_N_ ref


_K_ ( **s** _,_ **s** _j_ ) (16)

_j_



The OPES-explore bias potential at time step _n_ can then be calculated from the probability


density as

_Vn_ ( **s** ) = ( _γ −_ 1) [1]             - _p_ WT _n_ ( **s** ) + _ϵ_             - (17)

_β_ [log] _Zn_


_Zn_ is a modified normalisation constant computed from numerically integrating the density


with the kernel centers as integration points. The parameter _ϵ_ contains a barrier parameter


∆ _E_ via _γ_ = _β_ ∆ _E_, where _β_ = 1 _/_ ( _k_ B _T_ ) is the inverse thermal energy.




   - _−γ_
_ϵ_ = exp

_γ −_ 1




(18)



The incorporation of ∆ _E_ allows the method to place a soft limit on the maximum strength


of the bias potential. OPES-explore offers several advantages over metadynamics for config

urational space exploration. While metadynamics and its variants slowly deposit Gaussian


bias hills, OPES-explore models the probability density. As _p_ [WT] _n_ is normalized, the simula

tion starts with a strong bias right from the start. Further, the normalization constant is


modified in such a way that it only increases when kernels overlap, improving exploration.


A more detailed discussion of these advantages and the construction of _Zn_ can be found in


the original publication by Invernizzi and Parrinello. [56] The conceptual steps involved in the


ERBS method are illustrated in Figure 1. It should be noted that a simulation can either


use a fixed PCA basis constructed from a preexisting dataset or use a variable one that


is recomputed every time a new reference descriptor is added. For a fixed dimensionality


reduction function, the target-probability density in Equation (16) is well-defined. As the


goal of ERBS is not to be a free energy method, but to sample the most structurally diverse


configurations within a physically meaningful energy range, we use a variable basis for all


experiments.


13


Finally, to analyse the scalability of ERBS, we consider the computational cost associated


with the bias force evaluation. The gradient of the bias potential in Equation (17) can be


written as



_∂Vn_

_[∂V][n]_
_∂_ **R** [=] _∂_ **s**



(19)
_∂_ **R** _[.]_



_∂Vn_




_[n]_

_[∂]_ **[s]**
_∂_ **s** _[·]_ _∂_



Here, _∂_ **s**
_∂_ **R** [is the Jacobian of the reduced descriptor vector] _[ s]_ [ with respect to the atomic]


positions _R_ . Notably, it is independent of the number of reference descriptors. The term


_∂Vn_

_∂_ **s** [, on the other hand, involves the sum of kernel gradient contributions from each of the]


reference descriptors and thus scales linearly with the number of references. However, since


the first term consists of simple algebraic operations, its cost is negligible compared to that


of evaluating _∂_ **s**
_∂_ **R** [.]


As a result, the overall cost of ERBS using the GM descriptor is comparable to the


cost of a GMNN force evaluation and remains practically independent of the number of


reference descriptors used for even extensive active learning sampling runs. A comparison of


the scaling behaviour for ERBS and the method by Yoo et al. [28] can be found in the SI.

### **3 Results**

#### **3.1 Static dataset generation for Alanine Dipeptide**


Alanine dipeptide is a commonly used test system in the enhanced-sampling literature [43,58]


due to the thoroughly investigated FES in two of its dihedral angles, Φ and Ψ. Consequently,


the degree to which a trajectory covers the dihedral angle space serves as an indication of


how well a general-purpose sampling method is able to identify physically relevant degrees of


freedom. In order to examine the sensitivity of ERBS to the particular choice of parameters,


we perform an extensive parameter scan on alanine dipeptide in vacuum. The scanning


14


ranges for the parameters in Equations (14), (15) and (18) are selected as follows. For


the barrier parameter in Equation (18), we use ∆ _E_ = _{_ 5 _,_ 10 _,_ 15 _,_ 20 _,_ 25 _}_ eV. The covariance


matrix in Equation (15) is chosen as Σ = _σ_ [2] **I** _k_, with _σ_ = _{_ 0 _._ 05 _,_ 0 _._ 1 _,_ 0 _._ 2 _,_ 0 _._ 5 _}_, corresponding


to isotropic kernels in a reduced feature space. Finally, the dimensionality of the descriptor


space in Equation (14) is selected from _k_ = _{_ 2 _,_ 4 _,_ 6 _,_ 8 _,_ 10 _}_ .


For each possible parameter combination, an 80 ps trajectory is simulated using a Langevin


thermostat at 300 K with a timestep of 0 _._ 5 fs using the Amber99-SB force field [59,60] in CP2K. [61]


New kernels were deposited every 10,000 steps. In addition to the biased simulations, we


also compute unbiased trajectories at 300 K and 1200 K to set a baseline for the system’s ex

ploration and to verify whether the effect of the bias could also be reached by increasing the


temperature. The Φ _−_ Ψ-space coverage for the unbiased and biased trajectories is displayed


in Figure 2. In order to track and quantify the coverage, the space was tiled into 15°-by-15°


squares. Coverage is then calculated by the ratio of visited squares to their total amount.


Figure 2: a–c) Coverage vs parameter choices, d) Φ _−_ Ψ-space coverage of alanine dipeptide
over time using molecular dynamics and the enhanced sampling method.


The initial structure is located in a free energy minimum. The unbiased dynamics are


stuck there for the entire duration of the trajectory; hence, the coverage is severely limited.


A few parameter choices with _k_ = 2 lead to the dissociation of the molecule. This is expected


15


for high barrier heights and small Gaussian bandwidths, which lead to such strong forces


that the simulation becomes unstable. While the number of physically relevant collective


variables in this system is two, ERBS learns them on the fly and, thus, needs time to identify


them over the course of a simulation. It is thus advisable to initially overestimate the value


of _k_, since there is only a negligible cost associated with an increase in _k_ we largely find an


insensitivity of the coverage to the particular choice of the number of principal components.


The lack of a trend for _k ≥_ 2 demonstrates that the PCA successfully concentrated the


relevant slow dynamics into the first few principal components. Otherwise, the exploration


was drastically increased when compared to the unbiased simulation at 300 K, and most


parameter choices outperformed even the 1200 K trajectories exploration with coverages of


up to 75 %. The parameter set leading to the largest coverage used ∆ _E_ = 20 eV, _σ_ = 0 _._ 05


and _k_ = 4 We will refer to the trajectory with the best coverage as ERBS B. We also highlight


a second trajectory which also achieves good coverage but with a completely different set of


parameters, ∆ _E_ = 25 eV, _σ_ = 0 _._ 1 and _k_ = 10 and refer to it as ERBS A.


Next, the suitability of the biased trajectories as MLIP training data was investigated.


Four trajectories were chosen for dataset creation: the unbiased MD trajectories at 300 K


and 1200 K and the biased trajectories ERBS A and ERBS B The datasets contain 1800


training and 200 validation samples, respectively. In each case, the data points are selected


randomly, but the validation samples are taken from the last 20 percent of the trajectory.


GMNN models were trained on each dataset using identical hyperparameters. A radial


basis consisting of 16 spherical Bessel functions was chosen with a cut-off of 5 Å. Two


neural network layers of size 64 were used, and the model was trained with the AdamW


optimizer. [62,63] Further training details can be found in Table S1.


The force MAEs of each model evaluated on each dataset are displayed in Figure 3.



_N_ atoms



_i_ =1



1
_F_ MAE =
_N_ atoms _N_ structures



_N_ structures

 

_j_ =1


16



pred _,_ ( _j_ )
**F** _i_ _−_ **F** [ref] _i_ _[,]_ [(] _[j]_ [)] (20)
��� ���


For each validation dataset, the model trained on the corresponding training dataset


achieves the lowest error. While the model trained directly on the 300 K data achieves the


lowest errors on the 300 K validation set, it performs the worst on all other validation sets.


Conversely, the model trained on the 1200 K MD data generalizes better to other MD-derived


datasets but ranks near the bottom on the ERBS validation sets. In contrast, models trained


on ERBS-sampled data perform consistently well across biased and unbiased validation sets,


demonstrating strong transferability. More detailed prediction-error parity plots for the four


models can be found in Figure S5.


Figure 3: Cross-validation of force MAEs. The x-axis indicates the training dataset used to
generate each model. The colored bars within each group represent the model’s performance
on the four distinct validation datasets, as defined in the legend.


While validation metrics give a reasonable first estimate of model performance, the pur

pose of MLIPs is to compute physical observables, such as FESs. The FES in Φ and Ψ can be


evaluated using models trained on configurations from biased and unbiased dynamics. Well

tempered metadynamics simulations using Plumed [64,65] were conducted for the MD-300K,


MD-1200K, and ERBS models trained on the datasets from the previous experiment. A


biasing factor of 10, hill size of 1 _._ 2 kJ mol _[−]_ [1], and bandwidth of 0.35 rad were used. It should


be emphasized that the parameters for metadynamics fulfil different purposes and can’t be


directly compared to the ones used in ERBS despite similar names. Further, a ground truth


17


simulation was carried out with the Amber99-SB force field. All simulations were conducted


in the NVT ensemble at 300 K and lasted for 10 ns. The reference free-energy surface and the


errors of the FES produced by the models MD-1200K and the ERBS models are displayed


in Figure 4. All FES were shifted such that the global minimum of each surface is set to


zero. This alignment allows for consistent comparison of relative free energy differences, and


the error was computed with respect to the reference FES based on the aligned values.































Figure 4: (a) Free energy surface (FES) of alanine dipeptide computed with the reference
force field. (b–d) Signed errors (∆FES = FESMLIP - FESReference) between the reference and
the FES computed with models trained on data from (b) MD-1200 K, (c) ERBS A, and (d)
ERBS B. Blue regions indicate lower predicted free energies relative to the reference, while
red regions indicate overestimation. The red cross and green stars indicate the location of
the Global free energy minimum of the reference force field and trained MLIPs, respectively.


The model trained on the 300 K trajectory data is unstable. The system forms new


bonds during the trajectory, which keeps it stuck in a non-physical free-energy minimum.


Despite the different parameter choices for the bias and resulting differences in the Φ-Ψ space


coverage of models ERBS A and ERBS B, they achieve similar quality in their FESs. While


the MD-1200K model also achieves good metrics, it is surpassed by both models trained


on enhanced sampling data. In terms of localization, both the ERBS B and MD 1200 K


models correctly identify the global free energy minimum, whereas ERBS A exhibits a minor


deviation. However, the MD 1200 K model yields significantly larger errors in the broader


18


free energy surface. This discrepancy can be attributed to the training data: the 1200 K


trajectory remained trapped within the global minimum basin, failing to explore the right

hand side of Ramachandran space. The coverage of the three sampling trajectories is shown


in Figure S1. The best model, ERBS A, achieves a free energy MAE of 1 _._ 02 kcal mol _[−]_ [1], which


is almost chemically accurate, but could easily be refined for production accuracy within a


few active learning iterations.


In a recent study by Tan et al., [43] the data efficiency of MLIPs in reconstructing the


alanine dipeptide FES was investigated. Using their novel eABF method, they perform


active learning for the same system and conclude that an accurate FES may be accessible at


4000 data points. The datasets generated using the ERBS method were taken from static


80 ps trajectories and are comprised of 2000 datapoints, suggesting that chemical accuracy


could be achieved with significantly less than 4000 data points when using ERBS biasing in


an active learning setting.

#### **3.2 Accelerated Active Learning of Liquid Water**


In order to demonstrate the efficacy of ERBS in an active learning setting, we consider the


case of liquid water. The initial system was constructed using Packmol [66] with 32 water


molecules in a periodic box at the experimental density of 997 kg m _[−]_ [3] . Energy and forces


of all data points collected in the active learning scheme were computed using the revPBE0


hybrid functional [67] with a plane wave cutoff of 400 Ry, TZV2P-GTH basis sets, [68] Goedecker–


Teter–Hutter pseudo potentials [69–71] and the D3 dispersion correction [72,73] in CP2K. [61] All


DFT parameters are adapted from Cheng et al.. [36]


Starting from the Packmol structure, 200 configurations were bootstrapped by randomly


rotating and translating molecules and atomic displacements. The first 2 iterations of active


learning were performed on these bootstrapped samples as follows. A first model was trained


on 10 and validated on 6 randomly selected data points. From the remaining data pool,


another 5 training samples were chosen using the maximum distance selection with last

19


layer gradient features.


Afterwards, two AL workflows were set up. Both are identical apart from the use or lack


of the ERBS bias potential. For the biased workflow, a new kernel was placed every 5 ps with


a barrier height of 0 _._ 5 eV per atom and a bandwidth of 1.0. The first 3 PCA components


were used in the construction of the CVs. GMNN models were trained as shallow ensembles


with 16 members in order to have access to uncertainty estimates. The radial cutoff was


chosen to be 5 _._ 5 Å and a neural network with two hidden layers of 128 and 64 units was used


throughout. The remaining hyperparameters are listed in Table S1.


For every iteration, a 25 ps sampling simulation was set up using a time step of 0 _._ 5 fs and


a coupling constant of 500 fs for the Berendsen thermostat. A force uncertainty threshold of


_−_ 1
3 _._ 0 meV Å was used as a stopping criterion for the trajectories. The sampling simulations


were conducted with the `ASESafeSampling` node in IPSuite. Upon reaching the uncertainty


threshold, the geometry is reset to the starting configuration, the momenta are initialized,


and the simulation continues until the specified duration is reached. After each sampling


simulation, 4 data points were chosen randomly for the validation dataset, and 10 data


points were chosen using maximum distance selection with last-layer gradient features from


the trajectory. For the remainder of the active learning cycles, these two methods were


used for all training and validation data selections, respectively. In total, 10 active learning


iterations were conducted for both setups, resulting in 115 training and 46 validation samples


in each case. The prediction-error parity plots for the active learned models as well as the


model trained on the literature dataset can be found in Figure S6.


The water dataset by Cheng et al. [36] was generated with an emphasis on structural


diversity and at in part sampled using path-integral molecular dynamics. Models trained


on it were shown to achieve good agreement with experiment for structural properties of


liquid water, relative stabilities of different phases of water ice and other thermodynamic


observables. As we use the same level of theory and DFT code for labelling data in this


experiment, we can evaluate all models on their dataset. Figure 5 a) displays the Force MAE


20


for each model iteration from the active learning runs with and without enhanced sampling


compared to a model fitted directly to the literature training data. We observe that the


models trained using ERBS-enhanced sampling consistently achieve lower force errors over


all AL iterations compared to those trained using standard MD AL sampling. As the number


of AL cycles increases, the performance gap between the two approaches narrows. It is to


be expected that neither workflow achieves chemical accuracy on the literature test dataset.


The test dataset includes configurations of varying densities and bond breakages, neither of


which is contained in the datasets constructed here.


In addition to test set metrics, we simulate diffusion coefficients to investigate the abil

ity of the two AL setups in producing models that can accurately simulate experimental


observables. We simulate 5 ns trajectories of 256 water molecules in the NVT ensemble at


300 K using the models obtained from each AL iteration. Additionally, the diffusion was


also simulated with the model trained on the literature dataset in order to rule out model

architecture-specific limitations in describing the dynamics. The center-of-mass diffusion


coefficients, _D_ are obtained from a linear fit to the mean-squared displacement (MSD)



2 [�]



**R** ( _t_ ) _−_ **R** (0)

_i_ =1



�����



MSD(∆ _t_ ) =



1

_N_
������



_N_




= 6 _D_ ∆ _t_ (21)



Here, the sum goes over all _N_ equivalent particle positions, **R**, or in this case the mass centers


of the water molecules and ∆ _t_ is the simulation time. Based on the diffusion coefficient thus


calculated from a finite system, the infinite system-size limit is obtained via the Yeh–Hummer


correction. [74]


_ξ_
_D_ ( _∞_ ) = _D_ ( _L_ ) + (22)
6 _πβηL_


The first term, _D_ ( _L_ ) corresponds to the diffusion coefficient obtained from rearranging


21 for a finite system of side length _L_ . The correction term includes the geometry-dependent


constant _ξ_ = 2 _._ 837297 (for a cubic box) and the shear viscosity _η_ .


21


The first 200 ps of the trajectories are discarded for equilibration. To estimate uncer

tainties, block averaging is applied. All simulated diffusion coefficients and the experimental


value are displayed in Figure 5 b).


Figure 5: a) Force mean absolute errors with respect to the literature water dataset across
active learning iterations for the models created with MD and ERBS-based active learning.
b) Diffusion coefficients of water simulated using models from successive active learning
iterations, compared to the value obtained with the model trained on the literature dataset.


Diffusion coefficients simulated with the ERBS models are consistently closer the the


reference value obtained with the model trained on the literature dataset across all AL itera

tions. Starting from iteration 4, the ERBS model diffusion coefficients reach good agreement


with the value produced with the reference model, with the standard errors overlapping for


5 of the remaining 7 trajectories. To further validate the active-learned models, we have


computed the oxygen-oxygen radial distribution functions, which can be found in Figure S2.


While the reference and ERBS models yield similar diffusion coefficients, they both over
estimate the experimental value of 241 Å2 ns _−_ 1 75 by about 25 Å2 ns _−_ 1. Possible reasons could


either be limitations of the GMNN model or a tendency of the PBE0 hybrid functional to


overestimate the diffusion. Diffusion coefficients reported in AIMD studies strongly depend


on the choice of density functional. [76–78] In contrast, Daru et al. [79] report excellent agreement


between simulated and experimental diffusion coefficients using an MLIP trained on coupled


cluster data, with simulations that also account for nuclear quantum effects. It is worth


noting that, due to the small datasets used here, adding a few data points can change the


PES of the model quite drastically between iterations, and a monotonic convergence to the


22


reference model cannot be expected. Nevertheless, the rapid improvement and overall stabil

ity of the ERBS-trained models highlight the effectiveness of enhanced sampling in building


accurate MLIPs with minimal data.

#### **3.3 Comparison to Uncertainty Based Sampling**


To benchmark the sampling performance of ERBS against UDD, we selected the ionic liquid

BMIM [+] BF4 - as a test system. Its inherently high viscosity characteristic poses a particular


challenge for active learning, as long molecular dynamics trajectories are typically required


to adequately explore intermolecular interactions. [80–82]

A shallow ensemble is trained on a 200-structure subset of the BMIM [+] BF4  - dataset


created by Zills et al.. [83] The prediction-error parity plots for the model are shown in


Figure S7. The model hyperparameters are identical to the experiment on liquid water


from the previous section. It achieves energy and force MAEs on the validation set of


_−_ 1
0 _._ 75 meV atom _[−]_ [1] and 62 meV Å respectively. The quality of uncertainty estimates is dis

cussed in Section S5. Next, we perform sampling simulations with unbiased MD, UDD,


and ERBS. To ensure a fair comparison, we performed parameter scans for both UDD and


ERBS. We perform UDD for all combinations of _A_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _,_ 5 _._ 0 _,_ 10 _._ 0 _}_ eV atom _[−]_ [1]


and _B_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _}_ eV cf. Equation (11). As ERBS has more parameters, we fo

cus on the barrier height, bandwidth, and number of principal components. Starting from


∆ _E_ = 1 _._ 5 eV atom _[−]_ [1], _σ_ = 2 _._ 0, and _k_ = 2, one parameter at a time was scanned while holding


the others fixed to reduce the number of simulations. The individual parameter ranges were


chosen as ∆ _E_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 5 _,_ 2 _._ 5 _}_ eV atom _[−]_ [1], _σ_ = _{_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _,_ 5 _._ 0 _}_ and _k_ = _{_ 1 _,_ 2 _,_ 3 _,_ 4 _}_ .


The sampling simulations lasted for 100 ps and were simulated with a Berendsen thermostat


using a coupling constant of 500 fs and a time step of 0 _._ 5 fs.


To assess sampling efficiency, we use the mean squared displacement (MSD) of the BF4


center of mass, as defined in Equation (21), as a proxy measure. A higher MSD reflects


a larger deviation from the initial configuration, indicating more extensive sampling of the


23


intermolecular degrees of freedom. The maximum MSD for each trajectory is displayed in


Figure 6.


Figure 6: Final mean square displacements BF4 - center of masses from
BMIM [+] BF4 - trajectories simulated with MD and various parameters for ERBS and
UDD. For the three ERBS parameter scans, the size of the dots is proportional to the value
of the respective parameter. For ERBS, these ranges are ∆ _E_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 5 _,_ 2 _._ 5 _}_ eV atom _[−]_ [1],
_σ_ = _{_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _,_ 5 _._ 0 _}_ and _k_ = _{_ 1 _,_ 2 _,_ 3 _,_ 4 _}_ and for UDD _A_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _,_ 5 _._ 0 _,_ 10 _._ 0 _}_ and
_B_ = _{_ 0 _._ 1 _,_ 0 _._ 5 _,_ 1 _._ 0 _,_ 2 _._ 0 _}_ .


It can be seen that UDD is capable of enhancing the sampling of intermolecular degrees


of freedom to some degree, but only a few parameter choices offer a significant enhancement.

The trajectory with an MSD of 0 _._ 5 Å2 ns _−_ 1 terminated early due to bond breakage, leading


to small overall displacements. All runs with _B_ = 0 _._ 1 eV terminated within the first 100


simulation steps, before a configuration could be collected. In the case of ERBS, only


the ∆ _E_ = 2 _._ 5 eV atom _[−]_ [1] run is terminated early for the same reason. We find that for


most parameter choices, ERBS drastically enhances the intermolecular motions, while the


enhancement for UDD is significantly less pronounced. In the best cases, ERBS achieves an


MSD almost 5 times larger than unbiased MD, while UDD only shows an increase of a factor


of 1.8.


To visualize the differences in exploration strategies, we consider the distribution of aver

aged Gaussian moment descriptors. We compute the average descriptors for the MD trajec

24


tory and the highest MSD UDD, and ERBS trajectories, and project them into a common


2D PCA basis. Figure 7 reveals that the UDD trajectory samples configurations that are


shifted with respect to the MD trajectory in the dimensionality-reduced descriptor space. In


contrast, the ERBS trajectory displays a distinct clustering pattern that evolves over time,


a direct consequence of the iterative deposition of bias potential in descriptor space. Over

all, the ERBS trajectory exhibits the highest variance in descriptor space, highlighting the


method’s ability to drive the system out of local minima and explore a significantly larger


volume of the physically relevant configurational space.









Figure 7: First two principal components of the system average descriptors of BMIM [+] BF4 
for an MD, a UDD, and an ERBS trajectory. The ERBS descriptors are color-coded by the
simulation time to highlight the clustering.


To explain the sampling behaviour of UDD, we consider the bias forces


_−_ _[∂E]_ [UDD] = _−_ _[∂E]_ [UDD][ (] _[σ]_ _E_ [2] [)] _·_ _[∂σ]_ _E_ [2] (23)

_∂_ **R** _∂σE_ [2] _∂_ **R** _[,]_


where the derivative of the predicted energy variance _σE_ [2] [with respect to atomic positions] **[ R]**



is proportional to
_∂σE_ [2]            _∂_ **R** _[∝]_

_m_




- _E_ [(] _[m]_ [)] _−_ _E_ [¯] �� **F** [(] _[m]_ [)] _−_ **F** [¯] - _._ (24)



As such, the bias forces depend on the disagreement between the ensemble members’ force


25


predictions.


In molecular liquids, forces predictions can be analytically decomposed into vibrational,


rotational, and translational components, [84,85] with the intermolecular forces typically being


an order of magnitude smaller than the intramolecular ones. Hence, the intermolecular bias


forces depend on the disagreement of the ensemble members’ intermolecular force predic

tions. This decomposition is illustrated in the parity plots shown in Section S6. Both the


predicted intermolecular forces and their uncertainties remain small for a well-trained and


well-calibrated MLIP. Since the UDD forces along particular degrees of freedom are directly


linked to the model’s uncertainty along these degrees of freedom, the bias tends to vanish in


the directions of slow, collective motion. This limits the effectiveness of UDD in enhancing


sampling along shallow degrees of freedom.

### **4 Conclusion**


In this work, we have introduced ERBS, a general-purpose enhanced sampling strategy


that allows for the construction of diverse datasets for MLIPs. Starting from the model


descriptor averaged over the whole system, ERBS identifies the slowest collective modes in


descriptor space via principal component analysis. A bias potential is then constructed based


on the recently introduced OPES-Explore method, which combines desirable attributes such


as rapid exploration of the FES and limiting the maximum strength of the bias potential.


Unlike uncertainty-based enhanced sampling methods, ERBS does not require a preliminary


MLIP and can be used on top of any interatomic potential. Model independence makes ERBS


particularly convenient in early-stage dataset generation or for systems where classical force


fields or pre-trained MLIPs are readily available.


We first evaluated ERBS using a classical force field for alanine dipeptide. ERBS was


able to achieve up to 75 % coverage of the dihedral angle space in just 80 ps of simulation,


and the resulting data enabled the training of MLIPs that accurately reproduce the free


26


energy landscape.


To demonstrate utility in an active learning setting, we applied ERBS to the iterative


construction of a water dataset. Compared to unbiased sampling, ERBS dramatically ac

celerated convergence toward accurate diffusion coefficients, matching the quality of models


trained on literature datasets using an order of magnitude fewer training points. The model


trained exclusively with MD samples, on the other hand, showed significant deviations from


the reference diffusion coefficient throughout.


Finally, we compared the sampling behavior of ERBS and uncertainty-driven dynamics


for the highly viscous ionic liquid BMIM [+] BF4 - . We observed that ERBS more effectively


enhances sampling along slow, intermolecular degrees of freedom. The accelerated sampling


is attributed to the use of global, low-dimensional collective variables, which avoids overem

phasising high-frequency intramolecular modes, which are often the largest contributors to


mode uncertainty and error. ERBS offers a fast method for enhancing the quality of training


data both in static and active learning settings, even when no pre-trained model is avail

able. Crucially, although demonstrated here using the Gaussian Moment descriptor, the


framework is representation-agnostic and can be directly coupled to other state-of-the-art


descriptors or the feature layers of equivariant neural networks. The role of a pre-trained rep

resentation and extensions to constant-pressure simulations will be explored in subsequent


studies. While this study focuses on the exploration of molecular and liquid configuration


spaces, the variance-based identification of collective variables is a phase-agnostic principle,


suggesting that the method is applicable to solid-state sampling as well. We intend to further


investigate this line of research in future work.


Lastly, the use of enhanced sampling techniques, such as ERBS, may also prove valuable


in the context of constructing datasets for atomistic foundation models. By systematically


exploring underrepresented regions of configuration space, these methods can help ensure


broad coverage of structural motifs. This could lead to more compact and diverse datasets,


ultimately improving the transferability and robustness of foundation models across domains.


27


### **Associated Content**

#### **Data Availability**

The workflow notebook as well as all input files for the various software packages needed to


reproduce the work presented here can be found at `https://github.com/M-R-Schaefer/`


`erbs_experiments/` . All data generated during the iterative training and production simu

lations are stored on an S3-object storage. It can be obtained by cloning the repository and


executing `dvc pull` in the repository folder.

#### **Code Availability**


All software used throughout this work is publicly available. ERBS is available on Github


at `https://github.com/apax-hub/erbs` . The Apax repository is available on Github at


`https://github.com/apax-hub/apax` . IPSuite is available at `https://github.com/zincware/`


`IPSuite` . All three can be installed from PyPi _via_ `pip install erbs apax ipsuite` .

#### **Supporting Information**


Detailed table of model hyperparameters for all experiments; discussion of the computational


complexity of the method; Ramachandran space coverage for various models of the alanine


dipeptide experiment; calibration metrics of the shallow ensemble in the BMIM experiment;


decomposition of forces into translational, rotational and vibrational components for the


BMIM dataset; test set prediction-error parity plots for all experiments.


28


### **Author Information**

#### **Corresponding Author**

**Johannes Kästner** - Institute for Theoretical Chemistry, University of Stuttgart, Pfaf

fenwaldring 55, 70569 Stuttgart, Germany; https://orcid.org/0000-0001-6178-7669; Email:


kaestner@theochem.uni-stuttgart.de

#### **Author**


**Moritz R. Schäfer** - Institute for Theoretical Chemistry, University of Stuttgart, Pfaffen

waldring 55, 70569 Stuttgart, Germany; https://orcid.org/0000-0001-8474-5808

#### **Notes**


The authors have no conflicts of interest to declare.

### **Acknowledgements**


The authors would like to thank Nico Segreto and Fabian Zills for insightful discussion and


comments on an early version of the manuscript. J.K., and M.S. acknowledge support by


the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) in the frame

work of the priority program SPP 2363, “Utilization and Development of Machine Learning


for Molecular Applications - Molecular Machine Learning” Project No. 497249646. Further


funding was provided by Deutsche Forschungsgemeinschaft (DFG, German Research Foun

dation) under Germany’s Excellence Strategy - EXC 2075 – 390740016. We acknowledge the


support by the Stuttgart Center for Simulation Science (SimTech).


All authors acknowledge support by the state of Baden-Württemberg through bwHPC


and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG.


29


### **Supporting Information Available** **S1 Model Hyperparameters**

All hyperparameters used to train the models of each experiment are listed in Table S1. The


Huber loss function used for the alanine dipeptide experiment is given by



_N_ atoms [(] _[k]_ [)]



_i_
















12 [(] _**[F]**_ [ ref] _i,k_ _[−]_ _**[F]**_ _[ i]_ [(] _[S][k][,]_ _**[ θ]**_ [))][2] _[,]_ if _|_ _**F**_ [ref] _i,k_ _[−]_ _**[F]**_ _[ i]_ [(] _[S][k][,]_ _**[ θ]**_ [)] _[| ≤]_ _[δ]_



_δ_ - _|_ _**F**_ [ref] _i,k_ _[−]_ _**[F]**_ _[ i]_ [(] _[S][k][,]_ _**[ θ]**_ [)] _[| −]_ 2 [1]



_Lδ_ ( _**θ**_ ) =



_N_ train



_k_ =1



1

3 _N_ atoms [(] _[k]_ [)]



(S1)

[1] - _,_ otherwise

2 _[δ]_



In total, the number of trainable parameters was 32929 for the Alanine Dipeptide, 90244

for water, and 92548 BMIM [+] BF4 - experiments. The difference in parameter counts for the


last two experiments arises from the different number of elements in the systems, which


results in a different number of elemental embedding parameters.

### **S2 Scaling**


The method by Yoo et al. constructs per-atom bias potentials based on Behler-Parinello


descriptors and a metadynamics-like functional form of the bias. The use of uncompressed


per-atom descriptors significantly increases the memory requirements for storing the reference


descriptors. Further, the calculation of _E_ bias requires comparison of each atom’s descriptor


with all reference descriptors of the same element. This introduces an additional factor to the


cost calculation of the bias potential of _N_ atoms. ERBS uses diagonal kernels; the approach of


Yoo et al. [28] constructs an adaptive covariance matrix from descriptor Jacobians according


to:



_N_ at

  Σ _jk_ ( **G** ) = _σ_ [2]


_i_ =1






_α_ = _x,y,z_


S30



_∂Gk_
+ _εδjk,_ (S2)
_∂Ri,α_



_∂Gj_
_∂Ri,α_


Table S1: GMNN training hyperparameters used throughout the experiments of Alanine
Dipeptide, Water and BMIM [+] BF4 

**Hyperparameter** **Alanine Dipeptide** **Water, BMIM** **[+]** **BF4** **[–]**


_Training_


Epochs 10 000 10 000
Batch size 8 1
Gradient clipping 10.0 10.0


_Model_


Basis function Bessel, _n_ = 16, _r_ max = 5 _._ 0 Bessel, _n_ = 16, _r_ max = 5 _._ 5
Radial functions 5 6
NN layers 64, 64 128, 64
Ensemble     - Shallow (16 members)


_Optimizer_


Name AdamW AdamW
Embedding LR 0.001 0.0001
NN LR 0.001 0.0001
Scale LR 0.0005 0.0001
Shift LR 0.0005 0.0001
Weight decay 10 _[−]_ [5] 2 _·_ 10 _[−]_ [4]


_Schedule_


Name Cyclic cosine Cyclic cosine
Period 50 50
Decay factor 0.95 0.96


_Loss functions_


Energy loss Huber ( _δ_ = 0 _._ 5), weight=1.0 NLL, weight=1.0
Force loss Huber ( _δ_ = 0 _._ 1), weight=2.0 NLL, weight=2.0


where _σ_ is a hyperparameter controlling the scale of the kernel, and _ϵ_ is a regularization


constant added to the diagonal to ensure numerical stability. The covariance matrix has a


dimensionality of _N_ feat _× N_ feat, where _N_ feat is the number of descriptor components per con

figuration. Its inverse needs to be stored along with the reference descriptors or recomputed


on the fly, adding to either the memory or computational cost.


Additionally, we find that, at least for the GM descriptor, this matrix can be severely ill

conditioned, such that its inverse is dominated by the choice of _ϵ_, effectively suppressing the


geometry-dependent structure of the covariance. One possible cause for the ill-conditioning


S31


is that the GM descriptor constructs many body features via outer products, leading to


high correlation of the features. The Behler-Parinello descriptor, on the other hand, has


separate parts for two and three-body interactions of element pairs and triplets, reducing


the correlation.

### **S3 Ramachandran Space Coverage**


To further analyze the exploration behavior of the ERBS method in comparison to high

temperature molecular dynamics, we investigate the Ramachandran space coverage visually.


Figure S1 displays the coverage plots for the MD 1200 K, ERBS A, ERBS B trajectories, and


one that led to the dissociation of the molecule. The parameter choice for the dissociated


simulation was ∆ _E_ = 10 eV, _σ_ = 0 _._ 1, and _k_ = 2. We find that the MD simulation at


1200 K does not sample the free energy minimum in the lower right quadrant, while both


ERBS simulations do. The good description of the minimum location and high errors on


the right-hand side of the FES of the MD 1200 K model can be explained in this way. The


broken simulation achieves a high coverage, although this is merely due to the free rotation


of fragments.

### **S4 Radial Distribution Functions of Water**


We further investigate the capability of the active learned MLIP to reproduce structural


properties of liquid water. Specifically, we calculate the oxygen-oxygen radial distribution


functions (RDFs). As shown in Figure S2, the RDFs for all models are in good agreement


and visually indistinguishable, accurately reproducing the location and intensity of both the


first and second hydration shells.


We do not observe an underestimation of the hydration shells that would explain an


overestimated diffusion coefficient. The radial distribution function defines the effective 2

body potential of mean force (PMF), _W_ ( _r_ ) = _−k_ B _T_ log _g_ ( _r_ ). However, _W_ ( _r_ ) is a free energy


S32


a)
MD 1200 K


/2


0


/2


Coverage: 0.39


/2 0 /2
/ rad

c)
ERBS A


/2


0


/2


Coverage: 0.69


/2 0 /2
/ rad



b)
Unstable


/2


0


/2


Coverage: 0.84


/2 0 /2
/ rad

d)
ERBS B


/2


0


/2


Coverage: 0.69


/2 0 /2
/ rad



Figure S1: Ramachandran space coverage of the a) MD 1200K, b) ∆ _E_ = 10 eV, _σ_ = 0 _._ 1,
and _k_ = 2, c) ERBS A, and d) ERBS B.


S33


4


2


0



Radial Distribution Functions


Reference
ERBS
MD


0 2 4 6 8
Distance _r_ [Å]



Signed Error vs. Reference Model



0.075


0.050


0.025


0.000


0.025


0.050



ERBS MAE: 0.010
~~MD MAE: 0.010~~


0 2 4 6 8
Distance _r_ [Å]



Figure S2: Oxygen-oxygen radial distribution functions of the reference model and the MLIPs
produced by active learning on MD and ERBS data. The right panel shows the signed error
of the active learned models with respect to the reference.


S34


surface resulting from integrating out all other degrees of freedom, including higher body


order terms.


Diffusion, however, is a dynamical process governed by the underlying high-dimensional


potential energy surface, not the projected PMF. A transition event often requires a specific


many-body rearrangement, such as a rotation around a bond that constitutes a high-energy


barrier on the true PES.


The fact that the MD-trained model reproduces the RDF, and thus the potential of mean


force, but overestimates diffusion implies it has learned a ’flattened’ PES. It captures the


pairwise energetics correctly but lacks the explicit many-body repulsions that create the true


friction and transition barriers in the liquid.

### **S5 Calibration Metrics**


Assessing the quality of uncertainty estimates is crucial in the active learning context, both


for terminating sampling trajectories and for the appropriate biasing by UDD. Figure S3


displays the scatter plots for predicted uncertainties compared to empirical errors for energy


and force uncertainties for shallow ensembles trained with MSE and NLL losses. While the


validation errors are comparable for both cases, the calibration of uncertainty estimates is


significantly improved by the use of the probabilistic loss function.

### **S6 Force Decomposition**


In molecular liquids, the forces on each atom can be analytically decomposed into vibrational,


translational, and rotational forces. [84] Figure S4 shows the prediction errors of the model

trained on BMIM [+] BF4 - for the decomposed forces. The separation in magnitude between


inter- and intramolecular forces is evident. While the MAE for the intermolecular forces is


fairly small, so are the true forces. The mean absolute force components of the translational


_−_ 1 _−_ 1
and rotational are 20 _._ 6 meV Å and 19 _._ 5 meV Å respectively, resulting in large relative


S35


Figure S3: Predicted-empirical error scatter plot for energies and forces of the BMIM [+] BF4 
validation dataset. Panels a) and b) show the performance of the shallow ensemble trained
with an NLL loss, c) and d) that of the shallow ensemble trained with an MSE loss.


S36


errors.


Figure S4: Error of predicted compared to true force components for the BMIM [+] BF4 
validation dataset.

### **S7 Test Set Error Analysis**


To evaluate predictive quality, we present density-colored parity plots of energy and force


errors for key models. For the alanine dipeptide, where cross-validation results are detailed


in the main text, Figure S5 shows the self-validation performance. As the reference data


is derived from a classical force field, all models fit energies and forces well below chemical


accuracy.


For the water experiment, we compare the final MD and ERBS active learning models


against the model trained on the literature dataset. While the MD and ERBS models achieve


similar high accuracy, the literature-trained model exhibits consistently higher errors. This


is attributed to the broader diversity of the literature dataset, which includes path-integral


MD and constant-pressure simulations.


Finally, for BMIM [+] BF4  -, we analyze the shallow ensemble trained on the literature


dataset. It achieves improved energy metrics compared to the original publication (MAE:


_−_ 1
5 _._ 5 meV atom _[−]_ [1] ), though force metrics are slightly higher (reference MAE: 48 meV Å ).


37


Figure S5: Energy and force prediction errors of the MD 300 K, MD 1200 K, ERBS A and
ERBS B on their respective validation sets.



























Figure S6: Energy and force prediction errors of the reference model and the active learned
models trained on MD and ERBS data.


38


Figure S7: Energy and force prediction errors of the model trained on the BMIM [+] BF4 
literature dataset.


39


### **References**

(1) Unke, O. T.; Chmiela, S.; Sauceda, H. E.; Gastegger, M.; Poltavsky, I.; Schütt, K. T.;


Tkatchenko, A.; Müller, K.-R. Machine Learning Force Fields. _Chemical Reviews_ **2021**,


_121_, 10142–10186.


(2) Behler, J.; Csányi, G. Machine Learning Potentials for Extended Systems: A Perspec

tive. _The European Physical Journal B_ **2021**, _94_, 142.


(3) Behler, J.; Parrinello, M. Generalized Neural-Network Representation of High

Dimensional Potential-Energy Surfaces. _Physical Review Letters_ **2007**, _98_, 146401.


(4) Behler, J. Atom-centered symmetry functions for constructing high-dimensional neural


network potentials. _The Journal of Chemical Physics_ **2011**, _134_, 074106.


(5) Bartók, A. P. Gaussian Approximation Potential: an interatomic potential derived from


first principles Quantum Mechanics. PhD thesis, 2009.


(6) Bartók, A. P.; Payne, M. C.; Kondor, R.; Csányi, G. Gaussian Approximation Poten

tials: The Accuracy of Quantum Mechanics, without the Electrons. _Physical Review_


_Letters_ **2010**, _104_, 136403.


(7) Bartók, A. P.; Kondor, R.; Csányi, G. On representing chemical environments. _Physical_


_Review B_ **2013**, _87_, 184115.


(8) Chmiela, S.; Tkatchenko, A.; Sauceda, H. E.; Poltavsky, I.; Schütt, K. T.; Müller, K.

R. Machine Learning of Accurate Energy-Conserving Molecular Force Fields. _Science_


_Advances_ **2017**, _3_, e1603015.


(9) Drautz, R. Atomic Cluster Expansion for Accurate and Transferable Interatomic Po

tentials. _Physical Review B_ **2019**, _99_, 014104.


40


(10) Batatia, I.; Kovacs, D. P.; Simm, G.; Ortner, C.; Csanyi, G. MACE: Higher Order


Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields.


_Advances in Neural Information Processing Systems_ **2022**, _35_, 11423–11436.


(11) Batzner, S.; Musaelian, A.; Sun, L.; Geiger, M.; Mailoa, J. P.; Kornbluth, M.; Moli

nari, N.; Smidt, T. E.; Kozinsky, B. E(3)-Equivariant Graph Neural Networks for Data

Efficient and Accurate Interatomic Potentials. _Nature Communications_ **2022**, _13_, 2453.


(12) Schütt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko, A.; Müller, K.-R. SchNet


   - A Deep Learning Architecture for Molecules and Materials. _The Journal of Chemical_


_Physics_ **2018**, _148_, 241722.


(13) Wang, H.; Zhang, L.; Han, J.; E, W. DeePMD-kit: A Deep Learning Package for Many

Body Potential Energy Representation and Molecular Dynamics. _Computer Physics_


_Communications_ **2018**, _228_, 178–184.


(14) Dullinger, P.; Futterer, J.; Kappel, N.; Wenzel, W. Accelerating Molecular Dynamics


Simulations with Quantum Accuracy by Hierarchical Classification. 2024; `https://`


`chemrxiv.org/engage/chemrxiv/article-details/671934e183f22e4214ee3d78` .


(15) Batatia, I. et al. A foundation model for atomistic materials chemistry. _J. Chem. Phys._


**2025**, _163_, 184110.


(16) Eastman, P. et al. SPICE, A Dataset of Drug-like Molecules and Peptides for Training


Machine Learning Potentials. _10_, 11, Publisher: Nature Publishing Group.


(17) Zills, F.; Agarwal, S.; Goncalves, T. J.; Gupta, S.; Fako, E.; Han, S.; Britta Mueller, I.;


Holm, C.; De, S. MLIPX: machine-learned interatomic potential eXploration. _37_,


385901, Publisher: IOP Publishing.


(18) Deng, B.; Choi, Y.; Zhong, P.; Riebesell, J.; Anand, S.; Li, Z.; Jun, K.; Persson, K. A.;


41


Ceder, G. Systematic softening in universal machine learning interatomic potentials.


_11_, 9, Publisher: Nature Publishing Group.


(19) Smith, J. S.; Isayev, O.; Roitberg, A. E. ANI-1: An Extensible Neural Network Potential


with DFT Accuracy at Force Field Computational Cost. _Chemical Science_ **2017**, _8_,


3192–3203.


(20) Busk, J.; Schmidt, M. N.; Winther, O.; Vegge, T.; Jørgensen, P. B. Graph Neural


Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic


Uncertainty on Energy and Forces. _Physical Chemistry Chemical Physics_ **2023**, _25_,


25828–25837.


(21) Thaler, S.; Doehner, G.; Zavadlav, J. Scalable Bayesian Uncertainty Quantification


for Neural Network Potentials: Promise and Pitfalls. _Journal of Chemical Theory and_


_Computation_ **2023**, _19_, 4520–4532.


(22) Guan, Y.; Yang, S.; Zhang, D. H. Construction of Reactive Potential Energy Surfaces


with Gaussian Process Regression: Active Data Selection. _Molecular Physics_ **2018**,


_116_, 823–834.


(23) Vandermause, J.; Torrisi, S. B.; Batzner, S.; Xie, Y.; Sun, L.; Kolpak, A. M.; Kozin

sky, B. On-the-Fly Active Learning of Interpretable Bayesian Force Fields for Atomistic


Rare Events. _npj Computational Materials_ **2020**, _6_, 1–11.


(24) Zaverkin, V.; Kästner, J. Exploration of Transferable and Uniformly Accurate Neural


Network Interatomic Potentials Using Optimal Experimental Design. _Machine Learn-_


_ing: Science and Technology_ **2021**, _2_, 035009.


(25) Zaverkin, V.; Holzmüller, D.; Christiansen, H.; Errica, F.; Alesiani, F.; Takamoto, M.;


Niepert, M.; Kästner, J. Uncertainty-Biased Molecular Dynamics for Learning Uni

formly Accurate Interatomic Potentials. _npj Computational Materials_ **2024**, _10_, 1–18.


42


(26) Herr, J. E.; Yao, K.; McIntyre, R.; Toth, D. W.; Parkhill, J. Metadynamics for Train

ing Neural Network Model Chemistries: A Competitive Assessment. _The Journal of_


_Chemical Physics_ **2018**, _148_, 241710.


(27) Laio, A.; Parrinello, M. Escaping free-energy minima. _Proceedings of the National_


_Academy of Sciences_ **2002**, _99_, 12562–12566.


(28) Yoo, D.; Jung, J.; Jeong, W.; Han, S. Metadynamics Sampling in Atomic Environment


Space for Collecting Training Data for Machine Learning Potentials. _npj Computational_


_Materials_ **2021**, _7_, 1–9.


(29) Kulichenko, M.; Barros, K.; Lubbers, N.; Li, Y. W.; Messerly, R.; Tretiak, S.;


Smith, J. S.; Nebgen, B. Uncertainty-Driven Dynamics for Active Learning of Inter

atomic Potentials. _Nature Computational Science_ **2023**, 1–10.


(30) Novoselov, I. I.; Yanilkin, A. V.; Shapeev, A. V.; Podryabinkin, E. V. Moment Tensor


Potentials as a Promising Tool to Study Diffusion Processes. _Computational Materials_


_Science_ **2019**, _164_, 46–56.


(31) van der Oord, C.; Sachs, M.; Kovács, D. P.; Ortner, C.; Csányi, G. Hyperactive Learning


for Data-Driven Interatomic Potentials. _npj Computational Materials_ **2023**, _9_, 1–14.


(32) Waters, M. J.; Rondinelli, J. M. Energy contour exploration with potentiostatic kine

matics. _Journal of Physics: Condensed Matter_ **2021**, _33_, 445901.


(33) Waters, M. J.; Rondinelli, J. M. Benchmarking structural evolution methods for training


of machine learned interatomic potentials. _Journal of Physics: Condensed Matter_ **2022**,


_34_, 385901.


(34) Stewart, G. W. On the Early History of the Singular Value Decomposition. _SIAM_


_Review_ **1993**, _35_, 551–566.


43


(35) Invernizzi, M.; Parrinello, M. Exploration vs Convergence Speed in Adaptive-Bias En

hanced Sampling. _Journal of Chemical Theory and Computation_ **2022**, _18_, 3988–3996.


(36) Cheng, B.; Engel, E. A.; Behler, J.; Dellago, C.; Ceriotti, M. Ab Initio Thermodynamics


of Liquid and Solid Water. _Proceedings of the National Academy of Sciences_ **2019**, _116_,


1110–1115.


(37) Zaverkin, V.; Kästner, J. Gaussian Moments as Physically Inspired Molecular Descrip

tors for Accurate and Scalable Machine Learning Potentials. _Journal of Chemical The-_


_ory and Computation_ **2020**, _16_, 5410–5421.


(38) Zaverkin, V.; Holzmüller, D.; Steinwart, I.; Kästner, J. Fast and Sample-Efficient In

teratomic Neural Network Potentials for Molecules and Materials Based on Gaussian


Moments. _Journal of Chemical Theory and Computation_ **2022**, _17_, 6658–6670.


(39) Prodan, E.; Kohn, W. Nearsightedness of electronic matter. _Proceedings of the National_


_Academy of Sciences_ **2005**, _102_, 11635–11638.


(40) Kocer, E.; Mason, J. K.; Erturk, H. A novel approach to describe chemical environments


in high-dimensional neural network potentials. _The Journal of Chemical Physics_ **2019**,


_150_, 154102.


(41) Pernot, P. The long road to calibrated prediction uncertainty in computational chem

istry. _The Journal of Chemical Physics_ **2022**, _156_, 114109.


(42) Bigi, F.; Chong, S.; Ceriotti, M.; Grasselli, F. A prediction rigidity formalism for low

cost uncertainties in trained neural networks. _Machine Learning: Science and Technol-_


_ogy_ **2024**, _5_, 045018.


(43) Tan, A. R.; Dietschreit, J. C. B.; Gómez-Bombarelli, R. Enhanced sampling of robust


molecular datasets with uncertainty-based collective variables. _The Journal of Chemical_


_Physics_ **2025**, _162_, 034114.


44


(44) Heid, E.; Schörghuber, J.; Wanzenböck, R.; Madsen, G. K. H. Spatially Resolved Un

certainties for Machine Learning Potentials. _Journal of Chemical Information and Mod-_


_eling_ **2024**, _64_, 6377–6387.


(45) Kellner, M.; Ceriotti, M. Uncertainty Quantification by Direct Propagation of Shallow


Ensembles. _Machine Learning: Science and Technology_ **2024**, _5_, 035006.


(46) Kirsch, A.; van Amersfoort, J.; Gal, Y. BatchBALD: Efficient and Diverse Batch Ac

quisition for Deep Bayesian Active Learning. arXiv:1906.08158 [stat.ML], 2019; `https:`


`//arxiv.org/abs/1906.08158`, arXiv.org e-Print Archive. `https://arxiv.org/abs/`


`1906.08158` (accessed Jan 21, 2026).


(47) Zaverkin, V.; Holzmüller, D.; Steinwart, I.; Kästner, J. Exploring Chemical and Con

formational Spaces by Batch Mode Deep Active Learning. _Digital Discovery_ **2022**, _1_,


605–620.


(48) Gonzalez, T. F. Clustering to minimize the maximum intercluster distance. _38_ .


(49) Schäfer, M. R.; Segreto, N.; Zills, F.; Holm, C.; Kästner, J. Apax: A Flexible and Per

formant Framework for the Development of Machine-Learned Interatomic Potentials.


_J. Chem. Inf. Model._ **2025**, _65_, 8066–8078.


(50) De, S.; P. Bartók, A.; Csányi, G.; Ceriotti, M. Comparing molecules and solids across


structural and alchemical space. _Physical Chemistry Chemical Physics_ **2016**, _18_, 13754–


13769.


(51) Bartók, A. P.; De, S.; Poelking, C.; Bernstein, N.; Kermode, J. R.; Csányi, G.; Ce

riotti, M. Machine learning unifies the modeling of materials and molecules. _Science_


_Advances_ **2017**, _3_, e1701816.


(52) Rowe, P.; Deringer, V. L.; Gasparotto, P.; Csányi, G.; Michaelides, A. An accurate and


45


transferable machine learning potential for carbon. _The Journal of Chemical Physics_


**2020**, _153_, 034702.


(53) Chen, W.; Tan, A. R.; Ferguson, A. L. Collective variable discovery and enhanced


sampling using autoencoders: Innovations in network architecture and error function


design. _The Journal of Chemical Physics_ **2018**, _149_, 072312.


(54) Sicard, F.; Senet, P. Reconstructing the free-energy landscape of Met-enkephalin using


dihedral principal component analysis and well-tempered metadynamics. _The Journal_


_of Chemical Physics_ **2013**, _138_, 235101.


(55) Kingma, D. P.; Welling, M. Auto-Encoding Variational Bayes. 2022; `http://arxiv.`


`org/abs/1312.6114` .


(56) Invernizzi, M.; Parrinello, M. Rethinking Metadynamics: From Bias Potentials to Prob

ability Distributions. _The Journal of Physical Chemistry Letters_ **2020**, _11_, 2731–2736.


(57) Barducci, A.; Bussi, G.; Parrinello, M. Well-Tempered Metadynamics: A Smoothly


Converging and Tunable Free-Energy Method. _Physical Review Letters_ **2008**, _100_,


020603.


(58) Laio, A.; Gervasio, F. L. Metadynamics: a method to simulate rare events and recon

struct the free energy in biophysics, chemistry and material science. _Reports on Progress_


_in Physics_ **2008**, _71_, 126601.


(59) Wu, H.-N.; Jiang, F.; Wu, Y.-D. Significantly Improved Protein Folding Thermody

namics Using a Dispersion-Corrected Water Model and a New Residue-Specific Force


Field. _The Journal of Physical Chemistry Letters_ **2017**, _8_, 3199–3205.


(60) Guo, J.; Zhou, H.-X. Protein Allostery and Conformational Dynamics. _Chemical Re-_


_views_ **2016**, _116_, 6503–6515.


46


(61) Kühne, T. D. et al. CP2K: An Electronic Structure and Molecular Dynamics Software


Package - Quickstep: Efficient and Accurate Electronic Structure Calculations. _The_


_Journal of Chemical Physics_ **2020**, _152_, 194103.


(62) Kingma, D. P.; Ba, J. Adam: A Method for Stochastic Optimization. 2017; `https:`


`//arxiv.org/abs/1412.6980` .


(63) Loshchilov, I.; Hutter, F. Decoupled Weight Decay Regularization. 2019; `http://`


`arxiv.org/abs/1711.05101` .


(64) Tribello, G. A.; Bonomi, M.; Branduardi, D.; Camilloni, C.; Bussi, G. PLUMED 2:


New feathers for an old bird. _Computer Physics Communications_ **2014**, _185_, 604–613.


(65) Bonomi, M. et al. Promoting transparency and reproducibility in enhanced molecular


simulations. _Nature Methods_ **2019**, _16_, 670–673.


(66) Martínez, L.; Andrade, R.; Birgin, E. G.; Martínez, J. M. PACKMOL: A Package


for Building Initial Configurations for Molecular Dynamics Simulations. _Journal of_


_Computational Chemistry_ **2009**, _30_, 2157–2164.


(67) Adamo, C.; Barone, V. Toward Reliable Density Functional Methods without Ad

justable Parameters: The PBE0 Model. _The Journal of Chemical Physics_ **1999**, _110_,


6158–6170.


(68) Goedecker, S.; Teter, M.; Hutter, J. Separable Dual-Space Gaussian Pseudopotentials.


_Physical Review B_ **1996**, _54_, 1703–1710.


(69) Becke, A. D. Density-Functional Thermochemistry. V. Systematic Optimization of


Exchange-Correlation Functionals. _The Journal of Chemical Physics_ **1997**, _107_, 8554–


8560.


(70) Hartwigsen, C.; Goedecker, S.; Hutter, J. Relativistic Separable Dual-Space Gaussian


Pseudopotentials from H to Rn. _Phys. Rev. B_ **1998**, _58_, 3641–3662.


47


(71) Krack, M. Pseudopotentials for H to Kr Optimized for Gradient-Corrected Exchange

Correlation Functionals. _Theoretical Chemistry Accounts_ **2005**, _114_, 145–152.


(72) Grimme, S.; Antony, J.; Ehrlich, S.; Krieg, H. A Consistent and Accurate Ab Ini

tio Parametrization of Density Functional Dispersion Correction (DFT-D) for the 94


Elements H-Pu. _The Journal of Chemical Physics_ **2010**, _132_, 154104.


(73) Grimme, S.; Ehrlich, S.; Goerigk, L. Effect of the Damping Function in Dispersion


Corrected Density Functional Theory. _Journal of Computational Chemistry_ **2011**, _32_,


1456–1465.


(74) Yeh, I.-C.; Hummer, G. System-Size Dependence of Diffusion Coefficients and Viscosi

ties from Molecular Dynamics Simulations with Periodic Boundary Conditions. _Journal_


_of Physical Chemistry B_ **2004**, _108_, 15873–15879.


(75) Holz, M.; Heil, S. R.; Sacco, A. Temperature-dependent self-diffusion coefficients of


water and six selected molecular liquids for calibration in accurate 1H NMR PFG


measurements. _Physical Chemistry Chemical Physics_ **2000**, _2_, 4740–4742.


(76) Gillan, M. J.; Alfè, D.; Michaelides, A. Perspective: How good is DFT for water? _The_


_Journal of Chemical Physics_ **2016**, _144_, 130901.


(77) Fernández-Serra, M. V.; Artacho, E. Network equilibration and first-principles liquid


water. _The Journal of Chemical Physics_ **2004**, _121_, 11136–11144.


(78) Lee, H.-S.; Tuckerman, M. E. Dynamical properties of liquid water from ab initio


molecular dynamics performed in the complete basis set limit. _The Journal of Chemical_


_Physics_ **2007**, _126_, 164501.


(79) Daru, J.; Forbert, H.; Behler, J.; Marx, D. Coupled Cluster Molecular Dynamics of


Condensed Phase Systems Enabled by Machine Learning Potentials: Liquid Water


Benchmark. _Physical Review Letters_ **2022**, _129_, 226001.


48


(80) Salanne, M. Simulations of Room Temperature Ionic Liquids: From Polarizable to


Coarse-Grained Force Fields. _Physical Chemistry Chemical Physics_ **2015**, _17_, 14270–


14279.


(81) Bedrov, D.; Piquemal, J.-P.; Borodin, O.; MacKerell, A. D.; Roux, B.; Schröder, C.


Molecular Dynamics Simulations of Ionic Liquids and Electrolytes Using Polarizable


Force Fields. _Chemical Reviews_ **2019**, _119_, 7940–7995.


(82) Shayestehpour, O.; Zahn, S. Efficient Molecular Dynamics Simulations of Deep Eu

tectic Solvents with First-Principles Accuracy Using Machine Learning Interatomic


Potentials. _Journal of Chemical Theory and Computation_ **2023**, _19_, 8732–8742.


(83) Zills, F.; Schäfer, M. R.; Tovey, S.; Kästner, J.; Holm, C. Machine learning-driven


investigation of the structure and dynamics of the BMIM-BF4 room temperature ionic


liquid. _Faraday Discussions_ **2024**, _253_, 129–145.


(84) Magdău, I.-B.; Arismendi-Arrieta, D. J.; Smith, H. E.; Grey, C. P.; Hermansson, K.;


Csányi, G. Machine Learning Force Fields for Molecular Liquids: Ethylene Carbon

ate/Ethyl Methyl Carbonate Binary Solvent. _npj Computational Materials_ **2023**, _9_,


1–15.


(85) Thompson, A. P. et al. LAMMPS - a Flexible Simulation Tool for Particle-Based Ma

terials Modeling at the Atomic, Meso, and Continuum Scales. _Computer Physics Com-_


_munications_ **2022**, _271_, 108171.


49


### **TOC Graphic**



50


