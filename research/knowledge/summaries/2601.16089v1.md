## A forward-only scheme for online learning of proposal distributions in particle filters

Sylvain Procope-Mamert [∗] Nicolas Chopin [†] Maud Delattre [∗]

Guillaume Kon Kam King [∗]


**Abstract**


We introduce a new online approach for constructing proposal distributions in particle filters using a forward scheme. Our method progressively incorporates future observations to refine proposals. This is
in contrast to backward-scheme algorithms that require access to the
entire dataset, such as the iterated auxiliary particle filters (Guarniero
et al., 2017) and controlled sequential Monte Carlo (Heng et al., 2020),
which leverage all future observations through backward recursion. In
comparison, our forward scheme achieves a gradual improvement of
proposals that converges toward the proposal targeted by these backward methods. We show that backward approaches can be numerically
unstable even in simple settings. Our forward method, however, offers
significantly greater robustness with only a minor trade-off in performance, measured by the variance of the marginal likelihood estimator.
Numerical experiments on both simulated and real data illustrate the
enhanced stability of our forward approach.

### **1 Introduction**


Sequential Monte Carlo (SMC) methods (Doucet and Johansen, 2011; Chopin
and Papaspiliopoulos, 2020) are a class of particle algorithms designed to
approximate sequences of probability measures known up to a normalizing
constant with a sequence of weighted particles. They are widely used for
inference in _state-space models_ (SSMs), a class of models describing the evolution of an unobserved latent process through time, combined with a noisy
observation process (Durbin and Koopman, 2012). An SSM typically consists of a latent Markov process and an observation process conditionally
independent given the states. These models are particularly useful when
observations are highly noisy, but the underlying dynamics provide essential
structure to link observations together and share information across time,


∗Université Paris-Saclay, INRAE, MaIAGE, 78350, Jouy-en-Josas, France
†ENSAE, CREST, Institut Polytechnique de Paris


1


thereby reducing uncertainty and improving estimation. SSMs are ubiquitous in statistics, with applications in econometrics, signal processing, epidemiology, and robotics (Cappé et al., 2005). However, inference in nonlinear
and non-Gaussian SSMs is challenging because the likelihood, the filtering,
and the smoothing distributions lack closed-form expressions and are highdimensional. While exact solutions exist for linear Gaussian cases via the
Kalman filter (Kalman, 1960), Monte Carlo methods, such as particle filters
and sequential Monte Carlo methods were introduced to handle more general
settings.
SMC algorithms may be naturally derived for most SSMs from the sequential nature of the model. The simplest and canonical SMC algorithm
used to infer an SSM is the bootstrap particle filter (Gordon et al., 1993).
This algorithm is often subject to particle degeneracy, resulting in a poor
approximation of the target distribution when one or a few particles carry
most of the weight. Designing efficient and robust filtering algorithms is crucial, as poor approximations and particle degeneracy lead to high variances
for Monte Carlo estimates and unreliable inference, particularly in complex
models. There have been many attempts to develop improved SMC algorithms that adapt to the target distribution, i.e., to the model and the observed data. For example, some authors have proposed guided particle filters
(Doucet et al., 2000) or auxiliary particle filters (Pitt and Shephard, 1999).
Most of these methods involve looking one step (one observation) ahead to
sample and weight particles at each time step. There are various ways to construct such SMC algorithms manually (Chopin and Papaspiliopoulos, 2020),
but they require a deep understanding of the target distributions (for example, using Taylor expansions). Automatic and adaptive particle filters aim
to reduce manual tuning and improve reliability across diverse applications,
making them an active area of research.
One approach to automatically construct better proposals is to iterate
forward and backward passes over the data in order to progressively refine the
proposals, as the iterated auxiliary particle filter (IAPF) of Guarniero et al.
(2017), and the controlled sequential Monte Carlo (cSMC) of Heng et al.
(2020). We observe that, in moderately challenging cases, these methods
may fail to produce low-variance estimates of the marginal likelihood of the
SSM. One explanation is that it is often difficult to design the initial forward
pass that provides a good initialization to the next backward pass. This
happens, for instance, if one uses the bootstrap filter for this initial forward
pass.
In this article, we propose a forward scheme that produces an SMC algorithm without requiring a good initialization. The difference with backward
schemes can be understood as follows: the backward schemes start from the
last time step of the SSM and go backward in time, essentially using all future
observations to construct the proposals, whereas our forward scheme starts
one step ahead and proceeds forward in time, gradually incorporating future


2


observations to refine the proposals. Thus, our forward scheme iterates over
easier steps than the backward schemes and is much more robust to difficult
initializations. Although our scheme can be less efficient in easy cases —
it often converges more slowly than backward schemes that incorporate all
available information — we find this loss is modest. In Markovian models,
observations far ahead typically carry little information about the current
state, which limits the performance gap. Through extensive illustrations on
simulated and real data, we observe that our algorithm is able to produce
lower variance estimates than backward algorithms, which suffer from a lack
of robustness.
A recent proposal (Xue et al., 2025) uses the same forward decomposition to construct an online smoothing algorithm, relying on two simultaneous
particle systems. The implementation and goals differ from ours: they focus
on estimating smoothing distributions rather than the marginal likelihood
and parameter inference. Nonetheless, this work demonstrates that the forward decomposition can be used fruitfully to construct adaptive particle
algorithms for SSMs.
Building on these ideas, we now present our contribution in detail. The
remainder of the article is organized as follows. In the next section, we introduce notation, state-space models, and the Feynman-Kac formalism underlying SMC algorithms. Next, we introduce auxiliary Feynman-Kac models
and characterize globally and locally optimal proposals. Then, we derive
our forward-only iterative scheme and discuss its practical implementation.
The last section reports numerical experiments on simulated and real data,
comparing our method to existing backward-scheme algorithms and to a
reference bootstrap particle filter. We conclude with a brief discussion of
limitations and possible extensions.

### **2 Context**


**2.1** **Notations**


Given two measurable spaces ( _X_ 1 _, A_ 1) and ( _X_ 2 _, A_ 2), we define an unnormalized kernel (resp. a Markov kernel) _K_ from _X_ 1 to _X_ 2 as a function _X_ 1 _×A_ 2 _→_

[0 _, ∞_ ] such that for any set _A ∈A_ 2 the function _x ∈X_ 1 _�→_ _K_ ( _x, A_ ) is measurable and for any _x ∈X_ 1 the function _A ∈A_ 2 _�→_ _K_ ( _x, A_ ) is a finite
measure (resp. a probability measure). Given some positive measurable
function _f_ : _X_ 1 _× X_ 2 _→_ [0 _, ∞_ ], we define the function _K_ ( _f_ ): _X_ 1 _→_ [0 _, ∞_ ] as
follows:

             _K_ ( _f_ )( _x_ 0) = _f_ ( _x_ 0 _, x_ ) _K_ ( _x_ 0 _, dx_ ) _._


3


Similarly, given some measure K( _dx_ ) and some positive measurable function
_f_ : _X →_ [0 _, ∞_ ] we define the quantity K( _f_ ) _∈_ [0 _, ∞_ ] as follows:


              K( _f_ ) = _f_ ( _x_ )K( _dx_ ) _._


The notation _t_ : _s_ denotes the sequence ( _t, t_ + 1 _, . . ., s_ ), _xt_ : _s_ denotes a
sequence ( _xt, . . ., xs_ ), _x_ _[n]_ [:] _[m]_ denotes a sequence ( _x_ _[n]_ _, . . ., x_ _[m]_ ) and _x_ _[n]_ _t_ : [:] _s_ _[m]_ denotes
the collection of all _x_ _[j]_ _i_ [with] _[ t][ ≤]_ _[i][ ≤]_ _[s]_ [ and] _[ n][ ≤]_ _[j][ ≤]_ _[m]_ [, For spaces,] _[ X][t]_ [:] _[s]_ [ denotes]
the product space _Xt_ _×. . . Xs_ . Given a sequence of kernels ( _Kt_ ) _t≥_ 2 from _Xt−_ 1
to _Xt_ and a measure K1 on _X_ 1, _Kt_ : _s_ denotes the joint kernel from _Xt−_ 1 to
_Xt_ : _s_ as follows:


_Kt_ : _s_ ( _xt−_ 1 _, dxt_ : _s_ ) = _Kt_ ( _xt−_ 1 _, dxt_ ) _. . . Ks_ ( _xs−_ 1 _, dxs_ )


and the notation K1: _t_ denote the joint measure on _X_ 1: _t_ as follows:


K1: _t_ ( _dx_ 1: _t_ ) = K1( _dx_ 1) _. . . Kt_ ( _xt−_ 1 _, dxt_ ) _._


Given a sequence of positive measurable functions ( _Gt_ ) _t≥_ 2 on _Xt−_ 1 _× Xt_ and
_G_ 1 on _X_ 1, _Gt_ : _s_ (resp. _G_ 1: _t_ ) denotes the product function on _Xt−_ 1: _s_ (resp
_X_ 1: _t_ ):


_Gt_ : _s_ ( _xt−_ 1: _s_ ) = _Gt_ ( _xt−_ 1 _, xt_ ) _. . . Gs_ ( _xs−_ 1 _, xs_ ) _,_


_G_ 1: _s_ ( _x_ 1: _t_ ) = _G_ 1( _x_ 1) _. . . Gt_ ( _xt−_ 1 _, xt_ ) _._


For two kernels _K_ and _U_ from _X_ 1 to _X_ 2 such that, for each _x_ 1 _∈X_ 1, the
measure _K_ ( _x_ 1 _, dx_ 2) is absolutely continuous with respect to _U_ ( _x_ 1 _, dx_ 2), we
denote by _[dK]_

_dU_ [the Radon-Nikodym derivative between these two measures:]



_dK_



_U_ ( _x_ 1 _, dx_ 2) [(] _[x]_ [2][)]



_dK_ _[K]_ [(] _[x]_ [1] _[, dx]_ [2][)]

_dU_ [(] _[x]_ [1] _[, x]_ [2][) =] _U_ ( _x_ 1 _, dx_ 2)



We denote by **1** any constant unit function. We denote by _N_ ( _m,_ Σ)
or _N_ ( _dx, m,_ Σ) the multivariate Gaussian distribution with mean _m_ and
covariance matrix Σ. We denote Categorical( _w_ 1 _, . . ., wn_ ) the categorical
distribution that gives probability _wi/_ [�] _j_ _[n]_ =1 _[w][j]_ [ to] _[ i][ ∈]_ [1:] _[n]_ [.]


**2.2** **State-space models**


Our main motivation comes from the problem of performing inference for
state-space models (SSMs), which are a class of models that link a sequence
of observations _y_ 1: _T_ to a latent variable _x_ 1: _T_ that takes values in _X_ and
typically follows a Markovian dependency structure.
The first component of an SSM is the emission density _π_ ( _yt | xt_ ) (with
respect to, typically, the Lebesgue measure) of an individual observation
given the latent value at time _t_ . The second component is a latent Markov


4


chain, with an initial distribution denoted by _π_ ( _dx_ 1), and Markov transition
kernels denoted by _π_ ( _dxt | xt−_ 1). In an SSM, the standard tasks are to infer
the filtering distributions:


_π_ ( _dxt | y_ 1: _t_ ) _,_ _t ∈_ 1: _T_


the smoothing distribution:


_π_ ( _dx_ 1: _T | y_ 1: _T_ )


and the marginal likelihood:
_π_ ( _y_ 1: _T_ ) _._


In general, closed-form expressions for these tasks are not available, and the
typical solution is to construct Monte Carlo estimates of these targets.
Obtaining the smoothing distribution is an inference problem on a latent
space _X_ _[T]_, which can be very high-dimensional, but there is a lower intrinsic
dimension, thanks to the sequential dependency structure of the latent space.
To exploit this sequential dependency, we define a sequence of growing target
probability measures _νt_ on the space _X_ _[t]_ as:


_νt_ ( _dx_ 1: _t_ ) = _π_ ( _dx_ 1: _t | y_ 1: _t_ ) _._


The smoothing distribution corresponds to the terminal target _νT_ and the
filtering distributions correspond to marginals of each _νt_ .
To infer our targets _νt_ sequentially (from _ν_ 1 to _νT_ ), Bayes’ rule gives the
following relations:


_π_ ( _dx_ 1 _| y_ 1) = _[π]_ [(] _[y]_ [1] _[ |][ x]_ [1][)] _[π]_ [(] _[dx]_ [1][)] (1)

_π_ ( _y_ 1)

_π_ ( _dx_ 1: _t_ +1 _| y_ 1: _t_ +1)



= _[π]_ [(] _[y][t]_ [+1] _[ |][ x][t]_ [+1][)] _[π]_ [(] _[dx]_ [1:] _[t]_ [+1] _[ |][ y]_ [1:] _[t]_ [)]

_π_ ( _yt_ +1 _| y_ 1: _t_ )



(2)



= _[π]_ [(] _[y][t]_ [+1] _[ |][ x][t]_ [+1][)] _[π]_ [(] _[dx][t]_ [+1] _[ |][ x][t]_ [)] _[π]_ [(] _[dx]_ [1:] _[t][ |][ y]_ [1:] _[t]_ [)]

_π_ ( _yt_ +1 _| y_ 1: _t_ )


which can be seen as a recursive relation between _νt_ +1 and _νt_ .
Let us assume that we know how to sample from the transition kernels
_π_ ( _dxt_ +1 _| xt_ ) (as well as form the initial distribution _π_ ( _dx_ 1)) and that we can
compute the emission densities _π_ ( _yt | xt_ ). In this setting, equations (1) and
(2) describe the sequential construction of _ν_ 1: _T_ and define a Feynman–Kac
model. We will detail this notion in the following sections and explain how it
can be used to construct importance sampling approximations of the targets
_ν_ 1: _T_ within a sequential Monte Carlo algorithm.


5


**2.3** **The Feynman-Kac formalism**


We are interested in approximating the targets _ν_ 1: _T_ sequentially. To this
end, we begin by introducing Feynman-Kac models (Del Moral, 2004), which
provide a useful framework for describing sequentially constructed measures
and sequential Monte Carlo algorithms.
A Feynman-Kac model (M1: _T, G_ 1: _T_ ) up to time _T_ is composed of a
Markov chain M1: _T_ from which we can sample, and non-negative weight
functions _Gt_ ( _xt−_ 1 _, xt_ ) for _t ∈_ 1: _T_ .
We define the target path measures _ν_ 1: _T_ and the normalizing constants
_Zt_ associated with a Feynman-Kac model as:

_νt_ ( _dx_ 1: _t_ ) = [1] _G_ 1: _t_ ( _x_ 1: _t_ )M1: _t_ ( _dx_ 1: _t_ ) _,_

_Zt_

             _Zt_ = _G_ 1: _t_ ( _x_ 1: _t_ )M1: _t_ ( _dx_ 1: _t_ ) _._


These measures satisfy a relation similar to (2):


_Zt_
_νt_ +1 = _Gt_ +1 _νtMt_ +1 _._ (3)
_Zt_ +1


For an SSM, this leads to a canonical Feynman-Kac model (M1: _t, G_ 1: _t_ ) defined as:


M1( _dx_ 1) = _π_ ( _dx_ 1)



_Mt_ ( _xt−_ 1 _, dxt_ ) = _π_ ( _dxt | xt−_ 1)

_Gt_ ( _xt−_ 1 _, xt_ ) = _π_ ( _yt | xt_ ) _._



(4)



With this definition, the target path measures coincide with the targets
defined in the previous section, that is, _νt_ ( _dx_ 1: _t_ ) = _π_ ( _dx_ 1: _t | y_ 1: _t_ ) and the
normalizing constants correspond to the marginal likelihoods _Zt_ = _π_ ( _y_ 1: _t_ ).
For a given Feynman-Kac model, we define the cost-to-go function _Ht→s_
(from _t_ to _s_ ) as:
_Ht→s_ ( _xt_ ) = _Mt_ +1: _s_ ( _Gt_ +1: _s_ )( _xt_ ) (5)


which is, up to a multiplicative constant, the Radon-Nikodym derivative
between the target path measure _νs_ and the marginal of _νt_ over _x_ 1: _t_ . By
convention, we set _Hs→s ≡_ 1 and _H_ 0 _→t ≡_ _Zt_ .
For the canonical Feynman-Kac model associated with an SSM, a costto-go function is the likelihood of future observations given the current latent
state: _Ht→s_ ( _xt_ ) = _π_ ( _yt_ +1: _s | xt_ ).


**2.4** **Sequential Monte Carlo algorithms**


A sequential Monte Carlo (SMC) algorithm is a method for sampling from a
sequence of target measures _ν_ 1 _, . . ., νT_, known up to normalizing constants
_Z_ 1 _, . . ., ZT_ .


6


Given a Feynman-Kac model (M1: _T, G_ 1: _T_ ), since each target _νt_ is available in closed form from the previous target _νt−_ 1 (see (3)), we can construct
Monte Carlo approximations sequentially, from _ν_ 1 to _νT_ .
A standard sequential Monte Carlo algorithm (see Algorithm 1) follows
the evolution of _N_ particles _x_ [1:] _t_ _[N]_ with associated weights _wt_ [1:] _[N]_ over discrete
time _t_, starting at 1 and ending at _T_ . These particles are sampled and
weighted according to the Feynman-Kac model to construct an importance
sampling approximation of the targets _νt_ ( _dx_ 1: _t_ ). We refer to M1: _T_ as the
proposal Markov chain and _G_ 1: _T_ the weight functions.
If we were to reduce the algorithm to a sequence of importance sampling
steps (i.e., without resampling), we would observe particle degeneracy, that
is, a few particles would receive most of the weight, resulting in a poor
approximation. One of the key features of an SMC algorithm that mitigates
particle degeneracy is the resampling step: particles with low weights are
discarded, while particles with large weights are selected to produce offspring.
Formally, this can be described as selecting ancestors indices _a_ [1:] _t_ _[N]_ _∈_ N for
the next generation of _N_ particles at _t_ + 1, according to the weights _wt_ [1:] _[N]_ .
To simplify notation, when the context creates no ambiguity, we denote
by _ξ_ 1: _[n]_ _t_ [the trajectory of a particle, taking into account its ancestors:]

_ξ_ 1: _[n]_ _t_ [= (] _[ξ]_ 1: _a_ _[n]_ _tt−−_ 11 _[, x]_ _t_ _[n]_ [)] _[.]_


From the output ( _x_ [1:] 1: _[N]_ _T_ [,] _[ a]_ 1: [1:] _T_ _[N]_ [,] _[ w]_ 1: [1:] _T_ _[N]_ [) of an SMC algorithm, we construct]
the following estimators of _νt_ ( _φ_ ), for a R-valued function _φ_, and of _Zt_ :



1
_ν_ ˆ _t_ _[N]_ [(] _[φ]_ [) =] ~~�~~ _N_
_n_ =1 _[w]_ _t_ _[n]_



_N_


_wt_ _[n][φ]_ [(] _[ξ]_ 1: _[n]_ _t_ [)] (6)
_n_ =1



_N_


_wt_ _[n][.]_ (7)
_n_ =1



_N_

- 1

_ws_ _[n]_ [= ˆ] _[Z]_ _t_ _[N]_ _−_ 1
_N_
_n_ =1



_Z_ ˆ _t_ _[N]_ =



_t_



_s_ =1



1

_N_



When the targets _νt_ correspond to the previously introduced inference
targets _π_ ( _dx_ 1: _t | y_ 1: _t_ ) of an SSM, the resulting SMC algorithm is called a
particle filter. When the Feynman-Kac model is the canonical Feynman-Kac
model of an SSM defined by (4), an SMC algorithm is called a bootstrap
particle filter. The bootstrap particle filter is the simplest particle filter to
implement, and it will serve as a baseline reference algorithm throughout
this article.


**2.5** **Objectives**


The bootstrap particle filter is simple but often leads to inefficient algorithms
due to particle degeneracy, arising from the use of _π_ ( _dxt | xt−_ 1) as the proposal kernels _Mt_ ( _xt−_ 1 _, dxt_ ). The goal of our work is to automatically refine


7


**Algorithm 1:** Standard Sequential Monte Carlo
**Input:** A Feynman-Kac model (M1: _T, G_ 1: _T_ ), a number of particles _N_ .
**Output:** Particles _x_ [1:] 1: _[N]_ _T_ [, weights] _[ w]_ 1: [1:] _T_ _[N]_ [, ancestors index] _[ a]_ 1: [1:] _T_ _[N]_ [.]


**At time** _t_ = 1 **:**

**Sample** _x_ _[n]_ 1 _[∼]_ [M][1][(] _[dx]_ [1][)][ for] _[ n][ ∈]_ [1:] _[N]_ [.]
**Compute** weights _w_ 1 _[n]_ [=] _[ G]_ [1][(] _[x]_ 1 _[n]_ [)][ for] _[ n][ ∈]_ [1:] _[N]_ [.]
**Resample** to get ancestors _a_ _[n]_ 1 _[∼]_ [Categorical(] _[w]_ 1 [1:] _[N]_ ) for _n ∈_ 1: _N_ .

**At time** _t ∈_ 2: _T_ **:**

**Sample** _x_ _[n]_ _t_ _[∼]_ _[M][t]_ [(] _[x]_ _at−_ _[n]_ _t−_ 11 _[, dx][t]_ [)][ for] _[ n][ ∈]_ [1:] _[N]_ [.]

**Compute** weights _wt_ _[n]_ [=] _[ G][t]_ [(] _[x]_ _at−_ _[n]_ _t−_ 11 _[, x]_ _t_ _[n]_ [)][ for] _[ n][ ∈]_ [1:] _[N]_ [.]
**Resample** to get ancestors _a_ _[n]_ _t_ _[∼]_ [Categorical(] _[w]_ _t_ [1:] _[N]_ ) for _n ∈_ 1: _N_ .


the bootstrap particle filter by proposing a method to iteratively construct
better proposals, that is, better Feynman-Kac models.
More precisely, given a reference Feynman-Kac model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, the]
bootstrap filter in our case, we are interested in accurately estimating the
full target _νT_ [ref] and its normalizing constant _ZT_ [ref] [. Within the constraint of]
preserving these two, we will try to find the best Feynman-Kac model.

### **3 Method**


**3.1** **Auxiliary Feynman-Kac models with preserved final tar-**
**get**


Our goal is to develop a method for constructing optimal Feynman-Kac
models that sample from the full target distribution. We introduce auxiliary
Feynman-Kac models, inspired by the auxiliary particle filter (Pitt and Shephard 1999, see also Chap. 10 of Chopin and Papaspiliopoulos 2020). They are
defined by a proposal chain M1: _T_ and auxiliary weight functions _η_ 1: _T_, with
_η_ 0 _≡_ _ηT ≡_ 1, such that the auxiliary Feynman-Kac model (M1: _T, η_ 1: _T_ ) given
a reference model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, defines a Feynman-Kac model][ (][M][1:] _[T][, G]_ [1:] _[T]_ [ )]
that satisfies:


1 [(] _[dx]_ [1][)]
_G_ 1( _x_ 1) = _G_ [ref] 1 [(] _[x]_ [1][)] [M] M [ref] 1( _dx_ 1) _[η]_ [1][(] _[x]_ [1][)]



_Gt_ ( _xt−_ 1 _, xt_ ) = _G_ [ref] _t_ [(] _[x][t][−]_ [1] _[, x][t]_ [)]

_×_ _[M]_ _t_ [ref] ( _xt−_ 1 _, dxt_ ) _ηt_ ( _xt_ )
_Mt_ ( _xt−_ 1 _, dxt_ ) _ηt−_ 1( _xt−_ 1) _[.]_


In this context, at time _t_, the target path measure _νt_ is given by:



(8)



_G_ 1: _t_ _ηt_
_νt_ = ~~�~~ _G_ 1: _td_ M1: _t_ M1: _t_ = ~~�~~ _ηtdνt_ [ref] _νt_ [ref] (9)


8


the normalizing constant _Zt_ is:


          _Zt_ = _G_ 1: _td_ M1: _t_ = _Zt_ [ref]


and the cost-to-go function _Ht→T_ is:




_ηtdνt_ [ref]



_Ht→T_ = _Mt_ +1: _T_ ( _Gt_ +1: _T_ ) = _[H]_ _t_ [ref] _→T_ _._
_ηt_


Since we require that _ηT ≡_ 1 we have _νT_ = _νT_ [ref] and _ZT_ = _ZT_ [ref] [. The]
above relations describe a family of Feynman-Kac models with the same full
target and final normalizing constant. In fact, we describe all the FeynmanKac models satisfying these two properties. If we set M1: _T ≡_ _M_ 1: [ref] _T_ [and]
_η_ 1: _T_ _−_ 1 _≡_ 1, we recover exactly the reference. In practice, as shown by (9),
the auxiliary weights allow us to use a different sequence of measures that
still lead to _νT_ .
In the context of an SSM, it is crucial to emphasize that an SMC algorithm derived from an auxiliary Feynman-Kac model will not target the
usual particle filtering distributions, since the intermediate targets are modified, although it remains possible to produce filtering estimates. Therefore,
we do not expect to obtain optimal algorithms for the filtering task. Instead,
our objective is to achieve optimality for the marginal likelihood, that is, for
the estimator of _ZT_, and, since we preserve the final target, this approach
should also be efficient for the smoothing task.


**3.2** **Variance of the marginal likelihood estimator**


We denote by _Z_ [ˆ] _T_ _[N]_ [the Monte Carlo estimator of the normalizing constant] _[ Z][T]_
obtained from the output of an auxiliary Feynman-Kac model and defined
in (7).
This estimator is unbiased, E[ _Z_ [ˆ] _T_ _[N]_ [] =] _[ Z][T]_ [, and its variance satisfies (Doucet]
and Johansen, 2011):



V[ _Z_ [ˆ] _T_ _[N]_ []]
= [1]
_ZT_ [2] _N_



_T_



_t_ =1



��
_GtHt→T dνT_




- 1 - - 1
_dνT −_ 1 + _o_
_GtHt→T_ _N_




_._ (10)



The first integral term in (10) can be expressed, for an auxiliary FeynmanKac model, as:


_t_ [(] _[x][t][−]_ [1] _[, x][t]_ [)] _[H]_ _t_ [ref] _→T_ [(] _[x][t]_ [)] _Mt_ [ref] ( _xt−_ 1 _, dxt_ )
_Gt_ ( _xt−_ 1 _, xt_ ) _Ht→T_ ( _xt_ ) = _[G]_ [ref] (11)
_ηt−_ 1( _xt−_ 1) _Mt_ ( _xt−_ 1 _, dxt_ ) _[.]_


The variance of this estimator is of particular interest for various reasons. First, it determines the performance of the popular PMMH algorithm
(Andrieu et al., 2010), which may be used to sample from the posterior distribution of the parameters of the considered state-space model. Second, a


9


low variance also implies reduced weight degeneracy and suggests that the
full proposal distribution is close to the full target distribution. For this
reason, we use this variance to define optimality in the next section.
The summand in (10) is a (reverse) _χ_ [2] divergence between the target _νT_
and the proposal ( 1
_GtHt→T_ _[ν][T]_ [ up to a constant). More precisely, given two]
probability measures _ν_ and _µ_ on _X_, and a non-negative function _f_ on _X_, we
define a chi-squared divergence between _ν_ and _µ_, as follows:




      - _dν_
_Dχ_ [2] ( _ν | µ_ ) =



_dν_ - _dµ_

_dµ_ _[dµ]_ _dν_



_dν_ _[dµ][ −]_ [1]



and to simplify notation, we define a chi-squared divergence of _f_ relative to
_µ_ as the chi-squared divergence between _µ_ and _ν ∝_ _fµ_, as follows:


             - 1              _Dχ_ [2] _µ_ [(] _[f]_ [) =] _fdµ −_ 1 _._

_f_ _[dµ]_


Then, using (11), (10) may be rewritten as:



V[ _Z_ [ˆ] _T_ _[N]_ []]
= [1]
_ZT_ [2] _N_



_T_

- _Dχ_ [2] _νT_

_t_ =1




- _G_ ref _t_ _[H]_ _t_ [ref] _→T_
_ηt−_ 1



_dMt_ [ref]
_dMt_




- - 1
+ _O_

_N_ [2]




_._ (12)



See Appendix A for further details on how to derive (12) from Doucet and
Johansen (2011).
In our context, we aim to minimize the leading term in (12) with respect
to _ηt−_ 1 _Mt_, while keeping _G_ [ref] 1: _T_ [and] _[ M]_ [ ref] 1: _T_ [fixed.]
Therefore, to simplify the expressions, we introduce the following quantities:
U1 = _η_ 0M1 _Ut_ = _ηt−_ 1 _Mt_


that is, U1 is a finite measure and the _Ut_ ’s are unnormalized kernels. We
can recover both _ηt−_ 1 and _Mt_ from _Ut_ via:


              _ηt−_ 1( _xt−_ 1) = _Ut_ ( _xt−_ 1 _, dxt_ ) _,_


1
_Mt_ ( _xt−_ 1 _, dxt_ ) =
_ηt−_ 1( _xt−_ 1) _[U][t]_ [(] _[x][t][−]_ [1] _[, dx][t]_ [)] _[.]_


The auxiliary Feynman-Kac model notations allow, via (12), to precisely
characterize which quantity, which discrepancy, and which measure should
drive our learning algorithms, given that we want to minimize this variance.
The main consequence of this formulation is that optimality can be understood from (12) on the unconstrained set of unnormalized measures _U_ 1: _T_
more directly than from (10), which is defined on the set of Feynman-Kac
models constrained by their full target distribution.


10


**3.3** **Optimality**


**3.3.1** **Global optimality**


We define optimality in our context as finding _Ut_ (or equivalently _Mt_ and _ηt_ )
that minimizes the leading term of (10). The minimizer _Ut_ _[⋆]_ [follows directly]
from (12) and can be written as:


U _[⋆]_
1 [=] _[ G]_ 1 [ref] _[H]_ 1 [ref] _→T_ _[M]_ 1 [ref] _[,]_

(13)
_Ut_ _[⋆]_ [=] _[ G]_ _t_ [ref] _[H]_ _t_ [ref] _→T_ _[M]_ _t_ [ref] _,_ _t ≥_ 2 _._


By integration, the normalizing constant is, by definition of the cost-to-go
function (5):
_ηt_ _[⋆]_ _−_ 1 [=] _[ M]_ _t_ [ref] ( _G_ [ref] _t_ _[H]_ _t_ [ref] _→T_ [) =] _[ H]_ _t_ [ref] _−_ 1 _→T_ (14)


This leads to the following optimal proposal kernels:

M _[⋆]_ 1 [=] _[G]_ 1 [ref] _[H]_ 1 [ref] _→T_ M [ref] 1 _[,]_
_ZT_

_t_ _[H]_ _t_ [ref] _→T_
_Mt_ _[⋆]_ [=] _[G]_ [ref] _Mt_ [ref] _,_ _t ≥_ 2 _._
_Ht_ [ref] _−_ 1 _→T_


If the reference is the bootstrap particle filter, this yields:


_M_ 1 _[⋆]_ [(] _[dx]_ [1][) =] _[ π]_ [(] _[dx]_ [1] _[|][ y]_ [1:] _[T]_ [)]

_Mt_ _[⋆]_ [(] _[x][t][−]_ [1] _[, dx][t]_ [) =] _[ π]_ [(] _[dx][t]_ _[|][ x][t][−]_ [1] _[, y][t]_ [:] _[T]_ [)]

_ηt_ _[⋆]_ _−_ 1 [(] _[x][t][−]_ [1][) =] _[ π]_ [(] _[y][t]_ [:] _[T]_ _[|][ x][t][−]_ [1][)] _[.]_


This corresponds to sampling directly from the full target distribution, i.e.,
the smoother in this context (Guarniero et al., 2017; Heng et al., 2020):


_νt_ _[⋆]_ [(] _[dx]_ [1:] _[t]_ [) =] _[ π]_ [(] _[dx]_ [1:] _[t]_ _[|][ y]_ [1:] _[T]_ [)] _[.]_


Recast in our formalism, the approaches of Guarniero et al. (2017) and
Heng et al. (2020) approximate the optimal sequence U1: _T_ by means of the
following explicit backward recursion, for _t_ = _T −_ 1 _, T −_ 2 _, . . .,_ 1:


_Ut_ _[⋆]_ [=] _[ G]_ _t_ [ref] _[U]_ _t_ _[⋆]_ +1 [(] **[1]** [)] _[M]_ _t_ [ref] (15)


which follows from (13) and (14). This implies the following relations for
_ηt_ _[⋆]_ _−_ 1 [and] _[ M][ ⋆]_ _t_ [:]


M _[⋆]_ _G_ [ref] 1 _[η]_ 1 _[⋆]_ M [ref]
1 [=] ~~�~~ _G_ [ref] 1 _[η]_ 1 _[⋆][d]_ [M][ref] 1 1

_G_ [ref] _t_ _[η]_ _t_ _[⋆]_
_Mt_ _[⋆]_ [=] _t_
_Mt_ [ref] ( _G_ [ref] _t_ _[η]_ _t_ _[⋆]_ [)] _[M]_ [ref]

_ηt_ _[⋆]_ _−_ 1 [=] _[ M]_ _t_ [ref] ( _G_ [ref] _t_ _[η]_ _t_ _[⋆]_ [)] _[.]_


11


If the reference is the bootstrap particle filter, we recover the backward
smoothing recursion (Doucet and Johansen, 2011):


_π_ ( _xt | xt−_ 1 _, yt_ : _T_ ) = _[π]_ [(] _[y][t][ |][ x][t]_ [)] _[π]_ [(] _[y][t]_ [+1:] _[T][ |][ x][t]_ [)] _[π]_ [(] _[x][t][ |][ x][t][−]_ [1][)] _._

_π_ ( _yt_ : _T | xt−_ 1)


**3.3.2** **Tractability issues and local optimality**


Global optimality introduces some computational difficulties. The main difficulty is that this optimum is typically intractable, since the optimal target
at time _t_, _νt_ _[⋆]_ [, is precisely the marginal of the full target] _[ ν]_ _T_ _[⋆]_ [=] _[ ν][T]_ [ . This]
can be addressed by building approximations iteratively, alternating forward
and backward passes over the data, as in Guarniero et al. (2017) and Heng
et al. (2020). However, as we shall observe in our experiments, this approach
requires a sufficiently good initialization (i.e., the first forward pass), which
is non-trivial in cases where the reference Feynman-Kac model leads to a
very inefficient particle filter (e.g., a bootstrap filter for a state-space model
with highly informative data).
Instead, our method relies on the concept of local optimality, which can
be intuitively understood as optimality with respect to a finite horizon of _L_
steps ahead ( _Ht_ [ref] _→t_ + _L_ [in (13)), rather than with respect to the full remaining]
sequence _Ht_ [ref] _→T_ [.]
This allows us to follow a forward scheme, which can be used either as
an initial proposal for backward-scheme algorithms or to construct an online
algorithm — something not achievable with a purely backward scheme.
More precisely, local optimality is defined in the same sense as the local
optimality for a guided particle filter, described in Doucet et al. (2000). The
main idea underlying this notion of optimality is to ignore knowledge of the
full target and only include weight functions up to time _t_ . Namely, we aim
to optimize the variance of the estimator _Z_ [ˆ] _t_ _[N]_ of _Zt_ instead of _Z_ [ˆ] _T_ _[N]_ [of] _[ Z][T]_ [ .]
We look for an optimum with the target path _ν_ 1: _T_ fixed (i.e., at _η_ 1: _T_
fixed). By reducing this problem to minimizing the variance of the importance weights _wt_ _[n]_ [, it is well known (Doucet et al., 2000) that this leads to]
the following optimum:

M [loc] 1 = ~~�~~ _G_ [ref] 1 _G_ [ref] 1 _[η]_ [1] _[η][d]_ [1][M] 1 [ref] M [ref] 1

(16)
_G_ [ref] _t_ _[η][t]_
_Mt_ [loc] = _t_ _._
_Mt_ [ref] ( _G_ [ref] _t_ _[η][t]_ [)] _[M]_ [ref]


Local optimality provides insight into how to construct the best proposals
for a fixed target path _ν_ 1: _T_ . In the next section, we develop a scheme that
connects this notion of local optimality to a global optimal path.


12


**3.4** **Iterated forward scheme**


The optimal auxiliary weight functions _ηt_ _[⋆]_ [=] _[ H]_ _t_ [ref] _→T_ [correspond to the cost-]
to-go from time _t_ to final time _T_ . Therefore, if we fix a number _L_ of future
time points to take into account when constructing our proposals, we can
define auxiliary functions _η_ 0: [(] _[L]_ _T_ [)] [as the cost-to-go from time] _[ t]_ [ to time] _[ t]_ [ +] _[ L]_ [:]



_ηt_ [(] _[L]_ [)] =




_Ht_ [ref] _→t_ + _L_ if _t_ + _L ≤_ _T_
_Ht_ [ref] _→T_ [=] _[ η]_ _t_ _[⋆]_ else.



These auxiliary functions converge to _ηt_ _[⋆]_ [as] _[ L]_ [ increases.]
When the bootstrap particle filter is used as a reference, _ηt_ [(] _[L]_ [)] satisfies


_ηt_ [(] _[L]_ [)] ( _xt_ )

       = _G_ [ref] _t_ +1: _t_ + _L_ [(] _[x][t]_ [:] _[t]_ [+] _[L]_ [)] _[M]_ _t_ [ref] +1: _t_ + _L_ [(] _[x][t][, dx][t]_ [+1:] _[t]_ [+] _[L]_ [)]


_t_ + _L_

       
    = _π_ ( _yt_ + _s | xt_ + _s_ ) _π_ ( _dxt_ + _s | xt_ + _s−_ 1)


_s_ = _t_ +1

= _π_ ( _yt_ +1: _t_ + _L | xt_ )


for _t_ + _L ≤_ _T_, so that _ηt_ [(] _[L]_ [)] weights _xt_ according to the information contained
in _L_ future observations.
This establishes a connection with the fixed-lag SMC literature (Doucet
and Sénécal, 2004), which focuses on sampling from _π_ ( _dx_ 1: _t | y_ 1: _t_ + _L_ ), which
is precisely the target _νt_ [(] _[L]_ [)] associated with the auxiliary weight _ηt_ [(] _[L]_ [)] . Rapid
convergence of this target toward the smoothing distribution was shown by
Olsson et al. (2008), and by extension, the target _νt_ [(] _[L]_ [)] should rapidly approximate the target induced by the backward scheme.
There are two main differences between our approach and fixed-lag SMC.
First, instead of sampling blocks _xt_ : _t_ + _L_ to estimate _νt_ [(] _[L]_ [)], we sample from
an approximation of its _xt_ -marginal. Second, we build our approximation
iteratively over _L_ . As a result, our algorithm uses an increasing value of _L_,
compared to the standard block sampling method, which uses a fixed _L_ .
These auxiliary weights may be computed recursively, using a backward
scheme:
_ηt_ [(] _[L]_ [+1)] = _Mt_ [ref] +1 [(] _[G]_ [ref] _t_ +1 _[η]_ _t_ [(] +1 _[L]_ [)] [)]

starting from _ηt_ [(0)] _≡_ 1, corresponding to the reference. We recall that _ηT ≡_ 1
is required to preserve the last target. One crucial difference compared to
the backward scheme is that we only need to integrate up to _L_ times to
compute the auxiliary weight as opposed to _T_ times under global optimality.


13


By applying local optimality (16) to fixed auxiliary weights _ηt_ [(] _[L]_ [)], we
obtain an auxiliary model (M [(] 1: _[L]_ _T_ [+1)] _, η_ 0: [(] _[L]_ _T_ [)][)][ defined by:]

M [(] 1 _[L]_ [+1)] = _G_ [ref] 1 _[η]_ 1 [(] _[L]_ [)] M [ref] 1

~~�~~ _G_ [ref] 1 _[η]_ 1 [(] _[L]_ [)] _d_ M [ref] 1

_G_ [ref] _t_ _[η]_ _t_ [(] _[L]_ [)]
_Mt_ [(] _[L]_ [+1)] = _Mt_ [ref] _._
_Mt_ [ref] ( _G_ [ref] _t_ _[η]_ _t_ [(] _[L]_ [)] )


This leads to the following relation, similar to (15):


_Ut_ [(] _[L]_ [+1)] = _G_ [ref] _t_ _[U]_ _t_ [(] +1 _[L]_ [)][(] **[1]** [)] _[M]_ _t_ [ref] _._ (17)


Although this equation remains a backward recursion with respect to
time _t_, it can also be interpreted as a forward recursion with respect to
_L_ . We use this latter interpretation to derive our practical, forward-only
algorithm, as explained in the next section.

### **4 Implementation**


**4.1** **Parametrization using twisted Feynman-Kac models**


We use the same family of approximating proposal kernels as in Guarniero
et al. (2017) and Heng et al. (2020), that is, we parametrize the unnormalized
kernel in terms of a twisting function _φt_ :


_Ut_ ( _xt−_ 1 _, dxt_ ) = _φt_ ( _xt−_ 1 _, xt_ ) _Mt_ [ref] ( _xt−_ 1 _, dxt_ ) _._


This choice is motivated by the fact that the optimal kernel satisfies


_Ut_ _[⋆]_ [(] _[x][t][−]_ [1] _[, dx][t]_ [) =] _[ G]_ _t_ [ref] [(] _[x][t][−]_ [1] _[, x][t]_ [)] _[η]_ _t_ _[⋆]_ [(] _[x][t]_ [)] _[M]_ _t_ [ref] ( _xt−_ 1 _, dxt_ )


according to (13). Hence, the problem reduces to learning a strictly positive
function _φt_ of the form:


_Ut_ ( _xt−_ 1 _, dxt_ )
_φt_ ( _xt−_ 1 _, xt_ ) =
_Mt_ [ref] ( _xt−_ 1 _, dxt_ )

_≈_ _G_ [ref] _t_ [(] _[x][t][−]_ [1] _[, x][t]_ [)] _[η]_ _t_ _[⋆]_ [(] _[x][t]_ [)] _[.]_


When the reference is derived from the bootstrap particle filter, _G_ [ref] _t_ depends
only on _xt_, and is constant with respect to _xt−_ 1. In that case, we restrict _φt_
to be constant in _xt−_ 1.
This parametrization is especially useful when the transition kernels admit conjugacy properties, e.g., when _Mt_ [ref] ( _xt−_ 1 _, dxt_ ) is Gaussian (conditional


14


on _xt−_ 1). In that case, if _φt_ is chosen in the space of log-quadratic functions,
that is, provided


M1( _dx_ 1) = _N_ ( _m_ 1 _,_ Σ1)

_Mt_ ( _xt−_ 1 _, dxt_ ) = _N_ ( _mt_ ( _xt−_ 1) _,_ Σ _t_ ( _xt−_ 1))



and




    _φt_ ( _xt_ ) = exp _−_ [1]




[1] _t_ _[A][t][x][t]_ _[−]_ _[x][′]_ _t_ _[b][t]_ _[−]_ [1]

2 _[x][′]_ 2



2 _[c][t]_




(18)



under the condition Σ _t_ ( _xt−_ 1) + _At ≫_ 0 for all _t_, all quantities of interest are
available in closed form:

1 _φt_
_Mt_ = _Ut_ ( **1** ) _[U][t]_ [ =] _Mt_ [ref] ( _φt_ ) _[M]_ _t_ [ref]

_ηt_ = _Ut_ ( **1** ) = _Mt_ [ref] ( _φt_ ) _._


Function _φt_ with nice conjugacy properties seems to be typical of Gaussian applications: we are not aware of non-Gaussian examples that admit
such functions. The methods presented below use this parametrization, but
could be adapted to optimize over a class of unnormalized kernels _Ut_ instead
of _φt_ when necessary.


**4.2** **Recursively computing the twisting functions**


The main idea in approximating the schemes is to substitute approximations ˆ _φt_ (resp. _φ_ ˆ [(] _t_ _[L]_ [)] ) of the optimal twisting functions _φ_ _[⋆]_ _t_ [=] _[ dU]_ _t_ _[⋆][/dM]_ _t_ [ref]
(resp. _φ_ [(] _t_ _[L]_ [)] = _dUt_ [(] _[L]_ [)] _/dMt_ [ref] ) into the recursions (15) (resp. (17)). Several
methods exist to construct these approximations. In this section, we detail
an adaptation to the forward scheme (17) of the approach proposed by Heng
et al. (2020) for the backward scheme (15). See Appendix B for more details
on the backward implementation of Heng et al. (2020).
Expressing the forward scheme (17) in terms of the twist functions _φ_ [(] _t_ _[L]_ [)] =
_dUt_ [(] _[L]_ [)] _/dMt_ [ref], we obtain:

_φ_ [(] _t_ _[L]_ [+1)] _Mt_ [ref] = _G_ [ref] _t_ _[M]_ _t_ [ref] +1 [(] _[φ]_ [(] _t_ +1 _[L]_ [)] [)] _[M]_ _t_ [ref] _._


This gives the following recursion:

_φ_ [(] _t_ _[L]_ [+1)] = _G_ [ref] _t_ _[M]_ _t_ [ref] +1 [(] _[φ]_ [(] _t_ +1 _[L]_ [)] [)] _[.]_ (19)


This equation expresses the scheme entirely in terms of the twist functions
_φt_ . Following (19), we iteratively construct ˆ _φ_ [(] 1: _[L]_ _T_ [+1)], an approximation of
_φ_ [(] 1: _[L]_ _T_ [+1)], using the previous approximation ˆ _φ_ [(] 1: _[L]_ _T_ [)] [as input:]

_φ_ ˆ [(] _t_ _[L]_ [+1)] _≈_ _G_ [ref] _t_ _[M]_ _t_ [ref] +1 [( ˆ] _[φ]_ [(] _t_ +1 _[L]_ [)] [)]

_. . ._

_φ_ ˆ [(] _T_ _[L]_ [+1)] _≈_ _G_ [ref] _T_


15


**Algorithm 2:** Forward SMC Training

**Input:** A reference Feynman-Kac model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, a number of]
particles _N_, a number of iterations _L_ [max] and sets of strictly
positive functions Ψ1: _T_ .
**Output:** Approximations ˆ _φ_ [(] 1: _[L]_ _t_ [)] [for][ 1] _[ ≤]_ _[L][ ≤]_ _[L]_ [max][ of the forward]
scheme.

**Set** ˆ _φ_ [(0)] 1: _T_ _[≡]_ [1][,][ ˆ][M] 1 [(0)] _∼_ M [ref] 1 [,][ ˆ] _M_ 2: [(0)] _T_ _[≡]_ _[M]_ 2: [ref] _T_ [and][ ˆ] _[η]_ 1: [(0)] _T_ _[≡]_ [1][.]
**For** _L ∈_ 0: _L_ [max] _−_ 1 **:**

**For** _t ∈_ 1: _T_ **:**

**Approximate** _G_ [ref] _t_ _[η]_ [ˆ] _t_ [(] _[L]_ [)] by ˆ _φ_ [(] _t_ _[L]_ [+1)] _∈_ Ψ _t_ .

**Set** ˆ _ηt_ [(] _−_ _[L]_ [+1)] 1 = _Mt_ [ref] ( ˆ _φ_ [(] _t_ _[L]_ [+1)] ).


_t_
**Set** _M_ [ˆ] _t_ [(] _[L]_ [+1)] = _[φ]_ [ˆ][(] _[L]_ [+1)] _Mt_ [ref] .
_η_ ˆ _t_ [(] _−_ _[L]_ [+1)] 1
```
  // Sample particles according to trained proposal
```

**Sample** and **Compute** _x_ [(] _t_ _[L]_ [+1)] _[,n]_, _wt_ [(] _[L]_ [+1)] _[,n]_ and _a_ [(] _t_ _[L]_ [+1)] _[,n]_ according
to the _t_ -th SMC step (Algorithm 1) with _Mt_ = _M_ [ˆ] _t_ [(] _[L]_ [+1)],
_ηt−_ 1: _t_ = _ηt_ [(] _−_ _[L]_ [)] 1: _t_ [and ancestor] **[ ˜]** _**[x]**_ _t_ [(] _−_ _[L]_ [+1)] 1 _[,n]_ .

**Denote ˜** _**x**_ [(] _t_ _[L]_ [+1)] _[,n]_ = _xt_ [(] _[L]_ [+1)] _[,a]_ _t_ [(] _[L]_ [+1)] _[,n]_ .


The learned auxiliary Feynman-Kac model ( _M_ [ˆ] 1: [(] _[L]_ _T_ [+1)] _,_ ˆ _η_ 1: [(] _[L]_ _T_ [+1)] ) follows:

_M_ ˆ _t_ [(] _[L]_ [+1)] = _φ_ ˆ [(] _t_ _[L]_ [+1)] _Mt_ [ref] _,_
_Mt_ [ref] ( ˆ _φ_ [(] _t_ _[L]_ [+1)] )

_η_ ˆ _t_ [(] _−_ _[L]_ [+1)] 1 = _Mt_ [ref] ( ˆ _φ_ [(] _t_ _[L]_ [+1)] ) _._


The simplified Algorithm 2 summarizes how functions ˆ _φ_ [(] _t_ _[L]_ [)] are computed
recursively. Since the scheme is forward, the target at each step is available
directly from previously computed quantities. In particular, ˆ _φ_ [(] _t_ _[L]_ [)] does not
depend on any future approximation ˆ _φ_ [(] _t_ + _[L]_ [)] _h_ [, such that the algorithm is a suc-]
cession of forward passes over the data. Furthermore, with fixed depth _L_ of
the training, we can implement an online version of this algorithm that does
exactly the same computations, the construction and some implementation
details can be found in Appendix C, including the online version Algorithm 5
of Algorithm 2. In practice, the approximations depend on a certain learning
procedure, _A_ [fwd], which we describe in the next section.


**4.3** **Learning the twisting functions from particle samples**


We use a learning procedure _A_ [fwd], in Algorithm 3, similar to Controlled
SMC (see Appendix B), but the sampling part is more involved. Particles


16


**Algorithm 3:** Forward Iterated SMC

**Input:** A reference Feynman-Kac model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, a number of]
particles _N_, a number of iterations _L_ [max] .
**Output:** Approximations ˆ _φ_ [(] 1: _[L]_ _t_ [)] [for][ 1] _[ ≤]_ _[L][ ≤]_ _[L]_ [max][ of the forward]
scheme.

**Set** ˆ _φ_ [(0)] 1: _T_ _[≡]_ [1][,][ ˆ][M] 1 [(0)] _∼_ M [ref] 1 [,][ ˆ] _M_ 2: [(0)] _T_ _[≡]_ _[M]_ 2: [ref] _T_ [and][ ˆ] _[η]_ 1: [(0)] _T_ _[≡]_ [1][.]
**For** _L ∈_ 0: _L_ [max] _−_ 1 **:**

**For** _t ∈_ 1: _T_ **:**

```
  // Sample training particles according to previous
    iterations
```

**Sample** ¯ _x_ [(] _t_ _[L]_ [)] _[,n]_ and **Compute** ¯ _wt_ [(] _[L]_ [)] _[,n]_ according to the _t_ -th SMC
step (Algorithm 1) with _Mt_ = _M_ [ˆ] _t_ [(] _[L]_ [)] and _ηt−_ 1: _t_ = _ηt_ [(] _−_ _[L]_ [)] 1: _t_ [and]

ancestors **˜** _**x**_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ .
```
  // Compute the approximations
```

**Set** _f_ ( _xt−_ 1 _, xt_ ) = _G_ [ref] _t_ [(] _[x][t][−]_ [1] _[, x][t]_ [)ˆ] _[η]_ _t_ [(] _[L]_ [)] ( _xt_ )

**Set** ˆ _φ_ [(] _t_ _[L]_ [+1)] = _A_ [fwd] _t_ ( _f,_ **˜** _**x**_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ _,_ ¯ _x_ [(] _t_ _[L]_ [)] _[,]_ [1:] _[N]_ _,_ ¯ _wt_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ ).

**Set** ˆ _ηt_ [(] _−_ _[L]_ [+1)] 1 = _Mt_ [ref] ( ˆ _φ_ [(] _t_ _[L]_ [+1)] ).


_t_
**Set** _M_ [ˆ] _t_ [(] _[L]_ [+1)] = _[φ]_ [ˆ][(] _[L]_ [+1)] _Mt_ [ref] .
_η_ ˆ _t_ [(] _−_ _[L]_ [+1)] 1
```
  // Sample particles according to trained proposal
```

**Sample** and **Compute** _x_ [(] _t_ _[L]_ [+1)] _[,n]_, _wt_ [(] _[L]_ [+1)] _[,n]_ and _a_ [(] _t_ _[L]_ [+1)] _[,n]_ according
to the _t_ -th SMC step (Algorithm 1) with _Mt_ = _M_ [ˆ] _t_ [(] _[L]_ [+1)],
_ηt−_ 1: _t_ = _ηt_ [(] _−_ _[L]_ [)] 1: _t_ [and ancestor] **[ ˜]** _**[x]**_ _t_ [(] _−_ _[L]_ [+1)] 1 _[,n]_ .

**Denote ˜** _**x**_ [(] _t_ _[L]_ [+1)] _[,n]_ = _xt_ [(] _[L]_ [+1)] _[,a]_ _t_ [(] _[L]_ [+1)] _[,n]_ .


17


are sampled as we construct the approximation of the scheme: for each time
_t_ we sample and weight training particles ( _x_ ¯ [(] _t_ _[L]_ [)] _[,]_ [1:] _[N]_, ¯ _wt_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ ) according to
the current auxiliary Feynman-Kac model ( M [ˆ] [(] 1: _[L]_ _T_ [)] _[,]_ [ ˆ] _[η]_ 1: [(] _[L]_ _T_ [)][)][. After computing an]
approximation ˆ _φ_ [(] _t_ _[L]_ [+1)] from these particles, we sample new particles and ancestors ( _x_ [(] _t_ _[L]_ [+1)] _[,]_ [1:] _[N]_, _a_ [(] _t_ _[L]_ [+1)] _[,]_ [1:] _[N]_, _wt_ [(] _[L]_ [+1)] _[,]_ [1:] _[N]_ ) with the trained proposal _M_ [ˆ] _t_ [(] _[L]_ [+1)]
and the same fixed auxiliary weights ˆ _η_ 1: [(] _[L]_ _T_ [)][.]
At each step _t_ of iteration _L_ + 1 of Algorithm 3, ancestor particles **˜** _**x**_ [1:] _t−_ _[N]_ 1
with indices _a_ [1:] _t−_ _[N]_ 1 [and particles][ ¯] _[x]_ _t_ [1:] _[N]_ weighted by ¯ _wt_ _[n]_ [from the previous iter-]
ation ( _L_ ) are used to build approximations as:


_φ_ ˆ [(] _t_ _[L]_ [+1)] = _A_ [fwd] _t_ ( _G_ [ref] _t_ _[η]_ [ˆ] _t_ [(] _[L]_ [)] _,_ **˜** _**x**_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ _,_ ¯ _x_ [(] _t_ _[L]_ [)] _[,]_ [1:] _[N]_ _,_ ¯ _wt_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ )


where, for a function _ft_ that will be approximated in the function class Ψ _t_,
particles _x_ [1:] _t−_ _[N]_ 1 [and] _[ x]_ _t_ [1:] _[N]_, and weights _wt_ [1:] _[N]_, _A_ [fwd] _t_ is defined as follows:


_A_ [fwd] _t_ ( _ft, x_ [1:] _t−_ _[N]_ 1 _[, x]_ [1:] _t_ _[N]_ _, wt_ [1:] _[N]_ )



= arg min
_f_ ˆ _∈_ Ψ _t_



_N_


_wt_ _[n]_
_n_ =1




- �2
log _f_ [ˆ] ( _x_ _[n]_ _t−_ 1 _[, x][n]_ _t_ [)] _[ ∗−]_ [log] _[ f][t]_ [(] _[x]_ _t_ _[n]_ _−_ 1 _[, x][n]_ _t_ [)] _._



When the function class Ψ _t_ consists of single log-quadratic functions
(i.e., Ψ _t_ = _{x �→_ exp( _−_ [1] _[x][′][Ax][ −]_ _[x][′][b][ −]_ [1] _[c]_ [)] _[ |][ A][ ∈]_ [S] _[d][, b][ ∈]_ [R] _[d][, c][ ∈]_ [R] _[}]_ [), the]




[1]

2 _[x][′][Ax][ −]_ _[x][′][b][ −]_ [1] 2



(i.e., Ψ _t_ = _{x �→_ exp( _−_ [1] 2 _[x][′][Ax][ −]_ _[x][′][b][ −]_ [1] 2 _[c]_ [)] _[ |][ A][ ∈]_ [S] _[d][, b][ ∈]_ [R] _[d][, c][ ∈]_ [R] _[}]_ [), the]

minimization above amounts to a simple linear regression problem (with
a semi-definite constraint that is addressed by doing a projection of the
unconstrained solution) and is relatively easy to compute.
The optimization problem described by _A_ [fwd] _t_ is ill-defined when the importance weights _wt_ [1:] _[N]_ are too degenerate. A measure of degeneracy is the
effective sample size (ESS):



_n_ _[w][n]_ [)][2]
ESS( _w_ [1:] _[N]_ ) = [(][�]

~~�~~ _[.]_
_n_ [(] _[w][n]_ [)][2]


It represents an effective number of different particles in the sample and
ranges from 1 (highly degenerate) to _N_ (optimal). Whenever the ESS falls
below _N_ 0 = 2 _d_, where _d_ is the dimension of the space of functions _F_, the
following tempered update is used instead:



_A_ [fwd] _[α]_ ( _. . ._ ) = arg min
_f_ ˆ _∈_ Ψ _t_



_N_ - �2

- _e_ _[α]_ [ log(] _[w]_ _t_ _[n]_ [)] log _f_ [ˆ] ( _x_ _[n]_ _t−_ 1 _[, x][n]_ _t_ [)] _[ −]_ [log] _[ f][t]_ [(] _[x]_ _t_ _[A]_ _−t_ _[n]_ 1 _[, x]_ _t_ _[n]_ [)]

_n_ =1



where _α ∈_ (0 _,_ 1) is a tempering weight. The exponent _α_ is calibrated to
ensure that the ESS of ( _e_ _[α]_ [ log(] _[w]_ _t_ _[n]_ [)] ) [1:] _[N]_ is approximately _N_ 0.
The Algorithm 3 can be used with either _A_ [fwd] 1: _T_ [or] _[ A]_ 1: [fwd] _T_ _[α]_ [, but will definitely]
be less robust without the tempering weight.


18


The computational cost of Algorithm 3 is comparable to that of the backward Algorithm 4, although the Forward Iterated SMC algorithm samples
roughly twice as many particles as controlled SMC. If the number of iterations is fixed in advance, or multiple iterations are performed at once, an
online version of Algorithm 3 can be used, reusing particles for both sampling and training, achieving a computational cost comparable to controlled
SMC. These computational cost considerations and online versions of the
algorithm are detailed in Appendix D.

### **5 Numerical experiments**


In this section, we compare our forward-only training algorithm with alternative methods.
We first consider a simple one-dimensional nonlinear model, illustrating
the difficulties encountered by backward algorithms. Then, we study a real
data application of the multivariate stochastic volatility used in Guarniero
et al. (2017). Primarily, the methods used for comparison are controlled
SMC (Heng et al., 2020) as a backward training algorithm and the bootstrap
particle filter as a baseline reference.
Our primary performance criterion is an estimate of the variance of the
log-normalizing-constant estimator, i.e., Var[log _Z_ [ˆ] _T_ ] (marginal log-likelihood
in the bootstrap case). This quantity serves as a proxy for the relative variance V[ _Z_ [ˆ] _T_ ] _/_ E[ _Z_ [ˆ] _T_ ] [2], which motivates the methods but is harder to estimate
directly. Moreover, it is a commonly used criterion for tuning the number
of particles in particle Metropolis-Hastings algorithms (Doucet et al., 2015;
Sherlock, 2016).
We also consider the variance of the importance weights as a measure of
degeneracy. High weight variance indicates high variability in the estimators.
More precisely, we consider the normalized quantity:



_N_ 1 - _Nn_ =1 [(] _[w]_ _t_ _[n]_ [)][2] _[−]_ [(] _N_ [1]



_._
_N_ [1] ~~�~~ _Nn_ =1 _[w]_ _t_ _[n]_ [)][2]



1

_T_



_T_



_t_ =1



( [1]



_N_ [1] - _Nn_ =1 _[w]_ _t_ _[n]_ [)][2]



**5.1** **Nonlinear observation model**


In this section, we are interested in a simple state space model with nonlinear
observations, with latent space _X_ = R and observation space _Y_ = R, and
the following autoregressive transition model:

_x_ 1 = ~~_√_~~ _σx_ _[ε][x,]_ [1]

1 _−_ _α_ [2]

_xt_ = _αxt−_ 1 + _σxεx,t,_


with _εx,t ∼N_ (0 _,_ 1). The nonlinear observation model is defined as follows:


_yt_ = _f_ ( _xt_ ) + _σyεy,t_


19


with _εy,t ∼N_ (0 _,_ 1) and _f_ is a non-decreasing function.
The choice of _f_ controls the level of difficulty for SMC inference. For
example, if _f_ is a linear function, we get a linear Gaussian model for which
the backward path can be easily and exactly computed.
We set _f_ ( _x_ ) = exp( _x_ ) + _x/_ 10 as an example. With this function, the
model is more informative with larger values of _y_ . The intuition is that for
small _y_ ’s, the optimal path will be more difficult to learn and to infer from.
The model is motivated by the observation that most studies in the literature consider non-linear transitions with linear (or quasi-linear) observations. Often, the class of functions approximates well the optimal twisting
functions in this case. Non-linear observations, which are very common in
applications, introduce additional difficulty due to potentially ill-behaved
emission densities.
We try different scenarios with parameter values of _α ∈_ [0 _._ 9 _,_ 0 _._ 95 _,_ 0 _._ 98 _,_
0 _._ 99 _,_ 0 _._ 995], 11 values for _σx_ [2] [ranging from][ 0] _[.]_ [05][ to][ 0] _[.]_ [15][ and][ 6][ values for] _[ σ]_ _y_ [2]
ranging from 0 _._ 005 to 0 _._ 055. For each scenario, we generated 10 simulated
datasets (over _T_ = 100 time points). We ran each algorithm on each dataset
and compared their performance. More precisely, we compare the first iterations of Algorithm 3 based on the forward scheme and controlled SMC (Algorithm 4) based on the backward scheme. We ran each algorithm 64 times
to evaluate the variances of the estimator with _N_ = 1024. For each iteration
and each run of the algorithm, we get an SMC output ( _x_ [1:] 1: _[N]_ _T_ _[, a]_ 1: [1:] _T_ _[N]_ _[, w]_ 1: [1:] _T_ _[N]_ [)][ and]
an associated estimator _Z_ [ˆ] _T_ .
We measure "dataset difficulty" by the variance of the bootstrap particle
filter weights. Large weight variance indicates that the dataset is challenging to process. The Figure 1 shows that the backward algorithm tends to
fail (i.e., to have a high standard deviation) more often than our forward
algorithm, especially in harder data and parameter sets. In easy cases, i.e.,
when the bootstrap particle filter does well, our forward algorithm does not
improve on the backward algorithm. This is expected since low weight variances suggest less informative data and slower convergence of the forward
scheme toward the optimal scheme. The Figure 2 shows that for a low number of iterations, the backward scheme will often return something that is
worse than the bootstrap particle filter (for 80% of the datasets at iteration _L_ = 1). The backward scheme is 10 times worse than the bootstrap
particle filter in about 25% of the datasets across all iterations. In most of
these cases, the actual trained Feynman-Kac model will end up being either
highly degenerate or will provoke ill-defined inversions in the training. With
the forward scheme, we also observe these issues, but for only 20 datasets
(over 3300). This showcases the forward robustness to the dataset and robustness to the number of iterations: with a lower number of iterations, we
do not make things worse than the bootstrap.


20


Figure 1: Comparison of the standard deviation of _Z_ [ˆ] _t_ over 64 runs after _L_ =
4 training iterations between a forward algorithm (ours) and controlled SMC
(Heng et al., 2020). Better performance is associated with lower standard
deviation. Each point represents a single simulated dataset colored by the
mean (over 64 iterations) of the sum of the relative empirical weight variances
of the bootstrap particle filter (BPF). This variance is expected to be lower
for an easier dataset.


21


Figure 2: Proportion of simulated datasets on which the standard deviation
of _Z_ [ˆ] _t_ over 64 runs is lower than with the bootstrap particle filter (BPF) after
each iteration of the training algorithms (from 1 to 10). Denoting sd [BPF] the
standard deviation of _Z_ [ˆ] _t_ for the BPF, lighter dots show _P_ (sd [alg] _≤_ 0 _._ 1sd [BPF] ),
the proportion of simulated datasets for which the standard deviation of alg
is 10 time lower than the BPF standard deviation (below the main dots),
and _P_ (sd [alg] _≤_ 10sd [BPF] ) (above the main dots).


22


**5.2** **Multivariate Stochastic Volatility model**


In this section, we study the application of our forward-only approach to a
multivariate stochastic volatility model, on a real dataset. We use the same
model and data as Guarniero et al. (2017). We observe data _y_ 1: _T_ consisting
of monthly log returns on the exchange rate of _d_ currencies. We infer the
stochastic volatility _x_ 1: _T_ of dimension _d_ of this log-return as follows:


_yt,i | xt ∼N_ (0 _,_ exp( _xt,i_ )) _._


The latent space follows an autoregressive transition model:


_x_ 1 _∼N_ ( _m,_ Σ _∞_ )

_xt | xt−_ 1 _∼N_ (( _I −_ diag( _α_ )) _m_ + diag( _α_ ) _xt−_ 1 _,_ Σ)


with parameters _m ∈_ R _[d]_, _α ∈_ R _[d][−]_ [1] and Σ _∈_ R _[d][×][d]_ positive definite, and with
Σ _∞_ the covariance matrix such that we sample from the invariant measure
at time 1.
To reduce the dimensionality of the parameter space, Guarniero et al.
(2017) constrain Σ to have variances _σ_ [2] _∈_ R _[d]_ and zero correlations except
for the sub- and super-diagonals, denoted _ρ ∈_ R _[d][−]_ [1] . We proceed similarly.
We use data from the Federal Reserve [1] from March 2000 to August 2008
( _T_ = 102). Like in Guarniero et al. (2017), we set a uniform improper
prior on _m_, [0 _,_ 1] uniform priors on each coordinate of _α_, inverse gamma
priors with mean 0 _._ 2 and variance 1 on each coordinate of _σ_ [2], and a [ _−_ 1 _,_ 1]
triangular prior on each coordinate of _ρ_ . To infer parameters, we do 1 _._ 2 _×_ 10 [5]

iterations of particle marginal Metropolis-Hastings (PMMH) on this model
that uses estimators _Z_ [ˆ] _t_ obtained from running an SMC algorithm.
The different methods of estimating _Z_ [ˆ] _t_ that we compare are: the bootstrap particle filter with _N_ particles as a baseline and an SMC algorithm
using _N_ sample particles with a twisted Feynman-Kac model as input, trained
on the class of log-quadratic twisting functions (18) with a diagonal quadratic
term matrix _A_ . For the training, we use either the controlled SMC algorithm
(Algorithm 4) or our forward iterated SMC algorithm (Algorithm 3), both
algorithms use _L_ iterations and _N_ train particles. We try to match the computational cost of the forward-based SMC algorithm and the bootstrap particle
filter.
We use PMMH with Gaussian random-walk proposals tuned using the
forward algorithm. Beforehand, we transform the model parameters to ensure that proposals respect the parameter constraints. We initialize the
PMMH with the empirical variance of _y_ 1: _T_ for _m_, 0 _._ 9 for _α_, 0 _._ 2 for _σ_ [2] and
0 _._ 25 for _ρ_ .
To assess the variability of the estimates, we perform 10 additional SMC
runs every 100 PMMH steps (incurring an extra 10% computational cost)
to estimate the empirical variance of _Zt_ .


1 `[https://www.federalreserve.gov/releases/h10/hist/](https://www.federalreserve.gov/releases/h10/hist/)`


23


Figure 3: Empirical cumulative distribution of the empirical variances of
log _Z_ [ˆ] _T_ for _d_ = 8, measured every 100 steps of the PMMH algorithm over 10
evaluation.


In the experiment, we studied _d_ = 8 currencies. We chose _N_ = 4500 for
the bootstrap particle filter, _N_ sample = 600, and either ( _L_ = 8 _, N_ train = 100),
( _L_ = 4 _, N_ train = 200), or ( _L_ = 2 _, N_ train = 400) such that we use the same
number of training particles in each algorithm. Trace plots indicate that the
PMMH chains have reached stationarity, and the autocorrelation function
(ACF) suggests adequate tuning. In Figure 3 we observe the empirical CDF
of the estimated variances of log _Z_ [ˆ] _T_, measured every 100 PMMH steps. We
recall that, when we perform PMMH, we want V[log _Z_ [ˆ] _T_ ] _≪_ 1.
Our forward iterated SMC algorithm achieves significantly lower variance than the bootstrap particle filter. Moreover, as observed on simulated
data, the forward algorithm proves more robust than the backward algorithm: while the backward algorithm can potentially reach lower variances,
it frequently produces very large variances, which can cause PMMH chains
to become trapped in regions of parameter space where _Z_ [ˆ] _t_ estimates are
unreliable. At a fixed training cost, the backward algorithm is also more
sensitive to the number of iterations than the forward one. The behavior of
controlled SMC for ( _L_ = 2 _, N_ train = 200) is explained by the chain getting
stuck in a high variance region. A further comparison on the robustness
of the forward and backward algorithms on a slightly different dataset is
presented in Appendix E.


24


### **References**

Andrieu, C., Doucet, A., and Holenstein, R. (2010). Particle markov chain
monte carlo methods. _Journal of the Royal Statistical Society Series_
_B: Statistical Methodology_, 72(3):269–342. `[https://doi.org/10.1111/](https://doi.org/10.1111/j.1467-9868.2009.00736.x)`
`[j.1467-9868.2009.00736.x](https://doi.org/10.1111/j.1467-9868.2009.00736.x)` .


Cappé, O., Moulines, E., and Rydén, T. (2005). _Inference in Hidden Markov_
_Models_ . Springer Series in Statistics. Springer, New York, NY. `[https:](https://doi.org/10.1007/0-387-28982-8)`
`[//doi.org/10.1007/0-387-28982-8](https://doi.org/10.1007/0-387-28982-8)` .


Chopin, N. and Papaspiliopoulos, O. (2020). _An Introduction to Sequential_
_Monte Carlo_ . Springer Series in Statistics. Springer, Cham. `[https://doi.](https://doi.org/10.1007/978-3-030-47845-2)`
`[org/10.1007/978-3-030-47845-2](https://doi.org/10.1007/978-3-030-47845-2)` .


Del Moral, P. (2004). _Feynman-Kac Formulae_ . Probability and Its
Applications. Springer, New York, NY. `[https://doi.org/10.1007/](https://doi.org/10.1007/978-1-4684-9393-1)`
`[978-1-4684-9393-1](https://doi.org/10.1007/978-1-4684-9393-1)` .


Doucet, A., Godsill, S., and Andrieu, C. (2000). On sequential Monte
Carlo sampling methods for Bayesian filtering. _Statistics and Computing_,
10(3):197–208. `[https://doi.org/10.1023/A:1008935410038](https://doi.org/10.1023/A:1008935410038)` .


Doucet, A. and Johansen, A. M. (2011). A tutorial on particle filtering and
smoothing : Fifteen years later. In _The Oxford Handbook of Nonlinear_
_Filtering_, pages 656–705. Oxford University Press, Oxford ; N.Y.


Doucet, A., Pitt, M. K., Deligiannidis, G., and Kohn, R. (2015). Efficient
implementation of markov chain monte carlo when using an unbiased likelihood estimator. _Biometrika_, 102(2):295–313. `[https://doi.org/10.1093/](https://doi.org/10.1093/biomet/asu075)`
`[biomet/asu075](https://doi.org/10.1093/biomet/asu075)` .


Doucet, A. and Sénécal, S. (2004). Fixed-lag sequential Monte Carlo. In
_2004 12th European Signal Processing Conference_, pages 861–864, Vienna,
Austria.


Durbin, J. and Koopman, S. J. (2012). _Time Series Analysis by State Space_
_Methods_ . Oxford University Press, Oxford. `[https://doi.org/10.1093/](https://doi.org/10.1093/acprof:oso/9780199641178.001.0001)`
`[acprof:oso/9780199641178.001.0001](https://doi.org/10.1093/acprof:oso/9780199641178.001.0001)` .


Gordon, N., Salmond, D., and Smith, A. (1993). Novel approach to
nonlinear/non-Gaussian Bayesian state estimation. _IEE Proceedings F_
_(Radar and Signal Processing)_, 140(2):107–113. `[https://doi.org/10.](https://doi.org/10.1049/ip-f-2.1993.0015)`
`[1049/ip-f-2.1993.0015](https://doi.org/10.1049/ip-f-2.1993.0015)` .


Guarniero, P., Johansen, A. M., and Lee, A. (2017). The Iterated
Auxiliary Particle Filter. _Journal of the American Statistical Associa-_
_tion_, 112(520):1636–1647. `[https://doi.org/10.1080/01621459.2016.](https://doi.org/10.1080/01621459.2016.1222291)`
`[1222291](https://doi.org/10.1080/01621459.2016.1222291)` .


25


Heng, J., Bishop, A. N., Deligiannidis, G., and Doucet, A. (2020). Controlled
sequential Monte Carlo. _The Annals of Statistics_, 48(5):2904–2929. `[https:](https://doi.org/10.1214/19-AOS1914)`
`[//doi.org/10.1214/19-AOS1914](https://doi.org/10.1214/19-AOS1914)` .


Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction
Problems. _Journal of Basic Engineering_, 82(1):35–45. `[https://doi.org/](https://doi.org/10.1115/1.3662552)`
`[10.1115/1.3662552](https://doi.org/10.1115/1.3662552)` .


Olsson, J., Cappé, O., Douc, R., and Moulines, É. (2008). Sequential Monte
Carlo smoothing with application to parameter estimation in nonlinear
state space models. _Bernoulli_, 14(1):155–179. `[https://doi.org/10.](https://doi.org/10.3150/07-BEJ6150)`
`[3150/07-BEJ6150](https://doi.org/10.3150/07-BEJ6150)` .


Pitt, M. K. and Shephard, N. (1999). Filtering via Simulation: Auxiliary Particle Filters. _Journal of the American Statistical Association_, 94(446):590–
599. `[https://doi.org/10.1080/01621459.1999.10474153](https://doi.org/10.1080/01621459.1999.10474153)` .


Sherlock, C. (2016). Optimal Scaling for the Pseudo-Marginal Random
Walk Metropolis: Insensitivity to the Noise Generating Mechanism .
_Methodology and Computing in Applied Probability_, 18(3):869–884. `[https:](https://doi.org/10.1007/s11009-015-9471-6)`
`[//doi.org/10.1007/s11009-015-9471-6](https://doi.org/10.1007/s11009-015-9471-6)` .


Xue, L., Finke, A., and Johansen, A. M. (2025). Online rolling controlled
sequential monte carlo. Preprint at `[http://arxiv.org/abs/2508.00696](http://arxiv.org/abs/2508.00696)` .

### **A Expression of the variance**


If we translate equations of Doucet and Johansen (2011) into our notations
we get:



V[ _Z_ [ˆ] _T_ ]




_._ (20)



_ZT_ ]

= [1]
_ZT_ [2] _N_



_N_



_T_



_t_ =1



�� _νT_ ( _dx_ 1: _t_ ) - - 1
+ _O_
_νt−_ 1 _Mt_ ( _dx_ 1: _t_ ) _[ν][T]_ [ (] _[dx]_ [1:] _[t]_ [)] _[ −]_ [1] _N_ [2]



We define the chi-squared divergence between two probability measures
_ν_ and _µ_ as:



�� _dµ_
_Dχ_ [2] ( _ν | µ_ ) =

_dν_



�2 - _dµ_
_dν −_ 1 =

_dν_ _[dµ][ −]_ [1] _[.]_



We can express (20) with the chi-squared divergence as follows:



V[ _Z_ [ˆ] _T_ ]



_ZT_ ]

= [1]
_ZT_ [2] _N_



_T_





_._



_N_




- _Dχ_ [2] ( _νt−_ 1 _Mt | νT_ ) + _O_ - 1

_N_

_t_ =1



_N_ [2]



26


Let us introduce the chi-squared divergence of a non-negative function _f_
relative to a measure _µ_ as:



_f_ - - 1

_µ_ =
_µ_ ( _f_ ) _[µ]_ ���� _f_




     - _f_
_Dχ_ [2] _µ_ [(] _[f]_ [) =] _[ D][χ]_ [2]



1 
_fdµ −_ 1 _._
_f_ _[dµ]_



A crucial property of this quantity is that it is null if and only if _f_ is constant,
just like the chi-squared divergence is null when _ν_ = _µ_ .
Since, for a Feyman-Kac model (M1: _T, G_ 1: _T_ ),


1
_νt−_ 1 _Mt_ ( _dx_ 1: _t_ ) = _G_ 1: _t−_ 1( _x_ 1: _t−_ 1) _M_ 1: _t_ ( _dx_ 1: _t_ )
_Zt−_ 1

_νT_ ( _dx_ 1: _t_ ) = [1] _G_ 1: _t_ ( _x_ 1: _t_ ) _Ht→T_ ( _xt_ ) _M_ 1: _t_ ( _dx_ 1: _t_ )

_ZT_



we have
V[ _Z_ [ˆ] _T_ ]



_ZT_ ]

= [1]
_ZT_ [2] _N_



_T_





_._



_N_




- _Dχ_ [2] _νT_ [(] _[G][t][H][t][→][T]_ [) +] _[ O]_ - 1

_N_

_t_ =1



_N_ [2]



For an auxiliary Feynman-Kac model (M1 _, M_ 2: _T, η_ 1: _T_ ), we have:



_t_ _[H]_ _t_ [ref] _→T_
_GtHt→T_ = _[G]_ [ref]
_ηt−_ 1


so we get the following expression:



_dMt_ [ref]
_dMt_



V[ _Z_ [ˆ] _T_ ]



_dMt_ [ref]
_dMt_



_ZT_ ]

= [1]
_ZT_ [2] _N_



_N_



_T_

- _Dχ_ [2] _νT_

_t_ =1




- _G_ ref _t_ _[H]_ _t_ [ref] _→T_
_ηt−_ 1




- - 1
+ _O_

_N_ [2]




_._


### **B Backward algorithm: Controlled SMC**

In this section, we will explain the Controlled SMC algorithm developed by
Heng et al. (2020).
The backward scheme (15), in terms of twisting functions, yields:


_φ_ _[⋆]_ _T_ [=] _[ G]_ _T_ [ref]
_..._

_φ_ _[⋆]_ _t_ [=] _[ G]_ _t_ [ref] _[M]_ _t_ [ref] +1 [(] _[φ][⋆]_ _t_ +1 [)] _[.]_


The algorithm constructs an approximation with twisting functions ˆ _φ_ _[⋆]_ _t_ [. The]
successive approximations follow:


_φ_ ˆ _[⋆]_ _T_ _[≈]_ _[G]_ _T_ [ref]
_..._

_φ_ ˆ _[⋆]_ _t_ _[≈]_ _[G]_ _t_ [ref] _[M]_ _t_ [ref] +1 [( ˆ] _[φ][⋆]_ _t_ +1 [)] _[.]_


27


**Algorithm 4:** Controlled SMC (2020)

**Input:** A reference Feynman-Kac model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, a number of]
particles _N_, a number of iterations _L_ [max] .
**Output:** Approximations ˆ _φ_ _[⋆,]_ 1: [(] _t_ _[L]_ [)] for 1 _≤_ _L ≤_ _L_ [max] of the optimal
twisting functions _φ_ _[⋆]_ 1: _t_ [.]

```
// Sample initial training particles
```

**Sample** ( _x_ [0] 1: _[,]_ [1:] _t_ _[N]_, _w_ 1: [0] _[,]_ _T_ [1:] _[N]_, _a_ [0] 1: _[,]_ [1:] _T_ _[N]_ ) from the reference Feynman-Kac model
(M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][ with Algorithm 1.]

**Denote ˜** _**x**_ [(] _t_ _[L]_ [+1)] _[,n]_ = _x_ [(] _t_ _[L]_ [+1)] _[,a]_ _t_ [(] _[L]_ [+1)] _[,n]_ .
**For** _L_ **from** 1 **to** _L_ [max] **:**

```
 // Compute the approximations
```

**Set** ˆ _ηT_ _[⋆,]_ [(] _[L]_ [)] _≡_ 1.
**For** _t_ **from** _T_ **to** 1 **:**

**Set** _f_ ( _xt−_ 1 _, xt_ ) = _G_ [ref] _t_ [(] _[x][t][−]_ [1] _[, x][t]_ [)] _[η]_ _t_ _[⋆,]_ [(] _[L]_ [)] ( _xt_ ).

**Set** ˆ _φ_ _[⋆,]_ _t_ [(] _[L]_ [)] = _A_ [bck] ( _f,_ **˜** _**x**_ [(] _t−_ _[L][−]_ 1 [1)] _[,]_ [1:] _[N]_ _, x_ [(] _t_ _[L][−]_ [1)] _[,]_ [1:] _[N]_ ).

**Set** ˆ _ηt_ _[⋆,]_ _−_ [(] 1 _[L]_ [)] = _Mt_ [ref] ( ˆ _φ_ _[⋆,]_ _t_ [(] _[L]_ [)] ).


_t_
**Set** _Mt_ _[⋆,]_ [(] _[L]_ [)] = _[φ]_ [ˆ] _[⋆,]_ [(] _[L]_ [)] _Mt_ [ref] .
_η_ ˆ _t_ _[⋆,]_ _−_ [(] 1 _[L]_ [)]

```
 // Sample particles according to the last iteration
```

**Sample** ( _x_ [(] 1: _[L]_ _t_ [)] _[,]_ [1:] _[N]_, _w_ 1: [(] _[L]_ _T_ [)] _[,]_ [1:] _[N]_, _a_ [(] 1: _[L]_ _T_ [)] _[,]_ [1:] _[N]_ ) from the auxiliary
Feynman-Kac model ( M [ˆ] _[⋆,]_ 1: [(] _T_ _[L]_ [)] _[,]_ [ ˆ] _[η]_ 1: _[⋆,]_ _T_ [(] _[L]_ [)][)][ with Algorithm 1.]

**Denote ˜** _**x**_ [(] _t_ _[L]_ [+1)] _[,n]_ = _xt_ [(] _[i]_ [+1)] _[,a]_ _t_ [(] _[l]_ [+1)] _[,n]_ .


28


The learned auxiliary Feynman-Kac model ( _M_ [ˆ] 1: _T,_ ˆ _η_ 1: _T_ ) follows:


_M_ ˆ _t_ = _φ_ ˆ _t_ _t_ _,_ _η_ ˆ _t−_ 1 = _Mt_ [ref] ( ˆ _φt_ ) _._
_Mt_ [ref] ( ˆ _φt_ ) _[M]_ [ref]


The backward scheme is not naturally iterative. The idea of Guarniero
et al. (2017) and Heng et al. (2020) is that each iteration of the algorithm
samples from an auxiliary Feynman-Kac model ( M [ˆ] _[⋆,]_ 1: [(] _T_ _[L]_ [)] _[,]_ [ ˆ] _[η]_ 1: _[⋆,]_ _T_ [(] _[L]_ [)][)][ and gives us,]
applying the backward scheme, the next iteration ( M [ˆ] _[⋆,]_ 1: [(] _T_ _[L]_ [+1)] _,_ ˆ _η_ 1: _[⋆,]_ _T_ [(] _[L]_ [+1)] ). The
idea is that each of the iterations will bring _Ut_ closer to the optimal _Ut_ _[⋆]_ [and]
the SMC output closer to being directly sampled from the optimal _νT_ _[⋆]_ [. The]
validity of this convergence was proved under strong assumptions in Heng
et al. (2020).
Since the backward scheme starts from the end, the corresponding Algorithm 4 builds approximations from outputs of an SMC algorithm ( _x_ [1:] 1: _[N]_ _T_ _[,]_
_a_ [1:] 1: _[N]_ _T_ _[, w]_ 1: [1:] _T_ _[N]_ [)][ obtained with Algorithm 1 on the auxiliary Feynman-Kac model]
( M [ˆ] _[⋆,]_ 1: [(] _T_ _[L]_ [+1)] _,_ ˆ _η_ 1: _[⋆,]_ _T_ [(] _[L]_ [+1)] ) returned by the previous iteration. Such outputs provide
points used to make the approximations in Algorithm 4. Thus, if the initial
SMC algorithm is degenerate, there is a good chance that the approximation
will not learn anything useful from the target.
Approximations are built as follows:


_φ_ ˆ _[⋆,]_ _t_ [(] _[L]_ [)] = _A_ [bck] ( _Gtη_ ˆ _t_ _[⋆,]_ [(] _[L]_ [)] _, x_ ( _t−L−_ 1 1) _,a_ [(] _t−_ _[L][−]_ 1 [1)] _[,]_ [1:] _[N]_ _, x_ [(] _t_ _[L][−]_ [1)] _[,]_ [1:] _[N]_ )


where, given a function _ft_ that is approximated in a set of functions Ψ _t_,
particles _x_ [1:] _t−_ _[N]_ 1 [and] _[ x]_ _t_ [1:] _[N]_, _A_ [bck] is defined as the minimizer of the following
averaged loss over the particles:



_A_ [bck] ( _ft, x_ [1:] _t−_ _[N]_ 1 _[, x]_ [1:] _t_ _[N]_ ) = arg min
_f_ ˆ _∈F_



_N_



_n_ =1




- �2
log _f_ [ˆ] ( _x_ _[n]_ _t−_ 1 _[, x][n]_ _t_ [)] _[ −]_ [log] _[ f][t]_ [(] _[x]_ _t_ _[n]_ _−_ 1 _[, x][n]_ _t_ [)] _._


### **C Online version of the Forward Iterated Algorithm**

The forward algorithm (Algorithm 2) can be transformed into Algorithm 5
by changing the order of the operations and iterating over _t_ in the inner
loop. This algorithm is online since at iteration _t_ we can discard all previous
_y_ 1: _t−L_ max data.
One important distinction between the forward and the online versions
of our method is that we need to fix the depth _L_ [max] of our scheme before
we run the online Algorithm 5. In the forward Algorithm 2, we can decide
to increment _L_ [max] at the end of each inner loop.


29


**Algorithm 5:** Online SMC Training

**Input:** A reference Feynman-Kac model (M [ref] 1: _T_ _[, G]_ 1: [ref] _T_ [)][, a number of]
particles _N_, a number of iterations _L_ [max] and sets of strictly
positive functions Ψ1: _T_ .
**Output:** Approximations ˆ _φ_ [(] 1: _[L]_ _t_ [)] [for][ 1] _[ ≤]_ _[L][ ≤]_ _[L]_ [max][ of the forward]
scheme.

**Set** ˆ _φ_ [(0)] 1: _T_ _[≡]_ [1][,][ ˆ][M] 1 [(0)] _∼_ M [ref] 1 [,][ ˆ] _M_ 2: [(0)] _T_ _[≡]_ _[M]_ 2: [ref] _T_ [and][ ˆ] _[η]_ 1: [(0)] _T_ _[≡]_ [1][.]
**For** _t_ end _∈_ 1: _T_ **with** _t_ start = min(1 _, t_ end _−_ _L_ [max] + 1) **:**

**For** _t_ **from** _t_ end **to** _t_ start **with** _L_ = _t_ end _−_ _t_ **:**

**Approximate** _G_ [ref] _t_ _[η]_ [ˆ] _t_ [(] _[L]_ [)] by ˆ _φ_ [(] _t_ _[L]_ [+1)] _∈_ Ψ _t_ .

**Set** ˆ _ηt_ [(] _−_ _[L]_ [+1)] 1 = _Mt_ [ref] ( ˆ _φ_ [(] _t_ _[L]_ [+1)] ).


_t_
**Set** _M_ [ˆ] _t_ [(] _[L]_ [+1)] = _[φ]_ [ˆ][(] _[L]_ [+1)] _Mt_ [ref] .
_η_ ˆ _t_ [(] _−_ _[L]_ [+1)] 1
```
  // Sample particles according to trained proposal
```

**Sample** and **Compute** _x_ [(] _t_ _[L]_ [+1)] _[,n]_, _wt_ [(] _[L]_ [+1)] _[,n]_ and _a_ [(] _t_ _[L]_ [+1)] _[,n]_ according
to the _t_ -th SMC step (Algorithm 1) with _Mt_ = _M_ [ˆ] _t_ [(] _[L]_ [+1)],
_ηt−_ 1: _t_ = _ηt_ [(] _−_ _[L]_ [)] 1: _t_ [and ancestor] **[ ˜]** _**[x]**_ _t_ [(] _−_ _[L]_ [+1)] 1 _[,n]_ .

**Denote ˜** _**x**_ [(] _t_ _[L]_ [+1)] _[,n]_ = _xt_ [(] _[L]_ [+1)] _[,a]_ _t_ [(] _[L]_ [+1)] _[,n]_ .

### **D Computational cost of the Forward Iterated Al-** **gorithm**


The computational cost can be divided between two major operations: sampling particles and solving _A_ [fwd] and _A_ [bck] . Both the forward Algorithm 3
and the backward Algorithm 4 use the same number of minimizations per
iteration ( _T_ minimizations), but these algorithms do not sample the same
number of particles. At each iteration, the forward Algorithm 3 uses _N × T_
sampling of particles for the training and _N × T_ sampling of ancestors according to the trained proposal, whereas the backward Algorithm 4 uses the
same particles as training particles and as ancestors in the SMC.
If we denote by _C_ Sample and _C_ Solve the cost per particle of these two main
operations, the forward cost _C_ fwd and the backward cost _C_ bck are:


_C_ fwd

_NT_ [= 2] _[C]_ [Sample][ +] _[ C]_ [Solve]

_C_ bck

_NT_ [=] _[ C]_ [Sample][ +] _[ C]_ [Solve] _[.]_


This assumes that the cost of minimization is linear in _N_ (or with a negligible
constant part) and the same for _A_ [fwd] and _A_ [bck], which is the case in our
application.


30


**Algorithm 6:** Faster Online Forward Iterated SMC

**Input:** A reference Feynman-Kac model (M1 _, M_ 2: _t, G_ 1: _t_ ), a number of
particles _N_, a number of iterations _L_ [max] .
**Output:** Approximations ˆ _φ_ [(] 1: _[L]_ _t_ [)] [for][ 1] _[ ≤]_ _[L][ ≤]_ _[L]_ [max][ of the forward]
scheme.

**Set** ˆ _φ_ [(0)] 1: _T_ _[≡]_ [1][,][ ˆ][M] 1 [(0)] _∼_ M1, _M_ [ˆ] 2: [(0)] _T_ _[≡]_ _[M]_ 2: [(0)] _T_ [and][ ˆ] _[η]_ 1: [(0)] _T_ _[≡]_ [1][.]
**For** _t_ end _∈_ 1: _T_ **with** _t_ start = max(1 _, t_ end _−_ _L_ [max] + 1) **:**
```
 // Sample particles
```

**if** _t_ start = 1 **with** _L_ = _t_ end **:**

**Sample** _x_ [(] 1 _[L]_ [)] _[,n]_ _∼_ M [ˆ] [(] 1 _[L]_ [)][(] _[dx][t]_ [)][.]

**Compute** ¯ _w_ 1 [(] _[L]_ [)] _[,n]_ = _G_ [ˆ][(] 1 _[L]_ [)][(] _[x]_ _t_ [(] _[L]_ [)] _[,n]_ )
**else with** _t_ = _t_ start **and** _L_ = _L_ [max] **:**



**Resample** _x_ [(] _t−_ _[L]_ [)] 1 _[,]_ [1:] _[N]_ and get ancestors _a_ _[n]_ _t−_ 1 [according to] _[ w]_ _t_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ .



**Sample** _x_ [(] _t_ _[L]_ [)] _[,n]_ _∼_ _M_ [ˆ] _t_ [(] _[L]_ [)] (¯ _x_ ( _t−L_ )1 _,a_ _[n]_ _t−_ 1 _, dxt_ ).



**Compute** ¯ _wt_ [(] _[L]_ [)] _[,n]_ = _G_ [ˆ][(] _t_ _[L]_ [)] ( _x_ ( _t−L_ )1 _,a_ _[n]_ _t−_ 1 _, x_ [(] _t_ _[L]_ [)] _[,n]_ ).



**For** _t_ **from** _t_ start + 1 **to** _t_ end **with** _L_ = _t_ end _−_ _t_ **:**

**Resample** _x_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ to ¯ _x_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ according to ¯ _wt_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ .

**Sample** _x_ [(] _t_ _[L]_ [)] _[,n]_ _∼_ _M_ [ˆ] _t_ [(] _[L]_ [)] (¯ _x_ [(] _t−_ _[L]_ [+1)] 1 _[,n]_ _, dxt_ ).

**Compute** ¯ _wt_ [(] _[L]_ [)] _[,n]_ = _G_ [ˆ][(] _t_ _[L]_ [)] ( _x_ [(] _t−_ _[L]_ [+1)] 1 _[,n]_ _, x_ [(] _t_ _[L]_ [)] _[,n]_ ).

**Set** _wt_ [(0)] _[,t]_ [end] = ¯ _wt_ [(0)] _[,t]_ [end]
```
// Compute the approximations
```

**For** _t_ **from** _t_ end **to** _t_ start **with** _L_ = _t_ end _−_ _t_ **:**

**Set** _f_ ( _xt−_ 1 _, xt_ ) = _Gt_ ( _xt−_ 1 _, xt_ )ˆ _ηt_ [(] _[L]_ [)] ( _xt_ )

**Set** ˆ _φ_ [(] _t_ _[L]_ [+1)] = _A_ [fwd] ( _f,_ ¯ _x_ [(] _t−_ _[L]_ [+1)] 1 _[,]_ [1:] _[N]_ _, x_ [(] _t_ _[L]_ [)] _[,]_ [1:] _[N]_ _,_ ¯ _wt_ [(] _[L]_ [)] _[,]_ [1:] _[N]_ ).

**Set** ˆ _ηt_ [(] _−_ _[L]_ [+1)] 1 = _Mt_ ( ˆ _φ_ [(] _t_ _[L]_ [+1)] ).


_t_
**Set** _M_ [ˆ] _t_ [(] _[L]_ [+1)] = _[φ]_ [ˆ][(] _[L]_ [+1)] _Mt_ .
_η_ ˆ _t_ [(] _−_ _[L]_ [+1)] 1



**Set** _G_ [ˆ][(] _t_ _[L]_ [+1)] = _Gt_ _dMt_
_dMt_ [(] _[L]_ [+1)]



_ηt_ [(] _[L]_ [)] .
_ηt_ [(] _−_ _[L]_ [+1)] 1



**Compute** _wt_ [(] _−_ _[L]_ [+1)] 1 = ¯ _wt_ [(] _−_ _[L]_ [+1)] 1 _×_ _ηt_ [(] _−_ _[L]_ [+1)] 1 ( _x_ [(] _t_ _[L]_ [)] _[,n]_ ).
_ηt_ [(] _[L]_ [)]


31


Figure 4: Empirical cumulative distribution of the empirical variances of
log _Z_ [ˆ] _T d_ = 7 (bottom), measured every 100 steps of the PMMH algorithm
over 10 evaluation.


With log-quadratic twisting functions, the sampling cost is linear in the
dimension and the minimization cost is either _O_ ( _d_ [4] ) with a full quadratic
term _A_ or _O_ ( _d_ [2] ) with a diagonal quadratic term _A_ . Then, with problems of
higher dimension, the cost per iteration of the forward Algorithm 3 is nearly
identical to the cost per iteration of the backward Algorithm 4. Without
this log-quadratic setting, the cost per iteration is higher since it is more
involved than the computation of an inverse. Therefore, the difference in
computation time per iteration should also be small between the algorithms.
One way to reduce the computation time of Algorithm 3 is by doing
things online. If one wants to run many iterations of the Forward Iterated
Algorithm, one could recycle the sampled particles to use them for training.
It is done in Algorithm 6, which requires half the number of sampling particles than the initial Algorithm 3 (exactly the same amount as the backward
algorithm).

### **E Details on the robustness of the MSV experiment**


In this section, we keep the model and settings of the multivariate stochastic
volatility model defined in subsection 5.2.
We run the same experiment, but we remove one currency (INR – the


32


Indian rupee), which is especially difficult for the model. We end up with _d_ =
7, we choose _N_ = 3000 for the bootstrap particle filter, _N_ sample = 800, and
either ( _L_ = 8 _, N_ train = 50), ( _L_ = 4 _, N_ train = 100), or ( _L_ = 2 _, N_ train = 200)
for the iterated algorithms. In Figure 4, we observe that all versions of the
controlled SMC algorithm achieve similar variances, better than the forward
variances. By contrast with Figure 3, this shows that posterior sampling for
this model is not necessarily challenging, but that real datasets can be more
difficult than others. Controlled SMC, as shown in the simulated experiment
(Figure 1), does well with easier datasets but struggles with more difficult
ones. Conversely, the forward scheme is slower by nature (especially with
easier datasets), but can work with more difficult datasets, and here gives
similar performances in both cases. This is especially important with real
data applications, since practitioners are not always able to choose their
datasets, and our algorithm extends the sets of models that can be tackled
with iterated SMC methods.


33


