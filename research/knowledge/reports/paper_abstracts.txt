--- PAPER START ---
Category: rl_trading
File: 2024_Reinforcement Learning in Agent-Based Market Simulation: Unv.pdf
Content_Extract: ABSTRACT Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, partic- ularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based simulation framework employing reinforcement learning (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the simulation, offering insights into their response to significant market events. 1 Introduction Modern financial markets serve as vehicles for setting up-to-date prices, thus shaping economic landscapes worldwide. Thus understanding how markets react to external and internal events is crucial for investors and regulators. Designing a proper financial market simulator can answer ‚Äúwhat if‚Äù questions and help market participants make informed decisions in a fast-paced and volatile market. An extensive body of literature exists dedicated to methods of simulating market behavior [1, 2, 3, 4, 5, 6, 7]. Among these papers, agent-based market simulators stand out due to their ability to emulate dynamics of the real-world markets. Conventional agent-based systems use rule-based agents, e.g., [ 8, 9]. These systems have issues when calibrating to real markets and fail to capture realistic market dynamics. This limitation arises from the rigid, hard-coded nature of rule-based agents, which prevents them from adapting to changing market conditions. In contrast, agents which are capable of learning have the ability to optimize their goals by learning from the environment and the behavior of other agents. This adaptability closely mirrors the behavior of real-world investors, enhancing the realism of the simulation. Recently, we have seen several successful applications of machine learning techniques in financial problems such as portfolio management [10, 11], credit rating [12, 13], and order execution [14, 15]. Reinforcement learning (RL) is an important class of machine learning methods, where agents are capable of learning optimal strategies without knowing the underlying environment dynamics [16]. Recently, several papers have been published that use RL agents to construct a simulation environment for financial markets. Lussange et al. [7] model a market using hundreds of RL agents, each solving a simplified investment problem. However, the participants of real-world stock markets have different goals and use complex strategies. Ideally, we should let these agents le...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2025_Right Place, Right Time: Market Simulation-based RL for Exec.pdf
Content_Extract: Abstract Execution algorithms are vital to modern trading, they enable mar- ket participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more so- phisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reac- tive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent‚Äôs performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings high- light the potential of reinforcement learning as a powerful tool in the trader‚Äôs toolkit. CCS Concepts ‚Ä¢Computing methodologies ‚Üí Agent / discrete models;Rein- forcement learning. Keywords Reinforcement Learning, Agent-Based Models, Execution Algo- rithms, Efficient Frontier, Trading ACM Reference Format: Ollie Olby, Andreea Bacalum, Rory Baggott, and Namid R. Stillman. 2025. Right Place, Right Time: Market Simulation-based RL for Execution Op- timisation. In6th ACM International Conference on AI in Finance (ICAIF ‚Äô25), November 15‚Äì18, 2025, Singapore, Singapore.ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3768292.3770405 1 Introduction Execution algorithms are a vital tool for trading. They allow market participants to optimally execute large meta-orders by decomposing them into a series of smaller orders and submitting them over This work is licensed under a Creative Commons Attribution-NonCommercial- NoDerivatives 4.0 International License. ICAIF ‚Äô25, Singapore, Singapore ¬©2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2220-2/2025/11 https://doi.org/10.1145/3768292.3770405 a specified time horizon. This is essential for reducing slippage, which is defined as the difference between the expected and actual execution price of a trade. Slippage occurs when the market moves while a meta-order is being executed, incurring a cost to the trader, also known as an execution cost. Slippage, ùúÅ , can be defined as the sum of market risk,ùúÅ ùëÄùëÖ , and market impact,ùúÅ ùëÄùêº , ùúÅ=ùúÅ ùëÄùëÖ +ùúÅ ùëÄùêº (1) where market impact is described as the price movement caused by the execution of the order itself, and market risk is characterised as the price movement resulting from exogenous factors. This slippage or execution cost can often be significant, particularly when trying to execute large positions within a short horizon. Optimising these execution algorithms is therefore key for reduc- ing the adverse cost to a trading firm. Almgren and Chriss presented seminal work on the optimisation of these execution algorithms, where the...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2022_Understanding intra-day price formation process by agent-bas.pdf
Content_Extract: ABSTRACT This article presents XGB-Chiarella, a powerful new approach for deploying agent-based models to generate realistic intra-day artiÔ¨Åcial Ô¨Ånancial price data. This approach is based on agent-based models, calibrated by XGBoost machine learning surrogate. Following the Extended Chiarella model, three types of trading agents are introduced in this agent-based model: fundamental traders, momentum traders, and noise traders. In particular, XGB-Chiarella focuses on conÔ¨Åguring the simulation to accurately reÔ¨Çect real market behaviours. Instead of using the original Expectation- Maximisation algorithm for parameter estimation, the agent-based Extended Chiarella model is calibrated using XGBoost machine learning surrogate. It is shown that the machine learning surrogate learned in the proposed method is an accurate proxy of the true agent-based market simulation. The proposed calibration method is superior to the original Expectation-Maximisation parameter estimation in terms of the distance between historical and simulated stylised facts. With the same underlying model, the proposed methodology is capable of generating realistic price time series in various stocks listed at three different exchanges, which indicates the universality of intra-day price formation process. For the time scale (minutes) chosen in this paper, one agent per category is shown to be sufÔ¨Åcient to capture the intra-day price formation process. The proposed XGB-Chiarella approach provides insights that the price formation process is comprised of the interactions between momentum traders, fundamental traders, and noise traders. It can also be used to enhance risk management by practitioners. Keywords Agent-based Model¬∑ Financial Market Simulator¬∑ Extended Chiarella Model¬∑ Price Formation 1 Introduction 1.1 Motivation In the past decade algorithmic trading has grown rapidly across the world and has become the dominant way securities are traded in Ô¨Ånancial markets, currently generating more than half of the volume of U.S. equity markets. Constantly improving computer technology and its application by both traders and exchanges, together with the evolution of market micro-structure, automation of price quotation and trade execution have together enabled faster trading. Consequently, intra-day price formation underpinning this trading process has become the focus of intense research attention in recent years as market participants attempt to gain greater insight into how prices are determined and hence improve trading performance. Price formation determines the price of an asset through interactions between buyers and sellers. It is at the core of the efÔ¨Åcient and transparent operation of markets for goods and services. The balance between buyers and sellers provide an effective indicator of demand and supply in a market, where demand and supply are generally signiÔ¨Åcant arXiv:2208.14207v1  [q-fin.CP]  29 Aug 2022 AUGUST 31, 2022 but not the only driving factors behind price movem...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2025_Agent-Based Simulation of a Financial Market with Large Lang.pdf
Content_Extract: Abstract.In real-world stock markets, certain chart patterns‚Äîsuch as price declines near historical highs‚Äîcannot be fully explained by fun- damentals alone. These phenomena suggest the presence of path depen- dence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices lead- ing up to the present. Path dependence has drawn attention in behav- ioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individ- ual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading deci- sions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow stan- dard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Fur- thermore, an analysis of FCLAgents‚Äô behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior. Keywords:LLMs¬∑Financial market simulations¬∑Behavioral biases 1 Introduction An agent-based market simulation is an effective tool for modeling macro-scale financial phenomena. Researchers aim to constructively gain insight regarding the underlying mechanisms of these phenomena by designing investor behavior using heterogeneous agents and simulating their interactions [2,10,12,41]. One of the key factors in modeling investor behavior as agents is the incor- poration of behavioral biases. Agent-based approach focuses on bounded ratio- nality [46] to capture dynamics of complex systems such as financial markets. Unlike neoclassical economics, which assumes fully rational agents who opti- mize utility, the agent-based perspective requires behavioral principles that are arXiv:2510.12189v1  [cs.CE]  14 Oct 2025 2 R. Hashimoto et al. not necessarily based on optimization are required. Behavioral biases represent one manifestation of human behavior that reflects such bounded rationality. Research in behavioral finance [4,45,50] has identified various behavioral bi- ases among investors, suggesting that market participants‚Äô irrationality follows certain systematic patterns. Several studies in agent-based modeling have exam- ined the impact of incorporating behavioral biases, especially focusing on loss aversion [9,25,40], into agent models to assess their effects on macro-level phe- nomena. Loss aversion refers to the tendency of individuals to weigh potential losses more heavily than equivalent gains, often leading to risk-averse or -seeking behavior depending on the f...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2020_Model-free conventions in multi-agent reinforcement learning.pdf
Content_Extract: October 2020 Model-free conventions in multi-agent reinforcement learning with heterogeneous preferences Raphael K√∂ster1, 2, Kevin R. McKee1, Richard Everett1, Laura Weidinger1, William S. Isaac1, Edward Hughes1, Edgar A. Du√©√±ez-Guzm√°n1, Thore Graepel1, Matthew Botvinick1 and Joel Z. Leibo1, 2 1DeepMind, 2Corresponding authors: rkoster@google.com; jzl@google.com Game theoretic views of convention generally rest on notions of common knowledge and hyper-rational models of individual behavior. However, decades of work in behavioral economics have questioned the validity of both foundations. Meanwhile, computational neuroscience has contributed a modernized ‚Äúdual process‚Äùaccountofdecision-makingwheremodel-free(MF)reinforcementlearningtradesoÔ¨Äwithmodel- based(MB)reinforcementlearning. Theformercaptureshabitualandprocedurallearningwhilethelatter captures choices taken via explicit planning and deduction. Some conventions (e.g. international treaties) are likely supported by cognition that resonates with the game theoretic and MB accounts. However, con- vention formation may also occur via MF mechanisms like habit learning; though this possibility has been understudied. Here, we demonstrate that complex, large-scale conventions can emerge from MF learning mechanisms. This suggests that some conventions may be supported by habit-like cognition rather than explicitreasoning. WeapplyMFmulti-agentreinforcementlearningtoatemporo-spatiallyextendedgame with incomplete information. In this game, large parts of the state space are reachable only by collective action. However, heterogeneity of tastes makes such coordinated action diÔ¨Écult: multiple equilibria are desirable for all players, but subgroups prefer a particular equilibrium over all others. This creates a coor- dination problem that can be solved by establishing a convention. We investigate start-up and free rider subproblemsaswellastheeÔ¨Äectsofgroupsize, intensityofintrinsicpreference, andsalienceontheemer- gence dynamics ...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2022_Financial Vision Based Reinforcement Learning Trading Strate.pdf
Content_Extract: ABSTRACT Recent advances in artiÔ¨Åcial intelligence (AI) for quantitative trading have led to its general super- human performance in signiÔ¨Åcant trading performance. However, the potential risk of AI trading is a ‚Äúblack box‚Äù decision. Some AI computing mechanisms are complex and challenging to un- derstand. If we use AI without proper supervision, AI may lead to wrong choices and make huge losses. Hence, we need to ask about the AI ‚Äúblack box‚Äù, including why did AI decide to do this or not? Why can people trust AI or not? How can people Ô¨Åx their mistakes? These problems also highlight the challenges that AI technology can explain in the trading Ô¨Åeld. Keywords Financial Vision¬® Explainable AI (XAI)¬® Convolutional Neural Networks (CNN)¬® Gramian Angular Field (GAF)¬® Candlestick¬® Convolutional Neural Network (CNN)¬® Patterns Recognition¬® Proximal Policy Optimization (PPO)¬® Reinforcement Deep Reinforcement Learning (Deep RL)¬® Transfer Learning 1 Introduction Suppose investors want to directly predict the future transaction price or ups and downs. In that case, the fatal assump- tion is that the training data set is consistent with the data distribution that has not occurred in the future. However, the natural world will not let us know whether the subsequent data distribution will change. Because of this, even if researchers add a moving window to the training process, it is inevitable that ‚Äúmachine learning obstacles-prediction delay‚Äù will occur. Our method can avoid ‚Äúmachine learning obstacles-prediction delay‚Äù, We also propose auto trading by deep reinforcement learning. Our new article has the following contributions: 1. Our Ô¨Årst contribution is not to make future predictions but to focus on the current ‚Äúcandlesticks pattern detection‚Äù, such as EngulÔ¨Ång Pattern, Morning Star,. . . . 2. Our second contribution focuses on detecting trading entry and exit signals combined with related investment strategies. 3. Our third contribution found from experiments that the 15-minute price data of Ethereum train through trans- fer learning is suitable for US stock trading. With the rise of deep learning and reinforcement learning technology, breakthrough innovations are in computer trad- ing software [1, 2]. ArtiÔ¨Åcial intelligence (A.I.) is more efÔ¨Åcient than a calculation model that only uses static data [3]. arXiv:2202.04115v1  [cs.AI]  3 Feb 2022 Financial Vision Based Reinforcement Learning Trading Strategy A PREPRINT Investment companies have used computer algorithms to process transactions for several years. Both new and old investment companies have begun to use artiÔ¨Åcial intelligence to help customers process investments [4]. Recent advances in A.I. for quantitative trading have led to its widespread demonstration of superhuman performance in signiÔ¨Åcant trading performance [5]. Likewise, reinforcement learning (R.L.) [6] has found tremendous success and demonstrated a superhuman-level of capabilities in the trading Ô¨Åeld [7]. A.I. trading starts with supe...
--- PAPER END ---

--- PAPER START ---
Category: rl_trading
File: 2025_Quantum Reinforcement Learning Trading Agent for Sector Rota.pdf
Content_Extract: Abstract‚ÄîWe propose a hybrid quantum-classical reinforce- ment learning framework for sector rotation in the Taiwan stock market. Our system employs Proximal Policy Optimization (PPO) as the backbone algorithm and integrates both classi- cal architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV , QASA) as policy and value networks. An automated feature engineering pipeline extracts financial indica- tors from capital share data to ensure consistent model input across all configurations. Empirical backtesting reveals a key finding: although quantum-enhanced models consistently achieve higher training rewards, they underperform classical models in real-world investment metrics such as cumulative return and Sharpe ratio. This discrepancy highlights a core challenge in applying reinforcement learning to financial domains‚Äînamely, the mismatch between proxy reward signals and true investment objectives. Our analysis suggests that current reward designs may incentivize overfitting to short-term volatility rather than optimizing risk-adjusted returns. This issue is compounded by the inherent expressiveness and optimization instability of quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) con- straints. We discuss the implications of this reward-performance gap and propose directions for future improvement, including reward shaping, model regularization, and validation-based early stopping. Our work offers a reproducible benchmark and critical insights into the practical challenges of deploying quantum reinforcement learning in real-world finance. Index Terms‚ÄîQuantum reinforcement learning, sector rota- tion, hybrid quantum-classical model, Proximal Policy Optimiza- tion (PPO), quantum neural networks (QNN), Taiwan stock market, investment strategy optimization. I. INTRODUCTION Financial markets are inherently dynamic and noisy [1], driven by complex interactions among macroeconomic cycles, sector-specific trends, and investor behaviors. Sector rotation, the practice of reallocating capital between industry sectors based on their expected performance across economic phases, is a prominent strategy for generating alpha and managing risk in volatile markets. Reinforcement learning (RL) [2] offers a powerful paradigm for this challenge. Its ability to learn optimal decision policies through direct interaction with an environment aligns naturally with the sequential, stochastic nature of financial decision-making. Unlike traditional rule- based or supervised learning methods that depend on static datasets, RL agents dynamically adapt their strategies to maximize cumulative rewards over time. Recent advances in quantum machine learning (QML) [3] introduce new possibilities for enhancing these learning sys- tems. By integrating quantum principles like superposition and entanglement, quantum reinforcement learning (QRL) [4], [5] aims to create more expressive policy representations and accelerate optimization. However, the practic...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Realised quantile-based estimation of the integrated varianc.pdf
Content_Extract: Abstract In this paper, we propose a new jump robust quantile-based re alised variance measure of ex-post return variation that can be computed using potenti ally noisy data. The estimator is consistent for the integrated variance and we present fea sible central limit theorems which show that it converges at the best attainable rate and has exc ellent eÔ¨Éciency. Asymptotically, the quantile-based realised variance is immune to Ô¨Ånite act ivity jumps and outliers in the price series, while in modiÔ¨Åed form the estimator is applicable wi th market microstructure noise and therefore operational on high-frequency data. Simulation s show that it has superior robustness properties in Ô¨Ånite sample, while an empirical application illustrates its use on equity data. Keywords: Finite activity jumps; Market microstructure noise; Orde r statistics; Outliers; Realised variance. JEL ClassiÔ¨Åcation : C10; C80. ‚àó Podolskij gratefully acknowledges Ô¨Ånancial support from CREATES funded by the Danish National Research Foundation. We would like to thank Peter Bank, ¬¥Alvaro Cartea, Fulvio Corsi, Dick van Dijk, Dobrislav Dobrev, Bruce Lehman, Haikady Nagaraja, Kevin Sheppard (discussant), Neil Shephard and the seminar participants at the London-Oxford Financial Econometrics Study Group at Imperial C ollege, Humboldt-Universit¬® at zu Berlin, ECARES of Universit¬¥ e Libre de Bruxelles, LSE Department of Statistics, Fe deral Reverse Board, Washington, Amsterdam Business School, Tinbergen Institute, Rotterdam, the Workshop on Mathematical Finance for Young Researchers, Quantitative Products Laboratory, Berlin, and the SITE Summer W orkshop 2008, Stanford for valuable comments and discussions. ‚Ä† CREATES, School of Economics and Management, Aarhus Universit y, Building 1322, Bartholins All¬¥ e 10, 8000 Aarhus, Denmark. E-mail kchristensen@creates.au.dk. ‚Ä° Deutsche Bank AG, Winchester House, 1 Great Winchester Street , London EC2N 2DB, UK and aÔ¨Éli- ated with the Department of Quantitative Economics, the Universit y of Amsterdam, The Netherlands. E-mail: roel.ca.oomen@gmail.com. Phone: +44 (0) 207 54 56989. ¬ßPodolskij is with ETH Z¬® urich, Department of Mathematics, R¬® amistrasse 101, CH-8092 Z¬® urich, Switzerland and aÔ¨Éliated with CREATES, University of Aarhus, Denmark. E-mail: mark.podolskij@math.ethz.ch. arXiv:2601.13006v1  [econ.EM]  19 Jan 2026 1 Introduction In recent years, our understanding of asset price dynamics has b een signiÔ¨Åcantly enhanced by the increasing availability of intra-day Ô¨Ånancial tick data in conjunction w ith the rapid development and harnessing of the necessary econometric tools. Realised varia nce, deÔ¨Åned as the sum of squared intra-period returns, has been a key driver in this literature (e.g. A ndersen, Bollerslev, Diebold, and Labys, 2001; BarndorÔ¨Ä-Nielsen and Shephard, 2002) as it pro vides a simple yet highly eÔ¨Écient way to consistently estimate the quadratic variation of a price proc ess. However, when faced with the realities of high-frequency da...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_A unified theory of order flow, market impact, and volatilit.pdf
Content_Extract: Abstract We propose a microstructural model for the order flow in financial markets that distin- guishes betweencore ordersandreaction flow, both modeled as Hawkes processes. This model has a natural scaling limit that reconciles a number of salient empirical properties: persistent signed order flow, rough trading volume and volatility, and power-law market impact. In our framework, all these quantities are pinned down by a single statisticH 0, which measures the persistence of the core flow. Specifically, the signed flow converges to the sum of a fractional process with Hurst indexH 0 and a martingale, while the limiting traded volume is a rough process with Hurst indexH 0 ¬¥1{2. No-arbitrage constraints imply that volatility is rough, with Hurst parameter 2H0 ¬¥3{2, and that the price impact of trades follows a power law with exponent 2¬¥2H 0. The analysis of signed order flow data yields an estimateH 0 ¬´3{4. This is not only consistent with the square-root law of market impact, but also turns out to match estimates for the roughness of traded volumes and volatilities remarkably well. Keywords:Trading volume, order flow, core order flow, rough volatility, market impact, long memory, market microstructure, Hawkes processes, mixed fractional Brownian motion, limit the- orems, criticality. Mathematics Subject Classification (2020):60F05, 60G22, 60G55, 62P05, 91G15, 91G80 ‚àóYoussef Ouazzani Chahdi, Mathieu Rosenbaum and Gr¬¥ egoire Szymanski gratefully acknowledge support from the ILB ChairArtificial Intelligence and Quantitative Methods for Financeat University Paris Dauphine-PSL. The authors thank Pavel Chigansky and Marina Kleptsyna for key input on mixed fractional Brownian motion. They are also grateful to Jean-Philippe Bouchaud and Kevin Webster for inspiring discussions on order flow modeling, and thank BMLL Technologies for providing the historical market data used in this study. ‚Ä†Department of Mathematics, Imperial College London,j.muhle-karbe@imperial.ac.uk ‚Ä°MICS, CentraleSup¬¥ elec,youssef.ouazzani-chahdi@centralesupelec.fr ¬ßCeremade, Universit¬¥ e Paris Dauphine-PSL,mathieu.rosenbaum@dauphine.psl.eu ¬∂DMATH, Universit¬¥ e du Luxembourg,gregoire.szymanski@uni.lu 1 arXiv:2601.23172v2  [q-fin.ST]  2 Feb 2026 1 Introduction Prices and traded quantities are the fundamental observables in any financial market. In a vast body of research initiated by Bachelier [1] and Black and Scholes [8], (semi-)martingales have emerged as the canonical model for asset prices, reflecting the absence of arbitrage and limited predictability of returns. In contrast, there is no similar standard model class for the corresponding order flow yet. A key challenge is that any such model must at the same time capture the stylized properties of traded amounts (‚Äúunsigned volumes‚Äù) and their directionality (‚Äúsigned order flow‚Äù). Moreover, through the price impact of trades, order flow and price dynamics are intimately linked. A consistent model for the order flow therefore must ...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_WebCryptoAgent: Agentic Crypto Trading with Web Informatics.pdf
Content_Extract: Abstract Cryptocurrency trading increasingly depends on timely integration of heterogeneous web in- formation and market microstructure signals to support short-horizon decision making un- der extreme volatility. However, existing trad- ing systems struggle to jointly reason over noisy multi-source web evidence while main- taining robustness to rapid price shocks at sub- second timescales. The first challenge lies in synthesizing unstructured web content, so- cial sentiment, and structured OHLCV sig- nals into coherent and interpretable trading decisions without amplifying spurious corre- lations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt mar- ket shocks that require immediate defensive responses. To address these challenges, we propose WEBCRYPTOAGENT, an agentic trad- ing framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified ev- idence document for confidence-calibrated rea- soning. We further introduce a decoupled con- trol architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and pro- tective intervention independent of the trad- ing loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WE- BCRYPTOAGENTimproves trading stability, reduces spurious activity, and enhances tail- risk handling compared to existing baselines. Code will be available at https://github. com/AIGeeksGroup/WebCryptoAgent. 1 Introduction In recent years, the rapid development of large language models (LLMs) has catalyzed a new paradigm ofagentic trading systems(Shi et al., 2025; Zhang et al., 2025b; Lin et al., 2025; Ge et al., 2025; Zhang et al., 2025a), where autonomous Figure 1: Structural comparison between the horizon- tal firm-based debate model (TradingAgents) and our proposed vertical reflective two-tier architecture (We- bCryptoAgent). agents leverage textual and numerical information to make financial decisions. With the global expan- sion of the cryptocurrency market, characterized by extreme volatility and round-the-clock trading, the demand for intelligent trading assistants has intensified. These agents are designed not only to process heterogeneous data sources‚Äîsuch as news, social media sentiment, and historical mar- ket data‚Äîbut also to reason and act in dynamic environments. Early efforts in this direction in- clude domain-adapted financial assistants such as PIXIU (FinMA) (Xie et al., 2023), FinGPT (Yang et al., 2023b), and Instruct-FinGPT (Zhang et al., 2023a), which fine-tune general-purpose LLMs on financial corpora to enhance domain sensitiv- ity. Meanwhile, large-scale pretrained models such as BloombergGPT (Wu et al., 2023), Xu- anYuan 2.0 (Zhang et al., 2023b), and Fin-T5 (Lu et al., 2023) have demonstrated that hybrid do- main‚Äìgeneral corpora can achieve competitive rea- soning capabilitie...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Trading Electrons: Predicting DART Spread Spikes in ISO Elec.pdf
Content_Extract: Abstract We study the problem of forecasting and optimally trading day-ahead versus real- time (DART) price spreads in U.S. wholesale electricity markets. Building on the framework of [15], we extend spike prediction from a single zone to a multi-zone set- ting and treat both positive and negative DART spikes within a unified statistical model. To translate directional signals into economically meaningful positions, we de- velop a structural and market-consistent price impact model based on day-ahead bid stacks. This yields closed-form expressions for the optimal vector of zonal INC/DEC quantities, capturing asymmetric buy/sell impacts and cross-zone congestion effects. When applied to NYISO, the resulting impact-aware strategy significantly improves the risk‚Äìreturn profile relative to unit-size trading and highlights substantial heterogeneity across markets and seasons. 1 Introduction In U.S. wholesale electricity markets operated by Independent System Operators and Re- gional Transmission Organizations (ISOs/RTOs), trading is organized as a two-settlement system: a Day-Ahead Market (DAM), in which schedules and prices for the following oper- ating day are determined, and a Real-Time Market (RTM), in which actual imbalances are settled at higher frequency. The difference between these two prices‚Äîthe day-ahead real-time spread (DART)‚Äîis a central risk factor for both financial and physical market participants and can be interpreted as a market-implied forecast error up to an embedded risk pre- mium [27]. Large deviations often arise from transmission congestion, load forecast errors, unit commitment constraints, and network contingencies, and they can generate substantial profit opportunities for traders capable of anticipating extreme DART events. Empirical works document that such extreme price movements are short-lived, clustered, and closely linked to binding network constraints and unexpected demand shocks [4, 25, 30, 32, 9]. This clustering has in particular motivated self-exciting point-process (Hawkes-type) models for spike occurrences in electricity prices [5, 18, 6, 13, 10]. ‚àóCEREMADE, Universit¬¥ e Paris Dauphine & PSL Research University. ‚Ä†Department of Operations Research & Financial Engineering, Princeton University. 1 arXiv:2601.05085v2  [q-fin.TR]  1 Feb 2026 In NYISO (New York ISO), ISO‚ÄìNE (ISO New England) and ERCOT (Electric Relia- bility Council of Texas), market participants may take purely financial day-ahead positions through virtual bidding [19]. An INC trade (virtual demand) of sizeq >0 buys energy in the Day-Ahead Market at unit priceP DA and sells it back in the Real-Time Market at priceP RT, yielding a payoff (P RT ‚àíP DA)√óq, while a DEC trade (virtual supply) does the opposite, yielding (P DA ‚àíP RT)√óq. Thus, the problem faced by a virtual market participant is simultaneously predictive and operational: 1. reliably forecast where and when large DART spreads will occur, and 2. translate these forecasts into profitable day...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Who Restores the Peg? A Mean-Field Game Approach to Model St.pdf
Content_Extract: Abstract‚ÄîUSDC and USDT are the dominant stablecoins pegged to $1 with a total market capitalization of over $300B and rising. Stablecoins make dollar value globally accessible with secure transfer and settlement. Yet in practice, these stablecoins experience periods of stress and de-pegging from their $1 target, posing significant systemic risks. The behavior of market participants during these stress events and the collective actions that either restore or break the peg are not well understood. This paper addresses the question:who restores the peg?We develop a dynamic, agent-based mean-field game framework for fiat-collateralized stablecoins, in which a large population of arbitrageurs and retail traders strategically interacts across ex- plicit primary (mint/redeem) and secondary (exchange) markets during a de-peg episode. The key advantage of this equilibrium formulation is that it endogenously maps market frictions into a market-clearing price path and implied net order flows, allowing us to attribute peg-reverting pressure by channel and to stress- test when a given mechanism becomes insufficient for recovery. Using three historical de-peg events, we show that the calibrated equilibrium reproduces observed recovery half-lives and yields an order flow decomposition in which system-wide stress is predominantly stabilized by primary-market arbitrage, whereas episodes with impaired primary redemption require a joint recovery via both primary and secondary markets. Finally, a quantitative sensitivity analysis of primary-rail frictions identifies a non-linear breakdown threshold. Beyond this point, secondary- market liquidity acts mainly as a second-order amplifier around this primary-market bottleneck. Index Terms‚Äîstablecoins, mean-field games, market mi- crostructure, systemic risk, exploitability, DeFi I. INTRODUCTION Fiat-collateralized stablecoins such as USDC and USDT have become a fundamental component of the digital asset ecosystem. With the combined market capitalization exceed- ing $300 billion as of year 2025, these assets serve as a critical bridge between the traditional financial system and the decentralized finance (DeFi) sector [1]. They function as the primary unit of account, a medium of exchange for 24/7 global settlement, and a core form of collateral for decentralized lending and derivatives protocols. The perceived stability of these assets underpins the valuation and liquidity of thousands of other digital assets. However, this reliance introduces significant risk as these stablecoins are not immune to periods of market stress and can ‚Äúde-peg‚Äù from their $1 target, as evidenced by the acute de-pegging of USDC in ARBITRAGEURS RETAIL TRADERS TREASURY CEXDEXEXCHANGESMINTREDEEMBUY/SELLBUY/SELL1:1 ISSUANCE/NEW ASSETS LIQUIDITY/ PRICE DISCOVERY Fig. 1: Market structure for fiat-collateralized stablecoins. The primary market involves the treasury and arbitrageurs for 1:1 minting and redeeming, while the secondary market includes e...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Autonomous Market Intelligence: Agentic AI Nowcasting Predic.pdf
Content_Extract: Abstract Can fully agentic AI nowcast stock returns? We deploy a state-of-the-art Large Language Model to evaluate the attractiveness of each Russell 1000 stock daily, starting from April 2025 when AI web interfaces enabled real-time search. Our data contribution is unique along three dimensions. First, the nowcasting framework is completely out-of-sample and free of look-ahead bias by construction: predictions are collected at the current edge of time, ensuring the AI has no knowledge of future outcomes. Second, this temporal design is irreproducible‚Äîonce the information environment passes, it can never be recreated. Third, our framework is 100% agentic: we do not feed the model news, disclosures, or curated text; it autonomously searches the web, filters sources, and synthesises information into quantitative predictions. We find that AI possesses genuine stock selection ability, but only for identifying top winners. Longing the 20 highest-ranked stocks generates a daily Fama-French five-factor plus momentum alpha of 18.4 basis points and an annualised Sharpe ratio of 2.43. Critically, these returns derive from an implementable strategy trading highly liquid Russell 1000 constituents, with transaction costs representing less than 10% of gross alpha. However, this predictability is highly concentrated: expanding beyond the top tier rapidly dilutes alpha, and bottom-ranked stocks exhibit returns statistically indistinguishable from the market. We hypothesise that this asymmetry reflects online information structure: genuinely positive news generates coherent signals, while negative news is contaminated by strategic corporate obfuscation and social media noise. Keywords: Agentic AI, Large Language Models, Stock Return Nowcasting and Predictabil- ity, Look-Ahead Bias ‚àóThis draft is preliminary. We thank James Dow, Vikas Agarwal, Juhani Linnainmaa, and Wei Jiang for helpful comments. Results are continuously updated athttps://github.com/mapledust0/ AI-Stock-Nowcasting/. E-mails: Zefeng Chen (zefengchen@gsm.pku.edu.cn) and Darcy Pu (dar- cypu@pku.edu.cn). Errors are our own. 1 arXiv:2601.11958v1  [q-fin.GN]  17 Jan 2026 1 Introduction Can fully agentic AI synthesise complex, real-time information to generate actionable insights? We address this question by exploiting financial markets as a uniquely demanding laboratory. The ‚Äúefficient markets hypothesis‚Äù posits that prices rapidly incorporate all available information, making the market an adversarial environment where systematic outperformance is extraordinarily difficult. In this context, we deploy a state-of-the-art LLM as an autonomous investment analyst. Starting in April 2025, on each trading day, we task the model with evaluating the entire Russell 1000 universe. For each stock, it autonomously searches the live web, synthesises disparate information sources, and produces a quantitative attractiveness score between -5 and 5 to rank investment oppor- tunities. If the AI generates profitable trad...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_ProbFM: Probabilistic Time Series Foundation Model with Unce.pdf
Content_Extract: Abstract Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: cur- rent approaches either rely on restrictive distributional as- sumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs em- ploy sophisticated techniques such as mixture models, Stu- dent‚Äôs t-distributions, or conformal prediction, they fail to ad- dress the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer- based probabilistic framework,ProbFM(probabilistic foun- dation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncer- tainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantifica- tion approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic meth- ods: DER, Gaussian NLL, Student‚Äôs-t NLL, Quantile Loss, and Conformal Prediction. This design isolates the contribu- tion of different uncertainty quantification strategies while maintaining architectural consistency, providing clean evi- dence for DER‚Äôs effectiveness before integration into more complex architectures. Evaluation on cryptocurrency return forecasting demon- strates that DER maintains competitive forecasting accu- racy while providing explicit epistemic-aleatoric uncertainty decomposition. We demonstrate practical value through uncertainty-aware trading strategies, showing how epistemic- aleatoric decomposition enables effective risk management by filtering high-uncertainty predictions. This work establishes both an extensible framework for prin- cipled uncertainty quantification in foundation models and empirical evidence for DER‚Äôs effectiveness in financial ap- plications. 1 Introduction The emergence of Time Series Foundation Models (TSFMs) has advanced forecasting capabilities across domains, but a critical gap remains in uncertainty quantification for high- stakes applications. Current TSFMs‚Äîdespite employing so- phisticated techniques‚Äîfail to provide the principled uncer- tainty decomposition and calibration guarantees required for effective decision-making in quantitative finance. Current state-of-the-art TSFMs employ various sophisticated tech- niques. For example, Toto (Cohen et al. 2024) uses Student- T Mixture Models for observability data, MOIRAI (Woo et al. 2024) implements four-com...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Directional Liquidity and Geometric Shear in Pregeometric Or.pdf
Content_Extract: Directional Liquidity and Geometric Shear in Pregeometric Order Books Jo√£o P. da Cruz The Quantum Computer Company, Lisbon, Portugal ‚àó and Center for Theoretical and Computational Physics, Lisbon, Portugal (Dated: January 28, 2026) We introduce a structural framework for the geometry of financial order books in which liquidity, supply, and demand are treated as emergent observables rather than primitive market variables. The market is modeled as a relational substrate without assumed metric, temporal, or price coordinates. Observable quantities arise only through observation, implemented here as a reduction of relational degrees of freedom followed by a low-dimensional spectral projection. A one-dimensional projection induces a price-like coordinate and a projected liquidity density around the mid price, from which bid and ask sides emerge as two complementary restrictions. We show that directional liquidity im- balances decompose naturally into a rigid drift of the projected density and a geometric shear mode that deforms the bid‚Äìask structure without inducing price motion. Under a minimal single-scale hypothesis, the shear geometry constrains the projected liquidity to a gamma-like functional form, appearing as an integrated-gamma profile in discrete data. Empirical analysis of high-frequency Level II data across multiple U.S. equities confirms this geometry and shows that it outperforms standard alternative cumulative models under explicit model comparison and residual diagnostics. Keywords: order book geometry, emergent observables, pregeometric models, spectral graph methods, liq- uidity asymmetry, financial markets I. INTRODUCTION The structure of liquidity in financial order books has been a central object of study in market microstructure for more than two decades. Empirical investigations of high-frequency data have revealed remarkably robust regularities in the shape of order books, including con- vex liquidity profiles near the mid price, heavy tails at l...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Learning Market Making with Closing Auctions.pdf
Content_Extract: Abstract In this work, we investigate the market-making problem on a trading session in which a continuous phase on a limit order book is followed by a closing auction. Whereas standard optimal market-making models typically rely on terminal inventory penalties to manage end-of-day risk, ignoring the significant liquidity events available in closing auctions, we propose a Deep Q-Learning framework that explicitly incorporates this mechanism. We introduce a market-making framework designed to explicitly anticipate the closing auction, continuously refining the projected clearing price as the trading session evolves. We develop a generative stochastic market model to simulate the trading session and to emulate the market. Our theoretical model and Deep Q-Learning method is applied on the generator in two settings: (1) when the mid price follows a rough Heston model with generative data from this stochastic model; and (2) when the mid price corresponds to historical data of assets from the S&P 500 index and the performance of our algorithm is compared with classical benchmarks from optimal market making. Key words:optimal market making, auction trading, reinforcement learning, Markov Decision Process, Deep Q-Learning, regret analysis. 1 Introduction 1.1 Reinforcement learning and market making Market making is a cornerstone of modern electronic financial markets, providing liquidity by continuously posting buy and sell quotes while managing inventory and adverse selection risk with different participants having diverse objectives. Since the seminal work of Avellaneda and Stoikov [4] and its extension to explicit solutions in [30], optimal market making has been studied extensively through stochastic control frameworks, leading to tractable strategies that balance expected profit against inventory risk under stylized assumptions on order flow dynamics and price evolution. We refer to [16] for a review of the literature on market making and high- frequency trading and to [5, 6] for recent advances in the topic. These models, however, typically rely on parametric assumptions that are difficult to validate empirically and may fail to adapt to the non-stationarity and strategic complexity of real-world markets. The rapid growth of electronic trading and data availability combined with AI raising influence in financial industry has motivated the use of reinforcement learning as a flexible, data-driven alternative to classical control methods. Reinforcement learning allows a market maker to learn optimal quoting policies directly from interaction with the market, without requiring full knowl- edge of the underlying dynamics. Recent studies have demonstrated the promise of RL in market ‚àójulius.graf@berkeley.edu ‚Ä†mastrolia@berkeley.edu 1 arXiv:2601.17247v1  [q-fin.TR]  24 Jan 2026 making and related problems, showing improved adaptability to complex market conditions, la- tent regimes, and evolving order flow patterns. The seminal article [50] introduces a Q...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_An Explainable Market Integrity Monitoring System with Multi.pdf
Content_Extract: Abstract AbstractMarket manipulation remains difficult to detect in practice because the strongest surveillance systems rely on proprietary order-book data and opaque models, limiting re- producibility and independent validation. We presentAIMM-X, an explainable market- integrity monitoring framework that uses onlypublicly accessible signals‚Äîstandard OHLCV price/volume data combined with multi-source attention proxies‚Äîto surfacesus- picious windows: contiguous time intervals where returns, volatility, and public attention jointly deviate from their historical baselines. AIMM-X applies transparent statistical scor- ing andhysteresis-based segmentationto generate candidate windows, then ranks them using an interpretableIntegrity Scorethat decomposes into factor contributions (œï1‚Äìœï6), enabling auditability and analyst review. In a one-year demonstration on a 24-ticker universe (daily bars, 2024), AIMM-X detects 233 suspicious windows and produces fully reproducible artifacts (window lists, rankings, factor attributions, and case study reports). The system is intentionally framed astriage rather than accusation: outputs represent evidence for human investigation, not proof of wrongdoing. We conclude by outlining a validation roadmap that combines retrospec- tive checks against publicly documented enforcement actions, expert annotation studies, and higher-frequency data integration to support operational deployment and responsible regulatory engagement. Keywords:Market manipulation detection, explainable AI, market microstructure, social attention signals, algorithmic surveillance, financial market integrity, reproducible research 1 Introduction Financial markets serve as the lifeblood of modern economies, channeling capital from savers to productive enterprises, enabling risk management, and facilitating price discovery. Yet these critical functions depend entirely on market integrity‚Äîthe confidence that prices reflect genuine supply and demand rather than ma- nipulation, fraud, or coordinated distortion. When manipulative practices go undetected, the consequences extend far beyond individual losses: investor confidence erodes, capital allocation becomes inefficient, and market participation shrinks (Kyle, 1985; Allen & Gale, 1992; Fischel & Ross, 1991). The challenge of detecting market manipulation has intensified dramatically in recent years. Modern markets operate at speeds measured in microseconds, with algorithmic trading accounting for the majority of volume in major exchanges. Retail investors now coordinate through social media platforms, creating attention- driven volatility spikes that can overwhelm traditional surveillance systems (Hu et al., 2021; Eaton et al., 2022). Meanwhile, sophisticated actors have adapted their techniques, making manipulation harder to distinguish from legitimate trading (Putnin≈°, 2012; Scopino, 2015). 1 arXiv:2601.15304v1  [q-fin.RM]  10 Jan 2026 Explainable Market Integrity Monitoring via Multi-Source Attention ...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Bayesian Robust Financial Trading with Adversarial Synthetic.pdf
Content_Extract: Abstract Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes‚Äîe.g., monetary policy updates or unanticipated fluctua- tions in participant behavior. We identify two core challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically inte- grates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesiz- ing data with faithful temporal, cross-instrument, and macro corre- lations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent‚Äîguided by a quantile belief network‚Äîmaintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabi- lizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our frame- work outperforms 9 state-of-the-art baselines. In extreme events like the COVID pandemic, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and rapidly shifting market dynamics. ‚àóBoth authors contributed equally to this research. ‚Ä†Corresponding Author. CCS Concepts ‚Ä¢Computing methodologies ‚Üí Artificial intelligence;Dynamic programming for Markov decision processes. Keywords Quantitative Trading, Generative Model, Robust RL Resource Availability: The source code of this paper has been made publicly available at https:// github.com/XiaHaochong98/Bayesian-Robust-Financial-Trading-with-Adversarial- Synthetic-Market-Data. 1 INTRODUCTION Algorithmic trading systems have become an essential component of financial markets, with reinforcement learning (RL) emerging as a promising method for making financial decisions [34]. However, while these systems often excel in learning from large volumes of historical data, they usually fail to maintain similar performance in out-of-sample data. This overfitting arises from the highly dy- namic nature of financial markets, where the testing dynamics diverge from the training dynamics, as illustrated in Fig 1, because markets are continually reshaped by shifting macroecono...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_History Is Not Enough: An Adaptive Dataflow System for Finan.pdf
Content_Extract: Abstract‚ÄîIn quantitative finance, the gap between training and real-world performance‚Äîdriven by concept drift and distri- butional non-stationarity‚Äîremains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra ‚ÄúHistory Is Not Enough‚Äù underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning‚Äìbased adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner‚Äìscheduler that employs gradient-based bi-level optimization to control the sys- tem. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecast- ing and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk- adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data. Index Terms‚ÄîAdaptive dataflow, workflow automation, finan- cial time-series, data augmentation, I. INTRODUCTION Machine learning techniques have been widely applied to quantitative finance research, encompassing tasks such as trading [1] and forecasting [2]. The success of machine learning in quantitative finance largely stems from its abil- ity to leverage the vast amount of financial data generated by markets [3]. However, the constantly changing dynamics of the market present significant challenges. A fundamental assumption in applying machine learning techniques is that the data is independent and identically distributed (i.i.d.). When this i.i.d. assumption does not hold true, machine learning models tend to overfit the training data, which in turn reduces their robustness when applied to unseen data. The financial market, shaped by complex trader interactions, naturally evolves, leading to concept drift‚Äî inconsistency in the joint probability distribution between timetandt+k asP t(X, Y)Ã∏=P t+k(X, Y), k >0, whereXis the feature ‚àóEqual contribution. ‚Ä†Corresponding author. andYis the target variable. For data-driven financial sys- tems, such drift is not only a modeling challenge but also a data management problem: pipelines trained on static data lack mechanisms to adapt to distributional shifts over time. Addressing this gap requires an adaptive dataflow capable of managing evolving financial data streams. Data manipulation techniques, such as data augmentation, play a crucial role in enhancing the robustness and gen- eralization of machine learning models by expanding the...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Explainable Patterns in Cryptocurrency Microstructure.pdf
Content_Extract: Abstract We document stable cross-asset patterns in cryptocurrency limit-order-book mi- crostructure: the same engineered order book and trade features exhibit remarkably similar predictive importance and SHAP dependence shapes across assets spanning an order of magnitude in market capitalization (BTC, LTC, ETC, ENJ, ROSE). The data covers Binance Futures perpetual contract order books and trades on 1-second frequency starting from January 1st, 2022 up to October 12th, 2025. Us- ing a unified CatBoost modeling pipeline with a direction-aware GMADL objective and time-series cross validation, we show that feature rankings and partial effects are stable across assets despite heterogeneous liquidity and volatility. We connect these SHAP structures to microstructure theory (order flow imbalance, spread, and adverse selection) and validate tradability via a conservative top-of-book taker backtest as well as fixed depth maker backtest. Our primary novelty is a robust- ness analysis of a major flash crash, where the divergent performance of our taker and maker strategies empirically validates classic microstructure theories of adverse selection and highlights the systemic risks of algorithmic trading. Our results sug- gest a portable microstructure representation of short-horizon returns and motivate universal feature libraries for crypto markets. 1 Introduction Financial market microstructure studies how trading, information, and liquidity provision jointly determine short-horizon price dynamics. Across asset classes, a robust set of ‚àóbbieganowsk@student.uw.edu.pl, https://orcid.org/0009-0003-8671-8266 ‚Ä†rslepaczuk@wne.uw.edu.pl, https://orcid.org/0000-0001-5227-2014 1 arXiv:2602.00776v1  [q-fin.TR]  31 Jan 2026 features (order flow imbalance, bid‚Äìask spreads, depth, and trade arrival patterns) has been shown to explain a substantial fraction of return variation at very short horizons. Cryptocurrencies offer a unique space for testing whether these features are universal: assets vary widely in capitalization and liquidity, yet they are transacted through similar continuous double-auction mechanisms with transparent limit order books. This paper advances the hypothesis that short-horizon return predictability in crypto admits a universal representation. Concretely, we posit that a compact set of features engineered from the top of the order book and contemporaneous trade flow exhibits similar predictive importance and functional dependence shapes across assets spanning an order of magnitude in market capitalization. We evaluate this claim with flexible non-linear models and model-agnostic explanation tools designed for tabular data. Our empirical strategy is deliberately simple and portable. Given tick-level orderbook and trades data for five cryptocurrencies, we engineer a unified feature library capturing spreads and relative prices, order flow imbalances, and deviations of buy/sell volume- weighted average prices from the mid. We train gradient-boosted...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Resisting Manipulative Bots in Meme Coin Copy Trading: A Mul.pdf
Content_Extract: Abstract Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market‚Äôs extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from na√Øve copiers at scale. Despite its prevalence, bot-driven manip- ulation remains largely unexplored, and no robust defensive frame- work exists. We propose a manipulation-resistant copy-trading sys- tem based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wal- let selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading. CCS Concepts ‚Ä¢Computing methodologies ‚Üí Artificial intelligence;‚Ä¢Ap- plied computing‚ÜíEconomics. 1 Introduction While manipulative tactics such as rat trading are well-trodden ruses in traditional financial markets, these old-fashioned ploys have now become bot-driven and prey on na√Øve copy traders in a new hunting ground: the meme coin market. The launch of the $TRUMP meme coin on January 17, 2025 ignited a surge of speculation in the meme coin market, drawing millions of traders. However, many of these entrants lack prior trading experience. To ‚àóYebo Feng is the corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. lower the barrier to participation, meme coin tracking platforms such as GMGN introduced copy trading, an automated, one-click feature that allows users to replicate other wallets. In practice, copiers attempt to identify so-called smart money‚Äîwallets per- ceived to possess insider knowledge, trading expertise, and consis- tent profitability‚Äîand replicate their trades [2, 26]. However, the characteristics of the meme coin market, most notably extreme illiquidity and high volatility, combined with the na√Øvet√© of copy traders, create an ideal environment for exploitation. Sophisticated adversaries deploy automated trading bots to sys- tematically manipulate prices and induce predictable copy-trading behavior (see ¬ßA.1). In particular, some key opinion leaders (KOLs) featured on tracking platforms may abuse their perceived credi- bility by coordinating multiple bots to accumulate positions early at low prices, artificially inflate prices while concealing their true exposure...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Intraday Limit Order Price Change Transition Dynamics Across.pdf
Content_Extract: Abstract A quantitative understanding of the stochastic dynamics in limit order price changes is essential for meaningful advances in market microstructure research and effective execution strategy de- sign. This paper presents the first comprehensive empirical analysis of intraday limit order price change transition dynamics, treating ask and bid orders separately across different market capital- ization tiers. Using high-frequency tick data from NASDAQ100 stocks, we employ a discrete-time Markov chain framework to analyze the evolution of price adjustments throughout the trading day. We categorize consecutive price changes into nine distinct states and estimate transition probability matrices (TPMs) for six intraday intervals across High (HMC), Medium (MMC), and Low (LMC) mar- ket capitalization stocks. Elememt-wise comparison of TPMs reveals systematic intraday patterns: price inertia i.e. self-transition probability, peaks during opening and closing hours, stabilizing at lower levels during midday. A pronounced capitalization gradient is also observed:HMCstocks exhibit the strongest price inertia, whileLMCstocks demonstrate significantly lower stability and pronounced bid-ask spread. Markov chain metrics, including spectral gap, entropy rate, and mean recurrence times quantify these dynamics. Clustering analysis identifies three distinct temporal phases on the bid side ‚Äì Opening, Midday, and Closing and four phases on the ask side ‚Äì Open- ing, Midday, Pre-Close, and Close, indicating that sellers initiate end-of-day positioning strategies earlier than buyers. Stationary distributions reveal that limit order dynamics are predominantly characterized by neutral and mild price changes. Furthermore, Jensen-Shannon divergence com- puted between stationary distributions across time-intervals confirms the closing hour as the most distinct phase, with capitalization modulating the intensity of temporal contrasts and the degree of bid-ask asymmetry. These findings advance the understanding of evolving intraday limit order pricing behavior, offering direct applications for capitalization-aware and time-adaptive execution algorithms and risk management frameworks. Keywords:Limit orders, Markov chains, Intraday dynamics, Market capitalization, Bid-ask asymmetry, Clustering, Jensen-Shannon divergence Email addresses:salamrabindrajit@gmail.com(Salam Rabindrajit Luwang), arXiv:2601.04959v1  [q-fin.ST]  8 Jan 2026 1. Introduction In equity markets, limit orders represent a critical component of the trading ecosystem, allow- ing traders to specify precise execution prices while contributing to market liquidity [1, 2, 3, 4]. Unlike market orders that execute immediately at prevailing prices, limit orders remain active in the order book until matched or canceled. This creates a dynamic environment in which the intraday evolution of limit order prices is driven by traders‚Äô continuous strategic adjustments to liquidity conditions, information arrival, and prevaili...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Diverse Approaches to Optimal Execution Schedule Generation.pdf
Content_Extract: Abstract We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computationalresourcesperbehaviouralcellmayberequiredforrobustspecialistdevelopment across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities withR2 > 0.02out-of-sample. Within this environment, two Proximal Policy Optimization architectures‚Äîboth MLP and CNN feature extractors‚Äîdemonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods. KeywordsOptimal Execution ¬∑Reinforcement Learning¬∑Market Impact¬∑Transient Impact¬∑Quality- Diversity¬∑MAP-Elites¬∑Algorithmic Trading¬∑Robotics ‚àóThe views, opinions and conclusions expressed here are solely those of the authors and do not necessarily reflect the views or policies of the Bank of America, or any other institution with which the authors are affiliated. No responsibility should be attributed to those institutions . This article has not been reviewed, approved, or endorsed by the authors‚Äô employers or any affiliated organizations arXiv:2601.22113v2  [q-fin.TR]  30 Jan 2026 Diverse Approaches to Optimal ExecutionA Preprint Optimal Execution Empirical Impact Models (e.g., Bouchaud Propagator) Calibrated decay kernel, tran- sient impact, concave scaling Classical Models (e.g., Almgren‚ÄìChriss) Mean‚Äìvariance optimi- sation, constant volatil- ity, stationary impact Reinforcement Learn- ing Approaches Agent‚Äìenvironment loop, adaptive poli- cies, simulation-trained This Work: Novel RL Optimal Execution PPO, MAP-Elites, High-fidelity Gymnasium environment, calibrated transient impact, realistic order generation, vectorised simulation Figure 1: Evolution of optimal execution approaches: from classical models to empirical impact models to reinforcement learning, with this work positioned at the intersection of empirically calibrated models and RL methods. 1 Introduction Optimal execution (OE) is a central problem in algorithmic trading, influencing approximately a trillion dollars of daily turnove...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Fast Times, Slow Times: Timescale Separation in Financial Ti.pdf
Content_Extract: Abstract Financial time series exhibit multiscale behavior, with interaction between multiple processes operating on different timescales. This paper introduces a method for separating these processes using variance and tail stationarity criteria, framed as generalized eigenvalue problems. The approach allows for the identification of slow and fast components in asset returns and prices, with applications to parameter drift, mean reversion, and tail risk management. Empirical examples using currencies, equity ETFs and treasury yields illustrate the practical utility of the method. 1 Stationarity in Finance The study of multiscale processes is well-established in physics, with notable examples ranging from fluid dynamics to protein folding, where fast and slow dynamics interact [1]. The typical physical example involves fast oscillations inside a slowly moving envelope, where the cumulative effect of fast oscillations drives changes in the envelope [1, 2]. In finance, we can see multiple examples of similar multiscale behavior. For example, the S&P 500 exhibits mean reversion over miliseconds (due to low-latency arbitrage between e-mini S&P futures, ETFs and stock baskets) and over years (relative to gold or bonds). A notable example of feedback from high frequency to macro is of course the Flash Crash of 2010, where multiple feedback loops in high and mid frequency trading lead to the S&P 500 losing 9% of its value, only to recover later [3]. Therefore identification of such processes operating on different timescales, and the nature of their interaction, is of clear importance both to market practitioners and to economists. The key questions in this context are: ‚Ä¢Can we separate these processes? ‚Ä¢Can we estimate their relaxation timescales? ‚Ä¢Can we assess stationarity in a meaningful way? This paper addresses these questions by introducing a framework for timescale separation in financial time series, building on methods from signal processing and variational principles. On the trading side, financial strategies rely on calibrated parameters that drift over time, leading to strategy degradation. Two types of stationarity are relevant in this context: ‚Ä¢Stationarity of Returns: Implies stable distribution of returns, leading to stable parameters. ‚Ä¢Stationarity of Prices: Implies mean reversion in price levels. The same mathematical tools can be applied to both, though their interpretations differ. 2 Types of Stationarity The textbook definition of stationary processes involves the stationarity of the underlying distribution. 1 arXiv:2601.11201v1  [q-fin.PM]  16 Jan 2026 While this definition is attractive, it does not lend itself straightforwardly to data driven analysis. Primarily, this is due to the fact that we only have a finite amount of data available, and that any estimation of the underlying distribution is therefore fuzzy and imprecise. Rather than aiming for the entire distribution, it is then more practical to focus on particular aspect...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Market Making and Transient Impact in Spot FX.pdf
Content_Extract: Abstract Dealers in foreign exchange markets provide bid and ask prices to their clients at which they are happy to buy and sell, respectively. To manage risk, dealers can skew their quotes and hedge in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal market making with execution has previously been addressed within the Almgren-Chriss market impact model, which includes instantaneous and permanent components. However, there is overwhelming empirical evidence of the transient nature of market impact, with instantaneous and permanent impacts arising as the two limiting cases. In this note, we consider an intermediate scenario and study the interplay between risk management and impact resilience. Keywords:Market Making; Stochastic Optimal Control; Market Impact; Algorithmic Trading. 1 Introduction Foreign exchange (FX) markets continue to operate largely on an OTC (over-the-counter) basis where dealers offer bid and ask prices to their clients bilaterally or via aggregators. The dealer aims to make a spread but must manage inventory risk arising from the asynchronicity between client flow and market volatility. To attract risk-reducing flows, the dealer can skew their quotes, but may ultimately hedge excess inventory in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal strategies balancing spread capture and risk management have been a subject of recent active research (Avellaneda and Stoikov, 2008; Gu√©antet al., 2013; Carteaet al., 2014; Butz and Oomen, 2019; Bergault and Gu√©ant, 2021; Barzykinet al., 2023, 2025). In particular, the internalization versus externalization dilemma has been in focus (Butz and Oomen, 2019), and the Almgren-Chriss (2001) model with instantaneous cost and linear permanent impact has been employed to describe execution in the interbank market (Barzykin et al., 2023). One of the conclusions of this research is the existence of a pure internalization zone where it is not optimal for the dealer to execute. The inventory threshold, beyond which the dealer will begin to execute, depends on risk aversion, volatility, client flow, and, importantly, transaction costs and market impact. There is overwhelming empirical evidence for the transient nature of market impact, with the propagator model of Bouchaudet al.(2018) capturing its essence. A particular case of exponential relaxation in the Obizhaeva-Wang model (2013) is accompanied by a clear explanation in terms of the limit order book‚Äôs resilience. At the same time, large order execution demonstrates a universal square root dependence on the total executed quantity (T√≥thet al., 2011; Sato and Kanazawa, 2025). Both effects are found to be ‚àóEmail:alexander.barzykin@hsbc.com 1 arXiv:2601.13421v1  [q-fin.TR]  19 Jan 2026 very important in optimal execution (Neuman and Vo√ü, 2022; Heyet al., 2023; Webster, 2023). So, why Almgren-Chriss? First of all, the square root law ...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Manipulation in Prediction Markets: An Agent-based Modeling .pdf
Content_Extract: Abstract Prediction markets mobilize financial incentives to forecast binary event outcomes through the aggregation of dispersed beliefs and heterogeneous information. Their growing popularity and demonstrated predictive accuracy in political elections have raised speculation and concern regarding their susceptibility to manipulation and the potential consequences for democratic processes. Using agent-based simulations combined with an analytic characterization of price dynamics, we study how high-budget agents can introduce price distortions in prediction markets. We explore the persistence and stability of these distortions in the presence of herding or stubborn agents, and analyze how agent expertise affects market-price variance. Firstly we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting strategies in the market. The model exhibits stability across a broad parameter space, with complex agent behaviors and price interactions producing self-regulatory price discovery. Second, using this simulation framework, we investigate the conditions under which a highly resourced minority, or ‚Äúwhale‚Äù agent, with a biased valuation can distort the market price, and for how long. We find that biased whales can temporarily shift prices, with the magnitude and duration of distortion increasing when non-whale bettors exhibit herding behavior and slow learning. Our theoretical analysis corroborates these results, showing that whales can shift prices proportionally to their share of market capital, with distortion duration depending on non-whale learning rates and herding intensity. Keywords:prediction markets, agent-based model, betting markets, market manipulation 1. Introduction Prediction markets, where participants wager on binary outcomes, have been proposed as effective mechanisms for aggregating dispersed beliefs into accurate predic- tors in environments characterized by uncertainty and heterogeneous access to information [2, 44]. Their ap- peal rests on the idea that traders, motivated by finan- cial incentives, reveal private information through their willingness to buy or sell contracts corresponding to a particular outcome. Prediction market prices have been found to closely approximate the mean belief about an event‚Äôs proba- bility in theoretical settings with well-informed, con- strained and risk-adverse traders [21, 57]. In several real elections, prediction-market forecasts have matched or outperformed traditional polling or expert judgment [5, 12, 40, 55], although evidence is mixed [15, 28]. Posi- tive examples are sometimes attributed to the ‚Äúwisdom of crowds‚Äù, which relies on assumptions of indepen- dence, diversity, and the absence of dominant partic- ipants, conditions which are not always satisfied in practice [48]. ‚àóCorresponding autho...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Utility-Weighted Forecasting and Calibration for Risk-Adjust.pdf
Content_Extract: Abstract Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utility- weighted calibration reduces realised decision loss by over 30% relative to an uncalibrated baseline (t-stat -30.31) for loss differential and improves the Sharpe ratio from -3.62 to -2.29 during a drawdown regime. The mechanism is identified as a structural reduction in the frequency of binding constraints (from 16.0% to 5.1%), preventing the ‚Äùcorner solution‚Äù failures that characterize overconfident forecasts in high-friction environments. Keywords:calibration; probabilistic forecasting; decision loss; transaction costs; market impact; portfolio choice; financial econometrics. JEL:C10, C22, C53, G11, G17. 1 arXiv:2601.07852v1  [econ.EM]  9 Jan 2026 Contents 1 Introduction 4 1.1 Problem statement and motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Contribution and summary of results . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Positioning in financial econometrics . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 Economic environment, notation, and friction operator 8 2.1 Probability space, information, and outcomes . . . . . . . . . . . . . . . . . . . . 8 2.2 Predictive distributions as econometric objects . . . . . . . . . . . . . . . . . . . 9 2.3 Decisions and feasible set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4 Friction operator: costs and constraints . . . . . . . . . . . . . . . . . . . . . . . 10 2.5 Decision objective and decision loss . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3 Why accuracy is not the target: calibration and decision performance 14 3.1 Forecast metrics versus economic loss . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.2 Calibration concepts used in the paper . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3 Utility-weighted calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.4 Deriving the utility-weightœâin a canonical quadratic case . . . . . . . . . . . . . 16 4 Theory: dominance results under trading fricti...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_DeePM: Regime-Robust Deep Learning for Systematic Macro Port.pdf
Content_Extract: Abstract We proposeDeePM(DeepPortfolioManager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous ‚Äúragged filtration‚Äù problem via a Directed Delay(Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via aMacroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objectivewhere a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) ‚Äì awindow-robustutility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010‚Äì2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s ‚ÄúCTA (Commodity Trading Advisor) Winter‚Äù and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability. KeywordsSystematicMacro ¬∑PortfolioManagement ¬∑DeepLearning ¬∑Attention ¬∑GraphNeuralNetworks ¬∑Transaction Costs¬∑Robust Optimization¬∑Risk Measures 1 Introduction The central goal of systematic portfolio management is to construct asset allocations that generalize out-of-sample under heavy-tailed returns, regime shifts, and significant trading frictions. While classical mean‚Äìvariance optimization [Markowitz, 1952] provides a foundational framework, its practical deployment is plagued by ‚Äúerror maximization‚Äù [Michaud, 1989], where small estimation errors in covariance matrices lead to unstable, turnover-intensive portfolios. Consequently, modern approaches have increasingly pivoted toward machine learning pipelines. However, most existing methods adopt a disjoint two-stage approach ‚Äì forecasting returns first, then performing a portfolio construction step ‚Äì arXiv:2601.05975v1  [q-fin.TR]  9 Jan 2026 DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management which misaligns the training loss (Mean Squared Error) with the investor‚Äôs ultimate utility (Net Risk-Adjusted Return) [Gu et al., 2020, Elmachtoub and Grigas, 2022]. We proposeDeePM, aStructured,Risk-RobustDeep-learningPortfolioManager that learns a trading policy end-to-end. Unlike ‚Äúblack box‚Äù approaches that ...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Pregeometric Origins of Liquidity Geometry in Financial Orde.pdf
Content_Extract: abstract economic entities, and edges encode the possibility of interaction. Growth and reorganiza- tion proceed through inflationary updates that generate heterogeneous, hub-dominated structures without fine- tuning. Crucially, none of the standard market observ- ables are defined at this level. Observable quantities arise only through projection. By applying spectral embeddings based on the graph Laplacian, an observer assigns effective coordinates to the relational substrate. A one-dimensional projection induces a scalar coordinate naturally interpreted as price, while the distribution of projected density defines liquid- ity profiles. Successive projections of an evolving rela- tional system give rise to apparent time series, returns, and fluctuations, even though no fundamental time vari- able exists microscopically. Within this framework, supply and demand are not in- dependent behavioral curves but geometric branches of a single projected density. We show that, under minimal structural assumptions on regularity, vanishing liquidity at the mid, and exponential decay at large distances, the projected liquidity profiles necessarily adopt a gamma- like functional form. This result is structural and geo- metric in nature, and does not rely on equilibrium argu- ments, optimization principles, or agent-level behavior. arXiv:2601.17245v1  [q-fin.TR]  24 Jan 2026 2 We test these ideas empirically using high-frequency Level II data for several U.S. equities across different sec- tors. By focusing on short time windows, we extract in- stantaneous liquidity profiles relative to the mid price and show that their cumulative forms are well described by integrated gamma functions. The fitted parameters vary across assets and windows, indicating that they charac- terize local projected geometry rather than stationary market states. A minimal simulation of inflationary rela- tional dynamics reproduces the same functional structure in the absence of any market-specific assumptions. The goal of this work is not predictive. Instead, it is to demonstrate that standard order-book regularities can be understood as emergent geometric observables induced by projection. This perspective shifts the modeling focus from price dynamics to the geometry of observation and provides a unifying structural interpretation of liquidity in financial markets. II. RELATIONAL INFLATIONARY MODEL We introduce a minimal relational framework aimed at understanding how market observables arise without as- suming price, time, return, or risk as primitive variables. The construction is deliberatelypregeometric: no met- ric, ordering, or economic coordinate is postulated at the microscopic level. All quantities conventionally used to describe markets will be shown to emerge only through observation. A. Relational substrate The system is defined by a growing graphG= (V, E), where vertices represent economic entities (agents, venues, or abstract trading units) and edges encode the possibilit...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_A Formal Approach to AMM Fee Mechanisms with Lean 4.pdf
Content_Extract: Abstract Decentralized Finance (DeFi) has revolutionized financial markets by enabling complex asset- exchange protocols without trusted intermediaries. Automated Market Makers (AMMs) are a central component of DeFi, providing the core functionality of swapping assets of different types at algorithmically computed exchange rates. Several mainstream AMM implementations are based on the constant-product model, which ensures that swaps preserve the product of the token reserves in the AMM ‚Äî up to atrading feeused to incentivize liquidity provision. Trading fees substantially complicate the economic properties of AMMs, and for this reason some AMM models abstract them away in order to simplify the analysis. However, trading fees have a non-trivial impact on users‚Äô trading strategies, making it crucial to develop refined AMM models that precisely account for their effects. In this work, we extend a foundational model of AMMs by introducing a new parameter, the trading feeœï‚àà(0, 1], into the swap rate function. Fee amounts increase inversely proportional toœï. Whenœï= 1, no fee is applied and the original model is recovered. We analyze the resulting fee-adjusted model from an economic perspective. We show that several key properties of the swap rate function, including output-boundedness and monotonicity, are preserved. At the same time, other properties ‚Äî most notably additivity ‚Äî no longer hold. We precisely characterize this deviation by deriving a generalized form of additivity that captures the effect of swaps in the presence of trading fees. In particular, we prove that whenœï<1, executing a single large swap yields strictly greater profit than splitting the trade into smaller ones. Finally, we derive a closed-form solution to the arbitrage problem in the presence of trading fees and prove its uniqueness. All results are formalized and machine-checked in the Lean 4 proof assistant. 2012 ACM Subject ClassificationSoftware and its engineering‚ÜíFormal software verification Keywords and phrasesSmart contracts, Decentralized Finance, Verification, Blockchain Digital Object Identifier10.4230/OASIcs... FundingMassimo Bartoletti: Partially supported by project SERICS (PE00000014) and PRIN 2022 DeLiCE (F53D23009130001) under the MUR National Recovery and Resilience Plan funded by the European Union ‚Äì NextGenerationEU. Alberto Lluch-Lafuente: Partially supported by Copenhagen Fintech project CODeM - Challenges and Opportunities of Defi Models, special thanks to Troels Damgaard, Espen H√∏jsgaard, and Lasse Nisted for fruitful discussions on DeFi fee mechanisms. 1 Introduction Decentralized Finance (DeFi) has transformed the financial landscape by enabling peer-to- peer transactions without a central intermediary. One of the core mechanisms of many DeFi applications is the Automated Market Makers (AMMs), which enable token swaps through algorithmic pricing mechanisms. Among the various kinds of AMMs, the constant-product Market Makers (CPMMs) are the most used one...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_PredictionMarketBench: A SWE-bench-Style Framework for Backt.pdf
Content_Extract: Abstract Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introducePredictionMarketBench, a SWE- bench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshi- based episodes spanning cryptocurrency, weather, and sports. We report baseline results for a RandomAgent, a tool-calling LLM agent (gpt-4.1-nano), and a classic Bollinger Bands mean-reversion strategy, illustrating that naive activity can un- derperform due to transaction costs and settlement losses, while fee-aware algorithmic alphas can remain competitive in volatile episodes. 1 Introduction Prediction markets are exchange-traded contracts whose prices can often be interpreted as collective probabilistic forecasts about real-world events. Under appropriate design and participation, they can aggregate dispersed information and yield accurate predictions [17, 1]. Empirically, election prediction markets such as the Iowa Electronic Markets have been shown to outperform traditional polls at longer horizons [4], while the broader literature highlights the importance of market design and susceptibility to manipulation in determining forecast quality [3]. A key practical feature of modern prediction markets is that liquidity may be thin or frag- mented across many related questions. Automated market makers and cost-function market mak- ers (e.g., LMSR) provide continuous pricing under sparse order flow and have become foundational to the study of prediction market design [9]. At the same time, many venues (including regu- lated exchanges) operate as electronic limit-order markets, where execution outcomes depend on microstructure: spreads, queue position, and fees [16]. These properties make prediction markets a compelling but challenging environment for algo- rithmic and AI trading agents. Agents must reason under discrete settlement payoffs, horizon- dependent risk, and high transaction costs; furthermore, naive backtests can be misleading due to selection effects and overfitting [2]. Prior work has studied prediction markets through agent- based modeling and trader behavior, emphasizing how heterogeneous beliefs and bounded-rational strategies map into aggregate price dynamics [18]. ‚àóOddpool: oddpool.com|Code: PredictionMarketBench. 1 arXiv:2602.00133v1  [q-fin.ST]  28 Jan 2026 Recently, LLM-based and agentic AI sy...
--- PAPER END ---

--- PAPER START ---
Category: microstructure
File: 2026_Design and Empirical Study of a Large Language Model-Based M.pdf
Content_Extract: Abstract This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLMs) driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents‚Äîannouncement, event, price momentum, and market‚Äîeach conducting analysis from different dimensions; then the prediction agent integrates these multi-source sig- nals to output directional probability distributions across multiple time horizons; then the decision agent generates discrete position adjustment signals based on the pre- diction results and risk control constraints, thereby forming a closed loop of ‚Äùanaly- sis‚Äìprediction‚Äìdecision‚Äìexecution.‚Äù This study further compares two prediction model path- ways: for the prediction agent, directly calling the general-purpose large model DeepSeek- R1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from October 2024 to October 2025, both agent-based strategies significantly outperformed the buy-and-hold benchmark in terms of cumulative return, Sharpe ratio, and maximum drawdown. The results indicate that the multi-agent framework can effectively enhance the risk-adjusted return of REITs trading, and the fine-tuned small model performs close to or even better than the general- purpose large model in some scenarios. The project code and data are open-sourced; details can be found at:https://github.com/adennng/REITs-MAS-LLM 1 Introduction In recent years, the application of large language models (LLMs) in the financial field has gradu- ally expanded from ‚Äùtext understanding‚Äù to the complete chain of ‚Äùreasoning‚Äìdecision‚Äìexecution.‚Äù One line of research focuses on the construction and alignment of financial domain-specific LLMs, improving model reliability in financial Q&A, information extraction, and decision reasoning through financial corpus pre-training, instruction fine-tuning, and preference alignment. For example, works such as FinGPT, BloombergGPT, and PIXIU are frequently used as represen- tative base models and evaluation benchmarks for financial LLMs [8, 4, 7]. At the same time, an increasing number of studies emphasize that trading tasks require not only language un- derstanding but also the formation of auditable, executable decision logic amidst multi-source information, market noise, and uncertainty. Therefore, ‚Äùmulti-agent collaborative structures‚Äù and ‚Äùpost-training optimization (fine-tuning/RL)‚Äù have gained significant attention [6, 3]. 1.1 Structured Trading Systems with Multi-Agents In real trading processes, information sources are naturally diverse (news, announcements, macroeconomics, technical indicators, etc.). A single model often struggles to cover the en- tire chain of ‚Äùretrieval‚Äìanalysis‚Äìattribution‚Äìrisk control‚Äìexecution‚Äù simultaneously. Therefore, multi-agent systems (MAS) have become an important form for the systematic im...
--- PAPER END ---

--- PAPER START ---
Category: futures_arb
File: 2018_Optimal Dynamic Basis Trading.pdf
Content_Extract: Abstract We study the problem of dynamically trading a futures contract and its underlying asset under a stochastic basis model. The basis evolution is modeled by a stopped scaled Brownian bridge to account for non-convergence of the basis at maturity. The optimal trading strategies are determined from a utility maximization problem under hyperbolic absolute risk aversion (HARA) risk preferences. By analyzing the associated Hamilton-Jacobi-Bellman equation, we derive the exact conditions under which the equation admits a solution and solve the utility maximization explicitly. A series of numerical examples are provided to illustrate the optimal strategies and examine the eÔ¨Äects of model parameters. Keywords: futures stochastic basis cash and carry scaled Brownian bridge risk aversion JEL ClassiÔ¨ÅcationC41 G11 G12 1 Introduction Basis trading, also known ascash-and-carry trading in the context of futures contracts, is a core strategy for many speculative traders who seek to proÔ¨Åt from anticipated convergence of spot and futures prices. The practice usually involves taking a long position in the under-priced asset and a short position in the over-priced one, and closing the positions when convergence occurs. In reality, however, basis trading is far from a riskless arbitrage. Unexpected changes in market factors such as interest rate, cost of carry, or dividends can diminish proÔ¨Åtability. Moreover, market frictions, such as transaction costs and collateral payments, can turn seemingly certain arbitrage opportunities into disastrous trades. It is also possible that the basis does not converge at maturity. This non-convergence phenomenon was commonly observed in the grains markets. As reported in Irwin et al. (2011), Adjemian et al. (2013), and Garcia et al. (2015), for most of 2005-2010 futures contracts expired up to 35% above the spot price. As a result, some cash-and-carry traders may choose to close their positions prior to maturity to limit risk exposure. Early works on pricing of futures contracts, such as Cox et al. (1981) and Modest and Sundaresan (1983), established no-arbitrage relationships between the spot price and associated futures prices. Assuming imperfections, such as transaction cost, these relationships take the form of pricing bounds, which can be  Applied Mathematics Department, University of Washington, Seattle WA 98195.e-mail: bahmang@uw.edu y Applied Mathematics Department, University of Washington, Seattle WA 98195.e-mail: timleung@uw.edu 1 arXiv:1809.05961v3  [q-fin.PM]  25 May 2019 used for identifying proÔ¨Åtable trades. Among related studies on basis trading, Brennan and Schwartz (1988) and Brennan and Schwartz (1990) assumed that the basis of an index futures follows a scaled Brownian bridge and that trading the assets is subject to position limits and transaction costs. They calculated the value of the embedded timing options to trade the basis, and used the option prices to devise open-hold-close strategies involving t...
--- PAPER END ---

--- PAPER START ---
Category: futures_arb
File: 2020_An end-to-end data-driven optimisation framework for constra.pdf
Content_Extract: An end-to-end data-driven optimisation framework for constrained trajectories Florent Dewez Inria, Lille - Nord Europe Research centre, France Benjamin Guedj Inria, Lille - Nord Europe Research centre, France and Centre for ArtiÔ¨Åcial Intelligence, Department of Computer Science, University College London, United Kingdom Arthur Talpaert Inria, Lille - Nord Europe Research centre, France Vincent Vandewalle Inria, Lille - Nord Europe Research centre and Universit¬¥ e de Lille, France Many real-world problems require to optimise trajectories under constraints. Classical approaches are based on optimal control methods but require an ex- act knowledge of the underlying dynamics, which could be challenging or even out of reach. In this paper, we leverage data-driven approaches to design a new end-to-end framework which is dynamics-free for optimised and realistic trajectories. We Ô¨Årst decompose the trajectories on function basis, trading the initial inÔ¨Ånite dimension problem on a multivariate functional space for a parameter optimisation problem. A maximum a posteriori approach which incorporates information from data is used to obtain a new optimisation prob- lem which is regularised. The penalised term focuses the search on a region centered on data and includes estimated linear constraints in the problem. We apply our data-driven approach to two settings in aeronautics and sailing routes optimisation, yielding commanding results. The developed approach has been implemented in the Python library PyRotor. 1 arXiv:2011.11820v2  [stat.AP]  5 Feb 2021 Keywords: Statistical modelling; Functional data; Constrained optimisa- tion Contents 1. Introduction 2 2. An end-to-end optimisation workÔ¨Çow based on observed trajectories 4 2.1. Admissible trajectories modelling . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2. Projection for a Ô¨Ånite-dimensional optimisation problem . . . . . . . . . . . 5 2.3. Reference trajectories modelling . . . . . . . . . . . . . . . . . . . . . . ....
--- PAPER END ---

--- PAPER START ---
Category: futures_arb
File: 2020_The Fair Basis: Funding and capital in the reduced form fram.pdf
Content_Extract: Abstract A negative basis trade enters a long bond position and buys protection on the issuer of the bond through credit default swap (CDS), aiming at arbitrage profit due to the bond- CDS basis . To classic reduced form model theorists, the existence of the basis is  an abnormality or merely liquidity noise. Such a view, however, fails to explain large basis trading losses incurred during the financial crisis. Employing a bond continuously hedged by CDS under a dyna mic spread model with bond repo financing, we find th at there is unhedged and unhedgeable residual jump to default risk that can‚Äôt be diversified because of credit correlation. An economic capital approach has to apply and a charge on the use of capital follows. Together with the hedge funding cost, it allow s us to better understand the basis‚Äôs economics and to predict its fair level. Keywords: CDS-bond basis, negative basis, reduced form model, default risk, hedging error, FVA, KVA. 1. Introduction A negative basis trade enters a long bond position and buys protection on the issuer of the bond through credit default swap (CDS) , aiming at profit due on a positive carry when the bond‚Äôs funding cost is lower than the bond -CDS basis. Prior to the 2007 -2009 financial crisis, basis trading was a popular credit arbitrage trading strategy that excited many hedge funds and banks‚Äô prop trading desks. At least to some, it had turned out to be 1 The views and opinions expressed herein are strictly the views and opinions of the author, and do not reflect those of his employer and any of its subsidiaries . The author wishes to thank Joseph Langsam and Simon Juen for helpful comments. 2 elusive, when significant losses 2 were unfolded. The basis, defined as  the CDS spread minus same maturity bond spread, reflects a pricing discrepancy between the bond market and the CDS market on the same credit risk. Prior to the financial crisis, it was not a large number, rarely exceeding 20 bps. The financial crisis, how ever, saw an unprecendent ed basis widening, see Figure 1, with investment grade (IG) basis spiked at 250 bps and high yield (HY) at 650 bps. While there are certain factors such as CDS‚Äôs cheapest-to-delivery option, lack of voting rights conveyed to bond h olders, protection seller‚Äôs credit risk, and distortion created by LIBOR discounting (D.E. Shaw 2009), that could contribute to the basis,  earlier empirical researches (e.g., Longstaff, Mithal, and Neis, 2005) attribute the basis mostly to relative liquidity between bond and CDS markets.  Such is the case, the basis is considered non-economic and often termed the liquidity basis. At the time, CDS was generally considered of better market liquidity. Following the crisis, CDS trading has been shrinking, due to tightened regulatory scrutiny and capital rules. Many banks exited from single name CDS trading (Burne and Henning, 2014). At the same time, the push for electronic trading and trade data collection has improved cash bond liquidi...
--- PAPER END ---

--- PAPER START ---
Category: futures_arb
File: 2025_Agent-Based Simulation of a Perpetual Futures Market.pdf
Content_Extract: AbstractIintroduceanagent-basedmodelofaPerpetualFuturesmarketwithheterogeneousagentstradingviaa central-limitorderbook.PerpetualFutures(henceforthPerps)arefinancialderivativesintroducedbytheeconomistRobertShiller, designedto‚Äòpeg‚ÄôtheirpricetothatoftheunderlyingSpotmarket.ThispaperextendsthelimitorderbookmodelofChiarellaetal.(2002)bytakingtheiragentandorderbookparameters,designedfora simplestockexchange,andapplyingit tothemorecomplexenvironmentof a Perpmarketwithlongandshorttraderswhoexhibitbothpositionalandbasis-tradingbehaviors.I findthatdespitethesimplicityoftheagentbehavior, thesimulationisableto reproducethemostsalientfeatureof a Perpmarket,the‚Äòpegging‚Äôof thePerppriceto theunderlyingSpotprice.In contrastto fundamentalsimulationsof stockmarketswhichaimtoreproduceempiricallyobservedstylizedfactssuchastheleptokurtosisandheteroscedasticityofreturns,volatilityclusteringandothers,inderivativesmarketsmanyofthesefeaturesareprovidedexogenouslybytheunderlyingSpotpricesignal.ThisisespeciallytrueofPerpssincethederivativeisdesignedtomimicthepriceoftheSpotmarket.Therefore,thispaperwillfocusexclusivelyonanalyzinghowmarketandagentparameterssuchasorderlifetime,tradinghorizonandspreadaffectthepremiumsat whichPerpstradewithrespectto theunderlyingSpotmarket.I showthatthissimulationprovidesasimpleandrobustenvironmentforexploringthedynamicsofPerpetualFuturesmarketsandtheirmicrostructureinthisregard.Lastly, Iexploretheabilityofthemodeltoreproducetheeffectsofbiasinglongtraderstotradepositionallyandshorttraderstobasis-trade,whichwastheoriginalintentionbehindthemarketdesign,andis a tendencyobservedempiricallyin realPerpmarkets. 1 Table of Contents Abstract 1 1.0 Introduction 3 2.0 Literature Review2.1 Perpetual Futures 42.2 Agent-based Computational Economics (ACE) 52.3 Presenting ACE with Econometrics 52.4 Emerging Econometrics with ACE 62.6 A simulated analysis of the microstructure ofdouble auction markets, Chiarella et al. (2002) 7 3.0 Methods3.1 Agents 63.2 Trading Cohort 83.3 Long and Short Positions 83.4 Close positions and Exit 83.5 Positional trading vs Basis trading 93.6 Fundamentalist Trader 103.7 Chartist Trader 103.8 Noise Trader 103.9 Composite Forecast 103.10 11œÑ3.11 Geometric Brownian Motion for the Spot Market signal113.12 Simulation Runs 11 4.0 Simulations and Results 114.1 Analysing the accuracy of the Peg 134.2 Simulation Parameters 144.3 Exploratory Simulations 154.4 Simulations 174.5Varying the upper bound of the Chartist TimeHorizon Parameter18ùëôùëöùëéùë• 4.6 18ùëòùëöùëéùë• 4.7 19œÑ4.8 Bias 20 5.0 Conclusions 20 References 23 Appendix I 25 Appendix II 28 2...
--- PAPER END ---

--- PAPER START ---
Category: futures_arb
File: 2009_Defining, Estimating and Using Credit Term Structures. Part .pdf
Content_Extract: Berd, Mashal, Wang  | Defining, Estimating and Using Credit Term Structures  Part 3 November 2004  1 Defining, Estimating and Using Credit Term Structures Part 3: Consistent CDS-Bond Basis In the third part of this series we introduce consistent rel ative value measures for CDS-Bond basis trades using the bond-implied CDS term structure der ived from fitted survival rate curves. We explain why this measure is better than the tradi tionally used Z-spread or Libor OAS and offer simplified hedging and trading strategies  which take advantage of the relative value across the entire range of maturities of cash and synthetic credit markets. INTRODUCTION In a recent paper (Berd, Mashal and Wang [2004a], cited hereaft er as Part 1) we introduced a new methodology for direct estimation of implied term stru ctures of survival probabilities from credit bond prices. We have shown that this methodolog y is more robust than the traditional implementations of reduced-form default models (for the latter, see Jarrow and Turnbull [1995], and Duffie and Singleton [1999]). Mor e importantly, it is more consistent with the underlying bankruptcy resolution practices such as  debt acceleration and equal priority recovery for the same-seniority bonds. Our methodology is well suited to a direct comparison with  credit derivatives, particularly credit default swaps, whose valuation is driven by the model ing of default probabilities. In this paper we introduce new relative value measures, which take a dvantage of the internal consistency of this pricing methodology. In particular, we define: ‚Ä¢ Bond-Implied CDS (BCDS) term structure ‚Ä¢ CDS-Bond curve basis ‚Ä¢ Systematic and full bond-specific basis to CDS curve ‚Ä¢ Risk-free-equivalent coupon (RFC) streams for credit-risky bonds We also introduce and discuss static replication/hedging str ategies of credit risk in cash bonds using forward and spot CDS. In particular, we dem onstrate in detail how these strategies can be used to hedge the default ...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Signature Decomposition Method Applying to Pair Trading.pdf
Content_Extract: Abstract High-frequency quantitative trading strategies have long been of significant interest in futures market. While advanced statistical arbitrage and deep learning enhance high-frequency data pro- cessing, they diminish opportunities for traditional methods and yield less interpretable, unstable strategies. Consequently, developing stable, interpretable quantitative strategies remains a prior- ity in futures markets. In this study, we propose a novel pair trading strategy by leveraging the mathematical concept of path signature which serves as a feature representation of time series. Specifically, the path signature is decomposed into two new indicators: the path interactivity in- dicator segmented signature and the directional indicator covariation of increments, which serve as double filters in strategy design. Empirical experiments using minute-level futures data show our strategy significantly outperforms traditional pair trading, delivering higher returns, lower maximum drawdown, and higher Sharpe ratio. The proposed method enhances interpretability and robustness while maintaining strong returns, demonstrating the potential of path signatures in financial trading. Key words: Rough Path, Signature Method, Quantitative Finance, Pair Trading 1 arXiv:2505.05332v2  [econ.GN]  16 Oct 2025 1 Introduction In financial market, robust, consistently profitable strategies and indicators are pursued by participators. Among them, arbitrage strategies are generally considered as a comparatively lower-risk investment approach, hedging most market risks through simultaneously buying and selling multiple financial assets (Dybvig & Ross, 1989; Yadav & Pope, 1990; Liew & Wu, 2013; Krauss, 2017). Pair trading (Vidyamurthy, 2004; Elliott et al., 2005) is one of the popular and widely used statistical arbitrages in trading stocks (Chen et al., 2019), futures and options (Draper & Fung, 2002). The core concept of pair trading involves trading the price spread between two correlated assets. When their prices spread deviates from the expected value, the pair trading strategy involves going long on one relatively undervalued asset while shorting the other relatively overvalued one and closing the positions for profit after the spread reverts to normal levels. Arbitrage opportunities typically arise in imperfect markets. However, as more market participants exploit these opportunities, the price spread tends to disappear quickly (Krauss, 2017). What‚Äôs more, in highly complex financial markets, just capturing linear correlations alone is insufficient (Elliott et al., 2005), as the price spread is often not merely a simple price difference or ratio, but rather a nonlinear function composed of multiple features. So, to better capture arbitrage opportunities, an interpretable pair trading strategy that comprehensively incorporates both linear and nonlinear structures is of significant interest and importance. Extracting nonlinear features and capturing complex patter...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_A Comparison between Financial and Gambling Markets.pdf
Content_Extract: Abstract Financial and gambling markets are ostensibly similar and h ence strategies from one could potentially be applied to the other. Financia l markets have been extensively studied, resulting in numerous theorems a nd models, while gambling markets have received comparatively less attenti on and remain relatively undocumented. This study conducts a comprehens ive comparison of both markets, focusing on trading rather than regulation. F ive key aspects are examined: platform, product, procedure, participant and s trategy. The Ô¨Åndings reveal numerous similarities between these two markets. Fi nancial exchanges resemble online betting platforms, such as Betfair, and som e Ô¨Ånancial products, including stocks and options, share speculative traits wit h sports betting. We examine whether well-established models and strategies fr om Ô¨Ånancial markets could be applied to the gambling industry, which lacks compa rable frameworks. For example, statistical arbitrage from Ô¨Ånancial markets h as been eÔ¨Äectively applied to gambling markets, particularly in peer-to-peer betting exchanges, where bettors exploit odds discrepancies for risk-free pro Ô¨Åts using quantitative models. Therefore, exploring the strategies and approache s used in both markets could lead to new opportunities for innovation and optimiza tion in trading and betting activities. Keywords: Online Betting, Sports Betting, Peer-to-peer, Statistica l Arbitrage 1 1 Introduction Recently, a growing number of professional theoretical and empir ical research has illuminated surprising parallels between Ô¨Ånancial and gambling markets ( Arthur, Williams, & Delfabbro , 2016; Borna & Lowry , 1987; Cox, Kamolsareeratana, & Kouwenberg, 2020; Weidner, 2022). Both systems, Ô¨Ånancial markets and gambling markets ( Schwartz, 2013), have a long history. Financial markets facilitate the exchange of assets, including stocks and bonds ( Valdez & Molyneux , 2016) and gambling markets involve betting on uncertain outcomes ( Borna & Lowry , 1987). However, the literature on models and strategies for gambling mark ets is relatively limited compared to the widespread literature available on Ô¨Ånancial ma rkets. Several factors contribute to this discrepancy, including the inherent com plexity of the markets (Blau & Whitby, 2020) and the regulatory environment surrounding gambling (Weidner, 2022). Gambling markets represent a simpliÔ¨Åed form of Ô¨Ånancial markets (L.V. Williams, 1999). Therefore, the models and strategies proven eÔ¨Äective in Ô¨Ånanc ial markets could potentially be adapted and applied to gambling markets (J.N. Williams, Williams, Gooding, & Mix , 2023). A deeper comparison between these two markets could provide valuable insights into the potential transferability of Ô¨Å nancial market approaches to the gambling arena, and may uncover opportunities for developing new and innovative strategies for gambling. Historically, Ô¨Ånancial markets could be traced back to ancient civiliza tions including Mesopotamia and Egyp...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Pricing and hedging of decentralised lending contracts.pdf
Content_Extract: ABSTRACT . We study the loan contracts offered by decentralised loan protocols (DLPs) through the lens of financial derivatives. DLPs, which effectively are clear- inghouses, facilitate transactions between option buyers (i.e. borrowers) and option sellers (i.e. lenders). The loan-to-value at which the contract is initiated determ- ines the option premium borrowers pay for entering the contract, and this can be deduced from the non-arbitrage pricing theory. We show that when there are no market frictions, and there is no spread between lending and borrowing rates, it is optimal to never enter the lending contract. Next, by accounting for the spread between rates and transactional costs, we develop a deep neural network-based algorithm for learning trading strategies on the external markets that allow us to replicate the payoff of the lending contracts that are not necessarily optimally exercised. This allows hedge the risk lenders carry by issuing options sold to the borrowers, which can complement (or even replace) the liquidations mechanism used to protect lenders‚Äô capital. Our approach can also be used to exploit (statistical) arbitrage opportunities that may arise when DLP allow users to enter lending contracts with loan-to-value, which is not appropriately calibrated to market conditions or/and when different markets price risk differently. We present thorough simulation experiments using historical data and simulations to validate our approach. 1. I NTRODUCTION Decentralised lending protocols (DLPs) resemble a collateralised debt market (CDM) by pooling assets from lenders to enable over-collateralised loans to borrowers without having to rely on a central trusted entity [17, 26, 29, 18]. Protocol governance needs to monitor current market conditions to decide maximum loan-to-value ratios at loan origination for each pair of assets, which dictate how much of the debt asset can be borrowed at time zero, given posted collateral. One way to decide maximum loan-to- value ratios is to use statistical approaches involving coherent risk measures, which have been developed in [10] and which aim to control the probability of lenders losing assets and/or the amounts lost in the case of default happens. In this work, we study the loan contracts offered by DLPs through the lens of fin- ancial options. DLP - a clearinghouse - facilitates transactions between option buyers (i.e. borrowers) and option sellers (i.e. lenders). For example, protocols such as Aave [1], Compound [12], or Morpho [15] offer loan contracts that resemble stock loans in TradFi, which essentially are American perpetual barrier option with a barrier above the strike as has been demonstrated in [27]. The loan-to-value at which the contract is 1SIMTOPIA .AI 2SCHOOL OF MATHEMATICS , U NIVERSITY OF EDINBURGH 3THE ALAN TURING INSTITUTE 4INFINITAS BY KRUNGTHAI , BANGKOK 5IMPERIAL COLLEGE LONDON Acknowledgements: We would like to thank Samuel N. Cohen (University of Oxford), and David Siska...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Stochastic factors can matter: improving robust growth under.pdf
Content_Extract: Abstract Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strate- gies obtained from portfolio optimization are very sensitive to them. To mitigate this well- known phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price processX, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of ad- missible models allowsXto depend on a multivariate stochastic factorYand fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor processY. A principal motivation of this framework comes from pairs trading, whereXis the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certain partial differential equation (PDE). We demonstrate that utilizing the stochastic factor leads to im- provement in robust growth complementing the conclusions of the previous study [8], which additionally robustified the dynamics of the stochastic factor leading toY-independent opti- mal strategies. Our analysis leads to new financial insights, quantifying the improvement in growth the investor can achieve by optimally incorporating stochastic factors into their trad- ing decisions. We illustrate our theoretical results on several numerical examples including an application to pairs trading. Keywords:Robust finance, Growth maximization, Pairs trading, Statistical arbitrage, Stochastic factors, Calculus of variations, Ergodic process MSC 2020 Classification:91G10, 60G10, 60J46 1 Introduction In this paper we study an asymptotic growth-optimization problem under model uncertainty and ergodicity. Our focus is on an investor who seeks stability in (discounted) asset prices and trades ‚àóDepartment of Mathematics, ETH Zurich, balint.binkert@gmail.com ‚Ä†Department of Statistics, London School of Economics and Political Science, d.itkin@lse.ac.uk ‚Ä°Department of Statistics, London School of Economics and Political Science, p.j.mangers-bastian@lse.ac.uk ¬ßDepartment of Mathematics, ETH Zurich, josef.teichmann@math.ethz.ch 1 arXiv:2512.24906v1  [q-fin.MF]  31 Dec 2025 on this stability persisting, which is commonplace in pairs trading and certain statistical arbitrage strategies. In practice, investors estimate asset volatilities, distributions of asset returns and obtain noisy factors that provide partial information about price movements. However, direct estimation of the drifts of tradeable securities is usually inaccurate, due to the low signal-to-noise ratios present in financial data. Postulating a parametric model for asset returns and estimating only a few select parameters that pin down the drift process may seem plausible in some cases, but for the pur...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_An Application of the Ornstein-Uhlenbeck Process to Pairs Tr.pdf
Content_Extract: Abstract In this paper, we conduct preliminary analysis on a pairs trading strategy using the Ornstein-Uhlenbeck process to model stock price differences and compare that to a naive pairs trading strategy using a rolling window to calculate mean and standard deviation parameters. Our preliminary findings suggest that running a pairs trading strategy with the Ornstein-Uhlenbeck process outperforms the naive pairs trading strategy on a risk-return basis. Key further research can be conducted on the selection of pairs, augmenting the investment universe, finding different criteria in pairs selection, applying more rigorous machine learning techniques to assist with forecasting pricing trends, and in integrating portfolio optimization techniques. Introduction Pairs trading is a widely used market-neutral strategy in quantitative finance that capitalizes on the price convergence of two historically correlated securities. By simultaneously shorting the overpriced asset and longing the underpriced one, the strategy aims to profit from the mean-reverting behavior of their price spread. Unlike directional trading strategies, pairs trading does not depend on the overall market direction, allowing it to be robust in volatile or stagnant market conditions. This project seeks to improve the traditional pairs trading framework by leveraging statistical and econometric techniques. Specifically, we explore the application of cointegration tests, and modelling the spread as a mean-reverting processes using the Ornstein-Uhlenbeck (OU) model. The methodology is tested on historical market data to evaluate its performance and robustness under varying market conditions. The selection of pairs is critical. Pairs chosen should exhibit high correlation between the percentage change of the prices of each security, and the price spread should exhibit a mean reverting behavior. Once pairs are chosen, an algorithm can be developed to model the spread and to forecast future pric- ing trends to take a directional view on the spread. In our case, we employ the Ornstein-Uhlenbeck process to do so, and we compare our results to a basic computation of the z score using the rolling mean and standard deviation by backtesting our results on historical pricing data. Data We defined the universe of stocks to analyze as all US-based companies with a market capitalization greater than $1 billion. The stock data was obtained from Refinitiv‚Äôs Datastream, specifically from the 1 arXiv:2412.12458v1  [q-fin.TR]  17 Dec 2024 wrds ds2dsf table, which provides financial metrics such as market capitalization, returns, and prices. We limited the dataset to companies listed in the US, trading in USD, and filtered for data starting from 2018. This allowed us to focus on liquid, large-cap stocks for the analysis. Additionally, we retrieved metadata from the wrds ds names table, which includes key identifiers like company names, tickers, and other relevant securities information. After obtaining this...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Attention Factors for Statistical Arbitrage.pdf
Content_Extract: Abstract Statistical arbitrage exploits temporal price differences between similar assets. We develop a framework tojointlyidentify similar assets through factors, identify mispricing and form a trading policy that maximizes risk-adjusted performance after trading costs. Our Attention Factorsare conditional latent factors that are the most useful for arbitrage trading. They are learned from firm character- istic embeddings that allow for complex interactions. We identify time-series signals from the residual portfolios of our factors with a general sequence model. Estimating factors and the arbitrage trading strategy jointly is crucial to maximize profitability after trading costs. In a comprehensive empirical study we show that our Attention Factor model achieves an out-of-sample Sharpe ra- tio above 4 on the largest U.S. equities over a 24-year period. Our one-step solution yields an unprecedented Sharpe ratio of 2.3 net of transaction costs. We show that weak factors are important for arbitrage trading. Keywords Deep learning, attention, statistical arbitrage, latent factor model, sequence models, equities, investment ACM Reference Format: Elliot L. Epstein, Rose Wang, Jaewon Choi, and Markus Pelger. 2025. Atten- tion Factors for Statistical Arbitrage. In6th ACM International Conference on AI in Finance (ICAIF ‚Äô25), November 15‚Äì18, 2025, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3768292.3770398 1 Introduction Statistical arbitrage exploits temporal price differences between similar assets using statistical methods. Conceptually, these meth- ods are based on relative trades between a stock and a mimicking portfolio. The mimicking portfolio is constructed to be ‚Äúsimilar‚Äù to the target stock, usually based on historical co-movements in the price time-series. When the spread between the prices of the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICAIF ‚Äô25, Singapore, Singapore ¬©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2220-2/2025/11 https://doi.org/10.1145/3768292.3770398 two comparison assets widens, the arbitrageur sells the winner and buys the loser. If their prices move back together, the arbitrageur will profit. Statistical arbitrage trading has to solve the following three key problems: Given a large universe of assets, what are long- short portfolios of similar assets? Given these portfolios, what are time-series sig...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Finding Moving-Band Statistical Arbitrages via Convex-Concav.pdf
Content_Extract: Abstract We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time. 1 arXiv:2402.08108v1  [econ.EM]  12 Feb 2024 Contents 1 Introduction 3 1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 This paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Finding fixed-band stat-arbs 6 2.1 Formulation as convex-concave problem . . . . . . . . . . . . . . . . . . . . . 7 2.2 Interpretation via a simple trading policy . . . . . . . . . . . . . . . . . . . . 7 2.3 Solution method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Finding moving-band stat-arbs 9 3.1 Moving-band stat-arbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Finding moving-band stat-arbs . . . . . . . . . . . . . . . . . . . . . . . . . 10 4 Numerical experiments 10 4.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Simulation and metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.3 Results for fixed-band stat-arbs . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.4 Results for moving-band stat-arbs . . . . . . . . . . . . . . . . . . . . . . . . 18 5 Conclusions and comments 23 2...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2023_Linear and nonlinear causality in financial markets.pdf
Content_Extract: Chaos Linear and nonlinear causality in financial markets Haochun Ma, 1, 2 Davide Prosperino, 1, 2 Alexander Haluszczynski, 2 and Christoph R√§th 3 1)Ludwig-Maximilians-Universit√§t, Department of Physics, Schellingstra√üe 4, 80799 Munich, Germany 2)Allianz Global Investors, risklab, Seidlstra√üe 24, 80335, Munich, Germany 3)Deutsches Zentrum f√ºr Luft- und Raumfahrt (DLR), Institut f√ºr KI Sicherheit, Wilhelm-Runge-Stra√üe 10, 89081 Ulm, Germany (*Electronic mail: christoph.raeth@dlr.de) Identifying and quantifying co-dependence between financial instruments is a key challenge for researchers and practi- tioners in the financial industry. Linear measures such as the Pearson correlation are still widely used today, although their limited explanatory power is well known. In this paper we present a much more general framework for assessing co-dependencies by identifying and interpreting linear and nonlinear causalities in the complex system of financial markets. To do so, we use two different causal inference methods, transfer entropy and convergent cross-mapping, and employ Fourier transform surrogates to separate their linear and nonlinear contributions. We find that stock indices in Germany and the U.S. exhibit a significant degree of nonlinear causality and that correlation, while a very good proxy for linear causality, disregards nonlinear effects and hence underestimates causality itself. The presented framework enables the measurement of nonlinear causality, the correlation-causality fallacy, and motivates how causality can be used for inferring market signals, pair trading, and risk management of portfolios. Our results suggest that linear and nonlinear causality can be used as early warning indicators of abnormal market behavior, allowing for better trading strategies and risk management. Within the complex system of financial markets, under- standing the intricate ties between assets is crucial. Al- though the Pearson correlation has been a standard mea- sure for t...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Enhanced Momentum with Momentum Transformers.pdf
Content_Extract: AbstractThe primary objective of this research is to build aMomentum Transformer that is expected to outperformbenchmark time-series momentum and mean-reversiontrading strategies. We extend the ideas introduced in [6]to equities as the original paper primarily only buildsuponfuturesandequityindices. Unlikeconventional LongShort-Term Memory (LSTM) models, which operatesequentially and are optimized for processing localpatterns, an attention mechanismequips our architecturewith direct access to all prior time steps in the trainingwindow. This hybrid design, combining attention with anLSTM, enables the model to capture long-termdependencies, enhance performance in scenariosaccounting for transaction costs, and seamlesslyadapt toevolving market conditions, such as those witnessedduring the Covid Pandemic. The main technicalchallenges we faced aresomeof thesinsmentionedinthepaper ‚ÄúSeven Sins of Quantitative Investing ‚Äú[11] wherewe inadvertently facedinitial challenges, suchasthedatanot being trulyPoint-In-Time(PIT) duetoissueslikedataleakage, look-ahead biases, and possibly evensurvivorshipbias, all mentionedin[11]. Furthertechnicalchallenges were faced in the computation necessary forthisstrategy. Toaddressthese, thetimeperiodtrainedandtested on was reduced to 7 years and only onechangepoint lookback window of 21 was used. Afterrectifying all errors, our results show promise for a fewyears and are similarwiththeoriginal paper[6]althoughour best performing model doesn‚Äôt use changepointdetection. We average 4.14%returns which is similar totheir results. Our Sharpe is lower at an average of 1.12due to much higher volatility which may be due tostocksbeinginherentlymorevolatilethanfuturesandindices. 1.IntroductionThere arevariousoccurrencesinfinancial marketsthatcontradict the efficient market hypothesis. Of thesemarket anomalies, one of the most popular strategies isthe momentumstrategy. This strategy isbasedonthefactthat stocks with large returns over acertainperiodtendtohave higher average returns over the next period[1]. TheMoving Average Convergence Divergence (MACD)indicator is a popular method used in momentum strategies that helps identifybuyandsell signalsbasedonthe convergence and divergence of two different lengthmoving averages [2]. However, traditional momentumstrategies utilizing MACD often face challenges inadapting to rapidly changing market conditions and mayfail to capture complex temporal dependencies in pricedata.To address these limitations, researchers have utilizedadvancements in machine learning and artificialintelligence to apply a deep learning framework to thisproblem which can learn trends and size positions bydirectly optimizing the result with the Sharpe ratio[3, 4].These Deep MomentumNetworks typically compriseof aLong Short-TermMemory (LSTM) model and havebeenshown to outperform classical momentum strategies byWood et al. [4]. However, although the LSTMhas beenshown to perform well in short-termpatterns, it tends tostruggle...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_ESG driven pairs algorithm for sustainable trading: Analysis.pdf
Content_Extract: Abstract This paper proposes an algorithmic trading framework integrating Environmen- tal, Social, and Governance (ESG) ratings with a pairs trading strategy. It addresses the demand for socially responsible investment solutions by developing a unique al- gorithm blending ESG data with methods for identifying co-integrated stocks. This allows selecting profitable pairs adhering to ESG principles. Further, it incorporates technical indicators for optimal trade execution within this sustainability framework. Extensive back-testing provides evidence of the model‚Äôs effectiveness, consistently generating positive returns exceeding conventional pairs trading strategies, while upholding ESG principles. This paves the way for a transformative approach to algorithmic trading, offering insights for investors, policymakers, and academics. Keywords: ESG, Pairs Trading, Sustainable Investing 1 Introduction In a world that is increasingly interconnected and faced with challenges of climate change, the long term survival as well as success of a firm is contingent on factors beyond the realm of traditional metrics of financial performance. Accordingly, Environmental Social and Governance (ESG) considerations have emerged as a pivotal driver of sustainable per- formance of firms, not only from the perspective of investment decisions and risk manage- ment, but also from the viewpoint of consumer preferences, as well as regulatory practices, all of which, eventually has a bearing on the reputation and viability of a firms‚Äô operation [13]. At the heart of ESG standing of a firm lies the encompassing of the environmental im- pact, which can be attributed to the firm, the stakeholders of the firm and the consequent practices of corporate governance. In this context, particular consideration is extended to ‚àóDepartment of Mathematics, Indian Institute of Technology Guwahati, Guwahati-781039, India, e- mail: d.eeshaan@iitg.ac.in ‚Ä†Department of Mathematics, Indian Institute of Technology Guwahati, Guwahati-781039, India, e- mail: s.diwan@iitg.ac.in ‚Ä°Department of Mathematics, Indian Institute of Technology Guwahati, Guwahati-781039, India, e- mail: pratim@iitg.ac.in, Phone: +91-361-2582606 1 arXiv:2401.14761v1  [q-fin.TR]  26 Jan 2024 environmental dimension, including carbon footprint, energy consumption and use, and the effort towards management of emission, reduction of waste and conservation of natural resources. In order to facilitate the tangible assessment of the ESG performance of a firm, in case of the environment component, one can adopt metrics such as per unit (of production) energy consumption, extent of green house gas (GHG) emission and level of recycling as well as conservation efforts [14]. The social dimension of ESG, emphasizes on the relationship that a firm shares with its stakeholders, both internal (ethical workplace practices, diversity and inclusion) and external (engagement with the local community and philanthropic outreach activities). Some of ...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Automated Market Making and Decentralized Finance.pdf
Content_Extract: Automated Market Making and Decentralized Finance Marcello Monga St Hugh‚Äôs College University of Oxford Thesis submitted for the degree of DPhil in Mathematics Trinity 2024 arXiv:2407.16885v1  [q-fin.TR]  23 Jul 2024...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_CTBench: Cryptocurrency Time Series Generation Benchmark.pdf
Content_Extract: ABSTRACT Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeop- ardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classifica- tion and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce CTBench, the first comprehensive TSG benchmark tailored for the cryptocur- rency domain. CTBench curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading per- formance, risk assessment, and computational efficiency. A key innovation is a dual-task evaluation framework: (1) the Predictive Utility task measures how well synthetic data preserves temporal and cross-sectional patterns for forecasting, while (2) the Statistical Arbitrage task assesses whether reconstructed series support mean- reverting signals for trading. We benchmark eight representative models from five methodological families over four distinct mar- ket regimes, uncovering trade-offs between statistical fidelity and real-world profitability. Notably, CTBench offers model ranking analysis and actionable guidance for selecting and deploying TSG models in crypto analytics and strategy development. ACM Reference Format: Yihao Ang, Qiang Wang, Qiang Huang, Yifan Bao, Xinyu Xi, Anthony K. H. Tung, Chen Jin, and Zhiyong Huang. CTBench: Cryptocurrency Time Series Generation Benchmark. ACM Conference, XXX-XXX, 2020. doi:XX.XX/XXX.XX Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ACM Conference, ISSN XXXX-XXXX. doi:XX.XX/XXX.XX 1 INTRODUCTION Time Series Generation (TSG) has become a cornerstone tech- nique for tasks such as data augmentation [ 5, 51], anomaly de- tection [4, 65], privacy preservation [ 26, 60], and domain adap- tation [8, 33]. The core objective of TSG is to produce synthetic sequences that faithfully replicate the temporal dependencies and cross-dimensional correlations of real-world time series. Recent years have seen rapid advances in TSG models, supported by bench- marking frameworks like TS...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Is the difference between deep hedging and delta hedging a s.pdf
Content_Extract: Abstract The recent work of Horikawa and Nakagawa (2024) claims that under a complete market admitting statistical arbitrage, the difference between the hedging position provided by deep hedging and that of the replicating portfolio is a statistical arbitrage. This raises concerns as it entails that deep hedging can include a speculative component aimed simply at exploiting the structure of the risk measure guiding the hedging optimisation problem. We test whether such finding remains true in a GARCH-based market model, which is an illustrative case departing from complete market dynamics. We observe that the difference between deep hedging and delta hedging is a speculative overlay if the risk measure considered does not put sufficient relative weight on adverse outcomes. Nevertheless, a suitable choice of risk measure can prevent the deep hedging agent from engaging in speculation. JEL classification: C45, C61, G32. Keywords: Deep reinforcement learning, optimal hedging, arbitrage. ‚àóGauthier is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC, RGPIN- 2019-04029), a professorship funded by HEC Montr√©al, and the HEC Montr√©al Foundation. Godin is funded by NSERC (RGPIN-2024-04593). ‚Ä†Corresponding author. Email addresses: pascal.francois@hec.ca (Pascal Fran√ßois), genevieve.gauthier@hec.ca (Genevi√®ve Gauthier), frederic.godin@concordia.ca (Fr√©d√©ric Godin), carlos.octavio.perez92@gmail.com (Carlos Octavio P√©rez Mendoza). arXiv:2407.14736v3  [q-fin.CP]  21 Oct 2024 1 Introduction The seminal paper of Buehler et al. (2019), which proposes to use deep reinforcement learning (RL) methods to obtain optimal hedging procedures for financial derivatives, initiated a recent strand of literature.1 Deep RL methods are particulary well-suited to solve dynamic hedging problems because these methods can handle the curse of dimensionality, a problem that more traditional approaches (e.g., finite elements dynamic programming) struggle to overcome. They can also work with very general dynamics for asset prices, not being limited by mathematical tractability issues. While the ability of deep hedging strategies to outperform standard counterparts is well- documented, the existing literature has not yet extensively analyzed the structure of optimal policies and explained how such incremental performance is attained. Neagu et al. (2024) make a step in that direction by investigating the impact of the various features on optimal risk management decisions in the presence of illiquidity market impacts. In their recent work, Horikawa and Nakagawa (2024) investigate complete markets that allow for statistical arbitrage with respect to a specific risk measureœÅ. They assert that, within this framework, deep hedging strategies that minimize the chosen risk metric combine the traditional delta-hedging approach with a statistical arbitrage overlay. In a vector auto-regressive stochastic volatility model and in a GAN-simulated market model, Buehle...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Time-Varying Factor-Augmented Models for Volatility Forecast.pdf
Content_Extract: Abstract Accurate volatility forecasts are vital in modern finance for risk man- agement, portfolio allocation, and strategic decision-making. How- ever, existing methods face key limitations. Fully multivariate mod- els, while comprehensive, are computationally infeasible for realis- tic portfolios. Factor models, though efficient, primarily use static factor loadings, failing to capture evolving volatility co-movements when they are most critical. To address these limitations, we pro- pose a novel, model-agnosticFactor-Augmented Volatility Forecast framework. Our approach employs a time-varying factor model to extract a compact set of dynamic, cross-sectional factors from realized volatilities with minimal computational cost. These factors are then integrated into both statistical and AI-based forecasting models, enabling a unified system that jointly models asset-specific dynamics and evolving market-wide co-movements. Our frame- work demonstrates strong performance across two prominent asset classes‚Äîlarge-cap U.S. technology equities and major cryptocur- rencies‚Äîover both short-term (1-day) and medium-term (7-day) horizons. Using a suite of linear and non-linear AI-driven models, we consistently observe substantial improvements in predictive ac- curacy and economic value. Notably, a practical pairs-trading strat- egy built on our forecasts delivers superior risk-adjusted returns and profitability, particularly under adverse market conditions. CCS Concepts ‚Ä¢Applied computing ‚Üí Operations research;‚Ä¢Computing methodologies‚ÜíMachine learning. Keywords Volatility Forecasting, Factor Models, Long Short-Term Memory (LSTM), HAR, MIDAS, Volatility-Based Pairs Trading ‚àóCorresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICAIF ‚Äô25, Singapore, Singapore ¬©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2220-2/2025/11 https://doi.org/10.1145/3768292.3770407 ACM Reference Format: Duo Zhang, Jiayu Li, Junyi Mo, and Elynn Chen. 2025. Time-Varying Factor- Augmented Models for Volatility Forecasting. In6th ACM International Conference on AI in Finance (ICAIF ‚Äô25), November 15‚Äì18, 2025, Singapore, Sin- gapore.ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3768292. 3770407 1 Introduction From tick-by-tick hedging to long-term investment strategy, reliable volatility forecasts are indispensable for guiding financial decisions. A vast literature‚Äîranging from li...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_A Markowitz Approach to Managing a Dynamic Basket of Moving-.pdf
Content_Extract: Abstract We consider the problem of managing a portfolio of moving-band statistical arbi- trages (MBSAs), inspired by the Markowitz optimization framework. We show how to manage a dynamic basket of MBSAs, and illustrate the method on recent historical data, showing that it can perform very well in terms of risk-adjusted return, essentially uncorrelated with the market. ‚àóStanford University (   kasperjo@stanford.edu) ‚Ä†Abu Dhabi Investment Authority ‚Ä°Stanford University 1 arXiv:2412.02660v1  [econ.EM]  3 Dec 2024 Contents 1 Introduction 3 1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Moving-band stat-arbs 4 2.1 Midpoint price and alpha . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 MBSA lifetime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Multiple MBSAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3 Managing a dynamic basket of MBSAs 5 3.1 Portfolio holdings and trades . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Markowitz objective and constraints . . . . . . . . . . . . . . . . . . . . . . . 6 3.3 Markowitz formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4 Numerical experiments 9 4.1 Data and parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Simulation and metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5 Conclusion 17 2...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Market information of the fractional stochastic regularity m.pdf
Content_Extract: Abstract The Fractional Stochastic Regularity Model (FSRM) is an extension of Black-Scholes model describing the multifractal nature of prices. It is based on a multifractional process with a random Hurst exponent Ht, driven by a fractional Ornstein-Uhlenbeck (fOU) process. When the regularity parameter Ht is equal to 1/2, the efficient market hypothesis holds, but when Ht Ã∏= 1/2 past price returns contain some information on a future trend or mean-reversion of the log-price process. In this paper, we investigate some properties of the fOU process and, thanks to information theory and Shannon‚Äôs entropy, we determine theoretically the serial information of the regularity process Ht of the FSRM, giving some insight into one‚Äôs ability to forecast future price increments and to build statistical arbitrages with this model. An application to the forecast of future daily price returns of major stock indices shows promising results. Keywords: Fractional Ornstein-Uhlenbeck process, Hurst exponent, Shannon entropy, serial information, nonlinear serial dependence 1. Introduction In financial mathematics, the most famous model for option pricing is the Black-Scholes model [18, 58], which, under the no-arbitrage assumption, describes the price dynamics Pt of an underlying asset by means of the stochastic differential equation dPt Pt = rdt + œÉdWt, (1) where Wt is a standard Brownian motion. Outside the risk-neutral framework, the study of the stylized facts of price returns, among which self-similarity and long-range dependence [31, 32, 64, 27], has aroused in finance some interest in fractional processes such as the fractional Brownian motion (fBm) [57, 28]. An fBm BH t , for a Hurst exponent H ‚àà (0, 1/2] (respectively [1/2, 1)), is the fractional derivative (resp. integral) of order 1 /2 ‚àí H (resp. H ‚àí 1/2) of a standard Brownian motion. By substituting the Brownian measure dWt in equation (1) by the fractional measure dBH t , we obtain the fractional Black-Scholes model, in which one can adjust the serial dependence of the returns and obtain a process that exhibits long-range dependence when H > 1/2 as well as self-similarity. Since the fBm is a non-Markovian process, using it for describing log-prices supposes that one can use past prices to profitably forecast future price returns in average, thus contradicting the Efficient Market Hyphotesis (EMH) [33]. Does it mean that this model also induces pure arbitrage? This over- riding question has been the subject of a large literature [14]. Though pure arbitrages exist, according to this model, when trading in continuous or even in discrete time [66, 22], arbitrage opportunities disappear when one imposes specific transaction costs or a minimal, and possibly extremely small, interval of time between two consecutive transactions [22, 49]. This last condition reflects the reality of frictions in financial markets, so that one cannot argue from the no-arbitrage condition to discard the fBm for modelling log-pric...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_End-to-End Policy Learning of a Statistical Arbitrage Autoen.pdf
Content_Extract: Abstract In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (lin- ear) model is identiÔ¨Åed, a separate mean rever- sion strategy is then devised to generate a trad- ing signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb. As a Ô¨Årst approach, we employ a stan- dard Autoencoder trained on US stock returns to derive trading strategies based on the Ornstein- Uhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neu- ral network representation of a space of portfo- lio trading policies. This integration outputs port- folio allocations directly and is end-to-end train- able by backpropagation of the risk-adjusted re- turns of the neural policy. Our Ô¨Åndings demon- strate that this innovative end-to-end policy learn- ing approach not only simpliÔ¨Åes the strategy de- velopment process, but also yields superior gross returns over its competitors illustrating the po- tential of end-to-end training over classical two- stage approaches. 1. Introduction Quantifying relationships between Ô¨Ånancial assets using statistical techniques has long captivated both researche rs and practitioners. The concept of exploiting mean reverting prices in pairs trading, where one asset is traded against an - other upon divergence from their established relationship , laid the groundwork for this Ô¨Åeld. SigniÔ¨Åcant excess re- turns identiÔ¨Åed in pairs trading strategies between 1962 and 2002, as reported by Gatev et al. (2006), highlight the po- 1Department of Engineering, University of Oxford, Ox- ford, United Kingdom. Correspondence to: Fabian Krause <lina3477@ox.ac.uk>. tential of such approaches. A systematic review of pairs trading research is provided by Krauss (2017). Our fo- cus extends beyond pairs to the co-movement of groups of stocks or their underlying factors. Avellaneda & Lee (2010) formally generalised the idea of pairs to StatArb trading that is said to have been developed in the mid 1980‚Äôs in Morgan Stanleys trading group around Nunzio Tartaglia ( Gatev et al., 2006). StatArb trading assumes an asset pricing model incorporat- ing various statistical or fundamental factors to describe an asset‚Äôs price movement. There is a multitude of asset pric- ing options to choose from depending on context and mod- elling goals. The idea of StatArb is that deviations of an asset‚Äôs return from the model‚Äôs explained return are tempo- rary and will revert to the model returns. This necessitates modelling the reversion time series process and combining various asset signals into a tradable portfolio. Principal Component Analysis (PCA) is the work horse for uncovering the statistical factors in asset pricing mod - els. Our Ô¨Årst contribution is replacing PCA with an Au- toencoder to...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Reinforcement Learning Pair Trading: A Dynamic Scaling appro.pdf
Content_Extract: Abstract: Cryptocurrency is a cryptography-based digital asset with extremely volatile prices. Around USD 70 billion worth of cryptocurrency is traded daily on exchanges. Trading cryptocurrency is difficult due to the inherent volatility of the crypto market. This study investigates whether Reinforcement Learning (RL) can enhance decision-making in cryptocurrency algorithmic trading compared to traditional methods. In order to address this question, we combined reinforcement learning with a statistical arbitrage trading technique, pair trading, which exploits the price difference between statistically correlated assets. We constructed RL environments and trained RL agents to determine when and how to trade pairs of cryptocurrencies. We developed new reward shaping and observation/action spaces for reinforcement learning. We performed experiments with the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data separated by 1 min intervals ( n = 263,520). The traditional non-RL pair trading technique achieved an annualized profit of 8.33%, while the proposed RL-based pair trading technique achieved annualized profits from 9.94% to 31.53%, depending upon the RL learner. Our results show that RL can significantly outperform manual and traditional pair trading techniques when applied to volatile markets such as cryptocurrencies. Keywords: pair trading; reinforcement learning; algorithmic trading; deep learning; cryptocurrency 1. Introduction Arbitrage is a subdomain of financial trading that profits from price discrepancies in different markets (Dybvig and Ross 1989). Pair trading is one of the well-known arbi- trage trading methods in financial markets. Arbitrageurs identify two highly correlated assets to form a pair. When a price discrepancy happens, they buy the underpriced asset and sell the overpriced correlated asset to profit from the mean reversion of the prices. With the rise of high-frequency trading, the ability to conduct fast and accurate analyses has become critical. Arbitrage requires practitioners to constantly analyze the market conditions at the fastest speed possible, as arbitrageurs must compete for transi- tory opportunities (Brogaard et al. 2014). Therefore, we explore how Artificial Intelligence (AI) can enhance the process of pair trading, focusing on the speed and adaptability of decision-making. Reinforcement Learning (RL) is a captivating domain of AI. The idea of RL is to let the agent(s) learn to interact with an environment. The agent should learn from the environment‚Äôs responses to optimize its behavior (Sutton and Barto 2018). If we view the financial market from the perspective of the RL environment, actions in the financial market are investment decisions. By allowing agents to adapt dynamically to market conditions, RL has the potential to overcome the limitations of static, rule-based strategies in volatile and complex financial environments. For gaining profits, arbitrageurs are incentivized to train ...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Pairs Trading Using a Novel Graphical Matching Approach.pdf
Content_Extract: Pairs Trading Using a Novel Graphical Matching Approach Khizar Qureshi Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, kqureshi@mit.edu Tauhid Zaman Yale School of Management, Yale University, 165 Whitney Ave Ave., New Haven, CT 06511, tauhid.zaman@yale.edu Pairs trading, a strategy that capitalizes on price movements of asset pairs driven by similar factors, has gained sig- nificant popularity among traders. Common practice involves selecting highly cointegrated pairs to form a portfolio, which often leads to the inclusion of multiple pairs sharing common assets. This approach, while intuitive, inadver- tently elevates portfolio variance and diminishes risk-adjusted returns by concentrating on a small number of highly cointegrated assets. Our study introduces an innovative pair selection method employing graphical matchings designed to tackle this challenge. We model all assets and their cointegration levels with a weighted graph, where edges signify pairs and their weights indicate the extent of cointegration. A portfolio of pairs is a subgraph of this graph. We con- struct a portfolio which is a maximum weighted matching of this graph to select pairs which have strong cointegration while simultaneously ensuring that there are no shared assets within any pair of pairs. This approach ensures each asset is included in just one pair, leading to a significantly lower variance in the matching-based portfolio compared to a baseline approach that selects pairs purely based on cointegration. Theoretical analysis and empirical testing using data from the S&P 500 between 2017 and 2023, affirm the efficacy of our method. Notably, our matching-based strategy showcases a marked improvement in risk-adjusted performance, evidenced by a gross Sharpe ratio of 1.23, a significant enhancement over the baseline value of 0.48 and market value of 0.59. Additionally, our approach demonstrates reduced trading costs attributable to lower turnover, alongs...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Toward Quantum Utility in Finance: A Robust Data-Driven Algo.pdf
Content_Extract: Abstract.Clustering financial assets based on return correlations is a fundamental task in portfolio optimization and statistical arbitrage. However, classical clustering methods often fall short when dealing with signed correlation structures, typically requiring lossy transformations and heuristic assumptions such as a fixed number of clusters. In this work, we apply the Graph-based Coalition Structure Generation algo- rithm (GCS-Q) to directly cluster signed, weighted graphs without re- lying on such transformations. GCS-Q formulates each partitioning step as a QUBO problem, enabling it to leverage quantum annealing for ef- ficient exploration of exponentially large solution spaces. We validate our approach on both synthetic and real-world financial data, bench- marking against state-of-the-art classical algorithms such as SPONGE andk-Medoids. Our experiments demonstrate that GCS-Q consistently achieves higher clustering quality, as measured by Adjusted Rand In- dex and structural balance penalties, while dynamically determining the number of clusters. These results highlight the practical utility of near- term quantum computing for graph-based unsupervised learning in fi- nancial applications. Keywords:QuantumUtility¬∑SignedGraphs¬∑GraphClustering¬∑Asset Clustering¬∑Quantum Annealing 1 Introduction and Background Information about relationships among financial assets has long been central to strategies in portfolio optimization and algorithmic trading. Since companies and their stock prices are interdependent, exploiting these correlations, espe- cially in markets that are not perfectly efficient, can enhance decision-making in This work has been accepted for publication in the proceedings of the 1st Interna- tional Quantum Engineering conference and exhibition (QUEST-IS 2025), held 1‚Äì4 December 2025, Paris, France. arXiv:2509.07766v2  [quant-ph]  13 Sep 2025 2 S. Sharma et al. portfolio construction. The foundational work of Markowitz [17] introduced the mean-variance optimization framework, establishing diversification as a means to maximize return while minimizing risk [22]. More recent strategies, such as statistical arbitrage [2] and pair trading [5], use correlation or cointegration metrics but have become less effective due to in- creased market adoption. Consequently, graph-theoretic methods have emerged as robust alternatives [13], where assets are represented as nodes and correlation matrices are treated as signed, weighted graphs. Clustering these graphs aims to group assets with high intra-cluster and low inter-cluster correlations [14,27], re- cent works focusing on identifying the best-performing clustering algorithms [4]. Traditional clustering approaches such ask-Means,k-Medoids [10,15], or hi- erarchical clustering [18] face limitations when applied to signed graphs. These algorithms require positive-definite distance metrics and often rely on trans- forming signed correlation matricesœÅ‚àà[‚àí1,1]into non-negative distances using mon...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Deep reinforcement learning for optimal trading with partial.pdf
Content_Extract: Abstract Reinforcement Learning (RL) applied to financial problems has been the subject of a lively area of research. The use of RL for optimal trading strategies that exploit latent information in the market is, to the best of our knowledge, not widely tackled. In this paper we study an optimal trading problem, where a trading signal follows an Ornstein‚ÄìUhlenbeck process with regime-switching dynamics. We employ a blend of RL and Recurrent Neural Networks (RNN) in order to make the most at extracting underlying information from the trading signal with latent parameters. The latent parameters driving mean reversion, speed, and volatility are filtered from observations of the signal, and trading strategies are derived via RL. To address this problem, we propose three Deep Deterministic Policy Gradient (DDPG)‚Äìbased algorithms that integrate Gated Recurrent Unit (GRU) networks to capture temporal dependencies in the signal. The first, a one-step approach (hid- DDPG), directly encodes hidden states from the GRU into the RL trader. The second and third are two-step methods: one (prob-DDPG) makes use of posterior regime probability estimates, while the other (reg-DDPG) relies on forecasts of the next signal value. Through extensive simulations with increasingly complex Markovian regime dynamics for the trading signal‚Äôs parameters, as well as an empirical application to equity pair trading, we find that prob-DDPG achieves superior cumulative rewards and exhibits more interpretable strategies. By contrast, reg-DDPG provides limited benefits, while hid-DDPG offers intermediate performance with less interpretable strategies. Our results show that the quality and structure of the information supplied to the agent are crucial: embedding probabilistic insights into latent regimes substantially improves both profitability and robustness of reinforcement learning‚Äìbased trading strategies. 1 Introduction Optimal trading strategies have been the subject of an active and flourishing area of research since the advent of electronic markets several decades ago. The development of more efficient quoting systems and the faster spread of information have led major market participants, on both sell and buy sides, to embed ever more granular information into their trading algorithms. In model-based approaches to algorithmic trading, recent work includes the incorporation of trading signals into the classical optimal stochastic control formulation of trading, starting with the works of G√¢rleanuandPedersen2013indiscretetimeandCarteaandJaimungal2016andCasgrainandJaimungal 2019 (with latent signals) in continuous time. Thanks to the advent of machine learning (ML), research has begun to focus on model-agnostic approaches to incorporate trading signals. In this regard, among the many methods proposed in literature, those that propose forecasting algorithms that use Recurrent Neural Networks (RNNs) show the greatest promise. For example, Tsantekidis et al. 2020, investigate man...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Parameters Optimization of Pair Trading Algorithm.pdf
Content_Extract: ABSTRACT Pair trading is a market-neutral quantitative trading strategy that exploits price anomalies between two correlated assets. By taking simultaneous long and short po- sitions, it generates profits based on relative price movements, independent of overall market trends. This study explores the mathematical foundations of pair trading, focusing on identifying cointegrated pairs, constructing trading signals, and opti- mizing model parameters to maximize returns. The results highlight the strategy‚Äôs potential for consistent profitability, even in volatile market conditions. KEYWORDS Pair Trading; Correlation; Cointegration; Z-Score; Trading Signals; Parameters Optimization. Contents 1 Introduction 2 2 Summary of the model 3 2.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Temporal Division of the Algorithm . . . . . . . . . . . . . . . . . . . 4 2.3 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3.1 Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3.2 Test on the dataset . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.4 Co-Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.4.2 Co-integration test . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4.3 Test on the dataset . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.5 Spread and Z-Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.5.2 Visualisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.6 Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.7 Parameters optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.7.1 Motivation for Optimization . . . . . . . . . . . . . . . . . . . . 10 2.7.2 Point-In-Time (PIT) Considerations . . . . . . . . . . . . . . . 10 2.7.3 Optimization Framework . . . . . . . . . . . . . . . . . . . . . 11 2.7.4 Practical Implementation . . . . . . . . . . . . . . . . . . . . . 11 arXiv:2412.12555v1  [cs.CE]  17 Dec 2024 2.7.5 Conclusion on Optimization . . . . . . . . . . . . . . . . . . . . 12 3 Results 12 3.1 Correlation and p-value . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 Cumulative returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Parameters optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.4 Cumulative returns with the optimized parameters . . . . . . . . . . . 14 4 F urther improvements 15 4.1 Computational limitations . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.2 Profitability of pair trading . . . . . . . . . . . . . . . . . . . . . . . . 15 4.3 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5 Conclusion 16 1. Introduction Pair trading, a quantitative trading strategy, gained popularity...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Risk-sensitive Reinforcement Learning Based on Convex Scorin.pdf
Content_Extract: Abstract We propose a reinforcement learning (RL) framework under a broad class of risk objectives, characterized by convex scoring functions. This class covers many common risk measures, such as variance, Expected Shortfall, entropic Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue, we consider an augmented state space and an auxiliary variable and recast the problem as a two-state optimization problem. We propose a customized Actor-Critic algorithm and establish some theoretical approximation guarantees. A key theoretical contribu- tion is that our results do not require the Markov decision process to be continuous. Additionally, we propose an auxiliary variable sampling method inspired by the alternating minimization al- gorithm, which is convergent under certain conditions. We validate our approach in simulation experiments with a financial application in statistical arbitrage trading, demonstrating the ef- fectiveness of the algorithm. Keywords: Risk-sensitive reinforcement learning, Actor-Critic algorithm, convex scoring func- tion, augmented state process, two-stage optimization, error analysis 1 Introduction In the new trend of financial innovations, reinforcement learning (RL) has gained significant at- tention in wide financial decision-making when the environment is unknown (Hambly et al. (2023)). RL features the learning process when the agent perceives and interacts with the unknown envir- onment and updates the decisions based on the trial-and-error procedure (Garc¬¥ ƒ±a and Fern¬¥ andez (2015)). In the standard RL algorithms, one considers the linear accumulation of observed rewards in the optimal decision making: inf œÄ‚ààŒ† E " TX t=0 c(St, At, St+1) # , (1) ‚àóSchool of Mathematical Sciences, Peking University, Beijing, China.   hsy.1123@pku.edu.cn ‚Ä†Corresponding Author. School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), Shenzhen, China.   yangliu16@cuhk.edu.cn ‚Ä°Department of Applied Mathematics, The Hong Kong Polytechnic University, Kowloon, Hong Kong, China.   xiang.yu@polyu.edu.hk 1 arXiv:2505.04553v2  [q-fin.MF]  15 May 2025 where œÄ ‚àà Œ† is a policy and c is a cost function. Note that maximizing the reward is equivalent to minimizing the cost. This traditional formulation actually shows a risk-neutral attitude towards the cost. However, from a practical point of view, a decision maker often exhibits strong risk avoidance rather than risk neutrality throughout the decision process, such as in autonomous driving and clinical treatment planning, because some extreme scenarios during the learning process may cause a significant or fatal loss. As a remedy, risk-sensitive RL algorithms have caught a lot of attention; see, e.g., Garc¬¥ ƒ±a and Fern¬¥ andez (2015); Chow et al. (2018); Fei et al. (2020, 2021); Zhang et al. (2021); Jaimungal et al. (2022); B¬® auerle and Glauner (2022); Ni and Lai (2022); Du et al. (2022); Wang et al. (2024); Jia (2024) for some recent developments. Typic...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Statistical Arbitrage in Options Markets by Graph Learning a.pdf
Content_Extract: Abstract. Statistical arbitrages (StatArbs) driven by machine learning has garnered considerable attention in both academia and industry. Nevertheless, deep-learning (DL) approaches to directly exploit StatArbs in options markets remain largely unexplored. Moreover, prior graph learning (GL)‚Äîa methodological basis of this paper‚Äîstudies overlooked that features are tabular in many cases and that tree-based methods outperform DL on numerous tabular datasets. To bridge these gaps, we propose a two-stage GL approach for direct identification and exploitation of StatArbs in options markets. In the first stage, we define a novel prediction target isolating pure arbitrages via synthetic bonds. To predict the target, we develop RNConv, a GL architecture incorporating a tree structure. In the second stage, we propose SLSA‚Äîa class of positions comprising pure arbitrage opportunities. It is provably of minimal risk and neutral to all Black-Scholes risk factors under the arbitrage-free assumption. We also present the SLSA projection converting predictions into SLSA positions. Our experiments on KOSPI 200 index options show that RNConv statistically significantly outperforms GL baselines, and that SLSA consistently yields positive returns, achieving an average P&L-contract information ratio of 0.1627. Our approach offers a novel perspective on the prediction target and strategy for exploiting StatArbs in options markets through the lens of DL, in conjunction with a pioneering tree-based GL. Key words: Statistical Arbitrage, Graph Learning, Options Market, Deep Learning 1. Introduction We address the problem of directly identifying and exploiting statistical arbitrage (StatArb) oppor- tunities in options markets by graph learning (GL). The concepts of StatArb strategies, though defined differently across studies, can be summarized as a class of trading strategies that generate a positive expected return with an acceptable risk of loss by leveraging price discrepancies (Laz- zarino et al. 2018). StatArbs have attracted considerable attention in both academia and the financial industry, where they have been widely researched and applied (Zhan et al. 2022). To capture and exploit StatArbs in options markets, in this paper, we develop a GL method and trading strategy. Although various machine learning approaches have been proposed in the options markets, they did not focus on direct identification of StatArbs. Specifically, Zapart (2003) treated StatArbs as an ancillary subject, and although their strategy was labeled as such, it remained substantially exposed to the canonical risk factors of Black and Scholes (1973). Horikawa and Nakagawa (2024), Franc ¬∏ois et al. (2025) employed StatArbs solely as a lens through which to compare deep hedging and delta 1 arXiv:2508.14762v2  [q-fin.PR]  21 Aug 2025 2 hedging strategies, rather than as a phenomenon to be directly uncovered. Other machine learning studies in the options markets likewise fall short of addressing the ...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Application of Black-Litterman Bayesian in Statistical Arbit.pdf
Content_Extract: ABSTRACT In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilib- rium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S&P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner. Keywords Bayesian Estimation ¬∑ Black-Litterman Model ¬∑ Portfolio Management ¬∑ Statistical Arbitrage ¬∑ co-integration ¬∑ Mean Reversion ¬∑ Pairs Trading ¬∑ Quantitative Strategies ¬∑ Machine Learning 1 Introduction We found that integrating pairs trading with the Black-Litterman model portfolio optimization gives us more capibility and flexibility to control for risk and transcation cost. Our backtest shows that the optimized portfolio based on pairs trading strategy significantly enhances the return profile and minimize the down-side risk during financial crisis. In this paper, we first developed a traditional pair trading strategy with co-integration modelling to capture asset mispricing. Then we connected pairs trading strategy to a portfolio optimization problem using the Black-Litterman Model. Black-Litterman Model employs the Bayesian approach to estimate expected returns with both historical observation and investors‚Äô views on asset performance, which in our case, the views from pairs trading strategy. The way we connect traditional pairs trading to the Black-Litterman portfolio optimization framework brings innovation to pairs trading strategy, and orchestras the methodology of statistical arbitrage and portfolio optimization. In Section 2, we present the literature overview of methodology and empirical findings of Statistical Arbitrage and Black-Litterman model. So far very little literature focuses on establish a Black-Litterman model using statistical arbitrage strategy. In Section 3, we discuss how to select co-integrated stock pairs with rigorous Engle and Granger test. Then we transform the co-integration model into real world trading signals. Section 4 shows how to incorporate into the Black-Litterman model our prior views on stock returns based on pairs trading strategy. Finally, we calibrate model parameters and backtest the portfolio from two testing periods: out-of-sample 2016 to 2018 and stressful financial crisis in Section 5 We make our conclusions in Section 6. ‚àóauthor github: https://github.coecis.cornell.edu/qz247 arXiv:2406.06706v1  [q-fin.CP]  10 Jun 2024 Research RESEARCH 2 Literature Overview 2.1 Statistical Arbitrage 2.1.1 Definition Statistical arbitrage (StatArb) is a trading strategy that seeks to...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_LLMs for Time Series: an Application for Single Stocks and S.pdf
Content_Extract: Abstract Large Language Models (LLMs) have been adapted for time series prediction with significant success in pattern recognition. However, the common belief is that these models are not suitable for predict- ing financial market returns, which are known to be almost random. We aim to challenge this misconception through a counterexample. Specifically, we utilize the Chronos model from Ansari et al. (2024) and test both pretrained configurations and fine-tuned supervised fore- casts on the largest American single stocks using data from Guijarro- Ordonnez et al. (2021). We construct a long/short portfolio, and the performance simulation indicates that LLMs can in reality handle time series that are nearly indistinguishable from noise, demonstrating an ability to identify inefficiencies amidst randomness and generate alpha. Finally, we compare these results with benchmark models, highlight- ing significant room for improvement in LLM performance to further enhance their predictive capabilities. ‚àóMachina Capital, Paris, France ‚Ä†Valeyre Research, Cannes, France ‚Ä°University of Paris XIII ‚Äì Sorbonne Paris Nord, Villetaneuse, France 1 arXiv:2412.09394v2  [q-fin.PM]  3 Nov 2025 1 Introduction Large Language Models (LLMs) gained widespread popularity when Chat- GPT convinced many that machines were intelligent enough to reason like humans, even though the underlying techniques merely determine the most likely sequences of words that could respond to a prompt. Besides, with regards to the integration of generative AI in the financial industry, Citadel CEO Ken Griffin says ‚Äùthe tech hasn‚Äôt done much for hedge funds where it matters most‚Äî beating the market‚Äù 1. This study challenges this view by applying for the first time a LLM to a large portfolio of U.S stocks. Recently, the introduction of the transformer architecture by Vaswani et al. (2017) was a key development, as it enabled fast training on large datasets. LLMs are composed of many layers of transformers and have around a billion parame- ters. For instance, the T5 (Text-To-Text Transfer Transformer) architecture was introduced by Raffel et al. (2023), and further advanced the field from T5-small size with 60 million of parameters to T6-11B with 11 billion of pa- rameters). Amatrianin (2023) describes the catalog of LLMs models from Albert to ChatGPT and classifies T5 among others. The incorporation of Transformers into trading, risk, and the portfolio management industry is on the verge of transforming the financial landscape due to the oncoming technological gap. The reason is straightforward: They are reaching the state-of-the-art performance of learning from time series in many domains including Traffic, Climate, Energy, and recently, Finance. The transformer is indeed a deep learning architecture that enhances con- siderably the analysis and processing of huge volume of diverse and complex data that are valuable for businesses and decision-makers on a real time basis. For instance, it helps mak...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Temporal Representation Learning for Stock Similarities and .pdf
Content_Extract: July 19, 2024 arxiv main Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management Yoontae Hwang1, Stefan Zohren 2, and Yongjae Lee 1 1Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea 2University of Oxford, Oxford, UK (June 17, 2024 ) In the era of rapid globalization and digitalization, accurate identiÔ¨Åcation of similar stocks has become increasingly challenging due to the non-stationary nature of Ô¨Ånancial markets and the ambiguity in conventional regional and sector classiÔ¨Åcations. To address these challenges, we examineSimStock, a novel temporal self-supervised learning framework that combines techniques from self-supervised learning (SSL) and temporal domain generalization to learn robust and informative representations of Ô¨Ånancial time series data. The primary focus of our study is to understand the similarities between stocks from a broader perspective, considering the complex dynamics of the global Ô¨Ånancial landscape. We conduct extensive experiments on four real-world datasets with thousands of stocks and demonstrate the eÔ¨Äectiveness of SimStock in Ô¨Ånding similar stocks, outperforming existing methods. The practical utility of SimStock is showcased through its application to various investment strategies, such as pairs trading, index tracking, and portfolio optimization, where it leads to superior performance compared to conventional methods. Our Ô¨Åndings empirically examine the potential of data-driven approach to enhance investment decision- making and risk management practices by leveraging the power of temporal self-supervised learning in the face of the ever-changing global Ô¨Ånancial landscape. Keywords: Representation learning, Self-supervised learning, Temporal domain generalization, Financial time series, Stock similarity, Pairs trading, Index tracking, Portfolio optimization JEL ClassiÔ¨Åcation: C45, C58, C67, G11, G17 1. Introduction The identiÔ¨Åcation of similar stocks is an essent...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Cointegrated Matrix Autoregression Models.pdf
Content_Extract: Abstract We propose a novel cointegrated autoregressive model for matrix-valued time series, with bi-linear cointegrating vectors corresponding to the rows and columns of the matrix data. Com- pared to the traditional cointegration analysis, our proposed matrix cointegration model better preserves the inherent structure of the data and enables corresponding interpretations. To esti- mate the cointegrating vectors as well as other coefficients, we introduce two types of estimators based on least squares and maximum likelihood. We investigate the asymptotic properties of the cointegrated matrix autoregressive model under the existence of trend and establish the asymptotic distributions for the cointegrating vectors, as well as other model parameters. We conduct extensive simulations to demonstrate its superior performance over traditional meth- ods. In addition, we apply our proposed model to Fama-French portfolios and develop a effective pairs trading strategy. KEYWORDS: Cointegration; Multivariate Time Series; Matrix-valued Time Series; Pairs Trad- ing. 1 arXiv:2409.10860v1  [stat.ME]  17 Sep 2024 1 Introduction Cointegration refers to a long-run, stable relationship between two or more non-stationary time series data. It has become more and more observed and recognized in econometrics, finance, and other fields. The term ‚Äòcointegration‚Äô was first introduced in the late 1980s, see Granger (1981), Granger and Weiss (1983) and Engle and Granger (1987). Since then, the phenomenon of cointegra- tion has been widely studied and various methods have been developed for testing and estimation. Statistical analyses of cointegration models were developed by Johansen (1988), Johansen (1991), and Johansen et al. (1995), among others. They derived the testing of cointegration ranks between multiple time series variables and the maximum likelihood estimator of the cointegrating vectors through a vector error correction model (VECM), which is now widely used and considered to be standard in the literature. Recently, Wang (2014) studied the martingale limit theorem for a nonlinear cointegrated re- gression model. Doornik (2017) proposed a novel estimation procedure for the I(2) cointegrated vector autoregressive model. Franchi and Johansen (2017) studied the vector cointegration model allowing for multiple near unit roots using adjusted quantiles. Cai et al. (2017) studied the bivariate threshold VAR cointegration model. Zhang et al. (2019) proposed a new method for identifying cointegrated components of nonstationary time series. She and Ling (2020) studied the full rank and reduced rank least squares estimators of the heavy-tailed vector error correction models. Gao and Tsay (2021) proposed a new procedure to build factor models for high-dimensional unit-root time series. Guo et al. (2022) proposed an automated estimation method of heavy-tailed vector error correction models. To the best of our knowledge, there has been no prior research on cointe- gration for...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Graph Learning for Foreign Exchange Rate Prediction and Stat.pdf
Content_Extract: Abstract We propose a two-step graph learning approach for foreign ex- change statistical arbitrages (FXSAs), addressing two key gaps in prior studies: the absence of graph-learning methods for foreign exchange rate prediction (FXRP) that leverage multi-currency and currency‚Äìinterest rate relationships, and the disregard of the time lag between price observation and trade execution. In the first step, to capture complex multi-currency and currency‚Äìinterest rate rela- tionships, we formulate FXRP as an edge-level regression problem on a discrete-time spatiotemporal graph. This graph consists of currencies as nodes and exchanges as edges, with interest rates and foreign exchange rates serving as node and edge features, respec- tively. We then introduce a graph-learning method that leverages the spatiotemporal graph to address the FXRP problem. In the sec- ond step, we present a stochastic optimization problem to exploit FXSAs while accounting for the observation-execution time lag. To address this problem, we propose a graph-learning method that enforces constraints through projection and ReLU, maximizes risk- adjusted return by leveraging a graph with exchanges as nodes and influence relationships as edges, and utilizes the predictions from the FXRP method for the constraint parameters and node fea- tures. Moreover, we prove that our FXSA method satisfies empirical arbitrage constraints. The experimental results demonstrate that our FXRP method yields statistically significant improvements in mean squared error, and that the FXSA method achieves a 61.89% higher information ratio and a 45.51% higher Sortino ratio than a benchmark. Our approach provides a novel perspective on FXRP and FXSA within the context of graph learning. CCS Concepts ‚Ä¢ Computing methodologies ‚Üí Neural networks. Keywords Foreign Exchange, Graph Learning, Statistical Arbitrage, Deep Learning 1 Introduction We address the problem of predicting foreign exchange (FX) rates and exploiting FX statistical arbitrage (StatArb) opportunities. Con- sider a currency triplet: A, B, and C. We first exchange one unit of currency A for B, then convert the resulting amount of B to C, and finally exchange the resulting amount of C back into A. In theory, the final amount of A should be one [10], implying inherent inter- relationships among FX rates. However, in practice, it sometimes exceeds one [2], allowing potential profits. In this paper, we predict FX rates by leveraging the interrelationships among currencies and exploit FX StatArbs (FXSAs) involving three or more currencies, which arise in practice. Although the currencies in the triplet are theoretically interre- lated‚Äîboth among themselves and with interest rates (IRs)‚Äîno prior work has studied a graph learning (GL) method to exploit these relationships in FX rate prediction (FXRP). As multiple cur- rencies exist globally, combinatorially many triplets can be formed, each inducing its own theoretical interrelations. It is difficult to r...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Market-Bench: Evaluating Large Language Models on Introducto.pdf
Content_Extract: ABSTRACT We introduceMARKET-BENCH, a benchmark that evaluates large language mod- els (LLMs) onintroductory quantitative trading tasksby asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies‚Äîscheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT‚Äîand models must pro- duce code whose P&L, drawdown, and position paths match a verifiable reference implementation. We assess thirteen state-of-the-art models using a multi-round evaluation that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics), assigning failed outputs a duplicated-metrics baseline MAE. While most models reliably execute the simplest strategy (average Executable Passes of 4.08 out of 5 rounds), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.2 achieves strong overall performance with perfect executability, GPT-5.1 Codex- Max achieves the lowest best-run error on the easiest task, and Qwen3 Max attains perfect executability yet sometimes produces inaccurate P&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at:https://marketbench.ai. 1 INTRODUCTION The field of quantitative trading is extremely high-stakes. The hallucinations and incomplete inputs of LLMs cannot be used to trade in an environment where there are millions of dollars on the line. In order for these models to be used practically in this field, they must be able to understand and apply fundamental trading concepts and basic market dynamics, such as: ‚Ä¢ Trading rules and order execution ‚Ä¢ Implementing strategies in backtests ‚Ä¢ Understanding market and risk data The existing evaluations of large language models in finance focus on high-level tasks such as summarizing earnings calls, evaluating company fundamentals, modeling cash flows, or predicting sentiment of headlines. There has been little research that evaluates models on their ability to assist traders on a day-to-day basis. This paper takes a step toward closing that gap by introducing MARKET-BENCH, a benchmark that evaluates: Given a description of a trading strategy and market data, can a large language model construct a backtest whose output metrics match those of a verifiable imple- mentation? ‚àóCorrespondence to:samj@afterquery.com. 1 arXiv:2512.12264v2  [cs.CL]  20 Jan 2026 We design MARKET-BENCHaround three strategies that capture some of the fundamental aspects of market dynamics: 1. Scheduled tradingon a single stock (NASDAQ: MSFT), focusing on order-book interaction, position tracking, and P&L accounting. 2. Pa...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Optimal market-neutral currency trading on the cryptocurrenc.pdf
Content_Extract: Abstract: This research proposes a novel arbitrage approach in multivariate pair trading, termed the Optimal Trading Technique (OTT). We present a method for selectively forming a ‚Äúbucket‚Äù of fiat currencies anchored to cryptocurrency for monitoring and exploiting trading opportunities simultaneously. To address quantitative conflicts from multiple trading signals, a novel bi-objective convex optimization formulation is designed to balance investor preferences between profitability and risk tolerance. We understand that cryptocurrencies carry significant financial risks. Therefore this process includes tunable parameters such as volatility penalties and action thresholds. In experiments conducted in the cryptocurrency market from 2020 to 2022, which encompassed a vigorous bull run followed by a bear run, the OTT achieved an annualized profit of 15.49%. Additionally, supplementary experiments detailed in the appendix extend the applicability of OTT to other major cryptocurrencies in the post-COVID period, validating the model‚Äôs robustness and effectiveness in various market conditions. The arbitrage operation offers a new perspective on trading, without requiring external shorting or holding the intermediate during the arbitrage period. As a note of caution, this study acknowledges the high-risk nature of cryptocurrency investments, which can be subject to significant volatility and potential loss. Keywords: pair trading; multivariate; quantitative trading; cryptocurrency market 1. Introduction The secondary market functions as a modern, efficient form of double auction where participants intending to trade assets submit their acceptable bid and ask prices. The highest bid and lowest ask are matched if the prices meet. We believe in the Efficient Market Hypothesis (EMH), which posits that in the secondary market, prices should reflect all available information (Fama 1970). However, historical economic bubbles show that investors do not always act rationally. Dale et al. (2005) illustrated how an order book filled with irrational bids and asks could drive prices in the wrong direction. Identifying inefficiencies in an efficient market helps maintain trading equilibrium. Market directional trading involves astute traders estimating more accurate prices for instruments. They then place orders with the expectation that the market will revert to rationality. Traders who bet correctly are rewarded by the market mechanism, contributing to price discovery. Quantitative trading, instead of relying on experienced accountants or financial analysts, utilizes mathematical modeling for better price estimations. The cryptocurrency market is an emerging and yet immature secondary market. The recent cryptocurrency market trend benefits from technological advancements and global participation. Many people criticize the significant investment risk; however, its volatility, liquidity, and 24/7 trading make it an ideal platform for algorithmic trading. The cryptocur- ...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Advanced Statistical Arbitrage with Reinforcement Learning.pdf
Content_Extract: Abstract Statistical arbitrage is a prevalent trading strategy which takes advantage of mean reverse prop- erty of spread of paired stocks. Studies on this strategy often rely heavily on model assumption. In this study, we introduce an innovative model-free and reinforcement learning based framework for statistical arbitrage. For the construction of mean reversion spreads, we establish an empirical reversion time metric and optimize asset coefficients by minimizing this empirical mean reversion time. In the trading phase, we employ a reinforcement learning framework to identify the optimal mean reversion strategy. Diverging from traditional mean reversion strategies that primarily focus on price deviations from a long-term mean, our methodology creatively constructs the state space to encapsulate the recent trends in price movements. Additionally, the reward function is carefully tailored to reflect the unique characteristics of mean reversion trading. Keywords‚Äî Statistical Arbitrage, Mean Reversion Trading, Empirical Mean Reversion Time, Reinforcement Learning JEL: C14, C61 1 Introduction Statistical arbitrage, also known as mean reversion trading or pairs trading, is an important trading strategy in the financial markets. The essence of statistical arbitrage lies in creating spreads or port- folios from the market that exhibit mean-reverting characteristics, thereby unlocking opportunities for profit. For instance, if the price of a spread falls below its long-term mean, a trader might take a long position and then wait until its price correction, aiming to profit from this adjustment. The approach to statistical arbitrage unfolds in three distinct steps: First, it entails the identi- fication of two or more securities that have shown a historical pattern of moving together. Next, a mean-reverting spread is formulated from these correlated securities. The final step involves taking a position when the spread deviates from its long-term mean, leveraging the anticipated return to equilibrium to generate profits. Therefore, mean reversion trading is divided into three main elements: (1) the identification of securities with co-movements, (2) the construction of mean-reverting spreads, and (3) the development of a trading strategy based on these mean-reverting spreads. The first two components are referred to as the formation phase, while the third is considered the trading phase. The initial step in statistical arbitrage strategy is the identification of similar securities. Traditional methods predominantly utilize distance metrics, as highlighted by Gatev et al. (2006), where pairs are formed by selecting the securities that minimizes the sum of squared deviations (SSD) between their normalized price series. This principle of pair selection can be extended to encompass pairs of rep- resentative stocks and ETFs within specific sectors (Gatev et al. (2006), Avellaneda and Lee (2010), Montana and Triantafyllopoulos (2011), Leung and Li (2016)), as w...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_MTRGL:Effective Temporal Correlation Discerning through Mult.pdf
Content_Extract: ABSTRACT In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is dis- cerning temporal correlations among entities, necessitating the inte- gration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation iden- tification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies. Index Terms‚Äî Graph, finance, multimodal 1. INTRODUCTION Pair trading, a pivotal investment strategy, capitalizes on price dis- parities between related assets or markets [1]. Traders identify tem- porary price variances, executing simultaneous long and short posi- tions on correlated assets. In a long position, a trader anticipates a price rise, while in a short position, they expect a decline, aiming to repurchase the asset at a reduced price later. This strategy, illustrated in Figure 1, leverages market inefficiencies, enabling traders to profit from transient price deviations. By doing so, pair trading not only enhances market efficiency but also fosters improved returns and risk management in portfolios. The essence of pair trading lies in identifying temporal correla- tions among assets or markets. Recognizing pairs with synchronized price movements, which tend to converge or diverge, is complex due to the market‚Äôs dynamic nature and the vast number of poten- tial pairs. This complexity demands robust quantitative analysis and advanced statistical techniques, further complicated by the evolving nature of asset relationships influenced by market shifts, regulations, and macroeconomic events. Historically, experts manually identified these correlations. While traditional methods focused on statistical inferences, they often captured only basic data trends. The rise of machine learning has ushered in a new era, demonstrating its prowess in handling vast datasets, recognizing non-linear relationships, and outperforming traditional methods. However, the potential of machine learning in pair trading remains underexplored, necessitating further research. Applying machine learning to pair trading introduces chal- lenges. Firstly, while abundant data exists in finance, its simplicity *Corresponding author: Shan Wu, wus@hfut.edu.cn To appear as a conference paper at ICASSP 2024 Entity A Entity B Short Long Close short trade Close long trade Time Price Fig. 1 . Illustration of the Pair Trading Strategy. Entities A and B are two correlated entities who...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Statistical Arbitrage in Rank Space.pdf
Content_Extract: Abstract Equity market dynamics are conventionally investigated in name space where stocks are in- dexed by company names. In contrast, by indexing stocks based on their ranks in capitaliza- tion, we gain a different perspective of market dynamics in rank space. Here, we demonstrate the superior performance of statistical arbitrage in rank space over name space, driven by a robust market representation and enhanced mean-reverting properties of residual returns in rank space. Our statistical arbitrage algorithm features an intraday rebalancing mechanism for effective conversion between portfolios in name and rank space. We explore statistical arbitrage with and without neural networks in both name and rank space and show that the portfolios obtained in rank space with neural networks significantly outperform those in name space. 1 Introduction In equity markets, stocks are conventionally labeled by equity indices (company names). By relabeling stocks according to their ranks in capitalization, rather than their equity indices (company names), a different, more stable market structure can emerge. Specifically, we will gain a different perspective on market dynamics by focusing on the stock that occupies a certain rank in capitalization while the corresponding company name may change. We refer to a market labeled by the equity indices (company names) as a market in name space and one labeled by ranks in capitalization as a market in rank space . Market in rank space was explored by Fernholtz et al. who observed a stable distribution of capitalization across different ranks in the U.S. equity market over different time peri- ods [11, 16]. They further introduced an explanatory hybrid-Atlas model under stochastic portfolio theory, a framework that enables analyzing portfolios in rank space [5,15]. Empir- ically, B. Healy et al. analyzed the U.S. equity data and showed that the market in rank space is driven by a dominant single factor [14], in contrast to the multi-factor-driven market in name space [9,10,19]. While the primary market factor in rank space has been extensively studied, the residual returns ‚Äì those not explained by this primary factor in stock returns ‚Äì remain a fertile land of adventure. In addition to intellectual intrigue, the behavior of the residual returns is closely related 1Department of Applied Physics, Department of Statistics, Stanford University, Stanford, CA 94350 yingfeil@stanford.edu 2Department of Mathematics, Stanford University, Stanford, CA 94350 papanico@math.stanford.edu 1 arXiv:2410.06568v1  [q-fin.MF]  9 Oct 2024 to statistical arbitrage, a trading strategy that exploits temporary under-pricing or over- pricing of stocks. Specifically, statistical arbitrage relies on constructing market-neutral portfolios that reproduce the residual returns and make profits if the residual returns are mean-reverting. Various methodologies have been developed for designing and managing statistical arbitrage portfolios in name space...
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2024_Statistical arbitrage in multi-pair trading strategy based o.pdf
Content_Extract: Abstract The study seeks to develop an effective strategy based on the novel framework of statistical arbitrage based on graph clustering algorithms. The amalgamation of quantitative and machine learning methods, including the Kelly criterion, and an ensemble of machine learning classifiers have been used to improve risk-adjusted returns and increase the im- munity to transaction costs over existing approaches. The study seeks to provide an integrated approach to optimal signal detection and risk management. As a part of this approach, innovative ways of optimizing take profit and stop loss functions for daily frequency trading strategies have been proposed and tested. All of the tested approaches outper- formed appropriate benchmarks. The best combinations of the techniques and parameters demonstrated significantly better performance metrics than the relevant benchmarks. The results have been obtained under the assumption of realistic transaction costs, but are sensitive to changes in some key parameters. 1. Introduction This research aims to develop an effective algorithmic trading strategy, which would be built upon the novel frame- work involving graph clustering algorithms for statistical arbitrage. The aim is important as the study contributes to the growing body of research exploring the applications of graph theory in the algorithmic trading context. Furthermore, the detailed analysis of the strategy performance will allow for the accurate assessment of the strengths and weaknesses of the novel framework. In this study, the machine learning classifiers are utilized to enhance the average profitability of the trades executed. Moreover, throughout the implementation of the strategy, numerous optimizations are tested. Thus four research questions, around which the study is organized have been formulated: ‚Ä¢ RQ1: Does the usage of signal quality classifiers improve the quality of the graph-clustering-based strategy? ‚Ä¢ RQ2: To what extent, does the implementation of transaction and risk management measures to influence the performance of the strategy? ‚Ä¢ RQ3: What is the sensitivity of the strategy to changes in transaction costs? ‚Ä¢ RQ4: To what extent does the change of weights in the classifiers ensemble influence the strategy performance? The study is based on the daily data of constituents of S&P500, spanning from January 1, 2000, to December 31, 2022. To address the formulated questions, the proposed approach involves creating an ensemble classifier of the quality of signals generated by a graph-based strategy. Individual classifiers are trained and optimized using grid search cross-validation, then combined in the weighted ensemble based on their performance on the validation dataset. ‚àóCorresponding author: a.korniejczu@student.uw.edu.pl Email addresses: a.korniejczu@student.uw.edu.pl (Adam Korniejczuk), rslepaczuk@wne.uw.edu.pl (Robert ¬¥Slepaczuk) 1ORCID: 0000-0001-5227-2014 Preprint submitted to arXiv May 2024 arXiv:2406.10695v1  [q-fin....
--- PAPER END ---

--- PAPER START ---
Category: stat_arb
File: 2025_Statistical Arbitrage in Polish Equities Market Using Deep L.pdf
Content_Extract: Abstract We study a systematic approach to a popular Statistical Arbitrage technique of Pairs Trading. Instead of relying on2highly correlated assets, the latter one is substitute with the most accu- rate replication of the first with the use of so calledrisk-factors. Such factors can be determined by: Principal Components Analysis (PCA), actual market exchange traded funds (ETFs) or, as a authorial technique and thus our contribution to the literature, Long short-term memory networks (LSTMs). Residuals between the main asset and its replication‚Äô returns are analysed on a basis of their potential mean-reversion properties. Trading signals are later generated for sufficiently fast mean-reverting portfolios to profit from any technical mispricings. Besides the introduction of a new deep-learning based method, paper re-defines methods already presented by authors of 2008‚Äôs paperStatistical Arbitrage in the U.S. Equities Marketto match conditions of the polish stock exchange market. For that reason, instead ofSP500stocks‚Äô, com- ponents ofWIG20andmWIG40combined are in scope of trading activities with an addition of polish sector indices. Overall market factors such as the risk free rate or transaction costs are also adjusted from mentioned paper for better reality matching. After setting up the scope, all details of the strategy are explained: from the theory behind risk- factors representation, through the modelling of residuals with Ornstein-Uhlenbeck process till trading signals generation procedure. They are followed by a separate section concerning specifics of each replicating technique with a general overview of the method and its application for our pur- poses. Throughout the entire thesis various examples are graphically made for better understanding of discussed topics. The final part of the paper concerns testing of the overall Pairs Trading strategy and of its presented variations. To keep the results relevant and tested in different economic conditions, two backtesting periods are distinguished: 2017-2019 and a highly recessive 2020. All strategies manage to profit during the first interval with the PCA approach achieving around20%of combined return and even up to 2.63annualized Sharpe ratio (in 2017). Even though a lot of assumptions is changed in comparison to Avellaneda and Lee‚Äô 2008 paper, received results and main conclusions are highly comparable. During the COVID-19 recession, ETFs technique are the only profitable one achieving annual re- turn of5%- both the PCA and LSTM methods fail to produce any profits. All LSTM results can be seen as promising and should be optimized in future works, especially since it is possibly the first take on such application of recurrent neural networks....
--- PAPER END ---

