# Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs

**Authors**: Zheng Li
**Date**: 2026
**Topic**: Multi-Agent Systems, LLM Trading, REITs, Fine-Tuning, Reinforcement Learning

## Summary

The paper proposes a **Multi-Agent** investment system for the **Chinese Public REITs** market, driven by LLMs.

- **Architecture**:
  - **4 Analysis Agents**: Announcement, Event, Price Momentum, Market Macro.
  - **Prediction Agent**: Fuses signals to output probability of Up/Down/Side for T+1, T+5, T+20.
  - **Decision Agent**: Maps probabilities to discrete position adjustments (e.g., "Buy 20%").
- **Models Compared**:
  - **General Purpose**: DeepSeek-R1 (invoked directly).
  - **Specialized Small Model**: Qwen3-8B fine-tuned via SFT (Supervised Fine-Tuning) and **GSPO** (Reinforcement Learning Alignment).
- **Results**:
  - Both Agent strategies significantly outperform Buy & Hold (CR 15.5% vs 10.7%, Sharpe 1.71 vs 0.75).
  - The fine-tuned small model (Qwen3-8B) achieves slightly considerably better risk-adjusted returns (Sharpe 1.77) than the large model, proving that **specialized small models can match general large models** in specific domains.

## Key Concepts

1.  **Low-Volatility Adaptation**:
    - Chinese REITs have low volatility. The system uses a **Dynamic Volatility Threshold** ($\theta_t$) to define "Sideways" vs "Trend".
    - labels for training are adaptive: `up` if $R_k \ge \sqrt{k}\theta_t$.
2.  **Teacher Model Distillation**:
    - Training data for the small model is generated by a "Teacher" (DeepSeek-R1) which produces reasoning chains + structured JSON.
3.  **GSPO (Generative Supervised Policy Optimization)**:
    - A post-training step where the model is rewarded for **Directional Correctness** (did the price go up?) and **Format Compliance** (is the JSON valid?).

## Implications for Our Platform

- **Multi-Agent Framework**:
  - **Action**: Adopts the "Analyst -> Predictor -> Decider" pattern. Our current RL agents are monolithic. Breaking them into "Feature Agents" (e.g., OrderBook Agent, News Agent) feeding a "Master Agent" could improve interpretability.
- **Small Model Fine-Tuning**:
  - **Insight**: We don't need to query GPT-4 for every tick. We can distill GPT-4's reasoning into a localized, latency-friendly LLaMA/Qwen model for production trading.
- **Dynamic Labeling**:
  - **Alpha**: When training classifiers, do not use fixed thresholds (e.g., `> 10bps`). Use dynamic thresholds based on recent volatility (`> 1.5 * DailyVol`).

## Tags

#MultiAgent #LLMTrading #REITs #FineTuning #Distillation #DeepSeek #Qwen
