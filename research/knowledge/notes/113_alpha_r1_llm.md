# Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning

**Authors**: Zuoyou Jiang, Li Zhao, et al. (StepFun, SJTU)
**Date**: 2025-12
**Topic**: Large Language Models (LLMs), Reinforcement Learning (RL), Alpha Screening, Quantitative Trading, GRPO

## Summary
The paper proposes **Alpha-R1**, a reasoning-centric investment framework that uses an 8B-parameter LLM (Alpha-R1, based on Qwen 2.5) to dynamically screen alpha factors.
*   **Problem**: Factor investing faces "Alpha Decay" and "Regime Shifts". Static models (Lasso, XGBoost) fail to adapt to new economic nuances. Standard LLMs lack financial alignment.
*   **Methodology**:
    *   **Data Abstraction**: Converts Market Data + News into "Atomic Textual Units".
    *   **Iterative Memory**: Maintains a running "Global Market Description" (long-term memory) and "Daily Market State" (short-term context).
    *   **Semantic Profiling**: Each factor has a "Semantic Profile" (what it does, when it fails) generated by an LLM.
    *   **Reasoning Core**: Alpha-R1 takes the Current Market State + Factor Profiles and outputs a **Selected Subset of Factors** to use for the next period.
    *   **RL Optimization**: Trained using **Group Relative Policy Optimization (GRPO)** (a Critic-Free RL method used in DeepSeek-R1).
        *   **Reward**: Market Performance (Sharpe, Returns) + LLM-as-a-Judge (Reasoning Quality).
*   **Key Findings**:
    *   Alpha-R1 outperforms traditional factor selection (Lasso, boosting) and standard LLMs.
    *   It effectively "turns off" momentum factors during reversals and "turns on" value factors during recovery, based on news/reasoning.

## Key Concepts
1.  **Context-Aware Alpha Screening**:
    *   Instead of numeric feature selection, the model uses *Semantic Matching* between the "Factor's Rationale" and the "Market's Narrative".
2.  **GRPO for Finance**:
    *   Using RLHF/GRPO to align LLMs not just with human preference, but with **Profitability**. The "Prompt" is the market state, the "Response" is the portfolio, and the "Reward" is the PnL.

## Implications for Our Platform
-   **LLM Factor Selector**:
    *   We can implement a module where an LLM (e.g., DeepSeek-R1 or a fine-tuned Llama) reads our `Market_State_Summary.md` and selects which Alpha Factors from our `factors/` library to activate.
    *   This adds a "Meta-Learning" layer that is semantically driven.
-   **Implementation**:
    *   Use the **GRPO** loss (Equation 11) to fine-tune a small LLM (7B) on our backtest logs.
    *   Input: "Market is Volatile, CPI high." Output: "Enable LowVol and ShortMomentum factors." Reward: Validation Sharpe.

## Tags
#LLM #AlphaScreening #ReinforcementLearning #GRPO #FactorInvesting #ReasoningModel #FinLLM
