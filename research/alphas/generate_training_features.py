
import polars as pl
import numpy as np
import sys
import os

# Import Alpha Logic
sys.path.append(os.getcwd())
from research.alphas.batch_002_papers import compute_batch_002

DATA_PATH_PARQUET = "research/data/market_data_backup.parquet"
DATA_PATH_HBT = "research/data/hbt_multiproduct/TXFB6.npy"
OUTPUT_PATH = "research/data/hbt_multiproduct/TXFB6_features_v2.npy"

def generate_features():
    print("1. Loading Parquet & Computing Alphas...")
    try:
        df_raw = pl.read_parquet(DATA_PATH_PARQUET)
        df_raw = df_raw.filter(pl.col("symbol").str.contains("TXF"))
        
        # Alpha Compute
        # Needs Flattening First (copied from batch/validation script logic)
        if "bids_price" in df_raw.columns:
             # Robust Filter
             df_raw = df_raw.filter(pl.col("bids_price").is_not_null() & (pl.col("bids_price").list.len() > 0))
             df_raw = df_raw.with_columns([
                 pl.col("bids_price").list.first().alias("bid_px_0"),
                 pl.col("bids_vol").list.first().alias("bid_qty_0"),
                 pl.col("asks_price").list.first().alias("ask_px_0"),
                 pl.col("asks_vol").list.first().alias("ask_qty_0"),
             ])
        elif "bid_price" in df_raw.columns:
             df_raw = df_raw.filter(pl.col("bid_price").is_not_null() & (pl.col("bid_price").list.len() > 0))
             df_raw = df_raw.with_columns([
                 pl.col("bid_price").list.first().alias("bid_px_0"),
                 pl.col("bid_volume").list.first().alias("bid_qty_0"),
                 pl.col("ask_price").list.first().alias("ask_px_0"),
                 pl.col("ask_volume").list.first().alias("ask_qty_0"),
             ])
        
        # Compute Alphas (Returns columns: alpha_micro_dev, alpha_ofi_i, etc.)
        # Also returns 'exch_ts' sorted
        print("   Running Batch 002...")
        df_alphas = compute_batch_002(df_raw)
        
        # Cast TS for Join
        # batch_002 already casts to Datetime("ns")
        
        print(f"   Alphas Computed: {df_alphas.shape}")
        
    except Exception as e:
        print(f"Error computing alphas: {e}")
        return

    print("2. Loading HBT Trades (Target Index)...")
    # HBT Format: structured array? No, checking load from earlier
    # It might be 2D float array or structured.
    # process_parquet_v2 saved as "data=np_data" using savez_compressed -> .npz?
    # But path is .npy? Check earlier file list.
    # File list has TXFB6.npy AND TXFB6.npz.
    # train_rl_agent uses .npy.
    
    try:
        # Load NPY
        raw_data = np.load(DATA_PATH_HBT)
        
        # Check if structured
        if raw_data.dtype.names:
            ev = raw_data['ev']
            ts = raw_data['exch_ts']
        else:
            # Assume Generic HBT: [ev, ts, local_ts, side, price, qty]
            ev = raw_data[:, 0]
            ts = raw_data[:, 1]
            
        # Filter for Trades (ev == 2)
        print("   Filtering for Trades (ev=2)...")
        mask = (ev == 2)
        trade_ts = ts[mask]
        
        print(f"   Trade Events: {len(trade_ts)}")
        
        # Convert to Polars for Asof Join
        # HBT TS is usually uint64 (ns) or int64
        # Polars expects int64 for Datetime
        df_trades = pl.DataFrame({"exch_ts": trade_ts.astype(np.int64)})
        df_trades = df_trades.with_columns(pl.col("exch_ts").cast(pl.Datetime("ns")))
        
        # Sort just in case
        df_trades = df_trades.sort("exch_ts")
        
    except Exception as e:
        print(f"Error loading HBT data: {e}")
        return

    print("3. Aligning Features (As-Of Join)...")
    # Join Alphas to Trades
    # df_alphas has alpha_hawkes, alpha_micro_dev, alpha_ofi_i, alpha_trend, alpha_hurst
    
    # We want to form a Feature Vector for RL
    # RL Env currently expects 8 features.
    # New Vector: [Alpha_Prob (if computed), Imb_L3..L5, Mid_Mom, OFI_L1, Hawkes, Inv, MicroPrice, OFI_I]
    
    # Wait, batch_002 only computes 5 Alphas (Hawkes, Micro, OFI-I, Trend, Hurst).
    # WHERE are the *existing* features? (Imb, Mom)?
    # They were likely in the `_features.npy` generated by SOME OTHER script.
    # I should find that script or re-implement them here to have a unified vector.
    # Implementation Plan says: "Merge with existing features".
    
    # Load Existing Features to merge?
    existing_feat_path = DATA_PATH_HBT.replace('.npy', '_features.npy')
    try:
        print(f"   Loading Existing Features from {existing_feat_path}")
        existing_feats = np.load(existing_feat_path)
        # Apply mask
        existing_feats = existing_feats[mask] # Trades only
        
        # Existing stats
        print(f"   Existing Feats: {existing_feats.shape} (Dim 8)")
        # [0..7]
        
        # Now we need the NEW alphas for these trades.
        # Join df_trades with df_alphas
        df_joined = df_trades.join_asof(df_alphas, on="exch_ts", strategy="backward")
        
        # Extract new columns
        # Fill Nulls (Forward fill implies hold value, but if no history, 0)
        # Polars: join_asof creates nulls if no previous match? No, typical behavior.
        
        new_cols = ["alpha_micro_dev", "alpha_ofi_i"] # Only Micro and OFI-I desired for now?
        # Plan says: [Alpha_Prob, Imb... MicroPrice_Dev, OFI_I]
        
        # Convert to numpy
        new_feats_np = df_joined.select(new_cols).fill_null(0.0).to_numpy()
        
        print(f"   New Feats: {new_feats_np.shape}")
        
        # Concatenate
        # Result: [Existing(8), New(2)] -> Dim 10
        final_feats = np.hstack([existing_feats, new_feats_np])
        
        print(f"   Final Feature Matrix: {final_feats.shape}")
        
        # Save
        np.save(OUTPUT_PATH, final_feats)
        print(f"Saved to {OUTPUT_PATH}")
        
    except Exception as e:
        print(f"Error merging/saving: {e}")

if __name__ == "__main__":
    generate_features()
