## **Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of** **UAV**



[Amir Habel*](https://orcid.org/0009-0009-4239-219X)
Skolkovo Institute of Science and
Technology
Moscow, Russia
amir.habel@skoltech.ru


[Miguel Altamirano Cabrera](https://orcid.org/0000-0002-5974-9257)
Skolkovo Institute of Science and
Technology
Moscow, Russia
m.altamirano@skoltech.ru


Roohan Ahmed Khan
Skolkovo Institute of Science and
Technology
Moscow, Russia
roohan.khan@skoltech.ru


**Abstract**



Ivan Snegirev*
Skolkovo Institute of Science and
Technology
Moscow, Russia
ivan.snegirev@skoltech.ru


Jeffrin Sam
Skolkovo Institute of Science and
Technology
Moscow, Russia
jeffrin.sam@skoltech.ru


Muhammad Ahsan Mustafa
Skolkovo Institute of Science and
Technology
Moscow, Russia
Ahsan.Mustafa@skoltech.ru



Elizaveta Semenyakina*
Skolkovo Institute of Science and
Technology
Moscow, Russia
elizaveta.semenyakina@skoltech.ru


Fawad Mehboob
Skolkovo Institute of Science and
Technology
Moscow, Russia
Fawad.Mehboob@skoltech.ru


[Dzmitry Tsetserukou](https://orcid.org/0000-0001-8055-5345) [âˆ—]

Skolkovo Institute of Science and
Technology
Moscow, Russia
d.tsetserukou@skoltech.ru



This paper presents Glove2UAV, a wearable IMU-glove interface
for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed
thresholds. To promote safer and more predictable interaction
in dynamic flight, Glove2UAV is designed as a lightweight and
easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time
and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression
with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives
for directional flight (forward/backward and lateral motion) and,
when supported by the platform, to object-interaction commands.
Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing
glove signals with UAV telemetry in both simulation and real-world
flights. The results show fast gesture-based command execution,
stable coupling between gesture dynamics and platform motion,
correct operation of the core command set in our trials, and timely
delivery of vibratile warning cues.


**Keywords**


Wearable interface, IMU glove, gesture-based control, UAV, drone
teleoperation, haptic feedback, humanâ€“robot interaction, mobile
robots.


**1** **Introduction**


As robotic systems are increasingly deployed across work and everyday scenarios, the demand for intuitive and effective robot control interfaces continues to grow. Wearable interfaces [2] leverage


âˆ—These authors contributed equally to this research.



**Figure 1: Overview of the proposed IMU-glove UAV teleoper-**
**ation system.**


natural human motion and enable intuitive interaction with low
deployment overhead, making them attractive where rapid training
and operator mobility are important.
Unmanned aerial vehicles (UAVs) are among the most demanding
mobile robotic platforms: flight dynamics and sensitivity to control
inputs make operation precision-critical, and operator errors can be
particularly consequential. With expanding UAV applications, there
is a growing need for control interfaces that remain understandable
and reliable even for users with limited piloting experience [12].
Existing approaches range from conventional two-stick transmitters to immersive and eye-tracking interfaces [13]. AeroVR [16]
explores VR teleoperation with tactile feedback for aerial manipulation, using immersive visual context and vibrotactile cues for grasp
confirmation. DroneRacing [9] and OmniRace [10] demonstrates
vision-based low-level gesture control of a racing drone via 6D hand
pose estimation and gesture recognition, targeting highly dynamic


scenarios. A handheld motion-controller approach [14] provides
an alternative to traditional transmitters, emphasizing one-handed
operation and broad platform compatibility.
Against this background, we investigate an IMU-glove for UAV
control augmented with tactile cues intended to support operator awareness and confidence during dynamic flight. Unlike VR
approaches that require specialized hardware, the proposed interface preserves expressive gesture control with substantially lower
deployment overhead. Compared to vision-based interfaces that
can be sensitive to lighting, camera placement, and environmental
constraints, an IMU glove provides a stable input channel that does
not depend on external visual tracking or a specific spatial setup

[3, 4, 7]. Relative to handheld motion controllers, the wearable form
factor enables palm- and finger-level control while preserving user
mobility and freedom of movement.
Overall, our approach combines mobility, minimal external infrastructure, a compact set of control primitives, and an integrated
tactile warning channel, making it promising for scenarios beyond
laboratory settings.


**2** **System Architecture**


**Figure 2: System architecture.**


The system implements a wearable gesture-based interface for
UAV control with an integrated vibrotactile feedback channel. The
architecture comprises four components: the operator, the IMU
glove, the PC-based processing module, and the UAV (Fig. 2). The
glove captures palm and finger motions and streams inertial data
to the PC, where signals are filtered and mapped to control commands that are transmitted to the UAV. In the reverse direction,
UAV telemetry is used to generate vibrotactile warnings that complement the operatorâ€™s visual monitoring. Overall, the loop includes
three data flows: (i) glove-to-PC IMU measurements, (ii) PC-to-UAV
control commands, and (iii) UAV-to-PC telemetry for tactile cue
generation.
The glove hardware includes IMU sensors on the palm and fingers, an ESP32 microcontroller, vibrotactile actuators, and a power
module (Fig. 3(a)). The palm IMU serves as a reference for overall
hand motion, while finger IMUs capture relative movements for
gesture commands. The ESP32 performs synchronized sampling
and wireless transmission to the processing module. Vibrotactile
sensors notify the operator when potentially risky flight regimes
are detected.



A. A. Habel et al.


**Figure 3: (a) Wearable IMU-glove with integrated vibrotactile**
**actuators. (b) UAV platform used for validating the proposed**
**interface.**


The UAV platform executes high-level motion commands via its
onboard controller, while stabilization and low-level control remain
within the autopilot loop (Fig. 3(b)). Flight-state data, including velocity, are used by the processing module to trigger speed-threshold
vibrotactile warnings during operation.


**3** **Gesture Processing and Control Logic**


The unified processing and control pipeline of the proposed IMUglove interface is presented in Fig. 4. Raw gyroscope and Accelerometer measurements are first preprocessed on board. The GY-91 modules provide built-in digital low-pass filtering and initial calibration.
On the ESP32, we additionally suppress outliers using a slidingwindow median filter:


_ğ‘¦ğ‘˜_ = med( _ğ‘¥ğ‘˜_     - _ğ‘›,ğ‘¥ğ‘˜_     - _ğ‘›_ +1 _, . . .,ğ‘¥ğ‘˜, . . .,ğ‘¥ğ‘˜_ + _ğ‘›_ âˆ’1 _,ğ‘¥ğ‘˜_ + _ğ‘›_ ) _,_ (1)


followed by zero-offset compensation. The preliminary filtered
stream is packaged into JSON and transmitted over Wi-Fi via UDP
with a timestamp for reliable parsing on the PC.
After UDP reception, the main orientation estimation stage is
executed. For the finger IMUs, where motion is represented in a
simplified 2D setting [15], we employ a first-order complementary
filter. Accelerometer-based tilt angles are computed as:



where only roll _ğœƒğ‘¥_ and pitch _ğœƒ_ _ğ‘¦_ are estimated, while _ğœƒğ‘§_ is set to zero.
The gyroscope-based estimate is obtained by numerical integration:


_ğœƒğœ”,ğ‘¡_ = _ğœƒğ‘¡_ âˆ’1 + _ğœ”ğ‘¥_ Î” _ğ‘¡,_ (3)


and the fused angle is computed as:

_ğœƒ_ Ë† _ğ‘¡_ = _ğ›¼ğœƒğœ”,ğ‘¡_ + (1 âˆ’ _ğ›¼_ ) _ğœƒğ‘,ğ‘¡,_ _ğ›¼_ âˆˆ[0 _,_ 1] _._ (4)


For the wrist IMU, we employ the Madgwick filter [8] to estimate roll, pitch, and yaw. In parallel, the Reset pose, Zero-back
alignment, and Finger Lock procedures are applied to stabilize the
zero reference pose and reduce ambiguity in gesture recognition.
The Zero-back alignment mechanism continuously corrects the
reference pose by tracking deviations of the current orientation
from the initial zero state, thereby compensating for accumulated
errors and drift. The Finger Lock procedure temporarily disables
gesture recognition during active UAV control, preventing false
activations caused by unintentional finger movements. In addition,



ï£¹ï£ºï£ºï£ºï£ºï£ºï£»



arctan 2( _ğ‘ğ‘¦,ğ‘ğ‘§_ )

âˆšï¸ƒ
arctan 2(âˆ’ _ğ‘ğ‘¥,_ _ğ‘_ [2] _ğ‘¦_ + _ğ‘ğ‘§_ [2] )


0



_ğœƒğ‘¥_

_ğœ½_ _ğ‘_ = _ğœƒ_ _ğ‘¦_


_ğœƒğ‘§_

ï£®ï£¯ï£¯ï£¯ï£¯ï£°



ï£¹ï£ºï£ºï£ºï£ºï£»



=



ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°



_,_ (2)


Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV


a manual reference pose reset (Reset pose) is provided, increasing
the systemâ€™s robustness to unexpected failures and initialization
errors.


**Figure 4: Unified IMU-glove processing and control pipeline**
**for UAV teleoperation.**


Gesture commands have been explored for controlling highlevel UAV behavior [1]. Based on the estimated orientations, the
system implements three gesture-based control functions: gripper
open/close, step-based altitude control, and continuous wrist attitude control (roll/pitch/yaw). The recognized gesture type and wrist
orientation values are forwarded to a ROS-based drone connection
module for command delivery in both simulation and real-world
flights.


**4** **Experimental Evaluation**


This section presents the results of an initial validation of the proposed wearable UAV control interface. The experiments were aimed
at (1) verifying the consistency between gesture commands extracted from the IMU-glove data and UAV motion dynamics, and (2)
demonstrating the operability of the command set in flight scenarios and during object interaction. The evaluation was based on UAV
telemetry and the gloveâ€™s inertial measurement stream processed
in real time. This experimental assessment serves as a feasibility
validation of the gestureâ€“command loop rather than a detailed
quantitative accuracy analysis, which is left for future work.
Consistency between gesture kinematics and UAV telemetry was
assessed by comparing the UAV linear velocities with the corresponding angular velocities obtained from the IMU glove. The synchronized time-series plots show the expected coherent dynamics:
changes in gesture commands are accompanied by corresponding
changes in telemetry, supporting the selected logic for mapping
palm and finger movements to control commands.


**4.1** **Experiment 1: Trajectory Flight Control.**


In the first set of experiments, the operator flew the UAV along a predefined trajectory using the IMU glove. Control was implemented
by mapping hand orientation to directional motion commands:
pitch motion of the hand commanded UAV forward/backward
movement, while right/left roll commanded lateral motion to the
right/left. The operatorâ€™s task was to guide the UAV from the start



configuration to the finish while avoiding obstacles using these
gesture commands. A 6-DoF UAV was used in this experiment. All
trials were performed by a single operator and repeated five times
to assess consistency. One run was defined as a complete flight
along the prescribed trajectory with obstacle avoidance. In all repetitions, IMU-glove data and UAV telemetry were recorded and
synchronized in real time. Fig. 5 presents the results of Experiment
1 for two representative trajectories: the Real-world column shows
a photograph of the corresponding real run, while the Simulation
column shows a trajectory visualization replayed in a simulated
environment based on telemetry from that real flight. The righthand side shows synchronized time series of UAV telemetry (Data
from UAV) and IMU-glove measurements (Data from Glove) used
to compare gesture dynamics with platform motion. Overall, the
results confirm the operability of the continuous control loop and
a consistent platform response to changes in gesture commands.


**4.2** **Experiment 2: Object Interaction**


In the second set of experiments, we evaluated finger gestures intended for object-interaction commands when supported by the
platform. The grasp command was generated by a finger-closing
gesture: when the operator closed their fingers, the gripper closed
and grasped the object. In this experiment, the UAV had 7 DoF due
to an additional gripper for object manipulation. All trials were
performed by a single operator and repeated five times. One run
consisted of approaching the object, grasping it, and departing
the interaction area. Fig. 6 shows key phases of the Experiment
2 scenario (grasp initiation and object transport): for each phase,
the Real-world column provides a photograph of the real execution, while the Simulation column shows a trajectory visualization
replayed in a simulated environment from the telemetry of the
corresponding real run. The right-hand side presents synchronized
time series of IMU-glove data (Data from Glove) and UAV telemetry
(Data from UAV). The observed sequence confirms that finger gestures can serve as a separate informative control channel within the
demonstrated scenario, complementing palm-motion-based flight
control.
The results confirm the feasibility of gesture-primitive-based
control for directional flight and object interaction. Vibrotactile
warnings were triggered when flight speed exceeded predefined
thresholds and qualitatively encouraged more cautious behavior
(reduced gesture intensity and fewer abrupt maneuvers), yielding
lower speeds and smoother motion. A quantitative evaluation of
vibrotactile feedback is left for future work. The wearable form
factor reduces reliance on external infrastructure and may lower
the entry barrier for UAV operation, while vibrotactile feedback
may support operator confidence and perceived safety in dynamic
flight regimes [6].


**5** **Conclusion**


This work presents a wearable IMU-glove for gesture-based UAV
control with vibrotactile feedback. Simulation and real-world flights
demonstrate feasibility, with prompt gesture-to-command execution, consistent coupling between control primitives and UAV motion, and timely warning cues.


A. A. Habel et al.


**Figure 5: Experiment 1 (trajectory-following control): comparison of simulation and real-world flights for two representative**
**trajectories.**


**Figure 6: Experiment 2 (object grasp). Comparison of simulation and real-world execution using the proposed interface.**



The system uses a lightweight real-time pipeline with medianbased outlier suppression, Madgwick orientation estimation, and
complementary filtering in a compact, deployable form. Future
work will expand user studies, benchmark against alternative UAV
interfaces, test transferability across UAV platforms and settings,
and explore deep-learningâ€“based gesture recognition [5].


**Acknowledgements**


Research reported in this publication was financially supported by
the RSF grant No. 24-41-02039.


**References**


[1] John Akagi, Brady Moon, Xingguang Chen, and Cameron K. Peterson. 2019.
Gesture Commands for Controlling High-Level UAV Behavior. In _Proceedings of_
_the International Conference on Unmanned Aircraft Systems (ICUAS) (ICUAS â€™19)_ .
[IEEE, 1023â€“1030. doi:10.1109/ICUAS.2019.8797743](https://doi.org/10.1109/ICUAS.2019.8797743)




[2] Joseph DelPreto and Daniela Rus. 2020. Plug-and-Play Gesture Control Using
Muscle and Motion Sensors. In _Proceedings of the ACM/IEEE International Confer-_
_ence on Human-Robot Interaction (HRI â€™20)_ [. ACM, 439â€“448. doi:10.1145/3319502.](https://doi.org/10.1145/3319502.3374823)
[3374823](https://doi.org/10.1145/3319502.3374823)

[3] Boris Gromov, Gabriele Abbate, Luca M. Gambardella, and Alessandro Giusti.
2019. Proximity Human-Robot Interaction Using Pointing Gestures and a Wristmounted IMU. In _Proceedings of the IEEE International Conference on Robotics and_
_Automation (ICRA) (ICRA â€™19)_ [. IEEE, 8084â€“8091. doi:10.1109/ICRA.2019.8794399](https://doi.org/10.1109/ICRA.2019.8794399)

[4] Amy Harrison, Andrea Jester, Surej Mouli, Antonio Fratini, and Ali Jabran. 2025.
Systematic Evaluation of IMU Sensors for Application in Smart Glove System
for Remote Monitoring of Hand Differences. _Sensors_ 25, 1, Article 2 (2025).
[doi:10.3390/s25010002](https://doi.org/10.3390/s25010002)

[5] Ismael Espinoza Jaramillo, Jin Gyun Jeong, Patricio Rivera Lopez, Choong-Ho
Lee, Do-Yeon Kang, Tae-Jun Ha, Ji-Heon Oh, Hwanseok Jung, Jin Hyuk Lee,
Won Hee Lee, and Tae-Seong Kim. 2022. Real-Time Human Activity Recognition
with IMU and Encoder Sensors in Wearable Exoskeleton Robot via Deep Learning
Networks. _Sensors_ [22, 24, Article 9690 (2022). doi:10.3390/s22249690](https://doi.org/10.3390/s22249690)

[6] Luiza Labazanova, Akerke Tleugazy, Evgeny Tsykunov, and Dzmitry Tsetserukou.
2019. SwarmGlove: A Wearable Tactile Device for Navigation of Swarm of Drones
in VR Environment. In _Haptic Interaction_, Hiroyuki Kajimoto, Dongjun Lee, SangYoun Kim, Masashi Konyo, and Ki-Uk Kyung (Eds.). Lecture Notes in Electrical


Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV


[Engineering, Vol. 535. Springer, Singapore, 304â€“309. doi:10.1007/978-981-13-](https://doi.org/10.1007/978-981-13-3194-7_67)
[3194-7_67](https://doi.org/10.1007/978-981-13-3194-7_67)

[7] Bor-Shing Lin, I-Jung Lee, Shu-Yu Yang, Yi-Chiang Lo, Junghsi Lee, and Jean-Lon
Chen. 2018. Design of an Inertial-Sensor-Based Data Glove for Hand Function
Evaluation. _Sensors_ [18, 5, Article 1545 (2018). doi:10.3390/s18051545](https://doi.org/10.3390/s18051545)

[8] Sebastian O. H. Madgwick. 2010. _An Efficient Orientation Filter for Inertial and_
_Inertial/Magnetic Sensor Arrays_ . Technical Report. University of Bristol and x-io
Technologies.

[9] Christian Pfeiffer and Davide Scaramuzza. 2021. Human-Piloted Drone Racing:
Visual Processing and Control. _IEEE Robotics and Automation Letters_ 6, 2 (2021),
[3467â€“3474. doi:10.1109/LRA.2021.3064282](https://doi.org/10.1109/LRA.2021.3064282)

[10] Valerii Serpiva, Aleksey Fedoseev, Sausar Karaf, Ali Alridha Abdulkarim, and
Dzmitry Tsetserukou. 2024. OmniRace: 6D Hand Pose Estimation for Intuitive
Guidance of Racing Drone. In _2024 IEEE/RSJ International Conference on Intelligent_
_Robots and Systems (IROS)_ [. 2508â€“2513. doi:10.1109/IROS58592.2024.10801907](https://doi.org/10.1109/IROS58592.2024.10801907)

[11] Valerii Serpiva, Ekaterina Karmanova, Aleksey Fedoseev, Stepan Perminov, and
Dzmitry Tsetserukou. 2021. Swarmpaint: Human-swarm Interaction for Trajectory Generation and Formation Control by DNN-based Gesture Interface. In
_Proceedings of the International Conference on Unmanned Aircraft Systems (ICUAS)_



_(ICUAS â€™21)_ [. IEEE, 1055â€“1062. doi:10.1109/ICUAS51884.2021.9476795](https://doi.org/10.1109/ICUAS51884.2021.9476795)

[12] Dante Tezza and Marvin Andujar. 2019. The State-of-the-Art of Humanâ€“Drone
Interaction: A Survey. _IEEE Access_ [7 (2019), 167438â€“167454. doi:10.1109/ACCESS.](https://doi.org/10.1109/ACCESS.2019.2953900)
[2019.2953900](https://doi.org/10.1109/ACCESS.2019.2953900)

[13] Issatay Tokmurziyev, Valerii Serpiva, Aleksey Fedoseev, Miguel Altamirano
Cabrera, and Dzmitry Tsetserukou. 2024. GazeRace: Revolutionizing Remote
Piloting with Eye-Gaze Control. In _Proc IEEE Int. Conf. on Systems, Man, and_
_Cybernetics (SMC)_ [. 410â€“415. doi:10.1109/SMC54092.2024.10831987](https://doi.org/10.1109/SMC54092.2024.10831987)

[14] Daria Trinitatova, Sofia Shevelo, and Dzmitry Tsetserukou. 2025. Towards
Intuitive Drone Operation Using a Handheld Motion Controller. In _2025 20th_
_ACM/IEEE International Conference on Human-Robot Interaction (HRI)_ . 1690â€“1694.
[doi:10.1109/HRI61500.2025.10974242](https://doi.org/10.1109/HRI61500.2025.10974242)

[15] Zhicheng Yang, Bert-Jan F. van Beijnum, Bin Li, Shenggang Yan, and Peter H.
Veltink. 2020. Estimation of Relative Hand-Finger Orientation Using a Small
IMU Configuration. _Sensors_ [20, 14, Article 4008 (2020). doi:10.3390/s20144008](https://doi.org/10.3390/s20144008)

[16] Grigoriy A. Yashin, Daria Trinitatova, Ruslan T. Agishev, Roman Ibrahimov, and
Dzmitry Tsetserukou. 2019. AeroVr: Virtual Reality-based Teleoperation with
Tactile Feedback for Aerial Manipulation. In _2019 19th International Conference_
_on Advanced Robotics (ICAR)_ [. 767â€“772. doi:10.1109/ICAR46387.2019.8981574](https://doi.org/10.1109/ICAR46387.2019.8981574)


