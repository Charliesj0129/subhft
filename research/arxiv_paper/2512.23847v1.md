## A Test of Lookahead Bias in LLM Forecasts

### Zhenyu Gao, Wenxi Jiang, Yutong Yan [*] January 1, 2026

**Abstract**


We develop a statistical test to detect lookahead bias in economic forecasts generated by large


language models (LLMs). Using state-of-the-art pre-training data detection techniques, we es

timate the likelihood that a given prompt appeared in an LLM’s training corpus, a statistic we


term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP


and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the


test to two forecasting tasks: news headlines predicting stock returns and earnings call tran

scripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for


assessing the validity and reliability of LLM-generated forecasts.


*Gao, Jiang, and Yan are at the Department of Finance, CUHK Business School, The Chinese Univer

sity of Hong Kong. For helpful comments, we thank Chengwang Liao, Ron Kaniel, and seminar partici

pants at CUHK. Our correspondences are gaozhenyu@baf.cuhk.edu.hk, wenxijiang@baf.cuhk.edu.hk, and


yutong.yan@link.cuhk.edu.hk, respectively. First draft: December 2025.


## **1 Introduction**

Large language models (LLMs) are increasingly used by researchers and practitioners to


generate economic and financial forecasts. In a typical setting, an LLM such as ChatGPT is


prompted with textual inputs—such as news headlines or earnings call transcripts—to pre

dict future firm-level outcomes, including stock returns or capital expenditures (e.g., Lopez

Lira and Tang, 2023; Jha et al., 2024). A growing literature finds that LLM-based forecasts


often outperform both conventional econometric models and standard machine-learning


techniques.


Despite these promising results, their interpretation requires caution. When pre-trained,


off-the-shelf LLMs are used, the evaluation effectively occurs in-sample: the model may


have been exposed to overlapping or closely related data during training, making it diffi

cult to distinguish genuine reasoning from simple recall. This chance of lookahead bias is


especially high when forecasting variables such as stock returns that are widely reported in


public sources likely included in the model’s training corpus.


Several prompting strategies attempt to mitigate this issue, such as masking firm iden

tifiers, anonymizing contextual information, or limiting access to temporal cues (Sarkar,


2024; Engelberg et al., 2025; Wu et al., 2025). Yet their effectiveness remains mixed.


Ideally, one would evaluate forecasts strictly on out-of-sample data unseen by the LLM.


However, for widely used large-scale models, the available out-of-sample horizon is short,


limiting statistical power. Retraining such models on year-by-year corpora, a clean alterna

tive, is computationally prohibitive at current scales.


An additional complication, often overlooked, is that lookahead bias is task-specific


rather than an inherent characteristic across LLMs. As shown by Carlini et al. (2023),


memorization depends systematically on factors such as the nature and public visibility of


the input text and target variable, model size and architecture, and prompt design. These


variations underscore the need for a systematic and generalizable test to detect lookahead


bias in LLM forecasts on a case-by-case basis.


To address this challenge, we develop a statistical test that determines whether an


LLM’s forecasts arise from genuine reasoning or from training data leakage. Building on


state-of-the-art membership inference attack (MIA) techniques, the proposed method is


cost-efficient and requires neither model retraining nor access to proprietary training data.


It offers researchers and practitioners a practical diagnostic tool for assessing the reliability


1


and real-world validity of LLM-generated forecasts.


We first introduce the Lookahead Propensity (LAP), a measure of how likely the text in a


prompt appeared in an LLM’s training data. For each prompt, we compute the model-assigned


probability of every token conditional on its preceding tokens. We define LAP as the mean


token probabilities of the bottom _K_ % of tokens, i.e., those tokens with the lowest predicted


probabilities. This focus on uncommon tokens is key: frequent words such as “the” or


“and” are assigned high probabilities regardless of prior exposure, whereas rare tokens carry


more information about whether the text was previously seen. The intuition is that unseen


prompts tend to contain more low-probability (outlier) tokens under the model, whereas


seen prompts are less likely to have such extreme outliers, and those tokens instead re

ceive higher probability. Accordingly, a higher LAP indicates greater model familiarity and


a higher likelihood of training-data overlap. [1]


The LAP corresponds directly to the MIN-K% PROB statistic developed in the member

ship inference attack (MIA) literature (e.g., Shokri et al., 2017; Carlini et al., 2021, 2022;


Shi et al., 2024; Cheng et al., 2024). The MIA literature proposes various measures to detect


whether a given text was included in an LLM’s training data; among them, MIN-K% PROB


statistic stands out as a robust and benchmark-level baseline, demonstrating consistently


superior performance across reported evaluation settings (Shi et al., 2024). It has been


successfully applied in several LLM engineering contexts, including detecting copyrighted


material and auditing the effectiveness of machine-unlearning procedures. See Section 2.1


for a summary. Our contribution lies in applying this measure to prompt inputs used for


economic forecasting. We develop a formal statistical test to detect lookahead bias in LLM

based forecasts, which is a central concern for empirical economists.


In Section 2.2, we formally develop our test in an econometric framework. The main


proposition implies that if forecast accuracy is correlated with LAP, such a relationship in

dicates lookahead bias rather than genuine predictive reasoning. Let us first illustrate the


intuition using the following example. We prompt the model with:


_“Here is a piece of news on July 28, 2020: Kodak Triples on Loan to Make Covid-19_


_Drug Ingredients. Do you think this is good or bad for the stock price in the short_


_term?”_


Without access to future information, the LLM would need to reason about how govern

1Following Shi et al. (2024), we set _K_ % as 20%.


2


ment pharmaceutical contracts might influence firm value. However, if its training corpus


contains subsequent coverage of the same event, the task effectively becomes one of recall


rather than reasoning. Indeed, on the following day, July 29, a major news outlet published


the headline: “Kodak’s stock rose so fast it tripped 20 circuit breakers in a single day.” The


article body reported:


_“On Tuesday, President Donald Trump announced the company would receive a_


_$765 million loan to launch Kodak Pharmaceuticals ... Following a more than_


_200% jump in Tuesday trading, the rally continued on Wednesday and the shares_


_ended up 318%.”_


When presented with the July 28 headline, the LLM may recall textual patterns from the


July 29 report, such as “shares skyrocketed,” “tripped 20 circuit breakers,” or “318 percent


increase”, and consequently infer that the news is positive for the stock price. In this in

stance, what appears to be predictive ability actually reflects information recall from the


training data rather than reasoning over unseen information. Conceptually, a model that


achieves high predictive accuracy only when the prompt contains previously seen text is


analogous to a student excelling only on exam questions encountered during practice. This


pattern indicates memorization rather than genuine comprehension. A high LAP of the in

put text, therefore, indicates that the LLM has likely been exposed to the realized outcome.


In our econometric framework, the LLM’s prediction, denoted by ˆ _µ_, consists of two


indistinguishable components: genuine reasoning and potential memorization. A stan

dard forecast-accuracy regression of the realized outcome _Y_ on ˆ _µ_ cannot disentangle these


sources of predictability. Our proposed test augments the regression with an interaction


term between ˆ _µ_ and LAP. Given that LAP serves as a validated proxy for memorization in


the MIA literature, we prove that a positive coefficient on the interaction term is equivalent


to the presence of lookahead bias. In other words, if forecast accuracy systematically in

creases as the input text becomes more familiar to the model, the predictive signal is driven


at least partially by memory rather than reasoning.


In Section 3, we implement the LAP test on two forecasting exercises using Llama-3.3,


an open-source LLM released by Meta in December 2024. The first exercise adopts the


setup in Lopez-Lira and Tang (2023), which uses firm-specific news headlines to predict


next-day stock returns. Our results indicate that memorization substantially amplifies the


apparent predictive power of LLM-generated forecasts. Specifically, the baseline regression


3


shows that a one–standard-deviation increase in LLM prediction predicts a 0.197% higher


next-day return. By comparison, a one–standard-deviation increase in LAP increases the


marginal effect of LLM prediction on next-day stock return by 0.077%, which is about 37%


of the standalone LLM effect in the baseline test. Moreover, the stronger predictability


observed among small-cap stocks is also largely due to LAP amplification, reflecting stronger


lookahead bias.


The amplification effect remains robust when controlling for two confidence measures:


the first-token conditional probability of the LLM’s response (Chen et al., 2024) and the


model’s self-reported confidence level. This robustness indicates that memorization oper

ates through a mechanism distinct from the model’s internal measure of predictive certainty.


Finally, we conduct the LAP test in a genuinely out-of-sample period using Llama-2, from


September 2023 to December 2024. In the period after the release date, no lookahead bias


should arise. We first show that the coefficient on the interaction between LAP and the


model’s prediction indeed becomes statistically insignificant. Next, we conduct a bootstrap


analysis. The bootstrap distribution based on out-of-sample data is clearly separated from


the in-sample estimate, resulting in a one-sided bootstrap _p_ -value of 0.033 for the in-sample


interaction term. These findings further suggest that the in-sample predictability is largely


driven by look-ahead bias.


Our second forecasting exercise replicates the analysis in Jha et al. (2024), which uses


firms’ earnings conference call transcripts to predict subsequent capital expenditures. Con

sistent with our previous findings, LAP significantly amplifies the relationship between LLM


predictions and future investment. The model exhibits stronger predictive performance


when analyzing transcripts that contain linguistic patterns familiar from its training data,


suggesting that apparent foresight arises at least in part from memorization rather than


genuine inference. A one–standard-deviation increase in LLM prediction is associated with


a 0.324% higher future two-quarter capital expenditure ratio (scaled by total assets). By


comparison, a one–standard-deviation increase in LAP increases the marginal effect of LLM


prediction on future CapEx ratio by 0.149%, which is about 19% of the standalone LLM


effect in the baseline.


The two exercises demonstrate that at least part of the LLM’s apparent predictive power


stems from memorization rather than genuine reasoning, underscoring the need for cau

tion when applying LLMs to economic forecasting. However, this finding does not im

4


ply that forecasts by LLMs should be avoided altogether. The extent of lookahead bias


is task-specific, not a universal property of LLMs. It depends on factors such as the na

ture of the input text and target variable, model architecture, and prompt design. The LAP


measure and the statistical test we propose provide a simple, cost-effective diagnostic for


identifying cases in which model outputs are influenced by memorization rather than an

alytical reasoning. This distinction is crucial for ensuring the validity of inferences drawn


from LLM-based forecasts, especially in backtesting exercises that rely on historical data


potentially present in the model’s training corpus. As LLMs become increasingly integrated


into empirical finance and economic research, systematically distinguishing between mem

ory and reasoning will be essential for enhancing the credibility of their predictions.

### **Related Literature**


A growing body of work applies LLMs to extract economically meaningful signals from


financial text. Some studies use corporate disclosures and earnings calls as model inputs to


predict firm actions (e.g., Cao et al., 2023; Jha et al., 2024). Others apply LLMs to financial


news to forecast stock returns (e.g., Chen et al., 2022; Lopez-Lira and Tang, 2023) and


macroeconomic variables (e.g., Hansen and Kazinnik, 2024; Bybee, 2023). Chen et al.


(2024) analyze how LLMs’ conditional probabilities affect model predictability.


Meanwhile, recent studies have highlighted the potential for lookahead bias in LLM-based


predictions (e.g., Sarkar and Vafa, 2024). Lopez-Lira et al. (2025) detect LLM memoriza

tion by comparing the model’s recalled economic variables with their realized values. They


find that LLMs can sometimes perfectly reproduce exact historical values from their train

ing period, revealing that strong forecasting performance on pre-cutoff data may reflect


memorization.


Several studies also explore strategies to mitigate such bias. Engelberg et al. (2025) in

troduce an entity-neutering prompting approach, where identifying details such as firm


names and dates are removed by LLMs to reduce recognition while retaining informa

tional content. Another line of work develops self-developed models trained under con

trolled information sets (e.g., Sarkar, 2024; He et al., 2025). For example, He et al. (2025)


train leak-free language models—ChronoBERT (149M) and ChronoGPT (1.5B)—using data


available only up to each year-end, and confirm that the findings of Chen et al. (2022) in

volve only modest look-ahead bias.


5


Our study complements the existing literature by taking a different approach. Rather


than mitigating lookahead bias, we develop a statistical test to assess its likelihood and


magnitude. Since lookahead bias in LLM predictions is inherently task-specific, our test


provides a more generalizable and adaptable diagnostic tool. Moreover, the proposed LAP


test is cost-efficient, requiring neither model retraining nor access to proprietary training


data.


This paper also builds on the engineering literature on memorization in language mod

els. Carlini et al. (2021) demonstrate that LLMs can memorize portions of their training


data, complicating applications that require strict train–test separation or date-specific cut

offs. Subsequent studies, including Carlini et al. (2023) and Cheng et al. (2024), show


that token-level likelihoods and perplexity are effective indicators of memorization. The


MIA literature—reviewed comprehensively in Section 2.1—has further developed a suite


of quantitative measures and validation procedures to determine whether a given text is


likely part of an LLM’s training corpus. Our contribution lies in extending these insights


by applying MIA techniques to develop a statistical test for detecting lookahead bias in


LLM-based economic forecasts.

## **2 Lookahead Bias Detection**

### **2.1 Lookahead Propensity (LAP)**


We introduce the statistical properties of language models, the MIA literature, and LAP


construction in this subsection.


**Language Model** A large language model (LLM) is a probabilistic framework that pre

dicts the next token in a text sequence. [2] Given a sequence of observed tokens _w≤n−_ 1 :=


( _w_ 1, _w_ 2,..., _wn−_ 1), an LLM parameterized by _θ_ estimates the likelihood of the next token


_wn_ through the conditional probability


_Pθ_             - _wn | w≤n−_ 1� = _Pθ_ ( _wn | w_ 1, _w_ 2,..., _wn−_ 1) .


2In natural language processing (NLP), a _token_ is the smallest unit of text that carries meaning, such as a
word, subword, or character, depending on the tokenization scheme.


6


LLMs learn these conditional probabilities by training on massive text corpora, adjusting


internal parameters to maximize the likelihood of observed sequences.


Through repeated exposure to billions of examples, the model internalizes statistical


patterns—strengthening associations between words, phrases, and events that frequently


co-occur. This process does not involve “understanding” in the human sense; rather, LLMs


build a compressed representation of co-occurrence patterns across language. When pre

sented with a prompt, the model retrieves relevant patterns from this learned distribution


to generate contextually coherent continuations.


Critically for our analysis, this learning process absorbs not only contemporaneous as

sociations but also retrospective narratives written after the occurrence of the event of in

terest, creating the potential for lookahead bias in applications that treat model outputs as


real-time information.


**LLM Forecast and Lookahead Bias** Training data often contain both original news and


subsequent articles describing market reactions, allowing models to learn events and out

comes together. In the Kodak example from Section 1, training data likely include both


the loan announcement and next-day coverage of the stock surge. The model thus learns


not only that “a loan announcement happened” but also that “it was followed by a sharp


stock increase.” When the model encounters similar news later, it may effectively already


“know” what happens next. This creates a kind of lookahead bias, where predictions mirror


memorized outcomes rather than genuine reasoning.


When prompted with a headline consisting of _N_ tokens ( _w_ 1,..., _wN_ ), the model com

putes


_Pθ_ (prediction _|_ headline) = _Pθ_ ( _wN_ +1 _| w≤N_ )


where _wN_ +1 corresponds to the highest probability category (“positive,” “negative,” or “neu
tral”). [3] These probabilities reflect statistical associations from training rather than causal


reasoning. For the Kodak headline “Kodak Triples on Loan to Make Covid-19 Drug Ingredi

ents” (Figure I), the model’s prediction likely reflects memorized event-outcome pairs from


its July 2020 training data.


Because media coverage of an event and its subsequent outcome typically occur in close


succession, the inclusion of event-related articles in an LLM’s training corpus likely coin

3Since we select the maximum probability token at each step, output is deterministic given the same input.


7


cides with the inclusion of outcome-related coverage. This time-period overlap introduces a


form of lookahead bias, whereby the model’s apparent ability to anticipate future outcomes


may partly arise from exposure to post-event information during training. Building on this


intuition, we can infer lookahead bias from the model’s memorization of event-specific news


headlines, quantified using our LAP measure introduced below.


**MIA Techniques** Membership inference attacks (MIAs), introduced by Shokri et al. (2017),


formalize the task of determining whether a sample was included in a machine learning


(ML) model’s training set through a shadow-model framework. Subsequent studies such


as Yeom et al. (2018) refined attack methods and evaluation metrics. Later, Carlini et al.


(2022) framed MIA as a hypothesis-testing problem using likelihood-ratio principles, offer

ing a more rigorous theoretical foundation.


The rise of LLMs has spurred growing interest in testing whether specific texts were in

cluded in a model’s pre-training corpus, a setting distinct from classical MIAs developed for


conventional ML models. This interest is driven both by the opaque nature of LLM training


data and by concerns over the potential extraction of copyrighted or sensitive content (e.g.,


Carlini et al., 2021).


Shi et al. (2024) advance this line of work by formally defining reference-free pretraining

data detection problem and introducing the time-split WIKIMIA benchmark for reliable


validation. A key insight motivating their approach is that not all tokens’ conditional prob

abilities provide equal information about memorization. Common words such as “the,”


“is,” or “to” appear frequently across all texts and thus have high probabilities regardless of


whether the model has seen the specific content before. These high-frequency tokens add


noise rather than signal to detection efforts.


Building on this insight, Shi et al. (2024) propose MIN-K% PROB, a simple, training-free


detection score for assessing whether a given text has been memorized by an LLM. The


method focuses on the most informative tokens—those assigned unusually low probabil

ities by the model. Formally, for a passage with _n_ tokens, let the LLM assign each token


a conditional probability _pi_ . MIN-K% PROB sorts tokens by _pi_ and computes the average


log-probability of the bottom K%—the least likely tokens under the model. The hyperpa

rameter _K_ determines the sensitivity of the measure to low-probability “outlier” tokens.


Empirically, _K_ = 20 yields the most robust performance. Evaluated on WIKIMIA across


8


multiple large language models, Shi et al. (2024) show that MIN-K% PROB attains an av

erage AUC of 0.72, significantly higher than alternative MIA measures. [4] On an additional


copyrighted-book validation test, it achieves an AUC of 0.88. Also, its detection perfor

mance generally improves for larger models and longer passages. Therefore, we adopt


MIN-K% PROB with _K_ = 20 as our primary detection method given its strong empirical


performance.


**Construction of LAP** We now formally define LAP (i.e., MIN-K% PROB). Consider a lan

guage model with parameters _θ_, when presented with a prompt _w_ = ( _w_ 1,..., _wN_ ), LAP is


computed as,




            1

Lookahead Propensity( _w_, _K_ ) := exp

_|SK_ _|_



log _Pθ_ ( _wn | w≤n−_ 1)
_t∈SK_











, (1)



where _Pθ_ ( _wn | w≤n−_ 1) is the conditional probability assigned by model _θ_ to token _wn_ given


its preceding context _w<n_ := ( _w_ 1,..., _wn−_ 1), and _SK ⊂{_ 1,..., _N_ _}_ denotes the subset con

taining the lowest _K_ % of tokens ranked by _Pθ_ ( _wn | w≤n−_ 1), and _K_ = 20. The input _w_


encompasses any text provided to the model, including prompts, headlines, or full news


articles, as exampled in Figures I and II.

### **2.2 Econometric Framework**


We formalize lookahead bias through a contamination model where predictions incor

porate future information via memorization mechanisms. Formally, following Sarkar and


Vafa (2024), lookahead bias manifests when a model’s prediction ˆ _µt_ = ˆ _µ_ ( _X t_ ) for time _t_ + 1


violates the orthogonality condition:


Cov( _µ_ ˆ _t_, _ϵt_ +1) _̸_ = 0. (2)


Consider a standard forecasting environment with the following data-generating pro

cess:


_Yt_ +1 = _µ_ ( _X t_ ) + _ϵt_ +1, (3)


4AUC (area under the ROC curve) measures how well a detector distinguishes members from nonmembers. It equals the probability that a random member gets a higher score than a random non-member.
An AUC of 0.5 means no better than guessing; 1.0 means perfect detection.


9


where _Yt_ denotes the realized outcome and _X t_ observable information at time _t_ (e.g., news


headlines, earnings call transcripts), _µ_ ( _X t_ ) = �[ _Yt_ +1 _| X t_ ] is the true conditional expecta
tion, and _ϵt_ +1 _∼�_ (0, _σϵ_ [2][)][ represents future innovations unpredictable given the informa-]

tion set _�t_ available at time _t_, with _µ_ ( _X t_ ) _⊥_ _ϵt_ +1.


**Definition 1** (Lookahead Bias Contamination) **.** _When a LLM suffers from lookahead bias, its_


_predictions take the form:_


_µ_ ˆ _t_ = _µ_ ( _X t_ ) + _Ltϵt_ +1 (4)


_where Lt measures memorization strength._


The key implications of our framework are as follows. First, the contamination term


_Ltϵt_ +1 violates the orthogonality condition in Equation (2), thereby introducing lookahead


bias whenever _Lt >_ 0. Specifically, Cov( _Ltϵt_ +1, _ϵt_ +1) = _Lt_ Var( _ϵt_ +1) _>_ 0 implies that any pos

itive _Lt_ induces mechanical correlation between the model prediction and the irreducible


error.


Second, the magnitude of this bias scales with the degree of LLM memorization _Lt_ .


LLMs trained with greater exposure to future data implicitly encode information about later

period content, leading to stronger deviations from the true conditional process even when


no explicit future reference is present.


Third, the resulting predictor exhibits distorted accuracy, since its expected squared


prediction error


�[( _µ_ ˆ _t −_ _Yt_ +1) [2] ] = Var( _ϵt_ +1)[1 _−_ _Lt_ ] [2],


falls mechanically as _Lt_ increases.


Finally, two boundary cases clarify the mechanism: when _Lt_ = 0 for all _Lt_, pretraining


restricted to past data eliminates bias; whereas as _Lt →_ 1, complete memorization of future


information produces full contamination.


Based on our earlier discussion, we proxy _L_ with LAP defined in Equation (1). To detect


lookahead bias, we propose testing the significance of the interaction term in the regression:


_Yt_ +1 = _β_ 1 _µ_ ˆ _t_ + _β_ 2 _Lt_ + _β_ 3( _Lt ×_ ˆ _µt_ ) + _εt_ +1 (5)


10


**Proposition 1** (Detection Statistic) **.** _By the Frisch–Waugh–Lovell (FWL) theorem,_


_L_ ˆ _µ_ )
_β_ 3 = _[Cov]_ [(] [˜] _[y]_ [,][ �],

Var( [�] _L_ ˆ _µ_ )


_where_ ˜ _y and_ [�] _L_ ˆ _µ are the residuals from projecting Yt_ +1 _and L_ ˆ _µ onto_ ( _µ_ ˆ, _L_ ) _, respectively. Hence,_


_β_ 3 _>_ 0 _⇐⇒_ _Cov_ ( ˜ _y_, [�] _L_ ˆ _µ_ ) _>_ 0.


_The partial covariance admits the structural representation_


_Cov_ ( ˜ _y_, _L_ ˆ _µ_ ) = �� _L_ [2] Var( _ϵ |_ ˆ _µ_, _L_ )� . (6)

[�]


_Therefore,_


_β_ 3 _>_ 0 _iff_ �� _L_ [2] Var( _ϵ |_ ˆ _µ_, _L_ )� _>_ 0,


_which holds whenever L is nonzero on a set of positive probability and_ Var( _ϵ |_ ˆ _µ_, _L_ ) _>_ 0 _on_


_that set. In particular, if L ≥_ 0 _a.s. and_ Pr( _L >_ 0) _>_ 0 _, then β_ 3 _>_ 0 _. If L_ = 0 _a.s., then β_ 3 = 0 _._


_Proof: See Appendix B.1._


**Interpretation** Equation (6) implies that the covariance term is strictly positive whenever


_L >_ 0, indicating that lookahead bias arises whenever _β_ 3 _>_ 0. The magnitude of this co

variance scales jointly with the intensity of memorization _L_, and it vanishes when _L_ = 0,


corresponding to the absence of lookahead bias. This formulation provides a theoretically


grounded test statistic for empirically detecting such bias. The result also clarifies why


common mitigation strategies such as prompt engineering or identifier masking are inef

fective: they do not eliminate the structural contamination term _Ltϵt_ +1 that embeds future


information in LLM outputs.

## **3 Result**

### **3.1 Data**


**Stock Market Information** Daily stock returns, open prices, and close prices are obtained


from the Center for Research in Security Prices (CRSP), covering all common stocks listed


11


on the New York Stock Exchange (NYSE), the National Association of Securities Dealers


Automated Quotations (NASDAQ), and the American Stock Exchange (AMEX). We restrict


the sample to common stocks with a share code of 10 or 11, consistent with prior studies.


We obtain financial statement variables from Compustat.


**Stock News Headlines** Data on news headlines are collected via web scraping from Bloomberg


News. We select Bloomberg News because it is part of Bloomberg L.P., a leading global fi

nancial information provider whose real-time news service is updated more frequently than


traditional outlets. We collect the news headlines for all CRSP-listed companies in the sam

ple period and match them based on company names and ticker symbols. The final dataset


consists of 91,361 news headlines covering 1,587 unique companies from January 2012 to


December 2023.


**Earnings Call Transcripts** We obtain data on firms’ earnings call transcripts from Thom

son Reuter’s StreetEvents database. Then, we merge them with CRSP and Compustat data


using firm tickers and corresponding call dates, resulting in 135,541 matched observations.


After excluding records with missing values in the key variables, our final sample consists


of 74,338 firm-quarter observations from 3,897 unique U.S. publicly listed firms between


2006 and 2020.

### **3.2 LLM Setup and Prompt Design**


Our analysis adopts Llama-3.3, an open-source model released by Meta in December


2024. We choose Llama-3.3 over proprietary alternatives such as OpenAI’s API for two key


methodological reasons.


Most importantly, Llama-3.3 provides access to prompt token probabilities, which are


essential for computing our LAP measure. While OpenAI exposes token probabilities for


generated output tokens, it does not provide access to prompt token probabilities, making


it unsuitable for our research design.


Additionally, the open-source nature of Llama-3.3 ensures long-term replicability. Model


checkpoints, which are saved snapshots of trained model parameters, can be freely down

[loaded from platforms such as HuggingFace. This allows any researcher to reproduce our](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)


analysis using the identical December 2024 version. In contrast, proprietary API providers


12


may update or deprecate models over time (as OpenAI has done with ChatGPT 3.5), pre

venting future replication using the exact model version.


For the stock news exercise, we follow the prompt design in Lopez-Lira and Tang (2023).


To ensure robustness in our analysis, we also instruct the LLM to provide a confidence score


alongside its prediction. The prompt explicitly instructs the model to predict whether the


news is “good”, “bad”, or “neutral” for the stock price of the mentioned company, [5] accom

panied by a confidence score and a brief explanation, as shown in Figure I. This structured


format is intended to enhance interpretability, allowing for clear numerical indicators for


subsequent analysis.


For each headline, the model’s response is parsed and mapped to a numerical score,


where good is mapped to +1, neutral is mapped to 0, and bad is mapped to _−_ 1. The


confidence score is directly recorded from the LLM’s output, while the explanation is stored


for interpretive analysis.


For the earnings call exercise, we adopt the approach of Jha et al. (2024), similarly


incorporating confidence scores and limiting the input to the first 500 words of each tran

script, as shown in Figure II. The prediction signal generated by the LLM for firm _i_ in quarter


_q_ is based on its earnings call transcript. From the LLM prediction on earnings call data, the


variable takes values of _−_ 1, _−_ 0.5, 0, 0.5, or 1, indicating whether future capital expendi

tures are expected to significantly decrease, slightly decrease, not change, slightly increase,


or significantly increase before they are realized.


[We use vLLM to run the Llama-3.3 checkpoint locally, enabling efficient batched in-](https://docs.vllm.ai/en/latest/)


ference on GPU. For each headline prompt, we compute the LAP measure by requesting

#### token-level log probabilities via prompt_logprobs = 1 . We configure inference with do_sample = False to ensure deterministic token selection and full reproducibility across


runs.

### **3.3 Prompt News Headlines to Predict Stock Returns**


In this exercise, we generally follow the approach of Lopez-Lira and Tang (2023) except


the following minor differences. First, we use news headlines from Bloomberg, whereas


5We also conduct robustness tests by replacing “stock price” with “stock return”, as returns more directly reflect changes in market expectations. This aligns with Baker and Wurgler (2006) who showed that sentimentdriven mispricing in certain stock categories creates predictable return patterns—underpricing during low
sentiment periods leads to higher subsequent returns, while overpricing during high sentiment leads to lower
returns. The results remain qualitatively similar.


13


they collect headlines from multiple media sources. Second, their analysis uses ChatGPT

4 to classify each headline as good (+1), bad ( _−_ 1), or neutral (0); we adopt the same


classification scheme but use Llama-3.3 for the reasons we discussed above. As shown


later, these differences do not affect the results, since we are able to replicate their main


findings within our setting.


For simplicity, we use stock market close time 4 p.m. as the cutoff to classify headlines


on day _t_ . Specifically, if a headline is published before 4 p.m., it is considered news on day


_t_ and will link it to the next-day stock return; otherwise, it is treated as news on day _t_ + 1.


We begin with the in-sample analysis, as the LAP measure is designed to detect potential


memorization within the model’s own training period, spanning January 2012 to December


2023. We then repeat the analysis on the out-of-sample data as a placebo test. In contrast,


due to a different research focus, Lopez-Lira and Tang (2023) examine an out-of-sample


period from October 2021 to May 2024, consistent with ChatGPT-4’s reported knowledge


cutoff of September 2021.


Following the discussion in Section 2.2, the baseline regression is specified as follows:


_ri_, _t_ +1 = _αi_ + _βt_ + _γ ·_ LLM _i_, _t_ + _δ ·_ (LLM _i_, _t ×_ LAP _i_, _t_ ) + _η ·_ LAP _i_, _t_ + _ui_, _t_ +1, (7)


where _ri_, _t_ +1 is the stock return of firm _i_ on day _t_ + 1 (in %), LLM _i_, _t ∈{−_ 1,0, +1 _}_ derived


from the LLM’s evaluation of news headlines, and LAP _i_, _t_ is the lookahead propensity for


the news headline as specified in equation 1. Following Lopez-Lira and Tang (2023), we


includes firm and date fixed effects. Panel A of Table I shows summary statistics of main


variables.


Table II presents our results. The specification in Column (1) does not include the inter

action term and thus replicates the main finding of Lopez-Lira and Tang (2023). It shows


that LLMs can predict returns: a one–standard-deviation increase in the LLM prediction


measure predicts a 0.197% higher next-day return. Column (2) introduces our main test.


The coefficient before the interaction term LLM _i_, _t ×_ LAP _i_, _t_ is positive and highly significant.


This means that the LLM’s predictive power increases dramatically for high-LAP headlines.


Economically, a one–standard-deviation increase in LAP increases the marginal effect of


LLM on next-day stock return by 0.077%, which is about 37% of the standalone LLM effect


shown in Column (1). The coefficient of LLM itself become insignificant.


Lopez-Lira and Tang (2023) also find that LLM-based predictions exhibit stronger fore

14


casting power for small firms. They attribute this pattern to the relatively less efficient price


discovery in small-cap stocks, which tend to adjust more slowly to new information. We


revisit this result in Table III, Column (1), by adding an interaction term between LLM and


Small, a dummy variable equal to one for firms in the bottom quintile of market capital

ization, into the our main regression. The coefficient on the interaction term is positive,


consistent with their findings. In Column (2), we further introduce a triple interaction term,


LLM _×_ Small _×_ LAP, to examine whether the stronger predictability for small-cap stocks is


also driven by lookahead bias, as specified below:


_ri_, _t_ +1 = _αi_ + _βt_ + _γ_ 1LLM _i_, _t_ + _γ_ 2Small _i_, _t_ + _γ_ 3(LLM _i_, _t ×_ Small _i_, _t_ )


+ _δ_ 1LAP _i_, _t_ + _δ_ 2(LLM _i_, _t ×_ LAP _i_, _t_ ) + _δ_ 3(LAP _i_, _t ×_ Small _i_, _t_ )


+ _δ_ 4(LLM _i_, _t ×_ LAP _i_, _t ×_ Small _i_, _t_ ) + _ui_, _t_ +1. (8)


The results confirm this suspect: the coefficient on the triple interaction term is positive and


statistically significant. Moreover, once the triple interaction is included, the coefficient on


LLM _×_ LAP becomes negative.


These patterns are consistent with lookahead bias. The apparent predictability of the


LLM increases with higher LAP values, indicating that its forecasts are more accurate when


the input text is familiar to the model.


**Compare to LLM Prediction Confidence** Recent studies have examined how an LLM’s


self-reported or inferred confidence interacts with its forecast accuracy. For example, using


a similar financial-news and stock-return setting, Chen et al. (2024) show that the con

ditional probability of an LLM’s response contains economically meaningful information


and improves predictive performance. Although these confidence measures are conceptu

ally distinct from our LAP metric, it is worthwhile to test empirically whether they capture


related mechanisms.


Specifically, we incorporate two confidence measures into our analysis: (1) the first

token conditional probability in the LLM response _p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ), which reflects the


model’s inferred belief in its prediction given the wording of the prompt (Chen et al.,


2024), and (2) the model’s self-reported confidence level, Confidence _i_, _t_ . We then con

duct horse-race regressions between these confidence measures and LAP to assess whether


15


model confidence and memorization propensity explain overlapping or distinct components


of LLM forecasting performance.


Table IV shows the results. In Column (1), we include LLM prediction, _p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ),


and their interaction term. We find that the coefficient before the interaction term is sig

nificantly positive, consistent with findings of Chen et al. (2024). In Column (2), we add


LAP and LLM _×_ LAP. The main finding remains robust: the coefficient on LLM _×_ LAP is


still significantly posistive with a similar magnitude to that reported in Table II. The effect


of _p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ) on LLM do not change materially either. In Columns (3) and (4), we


repeat the analysis by using model’s self-reported confidence, and the results are similar.


Taken together, the result suggests that LAP and model confidence capture distinct com

ponents of LLM forecasts.


**Out-of-Sample and Bootstrap Analysis** Finally, we conduct a placebo test to validate


our proposition. Specifically, we repeat the analysis using news headlines released after the


model’s release date, during which forecasts should not be subject to lookahead bias. In


this out-of-sample period, the LAP measure should capture only random noise rather than


any memorization of the input text. Accordingly, in the placebo regression, the interac

tion term between LAP and LLM is expected to yield an insignificant coefficient, consistent


with the absence of lookahead bias once the model no longer has access to training-period


information.


To implement this idea, we use Llama-2 instead of Llama-3.3, which was released in


December 2024 and therefore offers too short a horizon for our test. Llama-2 was re

leased in July 2023 and updated with some of the files in Aug 2023, providing a longer


evaluation window. To be conservative, we use the model’s release date—rather than its


claimed knowledge cutoff—as the starting point for our out-of-sample period. Thus, the


out-of-sample period is from September 2023 to December 2024. Appendix Table A.1 sum

marizes the corresponding in-sample and out-of-sample periods for each model.


To ensure comparability across in-sample and out-of-sample periods, we standardize all


variables separately within each period and re-estimate the same set of regressions. Table


V reports the results. Columns (1) and (2) replicate our main in-sample findings, which


are qualitatively similar to those obtained using Llama-3.3. LLM predictions are positively


correlated with next-day stock returns, and this relationship is stronger when LAP is high.


16


In Columns (3) and (4), we re-estimate the regressions using the out-of-sample data.


As shown in Column (4), the coefficient on the interaction term LLM [std] _×_ LAP [std] is negative


and statistically insignificant, consistent with the absence of lookahead bias during the out

of-sample period.


Next, we conduct a bootstrap analysis. We use the pairs bootstrap approach (e.g., Efron


and Tibshirani, 1994). Specifically, we repeatedly sample observations from the out-of

sample data with replacement for 10,000 repetitions and re-estimate the regression in each


bootstrap sample. The bootstrap sample size is set equal to that of the out-of-sample period.


For each iteration, we record the estimated coefficient on the interaction term between


LLM [std] and LAP [std] .


Figure III shows the empirical distribution of the bootstrap coefficients. The blue, dotted


vertical line indicates the 95th percentile of the empirical bootstrap distribution, while the


red, solid vertical line represents the in-sample estimate of the interaction coefficient shown


in Column (2). Consistent with our baseline results, the distribution in the out-of-sample


placebo period is centered at a negative value. Moreover, the in-sample interaction coeffi

cient lies well outside the 95th percentile of the empirical distribution of the out-of-sample


interaction coefficients (with one-sided _p_ -value of 0.033), highlighting a clear difference


between the in-sample and out-of-sample distributions.

### **3.4 Prompt Earnings Call Transcripts to Predict Capex**


In our second forecast exercise, we follow the approach of Jha et al. (2024), who use


earnings call transcripts to predict firms’ capital expenditures two quarters ahead. We gen

erally follow their procedure of data cleaning and variable construction for consistency.


In their analysis, each transcript is segmented into 2,000-word chunks and classified by


ChatGPT-3.5 into five categories: significantly increase (+1.0), slightly increase (+0.5), no


change (0.0), slightly decrease ( _−_ 0.5), and significantly decrease ( _−_ 1.0). We use the same


classification scheme but replace ChatGPT-3.5 with Llama-3.3. Due to the context limita

tion of the self-hosted Llama-3.3 model, we restrict each input to the first 512 words of


the transcript, yet we are still able to replicate their baseline results. Our sample period is


2006–2020, identical to theirs. Summary statistics of main variables are reported in Panel


17


B of Table I. [6]


Jha et al. (2024) find that, when prompted with earnings call transcripts, LLMs can


effectively predict firms’ subsequent CapEx ratios. We examine whether this apparent pre

dictability is driven by memorization, as proxied by LAP. Column (1) of Table VI reproduces


their main finding: the LLM-generated prediction score LLM _i_, _q_ strongly forecasts future


capital expenditure.


Column (2) introduces the interaction term between LLM and LAP. The estimation in

dicates that the observed predictability is amplified by memorization: the coefficient on


the interaction term is positive and highly significant. In terms of economic magnitude,


a one-standard-deviation increase in the LLM prediction corresponds to a 0.324% higher


CapEx ratio, while a one-standard-deviation increase in LAP raises the marginal effect of


LLM by 0.149%—approximately 19% of the standalone LLM effect in Column (1). [7]


The pattern suggests that the model’s apparent ability to extract forward-looking in

formation from corporate communications may partly reflect its memory of firms’ invest

ment activities. This is not surprising, as articles discussing a company’s investments and


earnings-call communications are likely included in the model’s training corpus.

## **4 Conclusion**


Large language models (LLMs) are increasingly used to generate economic and financial


forecasts, often appearing to outperform conventional econometric and machine-learning


methods. However, recent studies suggest that this apparent predictive power may partly


reflect lookahead bias, as evaluation is effectively conducted in-sample, and the model may


have been exposed to overlapping information during training.


We develop a statistical test to distinguish genuine reasoning from training-data leakage


in LLM-based forecasts. Building on MIA techniques, we estimate the likelihood that a


given prompt appeared in an LLM’s pre-training corpus—a measure we term Lookahead


Propensity (LAP). We show that a positive correlation between LAP and forecast accuracy


indicates the presence and magnitude of lookahead bias.


6The distribution of LAP can vary substantially with the length of the input text, which explains why the
LAP distributions differ markedly across the two exercises.
7We were unable to replicate the baseline results using Llama-2, potentially due to limitations in its ability
to handle long-context inputs. As a result, we do not include out-of-sample test results for the earnings call
exercise.


18


Applying the LAP test to two forecasting tasks—news headlines predicting stock returns


and earnings-call transcripts predicting capital expenditures—we find that the predictive


power of LLMs partly reflects memorization rather than genuine inference.


Lookahead bias is task-specific, varying with factors such as input visibility, target vari

able, model scale, and prompt design. Our LAP test provides a cost-efficient, diagnostic tool


that requires no model retraining or proprietary data access, offering a practical framework


for assessing the validity and reliability of LLM-generated forecasts.


19


## **References**

M. Baker and J. Wurgler. Investor sentiment and the cross-section of stock returns. _The_


_journal of Finance_, 61(4):1645–1680, 2006.


J. L. Bybee. The ghost in the machine: Generating beliefs with large language models. _arXiv_


_preprint arXiv:2305.02823_, 2023.


S. Cao, W. Jiang, B. Yang, and A. L. Zhang. How to talk when a machine is listening:


Corporate disclosure in the age of ai. _The Review of Financial Studies_, 36(9):3603–3642,


2023.


N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,


D. Song, U. Erlingsson, et al. Extracting training data from large language models. In


_30th USENIX security symposium (USENIX Security 21)_, pages 2633–2650, 2021.


N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. Membership inference


attacks from first principles. In _2022 IEEE symposium on security and privacy (SP)_, pages


1897–1914. IEEE, 2022.


N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memoriza

tion across neural language models. In _The Eleventh International Conference on Learning_


_Representations_, 2023.


H. Chen, A. Didisheim, and L. Somoza. Out of the black box: Uncertainty quantification


for llms via conditional probabilities. _Available at SSRN_, 2024.


Y. Chen, B. T. Kelly, and D. Xiu. Expected returns and large language models. _Available at_


_SSRN 4416687_, 2022.


J. Cheng, M. Marone, O. Weller, D. Lawrie, D. Khashabi, and B. Van Durme. Dated data:


Tracing knowledge cutoffs in large language models. _First Conference on Language Mod-_


_eling_, 2024.


B. Efron and R. J. Tibshirani. _An introduction to the bootstrap_ . Chapman and Hall/CRC,


1994.


J. Engelberg, A. Manela, W. Mullins, and L. Vulicevic. Entity neutering. _Available at SSRN_,


2025.


20


A. L. Hansen and S. Kazinnik. Can chatgpt decipher fedspeak? _Available at SSRN 4399406_,


2024.


S. He, L. Lv, A. Manela, and J. Wu. Chronologically consistent large language models. _arXiv_


_preprint arXiv:2502.21206_, 2025.


M. Jha, J. Qian, M. Weber, and B. Yang. Chatgpt and corporate policies. _arXiv preprint_


_arXiv:2409.17933_, 2024.


A. Lopez-Lira and Y. Tang. Can chatgpt forecast stock price movements? return predictabil

ity and large language models. _arXiv preprint arXiv:2304.07619_, 2023.


A. Lopez-Lira, Y. Tang, and M. Zhu. The memorization problem: Can we trust llms’ economic


forecasts? _arXiv preprint arXiv:2504.14765_, 2025.


S. K. Sarkar. Storieslm: A family of language models with time-indexed training data.


_Available at SSRN 4881024_, 2024.


S. K. Sarkar and K. Vafa. Lookahead bias in pretrained language models. _Available at SSRN_


_4754678_, 2024.


W. Shi, A. Ajith, M. Xia, Y. Huang, D. Liu, T. Blevins, D. Chen, and L. Zettlemoyer. Detecting


pretraining data from large language models. _The Twelfth International Conference on_


_Learning Representations_, 2024.


R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against


machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages


3–18. IEEE, 2017.


K. Wu, B. Yang, Z. Ying, and D. Zhou. Anonymization and information loss. _arXiv preprint_


_arXiv:2511.15364_, 2025.


S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: An

alyzing the connection to overfitting. In _2018 IEEE 31st computer security foundations_


_symposium (CSF)_, pages 268–282. IEEE, 2018.


21


**Figure I.** Example Prompt and Response for Stock News Analysis


**Example Prompt:**


**Here is a piece of news:** _“(2020-07-28) Kodak Triples on Loan to Make Covid-19_
_Drug Ingredients.”_


**Do you think this news is good, bad, or neutral for the stock price of** _Eastman_
_Kodak Company (KODK)_ **in the short term?**


**Write your answer as:**
{ **good/ bad / neutral** }
{ **confidence (0-1):** }
{ **explanation (less than 25 words)** }


**Example Response:**


{ **good** }
{ **1.0** }
{ **Loan to produce Covid-19 drug ingredients boosts prospects.** }


22


**Figure II.** Example Prompt and Response for Earnings Call Analysis


**Example Prompt:**


**Here is an excerpt from the earnings call transcript of** _Oceaneering Interna-_
_tional Inc. (OII)_ **:**


_Q1 2020 Oceaneering International Inc. Earnings Call_
_“...We began 2020 with the expectation of marginal growth and improving business_
_fundamentals across all of our segments. And then the COVID-19 pandemic erupted_
_and fueled the further deterioration of the crude oil market fundamentals...”_


**Do you think the company Oceaneering International Inc. (OII) plans to in-**
**crease or decrease its capital expenditures over the next year?**


**Write your answer as:**
{ **significantly_increase / slightly_increase / no_change / slightly_decrease /**
**significantly_decrease** }
{ **confidence (0–1):** }
{ **explanation (less than 25 words):** }


**Example Response:**


{ **significantly_decrease** }
{ **0.8** }
{ **Due to COVID-19 pandemic and crude oil market deterioration.** }


23


**Figure III.** Results from Pairs Bootstrap Inference


This figure illustrates the bootstrap distribution of the interaction coefficient _β_ between standard
ized LLM and LAP, from the specification


                     -                      _ri_, _t_ +1 = _γ_ 1 LLM [std] _i_, _t_ [+] _[ γ]_ [2][ LAP][std] _i_, _t_ [+] _[ β]_ LLM [std] _i_, _t_ _[×]_ [ LAP][std] _i_, _t_ + _µi_ + _λt_ + _ui_, _t_ +1.


In each bootstrap replication _b_ = 1,...,10,000, out-of-sample observations are resampled with replacement, the regression above is re-estimated, and an interaction coefficient _β_ [(] _[b]_ [)] is obtained. The
histogram plots the empirical distribution of these _{β_ [(] _[b]_ [)] _}_ [10,000] _b_ =1 . The red solid vertical line denotes
the interaction coefficient estimated from in-sample, while the blue dotted vertical line marks the

95th percentile of the bootstrap distribution.





24


Table I. Descriptive Statistics


Panel A: Stock News Prediction


This table presents descriptive statistics for key variables used in the stock news analysis. Statistics

reported include mean, standard deviation (SD), and percentiles (P10, P25, Median, P75, P90).


Variable Mean SD P10 P25 Median P75 P90 N


LAP ( _×_ 10 [4] ) 0.073 0.027 0.041 0.053 0.069 0.089 0.110 91,361


_rt_ +1 (%) 0.061 4.301 -2.599 -1.036 0.035 1.136 2.626 91,361


LLM 0.084 0.937 -1.000 -1.000 0.000 1.000 1.000 91,361


_p_ ( _wN_ +1 _| w≤N_ ) 0.987 0.054 0.988 0.999 1.000 1.000 1.000 91,361


Confidence 0.801 0.073 0.700 0.800 0.800 0.800 0.900 91,361


Panel B: Earnings Call Prediction


This panel presents descriptive statistics for key variables used in the corporate investment predic
tion analysis. Statistics reported include mean, standard deviation (SD), and percentiles (P10, P25,

Median, P75, P90).


Variable Mean SD P10 P25 Median P75 P90 N


LAP (%) 2.049 1.007 0.947 1.330 1.872 2.573 3.373 74,338


CapEx (%) 2.823 3.967 0.108 0.479 1.427 3.471 7.104 74,338


LLM 0.076 0.407 -0.500 0.000 0.000 0.500 0.500 74,338


25


Table II. Regression of Next Day Returns on LLM Prediction and LAP


This table reports regression results from the specification


_ri_, _t_ +1 = _αi_ + _βt_ + _γ ·_ LLM _i_, _t_ + _δ ·_ (LLM _i_, _t ×_ LAP _i_, _t_ ) + _η ·_ LAP _i_, _t_ + _ui_, _t_ +1,


where _ri_, _t_ +1 is the next-day stock return for firm _i_ at time _t_, measured in percentage points. The
variable LLM _i_, _t_ represents the LLM-generated investment signal, taking values of _−_ 1, 0, or +1,
indicating negative, neutral, or positive predictions, respectively. The measure LAP _i_, _t_ is computed
by averaging the model’s token probabilities for the hardest-to-predict _K_ = 20% of prompt tokens.
Column (1) reports results from a specification that excludes the interaction term between LLM _i_, _t_
and LAP _i_, _t_ . Column (2) adds this interaction term to evaluate whether the predictive power of the
LLM signal varies with LAP. All regressions include firm and time fixed effects, and standard errors

are clustered by date. The sample includes all U.S. common stocks with at least one news headline

covering the firm from January 2012 to December 2023.


(1) (2)


_ri_, _t_ +1 _ri_, _t_ +1


LLM _i_, _t_ 0.210*** 0.00117


(12.24) (0.03)


LAP _i_, _t_ -1.297***


(-2.61)


LLM _i_, _t ×_ LAP _i_, _t_ 2.866***


(4.86)


Firm FE Yes Yes


Date FE Yes Yes


_R_ [2] 0.179 0.180


_N_ 91,361 91,361


26


Table III. Regression of Next-Period Returns on LLM Prediction, LAP, and Firm Size


This table reports regression results examining how the predictive power of LLM-based prediction
varies with LAP and firm size. The variable LLM _i_, _t_ denotes the prediction score generated by the
model for firm _i_ at time _t_, taking values of _−_ 1, 0, or +1 to indicate negative, neutral, or positive prediction, respectively. The measure LAP _i_, _t_ is computed by averaging the model’s token probabilities
for the hardest-to-predict _K_ = 20% of prompt tokens. The indicator Small _i_, _t_ equals one if the firm
is in the bottom quintile of market capitalization based on the previous day’s market cap, and zero

otherwise. Column (1) reports a specification that includes the LLM score, the small-firm indicator,

and their interaction to assess whether the return predictability of the LLM signal differs between
small and large firms. Column (2) adds the interaction between LLM _i_, _t_ and LAP _i_, _t_, as well as the
corresponding triple interaction with Small _i_, _t_, to evaluate whether the predictive power of the LLM
signal varies jointly with LAP and firm size. All regressions include firm and time fixed effects, and

standard errors are clustered by date. The sample includes all U.S. common stocks with at least one

news headline covering the firm from January 2012 to December 2023.


(1) (2)


_ri_, _t_ +1 _ri_, _t_ +1


LLM _i_, _t_ 0.154*** 0.0733**


(11.28) (2.37)


Small _i_, _t_ -0.128 0.00502


(-0.89) (0.03)


LLM _i_, _t ×_ Small _i_, _t_ 0.263*** -0.316**


(4.23) (-2.03)


LAP _i_, _t_ -0.8743**


(-2.21)


LLM _i_, _t ×_ LAP _i_, _t_ 1.116***


(2.67)


LAP _i_, _t ×_ Small _i_, _t_ -1.906


(-1.00)


LLM _i_, _t ×_ LAP _i_, _t ×_ Small _i_, _t_ 7.910***


(3.53)


Firm FE Yes Yes


Date FE Yes Yes


_R_ [2] 0.180 0.181


_N_ 91,361 91,361


27


Table IV. Regression of Next Day Returns on LLM Prediction, LAP, Conditional Probability in Response, and Confidence


This table reports regression results that assess the incremental predictive power of LAP and alterna
tive confidence measures for next-day stock returns. The main predictors include the LLM prediction
LLM _i_, _t_, the LAP measure LAP _i_, _t_, the probability of the first generated token _p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ), and
the model’s stated confidence Confidence _i_, _t_ . The variable LLM _i_, _t_ takes values of _−_ 1, 0, or +1, indicating negative, neutral, or positive prediction, respectively. The measure LAP _i_, _t_ is computed by
averaging the model’s token probabilities for the hardest-to-predict _K_ = 20% of prompt tokens.
The term _p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ) captures the probability of the model’s first generated token, conditional
on the headline (Chen et al., 2024). The variable Confidence _i_, _t_ reflects the model’s reported confidence in its prediction. Columns (1)–(4) report specifications that sequentially add these measures

and their interactions to assess whether the predictive content of the LLM signal depends on LAP,

the probability of the first generated token, or model-reported confidence. All regressions include

firm and time fixed effects, and standard errors are clustered by date. The sample includes all U.S.

common stocks with at least one news headline covering the firm from January 2012 to December

2023.


(1) (2) (3) (4)


_ri_, _t_ +1 _ri_, _t_ +1 _ri_, _t_ +1 _ri_, _t_ +1


LLM _i_, _t_ -0.499** -0.702*** -3.425*** -3.491***


(-2.21) (-3.05) (-11.11) (-11.15)


LAP _i_, _t_ -1.305*** -1.179**


(-2.63) (-2.41)


LLM _i_, _t ×_ LAP _i_, _t_ 2.860*** 1.772***


(4.85) (3.12)


_p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ) 0.143 0.143


(0.93) (0.93)


LLM _i_, _t × p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ) 0.714*** 0.710***


(3.13) (3.12)


Confidence _i_, _t_ 0.519* 0.575**


(1.80) (2.00)


LLM _i_, _t ×_ Confidence _i_, _t_ 4.520*** 4.442***


(11.46) (11.38)


Firm FE Yes Yes Yes Yes


Date FE Yes Yes Yes Yes


_R_ [2] 0.179 0.180 0.183 0.184


_N_ 91,361 91,361 91,361 91,361


28


Table V. Regression of Next-Period Returns on LLM Prediction Scores: In-Sample
and Out-of-Sample Analysis Using Llama-2


This table reports regression results evaluating the predictive performance of LLM predictions in

both in-sample (IS) and out-of-sample (OOS) periods using the Llama-2-70B model. All variables

are standardized separately within the in-sample and out-of-sample periods. The in-sample period

spans January 2012 to September 2022, and the out-of-sample period covers September 2023 to

December 2024; definitions of these periods are provided in Table A.1. Columns (1) and (3) report

in-sample estimates, while Columns (2) and (4) report the corresponding out-of-sample estimates

using an evaluation window that does not overlap with the training period. Columns (3)–(4) assess

whether the role of LAP in enhancing predictive performance carries over from the in-sample period

to the out-of-sample period. All regressions include firm and time fixed effects, and standard errors

are clustered by date.


(1) (2) (3) (4)


_ri_ [std], _t_ +1 _ri_ [std], _t_ +1 _ri_ [std], _t_ +1 _ri_ [std], _t_ +1


LLM [std] _i_, _t_ 0.0487*** 0.0489*** 0.0839*** 0.0838***


(12.09) (12.10) (7.45) (7.47)


LAP [std] _i_, _t_ -0.00163 0.000945


(-0.50) (0.09)


LLM [std] _i_, _t_ _[×]_ [ LAP][std] _i_, _t_ 0.0120*** -0.00736


(3.44) (-0.74)


OOS Bootstrap _p_ -value [0.033]


Sample Period In-sample In-sample Out-of-sample Out-of-sample


Firm FE Yes Yes Yes Yes


Date FE Yes Yes Yes Yes


Observations 82,234 82,234 9,942 9,942


29


Table VI. Regression of Future Capital Expenditures on LLM Predictions and LAP


This table reports regression results from the specification


CapEx _i_, _q_ +2 = _αi_ + _βq_ + _γ ·_ LLM _i_, _q_ + _δ ·_ (LLM _i_, _q ×_ LAP _i_, _q_ ) + _η ·_ LAP _i_, _q_ + _ui_, _q_ +2,


where CapEx _i_, _q_ +2 denotes firm _i_ ’s capital expenditures two quarters ahead, scaled by total assets.
The terms _αi_ and _βq_ represent firm and quarter fixed effects, respectively. The variable LLM _i_, _q_ is
the model-generated prediction score for firm _i_ in quarter _q_, taking values of _−_ 1, _−_ 0.5, 0, 0.5, or 1,

corresponding to significantly decrease, slightly decrease, no change, slightly increase, and significantly increase. The measure LAP _i_, _q_ is computed by averaging the model’s token probabilities for
the hardest-to-predict _K_ = 20% of prompt tokens. Column (1) reports results from a specification
that excludes the interaction between LLM _i_, _q_ and LAP _i_, _q_ . Column (2) adds this interaction term to
assess whether the predictive content of the LLM signal varies with LAP. All regressions include firm

and quarter fixed effects, and standard errors are clustered by firm.


(1) (2)


CapEx _i_, _q_ +2 CapEx _i_, _q_ +2


LLM _i_, _q_ 0.798*** 0.514***


(15.89) (5.88)


LAP _i_, _q_ -0.0160


(-0.57)


LLM _i_, _q ×_ LAP _i_, _q_ 0.148***


(3.59)


Firm FE Yes Yes


Quarter FE Yes Yes


_R_ [2] 0.642 0.643


_N_ 74,338 74,338


30


## **A Online Appendix**

### **A.1 Llama Model Family**

Table A.1. Timeline of the Llama model family.


The data cutoff marks the latest date of information included in the model’s training data, while the

release date shows when the model became publicly available. Chat versions may contain limited

knowledge beyond the cutoff due to additional tuning and reinforcement learning from human

feedback (RLHF).


Model Knowledge Cutoff Latest Update Date In-Sample Period Out-of-Sample Period


[Llama 2 (70B)](https://huggingface.co/meta-llama/Llama-2-70b-hf) Sept 2022 Aug 2023 Jan 2012 to Sept 2022 Sep 2023 to Dec 2024


[Llama 3.3 (70B)](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) Dec 2023 Dec 2024 Jan 2012 to Dec 2023 
### **A.2 Variable definitions**


Table A.2. Variable Definitions


This table summarizes the construction and definitions of all variables used in the analysis, including

LLM prediction, the LAP measure, the model’s predicted capital spending, and realized future capital

expenditure.


Variable Definition


News Headlines Predicting Stock Returns


_ri_, _t_ +1 Next-period stock return for firm _i_, measured in percentage points. If news is


released before 4:00 p.m. on day _t_, the return is computed between the closing


prices of day _t_ and day _t_ +1. If news is released after 4:00 p.m. on day _t_, the return


is computed between the closing prices of day _t_ +1 and day _t_ +2.


LLM _i_, _t_ Prediction score generated by the LLM for firm _i_ at time _t_ . Takes discrete values


of _−_ 1, 0, or +1, indicating negative, neutral, or positive prediction, respectively.


LAP _i_, _t_ Lookahead Propensity measure based on the model’s token probabilities for the


hardest-to-predict _K_ = 20% of prompt tokens.


Continued on next page


31


**Table A.3 – continued from previous page**


Variable Definition


_p_ ( _wi_, _N_ +1 _| wi_, _≤N_ ) Probability that the model generates the first output token _wi_, _N_ +1 (e.g., “good”,


“bad”, or “neutral”) given the input prompt _wi_, _≤N_ . It reflects the model’s inner


confidence (Chen et al., 2024).


Confidence _i_, _t_ LLM’s self-reported confidence in its prediction for firm _i_ at time _t_, expressed as a


percentage.


Small _i_, _t_ Binary indicator equal to 1 if firm _i_ falls into the bottom quintile of market capital

ization based on trading-day _t −_ 1 market cap, and 0 otherwise.


Earnings Call Predicting Capital Expenditure


CapEx _i_, _q_ +2 Capital expenditures reported at the end of the quarter _q_ + 2, scaled by total book


assets.


LLM _i_, _q_ Prediction generated by the LLM for firm _i_ in quarter _q_ based on its earnings call.


The variable takes values of _−_ 1, _−_ 0.5, 0, 0.5, or 1, indicating whether future capital


expenditures are expected to significantly decrease, slightly decrease, not change,


slightly increase, or significantly increase before they are realized.


LAP _i_, _q_ Lookahead Propensity measure based on the model’s token probabilities for the


hardest-to-predict _K_ = 20% of prompt tokens.


32


## **B Proofs of Theoretical Results**

### **B.1 Proof of Proposition 1**

_Proof._ By definition,


Cov( ˜ _y_, _L_ ˆ _µ_ ) = �[ ˜ _y ·_ _L_ ˆ _µ_ ] _−_ �[ ˜ _y_ ] _·_ �[ [�] _L_ ˆ _µ_ ].

[�] [�]


Since �[ ˜ _y_ ] = 0, this reduces to


Cov( ˜ _y_, _L_ ˆ _µ_ ) = �[ ˜ _y ·_ _L_ ˆ _µ_ ].

[�] [�]


Using the residualized forms,


˜ _y_ = _ϵ −_ �[ _ϵ |_ ˆ _µ_, _L_ ],              - _L_ ˆ _µ_ = _L_ [2] _ϵ −_ �[ _L_ [2] _ϵ |_ ˆ _µ_, _L_ ].


Taking conditional expectation with respect to ( _µ_ ˆ, _L_ ),


                  -                  �[ ˜ _y ·_ _L_ ˆ _µ_ ] = � �[ ˜ _y ·_ _L_ ˆ _µ |_ ˆ _µ_, _L_ ]

[�] [�]

= ��� _ϵ −_ �[ _ϵ |_ ˆ _µ_, _L_ ]� [�] _L_ [2] _ϵ −_ �[ _L_ [2] _ϵ |_ ˆ _µ_, _L_ ]� _|_ ˆ _µ_, _L_               
= _L_ [2] ��� _ϵ −_ �[ _ϵ |_ ˆ _µ_, _L_ ]�2 _|_ ˆ _µ_, _L_          

= _L_ [2] Var( _ϵ |_ ˆ _µ_, _L_ ).


Taking the outer expectation yields


Cov( ˜ _y_, _L_ ˆ _µ_ ) = �� _L_ [2] _·_ Var( _ϵ |_ ˆ _µ_, _L_ )�,

[�]


which is strictly positive whenever _L >_ 0.


33


