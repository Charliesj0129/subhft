## **THOR: A Versatile Foundation Model for Earth Observation Climate and** **Society Applications**



Theodor Forgaard
Norwegian Computing Center
Oslo, Norway


tforgaard@nr.no



Jarle Reksten
Norwegian Computing Center
Oslo, Norway


jarlebh@nr.no



Anders Waldeland
Norwegian Computing Center
Oslo, Norway


andersuw@nr.no



Valerio Marsocci
European Space Agency Φ-lab
Frascati, Italy


valerio.marsocci@esa.int


Michael Kampffmeyer
UiT - The Arctic University of Tromsø
Tromsø, Norway


michael.c.kampffmeyer@uit.no


**Abstract**


_Current Earth observation foundation models are architec-_
_turally rigid, struggle with heterogeneous sensors and are_
_constrained to fixed patch sizes. This limits their deploy-_
_ment in real-world scenarios requiring flexible compute-_
_accuracy trade-offs._ _We propose THOR, a "compute-_
_adaptive" foundation model that solves both input hetero-_
_geneity and deployment rigidity. THOR is the first archi-_
_tecture to unify data from Copernicus Sentinel-1, -2, and -3_
_(OLCI & SLSTR) satellites, processing their native 10 m to_
_1000 m resolutions in a single model. We pre-train THOR_
_with a novel randomized patch and input image size strat-_
_egy. This allows a single set of pre-trained weights to be de-_
_ployed at inference with any patch size, enabling a dynamic_
_trade-off between computational cost and feature resolution_
_without retraining. We pre-train THOR on THOR Pretrain,_
_a new, large-scale multi-sensor dataset and demonstrate_
_state-of-the-art performance on downstream benchmarks,_
_particularly in data-limited regimes like the PANGAEA 10%_
_split, validating that THOR’s flexible feature generation ex-_
_cels for diverse climate and society applications._


**1. Introduction**


Earth Observation (EO) enables large-scale monitoring of
Earth’s systems (e.g., [9, 11, 18, 23]), but this presents a
monumental computer vision challenge. Foundation mod

1



Nicolas Longépé
European Space Agency Φ-lab
Frascati, Italy


nicolas.longepe@esa.int


Arnt-Børre Salberg
Norwegian Computing Center
Oslo, Norway


salberg@nr.no


els (FM) promise to solve EO [16], but simply applying
models pre-trained on standard natural images is often suboptimal [24] as one must ingest a vast, heterogeneous data
stream from diverse sensors (e.g., optical, SAR) at scales
from meters to kilometers ground sampling distance (GSD).


Most current EO-specific FMs (e.g., [8, 13, 26, 30, 31]),
often built on Vision Transformers (ViT), are architecturally
rigid. They are trained using a fixed input image size and
a fixed patch size (e.g., 16 _×_ 16), which creates a critical bottleneck for data-efficient adaptation: coarse patching produces a low-resolution token sequence. Consequently, dense pixel-level tasks like segmentation require
large, complex decoders (e.g., UperNet [25]) to upsample the features. These decoders often demand significant
amounts of labeled data for fine-tuning, undermining the
core data-efficiency promise of FMs.


To address these shortcomings, we propose THOR
(Transformer based foundation model for Heterogeneous
Observation and Resolution), a versatile multi-modal foundation model designed for flexibility. THOR is the first
architecture to both unify the 10 m - 1000 m GSD range
of Sentinel-1, -2, and -3 (including the SLSTR sensor)
and integrate a compute-adaptive patching strategy, solving both input heterogeneity and deployment rigidity simultaneously. By incorporating a randomized patch size
and input image size during pre-training, THOR becomes
"compute-adaptive". A single set of weights can be deployed with various patch sizes and input image sizes. This


allows a user to select a smaller patch size at inference time,
producing a denser, higher-resolution token sequence that
can be processed by simpler, less data-hungry decoders.
This increased detail is crucial for tasks requiring highresolution understanding, such as fine-grained classification, and allows the dense representations to be paired with
simpler, lightweight decoders. Such lightweight decoders
are especially useful for cases with limited training data,
as they reduce the risk of overfitting compared to heavier
decoder architectures. Conversely, selecting a lower token
density significantly decreases the ViT memory and compute requirements, making it more applicable for globalscale tasks like climate trend analysis and ocean monitoring, or scenarios where sufficient training data is available
to support larger, more complex decoders. Moreover, the
multi-sensor integration allows THOR to leverage synergistic information: the all-weather radar sensing capability from Sentinel-1, the rich spectral detail of optics from
Sentinel-2, and the broad-scale climate context from the
Sentinel-3 OLCI and SLSTR instruments, all within a single, cohesive model.
To enable a model to learn this compute-adaptive, multiresolution capability, we created the THOR Pretrain dataset,
a new, large-scale dataset of 22TB that has been aligned
spatio-temporally and across modalities. It is the first to
unify data from Sentinel-1, -2, and Sentinel-3 (both OLCI
and SLSTR) satellites, processing their data at native resolutions from 10 m to 1000 m. THOR Pretrain also contains diverse land cover products, digital elevation models
(DEM), and ERA5-Land variables.
In summary, our key contributions are as follows:

- A flexible, multi-sensor architecture that is the first FM
to unify Sentinel-1 SAR, Sentinel-2 MSI, and Sentinel-3
OLCI & SLSTR data from 10 m - 1000 m GSD, built on
a compute-adaptive backbone (Sec. 4.1).

- A novel multi-modal pre-training framework that extends
the flexible patching to a MAE setup, combining pixellevel reconstruction with pretext tasks for land cover and
climate variables (Sec. 4.3).

- THOR Pretrain: A new, large-scale and diverse multimodal EO dataset, curated with a novel sampling strategy
to ensure geographic and thematic diversity (Sec. 3).
We demonstrate that this co-design achieves state-of-the-art
performance in limited training data regimes (Sec. 5).


**2. Related work**


**Self-supervised pre-training strategies in EO.** Recent EO
FMs leverage self-supervised learning (SSL) [28], primarily via Masked Autoencoders (MAE) (e.g., Prithvi-EO-2.0

[26], MMEarth [21], SatMAE [5]) and hybrid contrastive
methods (e.g., CROMA [8]). While powerful, these models are architecturally rigid. Prithvi-EO-2.0 is pre-trained
exclusively on 30 m GSD data [26], MMEarth adopts a



"resample-to-grid" strategy, harmonizing all data to a 10
m grid and discarding native resolution information [21],
and CROMA adopts a contrastive objective for radar-optical
sensor invariance with an MAE reconstruction objective [8].
They are all built on fixed patch sizes (e.g., 16 _×_ 16 or 8 _×_ 8).
This locks in a specific computational profile and, as argued
in the introduction, necessitates large, data-hungry decoders
for dense pixel-level tasks.


**Architectural solutions for input heterogeneity.** Stateof-the-art models like TerraMind [13], DOFA [31], and
Copernicus-FM [30] employ sophisticated input data processing strategies, such as TerraMind’s "dual-scale early
fusion" for nine modalities or DOFA’s wavelength dependent "dynamic weight generator" that functions as a flexible translation layer for heterogeneous sensor data. AnySat

[3] achieves this versatility by utilizing scale-adaptive spatial encoders and introducing Joint Embedding Predictive
Architecture [2] for multi-modal EO data and leverages
the spatial alignment of multiple modalities as a source of
self-supervision. Copernicus-FM unifies all major Copernicus Sentinel missions (Sentinel-1 SAR, Sentinel-2 MSI,
Sentinel-3 OLCI, Sentinel-5P), spanning the full 10 m to
1000 m GSD range [30]. Its "extended dynamic hypernetwork" generates weights based on sensor metadata, creating an "input-flexible" model. However, this flexibility is
primarily focused on handling diverse inputs rather than
deployment versatility. Scale-MAE [22] modifies the positional encoding to be "scale-aware" by scaling its positional encoding by the image’s GSD. However, it only handles one modality at a time, and its fixed patch size leads
to inconsistent sequence lengths and computational loads
for the same ground area. USat [12] uses separate patch
projection layers for different bands. While this effectively
ingests multi-modal data, USat’s architecture remains rigid.
Its "superpositional encoding" scheme is structurally constrained and, most importantly, incompatible with a flexible
patching strategy required for compute-adaptive inference.
While these models represent the best efforts to handle diverse inputs, they are "deployment-rigid", with a fixed patch
size and input image size during pre-training. For instance,
Copernicus-FM is trained with a fixed image footprint, limiting Sentinel-5P images to only a few pixels.


**Architectural rigidity and adaptive models.** FlexiViT [4]
demonstrated that by randomizing the patch size during pretraining, a single set of ViT weights can perform computeadaptive inference, allowing users to select the preferred
patch size during inference. This flexible-patching concept is only beginning to be adopted in EO-specific FMs.
Galileo [27] incorporates resizable patch embeddings from
FlexiViT, and pairs this architectural flexibility with a dualobjective training strategy to ensure the learned features
capture both the high-level semantic context (global) and
fine-grained detail (local) critical for diverse EO tasks. Sim


2


ilarly, FlexiMo [15] utilizes the FlexiViT strategy and includes a "wavelength-guided channel adaptation" module to
handle multi-sensor inputs, allowing the pre-trained model
to adapt to arbitrary spatial resolutions and maintain multiscale feature fidelity. While these models are a key step towards deployment versatility, they are focused on Sentinel1 and -2, without scaling to the full multi-resolution challenge (10 m – 1000 m) posed by sensors like Sentinel-3
OLCI & SLSTR.
**The gap: synthesizing input and deployment.** The related work reveals two powerful, yet until now, parallel lines
of research. On one side, models like Copernicus-FM and
USat solve input heterogeneity but are deployment-rigid.
On the other, models like Galileo and FlexiMo solve deployment versatility but have not been scaled to the full
multi-resolution (10 m - 1000 m) challenge. A critical gap
therefore exists: no single architecture has solved both input
heterogeneity and deployment versatility simultaneously.
Our work, THOR, is designed to be the first to fill this gap.
This challenge is non-trivial, as it requires co-designing the
positional encoding, per-band patch projection, and MAE
loss function to be mutually compatible across a 100x GSD
range (10 m to 1000 m). We propose a new architecture
that synthesizes state-of-the-art approaches for multi-sensor
input with compute-adaptive patching, enabling a single
model to operate efficiently across the full 10 m - 1000 m
GSD range. We detail this architecture in Sec. 4.


**3. THOR Pretrain**


THOR is pre-trained on a new, diverse, and large-scale
dataset named THOR Pretrain. This dataset is curated to
learn representations that are robust to variations in global
land cover, ocean phenomena, and cloud conditions.
THOR Pretrain unifies data from four major Copernicus Sentinel sensors: Sentinel-1 SAR, Sentinel-2 MSI,
Sentinel-3 OLCI and SLSTR. These sensors provide diverse
image modalities, including radar, multispectral and thermal, with resolutions ranging from 10 m to 1000 m.
Instead of stacking millions of small image crops, we
sample EO data using the Sentinel-2 tiles (110 _×_ 110 km)
as the sampling grid. This grounds the samples in a wellknown geographic unit. To ensure a rich, diverse dataset
not biased towards common land covers, we employed a
stratified sampling strategy based on k-means clustering of
land cover and RGB features. This actively over-samples
rare geographic and thematic classes. A total of 6273 globally distributed locations were sampled. For a sampled grid
location and time, we acquired the corresponding Sentinel2 data (Level 2A) along with overlapping Sentinel-1 SAR
(GRD), Sentinel-3 OLCI (Level 1C) and SLSTR data. To
ensure temporal consistency across modalities, we restrict
the acquisition window for Sentinel-1 and Sentinel-3 imagery to the be within _±_ 1 days of the Sentinel-2 anchor



timestamp for land areas and the same day for ocean areas.
Sentinel-3 data is selected from a nine times larger area to
account for the coarser resolution. For each location, we
also include the digital elevation model (DEM), and diverse
land cover maps: WorldCover [32], GlobCover [1], MODIS

[7], and ERA5-Land climate variables [19, 20].

To obtain temporal diversity in the dataset, each location is sampled a random number of times, leading to a total number of 18332 tile and date combinations, designed
to support compute-adaptive pre-training and downstream
generalization across diverse climate and social applications. The total size of the dataset is approximately 22 TB.
Full details on data processing, spatio-temporal alignment,
and the exact sampling weights are provided in the Supplementary Material.


**4. THOR foundation model**


As established in the related work, THOR is the first architecture designed to simultaneously solve input heterogeneity and deployment versatility. The core novelty of THOR is
the first successful extension, synthesis and scaling of three
state-of-the-art concepts: 1) Per-band patch projection strategy inspired by USat to handle heterogeneous sensor data.
2) Extension of the flexible patching and weight-resizing
strategy from FlexiViT to an MAE framework with random
input image sizes, enabling dynamic input image sizes and
patch sizes during inference. 3) GSD-aware 2D ALiBi encoding, inspired by CROMA, to maintain spatial context
across varying GSDs and patch sizes. This section details
the integration of these components and the multi-pretext
learning framework.


**4.1. Encoder architecture and flexible patching**


The core of THOR is a modified vision transformer (ViT)

[6], built to solve both input heterogeneity (multi-sensor,
multi-resolution) and deployment rigidity (fixed patch size
and flexible input image size) simultaneously (Fig. 1).


**4.1.1. Multi-sensor integration**


To handle the highly diverse sensor inputs, the model is inspired by the USat architecture [12]. As for USat, THOR
employs a separate patch projection layer for each input
band. This flexibility allows the model to process any subset
of bands during fine-tuning, accommodating computational
constraints or missing data. The encoder supports grouping arbitrary sets of bands with the same GSD, allocating a
larger number of patches for higher resolution bands to capture finer details, and fewer patches for coarser resolution
bands. To reduce the resulting long token sequence length,
an average pooling step is applied to aggregate corresponding patches from the same band group (Fig. 1).



3


Figure 1. THOR encoder uses a single ViT. Data is processed using a band-wise patch projection layer and group average pooling.


**4.1.2. Compute-adaptive inference**


To make THOR "compute-adaptive", allowing dynamic
trade-offs between computational cost and accuracy without retraining, we incorporate the FlexiViT approach by
randomizing the patch size during pre-training. The patch
embedding weights are resized accordingly during training,
enabling the resulting ViT to adapt to various patch sizes
(e.g., from 4 _×_ 4 to 32 _×_ 32) at inference time using a single set of pre-trained weights. This flexibility has a crucial
downstream benefit: a user can opt for a smaller patch size
at inference time, producing a denser, higher-resolution token sequence. This dense representation can be more effectively processed by simpler, more lightweight decoders,
potentially reducing the amount of labeled data needed for
fine-tuning pixel-level tasks and improving performance in
data-limited scenarios. We also randomize the input image
size during pre-training, allowing THOR to extrapolate to
larger input images than those used during fine-tuning.


**4.1.3. GSD-aware positional encoding**


USat’s superpositional encodings assumes fixed patch dimensions [12], and if you randomly change the patch size
this scheme become impractical as they require all patch
sizes to be multiples of the smallest possible patch size. We
therefore extend the 2D ALiBi (Attention by Linear Bias)
approach by CROMA [8] to be GSD-aware.
Let _ahij_ denote element ( _h, i, j_ ) of the attention matrix
corresponding to the _i_ th query **q** _hi ∈_ R _[d]_ and the _j_ th key
**k** _hj ∈_ R _[d]_, where _d_ is the head dimension. The attention
bias is calculated based on the real-world ground distance
between patch centers as



Figure 2. GSD-aware 2D-ALiBi for two groups: 10m GSD and
8x8 patches and 20m GSD and 4x4 patches. Left: Each sub-square
is the ALiBi values (Eq. 1) between a 20 m GSD patch and all 10
m GSD patches. Mid: Each sub-square is the ALiBi values (Eq. 1)
between a 10 m GSD patch and all 20 m GSD patches. Right: Full
GSD-aware 2D-ALiBi matrix for the full sequence of 8x8 + 4x4
= 80 patches, where the off-diagonal and diagonal blocks are the
intra product and inter product attention biases, respectively.


where dist( **x** _i,_ **x** _j_ ) denotes the distance in meters between
patch **x** _i_ and **x** _j_, max( _p_ ) is the largest patch size (in meters),
and _m_ ( _h_ ) denote the strength of the positional biases to the
_h_ th self-attention head, called slopes _m_ . We select slopes
as in [8]. The GSD-aware 2D-ALiBi is visually validated in
Fig. 2, which compares the ALiBi values for two configurations of 10 m GSD with 8 _×_ 8 patches and 20 m GSD with
4 _×_ 4 patches, showcasing the relative positional encoding
across tokens of different GSDs and patch dimensions.
For the lightweight decoder, which includes masked tokens for reconstruction, we modify the 2D sinusoidal positional encoding to be GSD-aware. Let _g_ denote the GSD
of the band we are reconstructing, and let _pos_ denote the
center position of a patch. The encoding _v_ for that patch is:




      _vx_ ( _pos,_ 2 _i_ ) = sin _g_ _[pos]_ [ + 0] _[.]_ [5]

10000 [2] _[i/D]_


      _vy_ ( _pos,_ 2 _i_ + 1) = cos _g_ _[pos]_ [ + 0] _[.]_ [5]

10000 [2] _[i/D]_






(2)




_√_
_ahij_ = **q** _[T]_ _hi_ **[k]** _[hj][/]_



_d −_ [dist(] **[x]** _[i][,]_ **[ x]** _[j]_ [)] _· m_ ( _h_ ) _,_ (1)

max( _p_ )



The GSD-aware 2D-ALiBi not only elegantly solves the
problem of handling flexible patch sizes and relating products of various resolutions, but also allows for test-time extrapolation to input sizes much larger than used during training [8]. As the decoder is discarded after pre-training, we
opt for the simpler 2D sinusoidal positional encoding.


**4.1.4. Randomized ground cover and patch size sampling**


To accommodate the large span in GSD between modalities, we devise a data sampling strategy where we sample
a random ground cover from the range (1000 _,_ 50000)m.
We then extract samples for the available modalities, making sure to only include data within a valid input image size
between (20 _,_ 500) pixels.
We then select random patch sizes per modality GSD,
makings sure not to exceed a predefined token budget. The
actual resizing of patch sizes is implemented as a modified
version of FlexiViT [4].



4


**4.2. Decoder architecture**


THOR is pre-trained using an extended MAE framework.
This approach applies a self-supervised reconstruction objective combined with novel multi-modal prediction tasks.
Following the standard MAE framework [10], the decoder is substantially lighter than the encoder, focusing
specifically on the reconstruction and land cover mapping
task. This asymmetric design ensures that the high-quality
feature representations reside solely within the heavier encoder, which is frozen for downstream tasks.
Unlike a standard MAE which, uses a linear projection
layer, our decoder head projects tokens back to the patch
space using a Conv2D-Transpose layer.


**4.3. Loss formulation**


THOR is trained from multiple pretext tasks to ensure generality and robustness across different applications (Fig. 3):

- Pixel-level reconstruction: Pixel-level input band reconstruction is performed using our proposed flexible VIT
MAE loss (Sec. 4.3.1). This task forces the model to learn
fine-grained details for pixel-level applications.

- Patch-level contrastive learning: We devise a patch level
guided soft (multi label) contrastive loss to leverage rich
semantic information in the available land cover products
(Sec. 4.3.2).

- Pixel-level map prediction: The model predicts land
cover maps, such as ESA WorldCover. This provides
dense, geo-semantic supervision across different GSDs
(e.g., WorldCover 10m maps predicted from Sentinel-1/2 groups, and MOD12Q1 maps predicted from Sentinel-3
SLSTR). The model also predicts elevation and slope derived from a DEM at 10 m and 60m GSD (Sec. 4.3.3).

- Image-level prediction [21, 27]: Predicts ERA5-Land
variables (from daily statistics, e.g., soil water, temperature, precipitation), latitude, longitude, and month, providing coarse-grained, climate-relevant feature learning.

- Image level SAR task: The learning includes Sentinel-1
SAR ascending/descending orbit direction classification
task and incidence angle prediction task.

Similar to USat [12] we use group specific projection
layers to map decoder tokens from group _g_ to patches of
shape ( _Pg, Pg, Cg_ ) where _Cg_ is the number of bands in this
group. The input bands are then patchified to the same patch
sizes and a channel-wise MSE loss over all the masked
patches is used.
We add linear heads to the pooled encoder tokens for
image-level tasks, using cyclic encoding for angular/temporal variables [21]



parameters, with the output being of dimension _∈_ R _[B][×]_ [2] _[C]_ .


**4.3.1. Flexible ViT MAE loss**


Using FlexiViT for processing the patches in the encoder
introduces a challenge in the MAE framework: the decoder
must reconstruct patches of varying sizes. Using arbitrary
patch sizes for patchifying the target input bands is trivial
and ensures that we have the same amount of decoder tokens as the number of target patches. However, the group
specific decoder projection layers mapping decoder tokens
is usually a linear layer resulting in a fixed output patch size.
To address this, we replace the linear projection layer with a
transposed Conv2D layer, enabling bilinear interpolation of
the projection weights. Finally, to ensure that the loss stays
the same, we introduce a scaling inspired by the FlexiViT
derivations.
Formally, the standard MAE uses a pixel-wise MSE loss
to reconstruct masked patches: _Lmae_ = (1 _/N_ ) _||_ vec( **x** ) _−_
_⟨_ **v** _,_ **z** _, ⟩||_ [2], where **x** _∈_ R _[p][×][p]_ is the input patch, **z** _∈_
R _[D][d]_ is the decoders predicted token embedding, and **v** _∈_
R _[D][d][×][p][×][p]_ is the weights mapping the decoder’s embeddings
to patch predictions. When the patch size _p_ changes to a
new size _p_ _[∗]_, the prediction weights **v** must also change to
**v** _[∗]_ to maintain the correct projection. We use a bilinear interpolation to change the weights, such that **v** _[∗]_ = **vB** _[T]_,
where **B** _∈_ R _[p]_ _∗_ [2] _[×][p]_ [2] . We prove that by scaling the entire
loss term with the pseudo-inverse **B** [+], the MSE loss for
the resized patch is mathematically equivalent to the original loss. This guarantees that our normalized reconstruction
target provides a consistent learning signal across all patch
sizes:



_L_ _[∗]_ _mae_ [=] [1]



_N_ _[||]_ **[B]** [+][(] **[B]** [vec(] **[x]** [)] _[ −⟨]_ **[vB]** _[T][,]_ **[ z]** [)] _[⟩||]_ [2]



(4)

[1]

_N_ _[||]_ [vec(] **[x]** [)] _[ −⟨]_ **[v]** _[,]_ **[ z]** _[⟩||]_ [2][ =] _[ L][mae][.]_



= [1]



cyclic_encoding( **x** _, s_ ) = �sin - 2 _sπ_



_π_ 
_s_ **[x]**



_π_ ��

_sπ_ **[x]** - _,_ (3)

_s_ **[x]**



This formulation allows THOR to seamlessly train with randomized patch sizes while maintaining a stable and mathematically consistent reconstruction objective.


**4.3.2. Patch-level contrastive loss**


Inspired by Galileo’s approach of using a patch-wise contrastive loss to amplify local details and enforce discrimination between tokens in a single sample, improving the
model’s ability to handle fine-grained features [27], we extend the multi-label guided approach from [29] to the patch
level. We randomly partition the unmasked patch tokens
into _K_ groups and compute an average embedding for each
group. Concurrently, we extract an average land cover histogram from the corresponding patch locations for each of
the _K_ groups, and soft similarity labels are then generated
using cosine similarity between these _K_ normalized histograms. A contrastive loss (Eq. 5) is applied to the _K_



cos - 2 _π_



where the factors **x** _∈_ R _[B][×][C]_ and _s ∈_ R [+] serve as scaling



5


average embeddings, forcing the model to produce similar representations for patch groups with similar land cover
compositions. This loss is computed per band group, for all
land cover tasks, adhering to the same viability constraints
as our map prediction task.
The contrastive loss for group _g_ and task _t_ is



_j∈P_ ( _i,j_ ) [exp] - _−h_ _[g,t]_ _i,j_ _[f]_ [(] **[x]** _[j][,]_ **[ x]** _[i]_ [)] _[/τ]_ 


_L_ _[g,t]_ _con_ [=] _[ −]_ [1]

_M_



_M_

- log


_i_ =1







_,_
_k∈Q_ ( _k,j_ ) [exp (] _[−][f]_ [(] **[x]** _[k][,]_ **[ x]** _[i]_ [)] _[/τ]_ [)]




~~�~~



(5)
where _M_ = _Bg,t × K_ is the total batch size times the
number of averaged tokens, _P_ ( _i, j_ ) = _{j_ = 1 _. . . M, j ̸_ =
_i, yj_ = _yi}_, _Q_ ( _i, k_ ) = _{k_ = 1 _. . . M, k ̸_ = _i, yk ̸_ = _yi}_, _f_ ( _·_ )
denotes the cosine similarity, _h_ _[g,t]_ _i,j_ [is the][ [0] _[,]_ [ 1]][ normalized]
soft similarity label between the averaged set of patches _i_
and _j_, and _yl_ is equal to the local GPU device. I.e., we only
select positive pairs from the same local device, and select
negatives from all other devices.
The total contrastive loss is thus




- _Lcon,g,t_ (6)


_t∈Tg_



_Lcon_ = [1]

_G_



_G_



_g_ =1



1
_|Tg|_



Figure 3. Pretext tasks used for learning THOR.


( _Lmap,t_ ), the ERA5 land, month, coordinate and S1 incidence regression loss ( _Lera_ 5 _, Lm, Lcoord, Linc_ ), S1 orbit
direction loss ( _Lorb_ ), and our novel contrastive loss ( _Lcon_ ).
_Ltotal_ = _λ_ 1 _Lmae_ + _λ_ 2 _Lcon_ + [�] _t_ _[λ]_ [3] _[,t][L][map,t]_ [ +] _[ λ]_ [4] _[L][era]_ [5][ +]

_λ_ 5 _Lm_ + _λ_ 6 _Lcoord_ + _λ_ 7 _Linc_ + _λ_ 8 _Lorb_ + _λ_ 9 _Lfft,_ where
_t ∈{WC, GC, MCD, DEM, SCL}_ is the land cover map
prediction tasks. _Lfft_ is a L1 MAE reconstruction loss in
the Fourier domain, included for stability during training

[14]. The loss weights can be found in the Supplementary
Material.


**5. Experiments**


Our experiments are designed to validate THOR’s core
hypotheses: **1)** Is THOR more data-efficient in low-label
regimes, validating our "data-hungry decoder" hypothesis?
**2)** Does our architecture successfully synthesize and learn
from the full 10 m - 1000 m S1, S2, and S3 sensor suite?,
and **3)** Is the compute-adaptive mechanism the key driver of
this performance?


**5.1. Experimental setup**


We evaluate on two main benchmarks: PANGAEA [17] and
Copernicus-Bench [30]. PANGAEA consists of a diverse
suite of 9 semantic segmentation tasks, including HLS,
MADOS, PASTIS, and Sen1Floods11. We follow its standard protocol, evaluating on the 10%, 50%, and 100% labeled data splits to test our data-efficiency hypothesis. To
specifically validate THOR’s ability to process Sentinel3 data, we evaluate on the four Sentinel-3 OLCI-specific
tasks (Cloud-S3, LC100Cls-S3, LC100Seg-S3, BiomassS3) from the Copernicus-FM benchmark.

We compare THOR against a wide range of state-ofthe-art (SOTA) FMs, including Copernicus-FM [30], DOFA

[31], and TerraMind [13], using the scores reported in [17].



where _G_ is the total number of band groups, _Tg_ is the set of
viable land cover tasks for the corresponding group


**4.3.3. Map prediction loss**


Following [13, 21, 27], we incorporate pixel-level pretext
tasks to provide dense, semantic supervision. These include
land cover classification using multiple land cover products
(e.g., WorldCover, GlobCover) at their native resolutions,
as well as elevation and slope regression from a DEM.
We integrate these targets by treating them as additional
output "bands". The decoder uses specialized projection
heads to predict each map, and we adapt our FlexiViT resizing strategy to project the decoder’s variable-GSD token
embeddings to the fixed GSD of the target map. We enforce a compatibility constraint, permitting only projections
that result in a target patch size within a [4 _,_ 32] pixel range.
For example, predicting a 10 m WorldCover map from a
16-pixel, 60 m GSD input patch is disallowed, as it would
require an unstable 96 _×_ 96 pixel projection.
Unlike the MAE reconstruction objective, which operates only on masked tokens, the map prediction loss is computed on all encoder patches (both masked and unmasked).
We apply a CE loss (0.1 label smoothing) for classification
tasks and an MSE loss for regression, standardizing DEM
targets (elevation and slope) using dataset statistics after a
per-sample min-normalization of the elevation. See Supplementary Material for details.


**4.3.4. Total loss**


Our final pre-training loss, _Ltotal_, is a weighted sum of the
MAE reconstruction loss ( _Lmae_ ), the map prediction losses



6


**5.2. Implementation details**


We pre-train a family of THOR models (Tiny, Small, Base,
Large) from scratch on the THOR Pretrain dataset for 400
epochs. All models are trained using the AdamW optimizer
with a base learning rate of 3e-4 for the base and large
model, and 4e-4 for small and tiny, a weight decay of 0.05,
and a linear warmup of 40, 20, 10, 10 epochs for ViT-Large,
Base, Small and Tiny model, respectively warmup followed
by a cosine decay schedule.
Training was conducted on 16 AMD MI250X GPUs,
with a total batch size of 1024. During pre-training, we randomly sample both input resolution (32 _×_ 32 – 1024 _×_ 1024)
and patch size (4 _×_ 4 – 32 _×_ 32) for every sample. To manage
computational constraints introduced by randomized patch
sizes and input image sizes across multiple band groups,
we implemented a simple token budget heuristic. During
training, a threshold for the maximum number of tokens is
enforced. When sampling a product group, a patch size is
drawn such that the resulting number of tokens does not exceed the remaining budget, ensuring efficient use of memory across heterogeneous inputs.


**5.3. Main result: State-of-the-art in data-limited**
**scenarios**


Our central hypothesis, introduced in the Introduction, is
that THOR’s flexible patching overcomes the "data-hungry
decoder" problem faced by rigid models. We test this directly on the 10% Pangaea benchmark split using an 6 _×_ 6
patch size.
In this low training data regime, THOR-B (Base) achieve
the best average rank across all datasets (Tab. 1). THOR-B
outperforms all other published models, including a +1.9
mIoU gain over the next-best model, TerraMind. This
strong performance, especially on fine-grained tasks like
sen1floods11 (86.29 mIoU), validates that THOR is significantly more data-efficient than its fixed-patch counterparts.


**5.4. Full-data benchmarking**


We next evaluate THOR’s performance in Pangaea using
full training data availability and for the Sentinel-3 OLCI
scenarios from Copernicus-Bench [30].
For 100% training data, THOR-B remains state-of-theart or highly competitive in the full-data regime. It achieves
the top rank on PASTIS (40.76% mIoU) and CropMap
(56.78% mIoU), demonstrating that its architecture scales
effectively with more data. A full comparison against all
baseline models is provided in the Supplementary Material.
The Copernius-Bench validates THOR’s ability to synthesize Sentinel-3 OLCI data (Tab. 2). Our model outperforms all baselines on two of the four OLCI specific
tasks, including a +4.1 mIoU gain on Cloud-S3 and a -6.6
RMSE (improvement) on Biomass-S3 over the CopernicusFM baseline. This confirms that THOR effectively learns



Figure 4. Test mIoU results for THOR-B model with varying patch
sizes using a fixed number of tokens equal to 18 with linear probing segmentation on the Sen1Floods11 dataset using Sentinel 1
and Sentinel 2 data, 10% of the training data, with mean aggregation of features.


from the challenging 10 m - 1000 m GSD range.


**5.5. Ablation studies**


**Value of compute-adaptivity.** We fine-tuned a single
THOR-B model on Sen1Floods11 (10% data) and evaluated
it at multiple patch sizes (Fig. 4) using linear probing. The
results shows simply by shrinking the patch size at inference time, from a coarse 16 _×_ 16 (61.9 mIoU) to a fine 4 _×_ 4
(81.1 mIoU), we gain nearly 20 mIoU points. This confirms
our hypothesis that a single model can be dynamically deployed, and smaller patches (producing denser tokens) are
critical for fine-grained tasks.
**Value of multi-sensor synthesis.** A core design principle of THOR is its ability to ingest and synthesize synergistic information from heterogeneous sensors, such as
Sentinel-1’s radar and Sentinel-2’s spectral details. To validate this capability, we conducted an ablation study on the
Sen1floods11 benchmark using the 10% data split. We finetuned the THOR-B model with an UperNet decoder on three
different input modality configurations: S1 only, S2 only,
and the combined S1 + S2 inputs. The results, presented in
Tab. 3, demonstrate the value of this multi-modal fusion.


**5.6. Test-time extrapolation to larger images**


We validated our GSD-aware 2D-ALiBi’s ability to extrapolate to larger input sizes than seen during training, a key
property of relative positional encodings [8]. We fine-tuned
a UperNet decoder on the Sen1floods11 (10% split) dataset
using a frozen THOR-B encoder with S1 + S2 inputs. The
model was trained only on 108 _×_ 108 pixel crops with a
patch size of 6.
During evaluation, we tested this fixed model on the
test set using various input sizes, applying Pangaea’s sliding window inference up to the full 512 _×_ 512 image size.
As shown in Fig 5, performance does not degrade at larger
scales; on the contrary, it shows a consistent improvement



7


Table 1. Pangaea results with 10% training data in mIoU. Bold/underline mark best/second-best per column.


Model HLS Burns MADOS PASTIS Sen1Floods11 FBP DynEarthNet CropMap SN7 AI4Farms Avg. Rank


CROMA 76.44 32.44 32.80 87.22 37.39 36.08 36.77 42.15 38.48 6.11
DOFA 71.98 23.77 27.68 82.84 27.82 **39.15** 29.91 46.10 27.74 10.22
GFM-Swin 67.23 28.19 21.47 62.57 55.58 28.16 27.21 39.48 32.88 12.56
Prithvi 77.73 21.24 33.56 86.28 29.98 32.28 27.71 36.78 35.04 10.22
RemoteCLIP 69.40 20.57 17.19 62.22 56.23 34.43 19.86 43.11 23.85 12.33
SatlasNet 74.79 29.87 16.76 83.92 37.86 34.64 29.08 49.78 13.91 10.22
Scale-MAE 75.47 21.47 22.86 64.74 48.75 35.27 13.44 49.68 26.66 10.78
SpectralGPT **83.35** 20.29 34.53 83.12 39.51 35.33 31.06 36.31 37.35 8.56
S12-MoCo 73.11 19.47 32.51 79.58 35.57 32.24 36.54 49.46 37.97 10.67
S12-DINO 75.93 23.47 36.62 84.95 34.63 32.78 38.44 41.15 37.91 8.33
S12-MAE 76.60 18.44 31.06 84.81 35.56 30.59 35.29 40.51 23.60 11.44
S12-Data2Vec 74.38 17.86 33.09 81.91 37.27 33.63 34.11 40.66 22.85 12.11
Terramind-B 77.39 **44.06** **39.96** 84.43 54.00 37.35 35.65 43.21 38.59 4.00


UNet Baseline 79.46 24.30 29.53 **88.55** 52.58 35.59 13.88 46.08 34.84 7.11
ViT Baseline 75.92 10.18 38.44 81.85 **56.53** 35.39 27.76 36.01 **39.20** 8.78


THOR-B 76.90 40.67 38.93 86.29 42.80 35.21 **42.23** 55.94 38.90 **3.78**
THOR-T 75.98 41.65 36.26 82.70 42.81 34.03 37.82 **58.52** 38.56 5.78



Table 2. Benchmark results on selected Copernicus-Bench benchmarks. _†_ : We use a patch size of 10 for Cloud-S3, 8 for LC100ClsS3, 6 for LC100Seg-S3 and 4 for Biomass-S3.


**Metric** **Supervised** **Supervised** **Random** **DOFA** **Copernicus** **THOR**
Backbone - ViT-S/16 ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/8 ViT-B/10 _†_
Modality - - - - All (spectral) All All (spectral)
Cloud-S3 mIoU 61.7 ± 0.7 63.0 ± 0.8 60.9 ± 0.0 58.2 ± 0.1 62.0 ± 0.7 **66.1 ± 0.1**
LC100Cls-S3 mAP 91.3 ± 0.3 91.4 ± 0.5 88.9 ± 0.1 89.5 ± 0.0 **93.3 ± 0.4** 91.0 ± 0.0
LC100Seg-S3 mIoU 20.1 ± 0.4 19.3 ± 0.5 18.2 ± 0.1 16.5 ± 0.1 **24.1 ± 0.0** 19.9 ± 0.0
Biomass-S3 RMSE ↓ 68.1 ± 0.3 68.3 ± 0.4 68.7 ± 0.5 74.1 ± 0.1 66.3 ± 0.1 **59.7 ± 0.0**


Table 3. Sen1floods11 10% test mIoU results for different modality configurations using THOR-B model with Upernet decoder.


Modalities mIoU


Sentinel-1 78.09
Sentinel-2 87.25
Sentinel-1 + Sentinel-2 **87.70**


Figure 5. Test mIoU results of a THOR-B (w/UperNet), trained on
a 108 _×_ 108 image size, evaluated on increasingly larger images.


as the input size increases, confirming the robust extrapolation capability of our positional encoding.



Table 4. RMSE snow cover fraction. Image size 128 _×_ 128 and
concatenated the tokens of the 500 m and 1000 m bands.


Decoder Patch size RMSE


UNet 12.4
UPerNet 16x16 14.0
8x8 12.4
4x4 9.90
Linear decoder 4x4 **9.88**


**5.7. Use case: mapping of snow cover**


We validate THOR’s compute-adaptive capability on a datascarce climate task: snow cover fraction regression using
500 m / 1000 m GSD Sentinel-3 SLSTR data.
We fine-tune THOR-B and compare a simple linear decoder against a UPerNet [25]. The results in Tab. 4 provide
two key insights:

- Deploying the same pre-trained THOR model with an
UPerNet decoder, but changing the inference patch size
from 16 _×_ 16 to 4 _×_ 4, reduced the RMSE from 12.4 to
9.90. This 29% reduction confirms that a denser token
sequence is beneficial, even for coarse-resolution data.

- A simple linear decoder with 4 _×_ 4 patches (9.99 RMSE)
performs identically to the much larger UPerNet with
4 _×_ 4 patches (9.90 RMSE). This confirms our hypothesis: the complex decoder was a crutch to compensate for
a "token-starved" encoder. By providing a dense token sequence, THOR’s flexible patching facilitates simpler decoders and validating its superior data-efficiency.


**6. Conclusion**


In this work, we addressed a weakness of current EO foundation models: their architectural rigidity. We argued that
fixed patch sizes lead to data-hungry decoders, limiting their
utility in data-scarce scenarios. We proposed THOR, the



8


Figure 6. Left: False color SLSTR image. Right: Snow cover
fraction, where green is less than 15% and purple is 100%.


first FM to synthesize a compute-adaptive patching strategy
with a multi-sensor architecture that unifies Sentinel-1, -2,
and -3 (OLCI & SLSTR) data.
Our experiments validate our central hypothesis. THOR
achieves state-of-the-art performance in the Pangaea 10%
benchmark, demonstrating superior data efficiency. This
confirms that the ability to use smaller patch sizes at inference provides a denser token sequence that is more effective
for fine-tuning on limited data. We also proved our complex
multi-sensor synthesis was successful, with THOR outperforming baselines on two of four Sentinel-3 OLCI specific
tasks, validating its unique 10 m - 1000 m GSD capability.
THOR, while versatile, has limitations that open clear
avenues for future work. While our dataset includes temporal samples, the architecture itself does not explicitly model
time. Future work will focus on extending this flexiblepatching concept to an explicit spatio-temporal backbone
and integrating other key modalities like Sentinel-5P (air
quality) or passive microwave data (climate) to further
strengthen THOR applicability to climate and society challenges.


**Acknowledgments**


This activity was funded and supported by European
Space Agency (ESA) Φ-lab (FM4CS project, contract no.
4000143489/24/I-DT), and the Research Council of Norway (KnowEarth project no. 337481).


**References**


[1] Olivier Arino, Jose Julio Ramos Perez, Vasileios
Kalogirou, Sophie Bontemps, Pierre Defourny, and
Eric Van Bogaert. Global Land Cover Map for 2009
(GlobCover 2009), 2012.

[2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr
Bojanowski, Pascal Vincent, Michael Rabbat, Yann
LeCun, and Nicolas Ballas. Self-supervised learning
from images with a joint-embedding predictive architecture. In _Proceedings of the IEEE/CVF Conference_



_on Computer Vision and Pattern Recognition_, pages
15619–15629, 2023.

[3] Guillaume Astruc, Nicolas Gonthier, Clement Mallet,
and Loic Landrieu. AnySat: One earth observation
model for many resolutions, scales, and modalities.
In _Proceedings of the Computer Vision and Pattern_
_Recognition Conference_, pages 19530–19540, 2025.

[4] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov,
Mathilde Caron, Simon Kornblith, Xiaohua Zhai,
Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One model
for all patch sizes. In _Proceedings of the IEEE/CVF_
_Conference on Computer Vision and Pattern Recogni-_
_tion_, pages 14496–14506, 2023.

[5] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick
Liu, Erik Rozi, Yutong He, Marshall Burke, David Lobell, and Stefano Ermon. Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery. _Advances in Neural Information Processing_
_Systems_, 35:197–211, 2022.

[6] Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers
for image recognition at scale. _arXiv preprint_
_arXiv:2010.11929_, 2020.

[7] Mark A. Friedl and Damien Sulla-Menashe. MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500
m SIN Grid V061 (MCD12Q1). NASA LP DAAC,
2022.

[8] Anthony Fuller, Koreen Millard, and James Green.
CROMA: Remote sensing representations with contrastive radar-optical masked autoencoders. _Advances_
_in Neural Information Processing Systems_, 36:5506–
5538, 2023.

[9] Matthew C Hansen, Peter V Potapov, Rebecca Moore,
Matt Hancher, Svetlana A Turubanova, Alexandra
Tyukavina, David Thau, Stephen V Stehman, Scott J
Goetz, Thomas R Loveland, et al. High-resolution
global maps of 21st-century forest cover change. _sci-_
_ence_, 342(6160):850–853, 2013.

[10] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,
Piotr Dollár, and Ross Girshick. Masked autoencoders
are scalable vision learners. In _Proceedings of the_
_IEEE/CVF conference on computer vision and pattern_
_recognition_, pages 16000–16009, 2022.

[11] Rainer Hollmann, Chris J Merchant, Roger Saunders, Catherine Downy, Michael Buchwitz, Anny
Cazenave, Emilio Chuvieco, Pierre Defourny, Gerrit
de Leeuw, René Forsberg, et al. The ESA climate
change initiative: Satellite data records for essential



9


climate variables. _Bulletin of the American Meteoro-_
_logical Society_, 94(10):1541–1552, 2013.

[12] Jeremy Irvin, Lucas Tao, Joanne Zhou, Yuntao Ma,
Langston Nashold, Benjamin Liu, and Andrew Y
Ng. USat: A unified self-supervised encoder
for multi-sensor satellite imagery. _arXiv preprint_
_arXiv:2312.02199_, 2023.

[13] Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio
Marsocci, Niklas Kopp, et al. Terramind: Large-scale
generative multimodality for earth observation. _arXiv_
_preprint arXiv:2504.11171_, 2025.

[14] Oren Kraus, Kian Kenyon-Dean, Saber Saberian,
Maryam Fallah, Peter McLean, Jess Leung, Vasudev
Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik,
Dominique Beaini, Maciej Sypetkowski, Chi Vicky
Cheng, Kristen Morse, Maureen Makes, Ben Mabey,
and Berton Earnshaw. Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology,
2024. arXiv:2404.10242 [cs].

[15] Xuyang Li, Chenyu Li, Pedram Ghamisi, and Danfeng
Hong. FlexiMo: A flexible remote sensing foundation
model. _arXiv preprint arXiv:2503.23844_, 2025.

[16] Nicolas Longepe, Hamed Alemohammad, Anca
Anghelea, Thomas Brunschwiler, Gustau CampsValls, Gabriele Cavallaro, Jocelyn Chanussot,
Jose Manuel Delgado, Begüm Demir, Nikolaos
Dionelis, Paolo Fraccaro, Anna Jungbluth, Robert E.
Kennedy, Valerio Marsocci, Muthukumaran Ramasubramanian, Raul Ramos-Pollan, Sujit Roy, Gencer
Sümbül, Devis Tuia, Xiao Xiang Zhu, and Rahul
Ramachandran. Earth action in transition: Highlights
from the 2025 esa-nasa international workshop on ai
foundation models for eo. 2025.

[17] Valerio Marsocci, Yuru Jia, Georges Le Bellier, David
Kerekes, Liang Zeng, Sebastian Hafner, Sebastian
Gerard, Eric Brune, Ritu Yadav, Ali Shibli, et al. Pangaea: A global and inclusive benchmark for geospatial
foundation models. _arXiv preprint arXiv:2412.04204_,
2024.

[18] Matthew F McCabe, Matthew Rodell, Douglas E Alsdorf, Diego G Miralles, Remko Uijlenhoet, Wolfgang
Wagner, Arko Lucieer, Rasmus Houborg, Niko EC
Verhoest, Trenton E Franz, et al. The future of earth
observation in hydrology. _Hydrology and earth system_
_sciences_, 21(7):3879–3914, 2017.

[19] J. Muñoz Sabater, E. Dutra, A. Agustí-Panareda, C.
Albergel, G. Arduini, G. Balsamo, S. Boussetta, M.
Choulga, S. Harrigan, H. Hersbach, B. Martens, D. G.
Miralles, M. Piles, N. J. Rodríguez-Fernández, E.
Zsoter, C. Buontempo, and J.-N. Thépaut. Era5-land:
a state-of-the-art global reanalysis dataset for land ap


plications. _Earth System Science Data_, 13(9):4349–
4383, 2021.

[20] Joaquín Muñoz-Sabater. ERA5-Land hourly data
from 1950 to present. Copernicus Climate Change
Service (C3S) Climate Data Store, 2019.

[21] Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, and Nico Lang.
MMEarth: Exploring multi-modal pretext tasks for
geospatial representation learning. In _European Con-_
_ference on Computer Vision_, pages 164–182. Springer,
2024.

[22] Colorado J Reed, Ritwik Gupta, Shufan Li, Sarah
Brockman, Christopher Funk, Brian Clipp, Kurt
Keutzer, Salvatore Candido, Matt Uyttendaele, and
Trevor Darrell. Scale-mae: A scale-aware masked
autoencoder for multiscale geospatial representation
learning. In _Proceedings of the IEEE/CVF Interna-_
_tional Conference on Computer Vision_, pages 4088–
4099, 2023.

[23] Markus Reichstein, Gustau Camps-Valls, Bjorn
Stevens, Martin Jung, Joachim Denzler, Nuno Carvalhais, and F Prabhat. Deep learning and process understanding for data-driven earth system science. _Nature_,
566(7743):195–204, 2019.

[24] Esther Rolf, Konstantin Klemmer, Caleb Robinson,
and Hannah Kerner. Position: Mission critical–
satellite data is a distinct modality in machine learning. In _Forty-first International Conference on Ma-_
_chine Learning_, 2024.

[25] Yang Ruiping, Liu Kun, Xu Shaohua, Yin Jian, and
Zhang Zhen. Vit-upernet: a hybrid vision transformer
with unified-perceptual-parsing network for medical
image segmentation. _Complex & Intelligent Systems_,
10(3):3819–3831, 2024.

[26] Daniela Szwarcman, Sujit Roy, Paolo Fraccaro,
Þorsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki
Ghosal, Pedro Henrique de Oliveira, Joao Lucas
de Sousa Almeida, Rocco Sedona, Yanghui Kang,
et al. Prithvi-eo-2.0: A versatile multi-temporal foundation model for earth observation applications. _arXiv_
_preprint arXiv:2412.02732_, 2024.

[27] Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry
Herzog, Patrick Beukema, Favyen Bastani, James R
Green, Evan Shelhamer, Hannah Kerner, and David
Rolnick. Galileo: Learning global and local features
in pretrained remote sensing models. _arXiv preprint_
_arXiv:2502.09356_, 2025.

[28] Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Lichao Mou, and Xiao Xiang Zhu. Selfsupervised learning in remote sensing: A review.
_IEEE Geosci. Remote Sensing Mag._, 10(4):213–247,
2022.



10


[29] Yi Wang, Conrad M. Albrecht, and Xiao Xiang
Zhu. Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining, 2024.
arXiv:2405.20462 [cs] version: 1.

[30] Yi Wang, Zhitong Xiong, Chenying Liu, Adam J.
Stewart, Thomas Dujardin, Nikolaos Ioannis Bountos,
Angelos Zavras, Franziska Gerken, Ioannis Papoutsis,
Laura Leal-Taixé, and Xiao Xiang Zhu. Towards a
Unified Copernicus Foundation Model for Earth Vision, 2025. arXiv:2503.11849 [cs].

[31] Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J.
Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, and
Xiao Xiang Zhu. Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities, 2024. arXiv:2403.15356 [cs].

[32] Daniele Zanaga, Ruben Van De Kerchove, Wanda
De Keersmaecker, Niels Souverijns, Carsten Brockmann, Ralf Quast, Jan Wevers, Alex Grosu, Audrey
Paccini, Sylvain Vergnaud, Oliver Cartus, Maurizio
Santoro, Steffen Fritz, Ivelina Georgieva, Myroslava
Lesiv, Sarah Carter, Martin Herold, Linlin Li, NandinErdene Tsendbazar, Fabrizio Ramoino, and Olivier
Arino. Esa worldcover 10 m 2020 v100, 2021.



11


## **THOR: A Versatile Foundation Model for Earth Observation Climate and** **Society Applications** Supplementary Material

with relevant metadata.



Figure S.1. Overview of THOR Pretrain sampled locations.


**A. THOR Pretrain**


The THOR FM is pre-trained on a new, diverse, and largescale dataset named THOR Pretrain. This dataset is curated to learn representations that are robust to variations in
global land cover, ocean phenomena, and cloud conditions.
THOR Pretrain unifies data from four major Copernicus Sentinel missions: Sentinel-1 SAR, Sentinel-2 MSI,
Sentinel-3 OLCI, and Sentinel-3 SLSTR. These sensors
provide diverse image modalities, including radar, multispectral and thermal sensors, with resolutions ranging from
10 m to 1000 m. In addition to the satellite data, the dataset
includes a digial elevation model (DEM), diverse land cover
maps, and ERA5-Land data. The dataset consists of 22TB
of data from globally distributed locations (Fig. S.1).


**A.1. Data, pre-processing and alignment**


Instead of stacking millions of small image crops, we sample EO data using the Sentinel-2 tiles (110 _×_ 110 km) as
the sampling grid. For a given grid location and time, we
sample the Sentinel-2 tile along with overlapping Sentinel1 SAR, Sentinel-3 OLCI, and Sentinel-3 SLSTR data.
Sentinel-3 data is selected from a 25 times larger area, centered at the Sentinel-2 tile, to account for its coarser resolution.
To ensure a diverse dataset of global land covers, ocean
phenomena, and cloud conditions, we employ a stratified
sampling strategy utilizing land cover and RGB maps of
the world (see Sec. A.2 for details). This methodology
is crucial to balance the dataset by actively prioritizing locations with high thematic and geographic diversity (e.g.,

[S1, S10]). A total of 6273 globally distributed locations
were sampled (Fig. S.1).


**A.1.1. Sensor data pre-processing**


The Sentinel data are downloaded from Copernicus Data
Space Ecosystem and preprocessed into netCDF files along



**Sentinel-1 SAR.** The SAR data is processed to sigmanaught, corrected for thermal noise, geocoded using the
Range Doppler algorithm. Two different resolutions of
Sentinel-1 are constructed: 10 m and 60 m GSD. The 10
m GSD Sentinel-image is aligned with the corresponding
Sentinel-2 data, whereas the 60 m GSD is processed to a
larger area, bounded by the Sentinel-3 footprint.


**Sentinel-2 MSI.** Level 2A Sentinel-2 data are acquired
and the reflectance bands are collected into a single netCDF
file along with metadata. The Scene Classification Map
(SCL) product, which includes various land cover classes
and a cloud mask, is also collected into the same netCDF
file.


**Sentinel-3 OLCI.** Level 1 OLCI data are acquired. The
top of atmosphere radiance ( _RT OA_ ) bands are converted to
reflectance ( _LT OA_ ) using


_RT OA_ ( _λ_ ) = _[πL][T OA]_ [(] _[λ]_ [)] (7)

_E_ 0( _λ_ ) _cos_ ( _ϕ_ ) _[,]_


where _E_ 0 is the solar spectral irradiance and _ϕ_ is the sun
zenith angle, both provided in the downloaded Sentinel-3
OLCI product file.
Further, the bands are resampled into the same UTM projection as the corresponding Sentinel-2 tile, but resampled
to a GSD of 250 m and a geographic extent of 25 times
larger area than the Sentinel-2 tile. This is done using the
bilinear algorithm implemented in the _pyresample_ Python
library.


**Sentinel-3 SLSTR.** Level 1 SLSTR data are acquired.
The Sentinel-3 SLSTR files are processed in the same manner as for OLCI: First, the top of atmosphere radiance bands
are converted to reflectance using Eq. (7). Then the reflectance and brightness temperature bands are resampled to
UTM projection and geographic extent similar to the OLCI
product, except that the GSDs are 500 m and 1000 m for the
reflectance and brightness temperature bands, respectively.
For SLSTR, cloud detection is performed using the
SCDA version 2.0 algorithm [S8].


**A.1.2. Auxiliary geospatial modalities (pretext targets)**


The dataset also includes auxiliary geospatial modalities for
reconstruction and prediction pretext tasks:



1


- Digital Elevation Model (pixel-level targets): DEMs are
included, and the model reconstructs both slope and elevation at 10 m and 60 m GSD as part of the MAE reconstruction objective from Sentinel-1 and Sentinel 2 bands.

- Land cover maps (pixel-level targets): Several land cover
products are incorporated to serve as pixel-level pretext
tasks, accommodating the range of satellite sensors by
varying in GSD from 10m to 500m. ESA WorldCover
(10 m) [S11] and the Sentinel-2 SLC map is predicted
from the Sentinel-1 and Sentinel-2 bands, the ESA GlobCover (300 m) [S2] is predicted from the Sentinel-3 OLCI
bands, and MOD12Q1 map (500 m) [S3] is predicted
from the Sentinel-3 SLSTR bands.

- ERA5-Land (image-level targets): The dataset includes
ERA5-Land data based on daily statistics, derived from
hourly land variables aggregated daily at 0.1 degrees resolution (approximate 9 km grid spacing). We select a diverse set of 17 variables covering temperature, hydrological cycles, snow cover, and vegetation indices (detailed
in Table S.1). This data is used for image-level prediction
pretext tasks.
To qualitatively validate our alignment pipeline, Fig. S.2
visualizes a complete sample tuple from the dataset. This
visualization highlights the extreme heterogeneity THOR
must resolve: the model must reconcile fine-grained textural details from the Sentinel-2 and Sentinel-1 (10 m) inputs with the broad-scale climatic context provided by the
Sentinel-3 sensors.
As illustrated by the bounding boxes, the dataset preserves the spatial hierarchy of the sensors. The Sentinel3 inputs cover a spatial footprint 25 times larger than the
Sentinel-2 anchor tile (Figs. S.2d - S.2f), ensuring that the
model captures large-scale atmospheric and thermal gradients that would be imperceptible in a narrow field-of-view
crop. The inclusion of aligned DEM and Land Cover maps
(Figs. S.2g - S.2k) further confirms that the model receives
dense topographic and semantic supervision alongside the
raw radiometric data


**A.2. Stratified sampling strategy**


The global land cover is not homogeneous, but highly imbalanced. Over 70% of the globe is covered with oceans,
and constructing the dataset using uniformly sampling the
Sentinel-2 tiles will result in a large part of ocean tiles. Even
if we only sample only tiles covering land, we will get a bias
towards forest, desert and shrublands. Since increasing the
pretraining data diversity enhances SSL performance [S1],
we need to capture the variation of the land cover and sample the Sentinel-2 tiles in a stratified manner.


**A.2.1. Land cover stratification**


First, we perform a ocean/land split, selecting 80% of the
Sentinel-2 tile from land areas.



To capture diversity of land areas, the strategy is based
on k-means clustering of extracted features [S6, S10]. We
use two data-sources to extract features from : ESA WorldCover maps and ESA Sentinel-2 RGB composite for 2022.
Each of them are treated independently.

- Feature extraction: For each tile location, we divide the
corresponding image data (WorldCover and RGB composite) into 224 _×_ 224 crops. For ESA WorldCover maps
we create a histogram of the 11 classes from each of
crop, using bin counts as the feature vector. For the ESA
Sentinel-2 2022 RGB composite, we use an ImageNet
pre-trained ViT-MAE model to create a 786-dimensional
embedding vector for each 224 _×_ 224 crop.

- Clustering and probability: K-means clustering (with
1000 clusters) is applied to group similar crops. The sampling probability for each tile location is determined as
the inverse of its cluster size, emphasizing rarity.

- Tile selection: Tile sampling probability is the average
of all crop probabilities within the tile, resulting in two
probabilities: one from WorldCover and one from the
Sentinel-2 RGB composite.


**A.2.2. Ocean data sampling**


To ensure comprehensive coverage of phenomena in the
ocean, sampling probabilities utilize various maps:

- World Bank Global Shipping Traffic Density maps are
used to calculate the normalized density of ship traffic and
oil and gas installations per Sentinel-2 tile.

- Areas with a higher probability of containing icebergs and
sea ice are defined based on existing maps and observations (e.g., specific longitudes for sea ice, and two large
regions in the southern hemisphere for icebergs).


**A.3. Final location sampling routine**


The final per-tile sampling probabilities are a weighted
combination of the land and ocean diversity scores, with an
80/20 split between land and ocean tiles. The detailed stratification for land and ocean samples is shown in Table S.2.


**A.3.1. Temporal sampling**


When a Sentinel-2 tile is sampled, the sampling routine selects an image from all available dates. While data constraints necessitate a balance between spatial and temporal
coverage, the goal is to obtain an average of two images per
tile. This is implemented by using a Poisson distribution
with an expectation of one to determine the number of _ad-_
_ditional_ images to sample, ensuring at least one image per
tile.
We generally aim to have as low cloud cover as possible,
but since the model will encounter clouded images in inference, we want THOR Pretrain to contain clouded images
as well. Hence, we assign sampling probabilities for each
10%-interval of cloud cover, and sample the image within
(or as close as possible) to that interval (Tab. S.3).



2


(a) Sentinel-1 MSI 10/20/60 m. (b) Sentinel-1 SAR 10 m. (c) Sentinel-1 SAR 60 m.


(d) Sentinel-3 OLCI 300 m. (e) Sentinel-3 SLSTR 500 m. (f) Sentinel-3 SLSTR 1000 m (thermal).


(g) DEM 10 m. (h) ESA WorldCover map 10 m. (i) Sentinel-2 SCL map 20 m.


(j) ESA GlobCover map 250 m (k) MOD12Q1 map 500 m


Figure S.2. Example images, from tile T19XDL on 2020-07-17


3


Table S.1. Description of ERA5-Land variables used to pre-train THOR, and included in THOR Pretrain.


**Variable Name** **Unit** **Description**


volumetric_soil_water_layer_1 None Volumetric soil water fraction for layer 1 (0–7cm).
volumetric_soil_water_layer_4 None Volumetric soil water fraction for layer 4 (100–289cm).
skin_temperature K Temperature of the surface of the Earth.
dewpoint_temperature_2m K Temperature at 2m to which air must be cooled for saturation.
temperature_2m K Air temperature at 2 meters above the surface.
soil_temperature_level_1 K Soil temperature at layer 1 (0–7cm).
soil_temperature_level_4 K Soil temperature at layer 4 (100–289cm).
snow_cover None Fraction of grid cell covered by snow.
snow_depth_water_equivalent m The depth of water that would result from melting the snow.
snowfall_sum m Accumulated snowfall (water equivalent).
snow_depth m Depth of the snowpack.
leaf_area_index_high_vegetation None Leaf area index fraction for high vegetation (e.g., trees).
leaf_area_index_low_vegetation None Leaf area index fraction for low vegetation (e.g., grass).
surface_pressure Pa Air pressure at the surface.
total_precipitation_sum m Accumulated total precipitation (rain and snow).
surface_runoff_sum m Accumulated water flowing over the land surface.
total_evaporation_sum m Accumulated evaporation from the surface.


Table S.2. Combined PR-tile sampling probabilities


**Category** **Sub-category** **Pct. of category**
Land Uniformly sampled from all land tiles 15%
(80%) Sampled from ESA WorldCover diversity 75%
Sampled from Sentinel-2 RGB composite 10%
Ocean Uniformly sampled from all ocean tiles 5%
(20%) Uniformly sampled from all coast tiles 40%
Sampled from ship-density probabilities 10%
Sampled from oil & gas installations-density 2%
Uniformly sampled from sea-ice areas 30%
Sampled from iceberg areas 13%


Table S.3. Sampling probabilities versus cloud coverage.


**Categories**
Cloud-cover interval [%] _>_ 10 10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 _<_ 90
Sampling probability [%] 97.66 0.99 0.49 0.25 0.12 0.06 0.06 0.06 0.06 0.25



**A.4. Dataset summary**


THOR Pretrain consists of a total of number of tile and date
combinations of 18332, with 6273 unique Sentinel-2 tiles
and 2926 unique dates, from 2016-01-01 to 2024-05-27.
Fig. S.3 illustrates the monthly distribution of the sampled observations, stratified by hemisphere. The distribution reveals two key characteristics of the dataset that align
with the physical realities of optical remote sensing:
The total volume of samples from the Northern Hemisphere Fig. S.3 (blue bars) is consistently higher than
that of the Southern Hemisphere (red bars). This reflects
the Earth’s geographical distribution, where approximately



68% of the global landmass resides in the Northern Hemisphere. Since our stratified sampling strategy prioritizes
land tiles (80% land / 20% ocean split), the dataset naturally mirrors this global land distribution.


To validate the multi-modal density of THOR Pretrain,
Tab. S.4 presents the co-occurrence matrix of all available
sensor modalities. This distribution reveals three critical
characteristics of the dataset that directly motivated our architectural choices:


- _High-volume multi-resolution alignment:_ Approximately
10,000 overlapping samples between Sentinel-2 and
Sentinel-3 (OLCI/SLSTR) bridge the 10 m – 1000 m res


4


Table S.4. Modality co-occurrence matrix (raw counts)


**Modality** **S2** **S1:10m** **S1:60m** **S3:OLCI** **S3:SLSTR** **LC** **DEM:10m** **DEM:60m**


S2 15310 3393 3528 9860 10952 14776 14556 14722
S1 10m 3393 4929 4896 2554 2919 4005 3966 3999
S1 60m 3528 4896 5121 2652 3032 4158 4099 4150
S3 OLCI 9860 2554 2652 11023 10574 10085 9927 10046
S3 SLSTR 10952 2919 3032 10574 12605 11366 11186 11321
LC 14776 4005 4158 10085 11366 16318 16095 16261
DEM 10m 14556 3966 4099 9927 11186 16095 16095 16094
DEM 60m 14722 3999 4150 10046 11321 16261 16094 16261


**B. THOR foundation model implementation**


**B.1. Band groups**



Figure S.3. Number of observations per month for northern (blue)
and southern (red) hemisphere.


olution gap. This alignment enables the model to propagate fine-grained optical textures to coarse thermal and
atmospheric readings.

- _Dense token supervision for radar-optical fusion:_ Although 3,400 aligned Sentinel-1/Sentinel-2 pairs appear
low in raw count, they represent full 110 _×_ 110 km tiles
rather than crops, yielding hundreds of millions of pixelaligned tokens. Combined with stratified sampling for
geodiversity, this provides a dense signal for learning
radar-optical distributions without the redundancy of uncurated datasets.

- _Natural sparsity as a regularizer:_ Variable sensor availability contrasts with the consistency of static auxiliary
variables (land cover, DEM) across approximately 16,300
locations. This natural sparsity validates our independent per-band projection layers, acting as a regularizer
that forces robustness to missing modalities and prevents
over-reliance on single sensors.


Is is important to note that we sample smaller crops from
the full tiles during pre-training, i.e., Fig. S.2 is only an
illustration of what the modalities available. During pretraining, random image locations in a given tile is sampled,
and smaller crops of each available modality corresponding
to the the same footprint is extracted.



To handle the heterogeneous resolutions of the input sensors efficiently, we organize the input bands into 10 distinct
groups as detailed in Tab. S.5. Grouping is primarily determined by the native GSD and the sensor source.
By grouping bands of identical resolution (e.g., Sentinel2 10 m bands in Group 1, Sentinel-1 10 m bands in Groups
4 & 5), we allow the encoder to process each group with
a patch number proportional to its information density.
For instance, the thermal bands from Sentinel-3 (Group
10, 960 m GSD) require significantly fewer tokens than
the optical bands from Sentinel-2 (Group 1, 10 m GSD)
for the same input image footprint. This grouping strategy is fundamental to our token budget heuristic, ensuring that high-frequency spatial details are preserved where
available, while minimizing computational waste on coarser
modalities.


**B.2. Multi-looking**


Multi-looking is often applied in SAR applications to reduce speckle noise, a granular distortion inherent to coherent imaging systems like radar [S9]. By averaging independent "looks" (images) of the same scene, the random noise
is smoothed out, which improves the image’s radiometric
quality at the expense of its spatial resolution. While aggregating the, say 10 m GRD pixels to 50 m, achieves a similar
result in terms of reducing speckle and lowering resolution,
it is technically not referred to as "multi-looking" in strict
SAR processing terminology.
THOR has been pretrained using a random "multilooking" by aggregating pixels to 10 m, 20 m, 30 m, 60 m,
120 m, 180 m or 240 m.


**B.3. Model configurations**


We train a family of THOR models ranging from Tiny to
Large to evaluate scaling laws and deployment versatility.
The specific architectural hyperparameters for each variant
are provided in Tab. S.6. All models share the same unified



5


Table S.5. THOR input band grouping. Input bands are organized into 10 groups based on sensor source and spatial resolution. Note that
Sentinel-1 data is split into coarse (60 m) and high-resolution (10 m) streams based on polarization/mode availability in the dataset. The
Sentinel-1 IW and EW more are mutually exclusive. _†_ During training the GSD of the SAR is aggregated ("multi-looked") to 10, 20, 30,
60, 120, 180 or 240 m.


**Group** **Sensor** **Bands** **Default GSD (m)**


1 Sentinel-2 Red, Green, Blue, NIR 10
2 Sentinel-2 RE1, RE2, RE3, RE4, SWIR1, SWIR2 20
3 Sentinel-2 CoastAerosol, WaterVapor 60


4 Sentinel-1 IW-VH, IW-VV, EW-VH, EW-VV 10/60 _†_
5 Sentinel-1 IW-HV, IW-HH, EW-HV, EW-HH 10/60 _†_


6 Sentinel-3 OLCI Oa01, Oa02, Oa03, Oa04, Oa05, Oa06, Oa07 240
7 Sentinel-3 OLCI Oa08, Oa09, Oa10, Oa11, Oa12, Oa13, Oa14 240
8 Sentinel-3 OLCI Oa15, Oa16, Oa17, Oa18, Oa19, Oa20, Oa21 240


9 Sentinel-3 SLSTR S1, S2, S3, S4, S5, S6 (reflectance) 480
10 Sentinel-3 SLSTR S7, S8, S9 (thermal BT) 960


Table S.6. **THOR model family configurations.** Hyperparameters for the Tiny, Small, Base, and Large variants. All models support dynamic input resolutions and patch sizes (4 [2] to 32 [2] ) during pre-training. The Token budget is an approximate cap enforced
during training to manage memory usage across heterogeneous inputs and is set to 1296 for all variants. The learning rate is set as
base_lr * ( batch_size * num_gpu) / 256.


**Model** **Layers** **Embed dim** **Heads** **MLP ratio** **Params** **Base Training LR** **Warmup epochs**


THOR-Tiny 12 192 3 4 _∼_ 7.6M 4e-4 10
THOR-Small 12 384 6 4 _∼_ 25.8M 4e-4 10
THOR-Base 12 768 12 4 _∼_ 94.1M 3e-4 20
THOR-Large 24 1024 16 4 _∼_ 314.4M 3e-4 40



encoder-decoder architecture but vary in embedding dimension, number of heads, and depth. Crucially, all variants
support the dynamic input resolution (32 [2] to 1024 [2] ) and
randomized patch sizes (4 [2] to 32 [2] ) described in the main
text.


**B.4. Token budget heuristic**


Processing multi-modal data with randomized input sizes
and patch sizes can lead to exploding sequence lengths if
left unchecked. To address this, we implement a dynamic
token budget heuristic, formally described in Algorithm 1.
The algorithm operates by first sampling a global spatial
footprint (C, C) in meters. For each band group _g_, we calculate number of tokens required based on a sampled patch
size _Pg_ and the GSD of the band group. If the cumulative
number of tokens approaches the pre-defined maximum token budget, the algorithm dynamically adjusts the minimum
allowable patch size for subsequent groups or caps the resolution. The ordering of the groups are randomly permuted
ensuring no bias in the algorithm. This ensures that every
training batch maximizes GPU utilization without causing
Out-Of-Memory errors, regardless of the random footprint
sampled.



**B.5. Loss Details**


Table S.7. Loss function weights used in pre-training


**Loss Component** **Lambda** **Weight Value**


Reconstruction _λ_ 1 1 _._ 5
Contrastive _λ_ 2 0 _._ 1
ERA5 _λ_ 4 0 _._ 1
Month _λ_ 5 0 _._ 1
Coordinates _λ_ 6 0 _._ 1
Orbit direction _λ_ 7 0 _._ 1
Incidence angle _λ_ 8 0 _._ 1
FFT _λ_ 9 0 _._ 01


**Map prediction losses:**
SCL _λ_ 3 _,_ SCL 0 _._ 05
World Cover _λ_ 3 _,_ WC 0 _._ 1
Global Canopy _λ_ 3 _,_ GC 0 _._ 1
MCD12Q1 _λ_ 3 _,_ MCD 0 _._ 1
DEM _λ_ 3 _,_ DEM 0 _._ 1


The total loss _Ltotal_ is a weighted sum of reconstruction, contrastive, and task-specific prediction losses. The



6


**Algorithm 1** THOR dynamic token budget heuristic (ground-cover based)


1: **Hyperparameters:**
2: _Bmax ←_ Maximum token budget (e.g., 1296)
3: _Crange ←_ [960 _,_ 46080] _▷_ Ground-cover range (meters)
4: _Prange ←_ [ _P_ min _, P_ max] = [4 _,_ 32] _▷_ Patch size range (pixels)

5: _Groups ←_ List of sensor groups (e.g., [S1, S2, S3-OLCI, ...])


6: **function** SAMPLEPATCHPARAMETERS( _Groups, C, Bmax_ )
7: _▷C_ is sampled: _C ∼U_ ( _Crange_ )
8: Randomly permute _Groups_ to get ( _g_ 1 _, . . ., gG_ )
9: _Tused ←_ 0

10: **for** each group _g_ in ( _g_ 1 _, . . ., gG_ ) **do**




       - _C_
11: _Hg ←_
_g._ GSD




; _Wg ←_ _Hg_



12: _Bremain ←_ _Bmax −_ _Tused_
13: **if** _Bremain ≤_ 0 **then**
14: **break**

15: **end if**
16: _▷_ Token range as in implementation (2–32 grid limit)

         -          - ��2
17: _Tmin ←_ max 2 _, ⌊Hg/P_ max _⌋_

          -          - ��2
18: _Tmax ←_ min 32 _, ⌈Hg/P_ min _⌉_

19: **if** _Tmin > Bremain_ **then**

20: **continue** _▷_ Not enough budget for this group
21: **end if**
22: _Ttarget ←_ min( _Tmax, Bremain_ )

     23: _Gtarget ←_ _Ttarget_ _▷_ Target grid size per side







�� _Hg_
24: _Pg ←_ clip
_Gtarget_




_, P_ min _, P_ max




        - _Hg_
25: _Tgroup ←_

_Pg_




- _×_ - _Wg_

_Pg_







26: _Tused ←_ _Tused_ + _Tgroup_
27: **end for**
28: **return** _{_ ( _Hg, Wg, Pg_ ) _| g ∈_ _Groups_ with allocated budget _}_
29: **end function**



specific weights ( _λ_ ) assigned to each component are listed
in Tab. S.7.We prioritize the reconstruction objective ( _λ_ 1 =
1 _._ 5) as it is the primary driver of feature learning in the
MAE framework. The auxiliary tasks (ERA5, map prediction, orbital regression) are weighted lower (0.05 - 0.1) to
act as regularizers and semantic guides without overwhelming the pixel-level reconstruction signal. The FFT loss [S5]
is included with a small weight to stabilize high-frequency
feature reconstruction.


**C. Experiments**


**C.1. Extensive Pangaea results**


We provide the complete tabulation of results for the Pangaea benchmark suite [S7] across three data availability
regimes: 10% (Tab. S.8), 50% (Tab. S.9), and 100%
(Tab. S.10). All THOR family model experiments are with



patch size 6, input size of 108 and concatenation of the output features. These experiments validate that THOR provides good performance in low training data regimes.
In the 10% regime (Tab. S.8), THOR-Base performs on
par with the current state-of-the-art, TerraMind-Base [S4],
by on fine-grained segmentation tasks. This confirms that
our flexible patching strategy, which allows for dense token
representations at inference time, compensates for the lack
of training labels by providing a richer signal to the decoder.
For the full training dataset, TerraMind show strong performance (achieving the top rank on average), but THOR-Base
remains highly competitive, outperforming the other models on PASTIS and CropMap tasks (Tab. S.10).


**Feature aggregation strategy** THOR processes input
groups independently, requiring a fusion strategy to com


7


Table S.8. Extended Pangaea results with 10% training data in mIoU. Bold/underline mark best/second-best per column.


Model HLS Burns MADOS PASTIS Sen1Floods11 FBP DynEarthNet CropMap SN7 AI4Farms Avg. Rank


CROMA 76.44 32.44 32.80 87.22 37.39 36.08 36.77 42.15 38.48 7.44
DOFA 71.98 23.77 27.68 82.84 27.82 **39.15** 29.91 46.10 27.74 11.78
GFM-Swin 67.23 28.19 21.47 62.57 55.58 28.16 27.21 39.48 32.88 14.11
Prithvi 77.73 21.24 33.56 86.28 29.98 32.28 27.71 36.78 35.04 11.67
RemoteCLIP 69.40 20.57 17.19 62.22 56.23 34.43 19.86 43.11 23.85 13.89
SatlasNet 74.79 29.87 16.76 83.92 37.86 34.64 29.08 49.78 13.91 12.11
Scale-MAE 75.47 21.47 22.86 64.74 48.75 35.27 13.44 49.68 26.66 12.33
SpectralGPT **83.35** 20.29 34.53 83.12 39.51 35.33 31.06 36.31 37.35 10.00
S12-MoCo 73.11 19.47 32.51 79.58 35.57 32.24 36.54 49.46 37.97 12.44
S12-DINO 75.93 23.47 36.62 84.95 34.63 32.78 38.44 41.15 37.91 9.78
S12-MAE 76.60 18.44 31.06 84.81 35.56 30.59 35.29 40.51 23.60 13.00
S12-Data2Vec 74.38 17.86 33.09 81.91 37.27 33.63 34.11 40.66 22.85 13.89
Terramind-B 77.39 **44.06** **39.96** 84.43 54.00 37.35 35.65 43.21 38.59 **4.56**


UNet Baseline 79.46 24.30 29.53 **88.55** 52.58 35.59 13.88 46.08 34.84 8.11
ViT Baseline 75.92 10.18 38.44 81.85 **56.53** 35.39 27.76 36.01 **39.20** 10.11


THOR-Tiny 75.98 37.63 36.26 82.70 42.81 34.03 37.82 58.52 38.56 7.11
THOR-Small 77.29 42.64 38.48 84.21 42.81 35.31 40.39 **59.41** 12.31 6.56
THOR-Base 76.90 40.67 38.93 86.29 42.80 35.21 **42.23** 55.93 38.90 4.67
THOR-Large 75.57 36.43 39.21 87.34 43.51 36.10 36.77 55.79 18.26 6.44


Table S.9. Extended Pangaea results with 50% training data in mIoU. Bold/underline mark best/second-best per column.


Model HLS Burns MADOS PASTIS Sen1Floods11 FBP DynEarthNet CropMap SN7 AI4Farms Avg. Rank


CROMA 81.52 57.68 32.33 90.57 48.01 38.30 42.20 59.31 28.19 **5.11**
DOFA 78.02 55.21 28.60 88.39 36.90 **39.20** 30.93 47.06 26.69 11.78
GFM-Swin 74.36 **63.37** 20.41 71.61 63.14 31.25 31.42 59.83 28.43 10.33
Prithvi 80.89 40.79 33.13 89.69 40.27 33.43 42.51 49.45 29.27 9.33
RemoteCLIP 74.28 53.26 17.46 71.67 **65.92** 30.91 36.3 50.83 25.11 13.11
SatlasNet 75.97 52.24 16.78 89.45 46.04 36.34 35.29 60.74 27.08 10.00
Scale-MAE 75.47 46.87 23.26 72.54 62.11 32.60 20.32 **61.24** 26.40 12.33
SpectralGPT 76.40 58.00 34.61 87.52 21.71 36.52 32.09 56.28 27.46 10.56
S12-MoCo 79.79 42.90 32.59 89.22 46.92 34.45 41.32 56.21 28.38 8.89
S12-DINO 80.12 40.42 35.71 88.93 44.85 32.76 31.13 55.14 25.68 12.33
S12-MAE 80.13 44.29 31.15 88.43 45.63 33.29 28.07 55.55 27.50 11.44
S12-Data2Vec 79.82 41.22 33.42 86.58 46.73 32.61 28.53 56.94 25.84 11.89


UNet Baseline **82.39** 43.87 30.25 **90.91** 55.42 35.14 36.30 46.82 **45.02** 7.89
ViT Baseline 78.17 28.77 38.71 86.08 57.32 37.33 39.53 49.21 38.37 9.00


THOR-Tiny 78.22 53.87 36.40 89.29 45.20 35.00 48.58 60.03 27.67 7.44
THOR-Small 79.14 52.14 38.20 90.42 44.99 36.64 45.41 59.46 27.95 7.11
THOR-Base 79.15 49.43 39.50 89.05 45.39 36.66 50.81 60.24 27.76 6.67
THOR-Large 76.69 52.30 **39.96** 89.92 46.03 37.36 **54.88** 59.67 27.96 5.78



bine features before the decoder. We compare mean aggregation (averaging token embeddings across groups) against
concatenation (stacking tokens along the channel dimension). As shown in Tab. S.11, concatenation consistently
outperforms mean aggregation, achieving 52.94% mIoU
(vs. 49.90%) in the 10% data regime. This may suggest that distinct sensor modalities contain complementary,
non-redundant information, and by averaging these features
high-frequency modality-specific signals may be "washed
out", whereas concatenation preserves the full feature variance often needed for fine-grained segmentation.


**Validation** **of** **compute-adaptive** **patching** A core
premise of THOR is that smaller patch sizes yield denser
feature maps, improving performance on pixel-level tasks.
Tab. S.12 validates this hypothesis: reducing the patch size
from 8 to 4 results in a significant performance boost, rising
from 54.69% to 58.63% mIoU on the full dataset.



While smaller patches increase the sequence length
(quadratic computational cost), they provide the necessary spatial granularity for segmentation tasks that coarse
patches (e.g., 16 _×_ 16) fail to resolve. This confirms that
THOR’s randomized patch pre-training successfully enables test-time adaptation to higher resolutions.


**Scaling and data efficiency** Fig. S.4 Figure S.4 illustrates the scaling behavior of the THOR family (Tiny,
Small, Base, Large) across data regimes. We observe a
clear "crossover" effect: In data-scarce regimes (10%), the
THOR-Base model is the most robust performer. Notably,
THOR-Large underperforms on the 10% data (lowest starting point in Fig. S.4), indicating that massive models may
be prone to overfitting when fine-tuning data is insufficient.
In data-rich regimes (50-100%), THOR-Large recovers and
surpasses all other variants, validating standard scaling laws
where capacity correlates with performance given sufficient



8


Table S.10. Extended Pangaea results with 100% training data in mIoU. Bold/underline mark best/second-best per column.


Model HLS Burns MADOS PASTIS Sen1Floods11 FBP DynEarthNet CropMap SN7 AI4Farms Avg. Rank


CROMA 82.42 67.55 32.32 90.89 51.83 38.29 49.38 59.28 25.65 7.00
DOFA 80.63 59.58 30.02 89.37 43.18 39.29 51.33 61.84 27.07 8.33
GFM-Swin 76.90 64.71 21.24 72.60 67.18 34.09 46.98 60.89 27.19 10.67
Prithvi 83.62 49.98 33.93 90.37 46.81 27.86 43.07 56.54 26.86 12.11
RemoteCLIP 76.59 60.00 18.23 74.26 **69.19** 31.78 52.05 57.76 25.12 12.56
SatlasNet 79.96 55.86 17.51 90.30 50.97 36.31 46.97 61.88 25.13 10.67
Scale-MAE 76.68 57.32 24.55 74.13 67.19 35.11 25.42 **62.96** 21.47 12.44
SpectralGPT 80.47 57.99 35.44 89.07 33.42 37.85 46.95 58.86 26.75 10.67
S12-MoCo 81.58 51.76 34.49 89.26 53.02 35.44 48.58 57.64 25.38 11.00
S12-DINO 81.72 49.37 36.18 88.61 51.15 34.81 48.66 56.47 25.62 12.11
S12-MAE 81.91 49.90 32.03 87.79 51.92 34.08 45.8 57.13 24.69 13.56
S12-Data2Vec 81.91 44.36 34.32 88.15 48.82 35.90 54.03 58.23 24.23 11.89
TerraMindv1-B 82.42 **69.52** 40.51 90.62 59.72 37.87 55.80 60.61 28.12 **3.56**


UNet Baseline **84.51** 54.79 31.60 **91.42** 60.47 **39.46** 47.57 62.09 **46.34** 5.00
ViT Baseline 81.58 48.19 38.53 87.66 59.32 36.83 44.08 52.57 38.37 11.11


THOR-Tiny 79.34 53.82 38.02 89.35 46.41 33.59 50.39 60.61 26.36 11.11
THOR-Small 79.26 52.66 39.54 90.14 47.25 34.84 59.49 60.01 26.91 9.11
THOR-Base 79.65 51.48 **40.76** 89.44 47.42 37.57 56.78 59.87 26.29 8.78
THOR-Large 79.47 53.73 39.88 89.55 47.62 37.29 **60.75** 59.71 26.75 8.33


Table S.11. Mean Pangaea test mIoU by output aggregation
method and training data, THOR base model.


**Output Aggregation** **10%** **100%**


concat 52.94 58.63
mean 49.90 56.02


Table S.12. Mean Pangaea test mIoU by patch size and training
data, THOR base model.


**Patch Size** **10%** **100%**



4 52.94 58.63
6 50.82 54.36
8 52.09 54.69


supervision. However, the performance depends strongly
on the dataset, as observed in Fig. S.5, where we show perdataset performance of each model in the THOR family.
To further investigate the trade-off between computational cost and downstream performance, we conducted a
series of experiments on four single-date Pangaea benchmarks using 10% of the training data. We compared
the performance of a standard UperNet decoder against a
lightweight linear probe decoder across varying patch sizes.
As illustrated in Figs. S.6a, S.6c, S.6e, and S.6g, while
the UperNet decoder yields a performance boost in certain
configurations, the linear decoder achieves competitive accuracy levels that are frequently on par with the much larger
architecture. Critically, when analyzing the computational
burden (Figs. S.6b, S.6d, and S.6f), the advantage of the
linear approach becomes clear. As detailed in Tab. S.13, the
UperNet architecture requires approximately 1000 _×_ more
parameters than the linear decoder. This drastic reduction



Figure S.4. Aggregated mIoU over all Pangaea benchmarks using 10, 50 and 100% training data for tiny, small, base and large
model. Patch size 6, concat feature aggregation and input image
size of 108 pixels.


in decoder parameter count validates THOR as a true foundation model. The ability of a simple linear probe to match
a complex non-linear decoder indicates that the pre-trained
encoder produces highly semantic, linearly separable features. It suggests that in data-limited regimes, the heavy
UperNet decoder is largely redundant and potentially prone
to overfitting, whereas THOR’s dense representations may
be deployed with minimal adaptation.


**C.2. Snow use-case**


We evaluate the regression capability of THOR on the fractional snow cover task (Tab. S.13). A linear decoder (using TerraTorch’s LinearDecoder) trained on frozen THOR
features consistently outperforms the fully supervised UNet
baseline (RMSE 12.4). Notably, THOR-Base with a linear decoder achieves the state-of-the-art RMSE of 9.88,
marginally surpassing the UPerNet head (RMSE 9.90).



9


Figure S.5. Per dataset mIoU for all Pangaea benchmarks using 10, 50 and 100% training data for tiny, small, base and large model. Patch
size 6, concat feature aggregation and input image size of 108 pixels.



Most critically, the linear decoder achieves this performance
using only 24.6k parameters, compared to the 22.9M parameters required by the UPerNet head. This 1000x reduction in decoder complexity demonstrates that THOR’s pretrained representations are linearly separable and semantically rich, requiring minimal adaptation for downstream
physical variable mapping.


**C.3. ERA5 Land analysis**


To validate the climate-awareness of the frozen encoder, we
analyze the performance of the linear probe on the holdout set against the ground truth ERA5-Land daily statistics
variables by sampling random crops with a ground cover
of 11520 m and extracting Sentinel-3 OLCI and SLSTR..
Fig. S.9 presents scatter plots for these targets, revealing a
clear distinction in performance: thermodynamic state variables (e.g., temperature_2m, surface_pressure)
exhibit strong linearity and tight clustering ( _R_ [2] _>_
0 _._ 8), whereas stochastic, accumulated phenomena (e.g.,
snow_depth, total_precipitation) remain challenging to regress from instantaneous optical/SAR snap


shots. This trend is quantified in Fig. S.7, which shows low
NRMSE and high _R_ [2] for thermal and vegetation indices,
contrasting with higher error rates for hydrological variables. However, the structural fidelity of the learned representation is confirmed in the correlation matrix of the predicted ERA5-Land values closely mirrors that of the ground
truth, demonstrating that THOR successfully captures the
physical inter-dependencies between these climatic variables (such as the coupling between soil moisture and temperature) (Fig. S.8). This suggests that the encoder moves
beyond visual texture matching to embed the broad climatological context required for downstream climate applications.


**Supplementary references**


[S1] Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio
Pizzati, Philip HS Torr, Adel Bibi, and Bernard
Ghanem. On pretraining data diversity for selfsupervised learning. In _European Conference on_
_Computer Vision_, pages 54–71. Springer, 2024.

[S2] Olivier Arino, Jose Julio Ramos Perez, Vasileios



10


(a) Sen1Floods11 - Patch Size (b) Sen1Floods11 - Model MACs


(c) MADOS - Patch Size (d) MADOS - Model MACs


(e) HLS Burns - Patch Size (f) HLS Burns - Model MACs


(g) AI4Smallfarms - Patch Size (h) AI4Smallfarms - Model MACs


11

Figure S.6. Model performance across different datasets. Left column shows test mIoU versus patch size, right column shows the respective
test mIoU versus model MACs (G). Using THOR Base frozen encoder and linear decoder _∼_ 0 _._ 2 M parameters (blue circles) and UPerNet
decoder _∼_ 67 _−_ 109M parameters (red squares) are compared across four benchmark datasets. Using a fixed input size of 128, and
concatenation of feature maps. All experiments run with 10% training data.


Table S.13. RMSE snow cover fraction. Image size 128 _×_ 128 and concatenated the tokens of the 500 m and 1000 m bands.


Decoder Encoder Patch size No. dec. param. Tot. no. param. RMSE


UNet 24.4M 12.4
UPerNet THOR Base 16x16 22.9M 0.1G 14.0
8x8 12.4
4x4 9.90
THOR Tiny 4x4 8.6M 16.2M 10.5
THOR Small 4x4 12.1M 37.9M **9.69**
THOR Large 4x4 33.1M 0.3G 12.2
Linear decoder THOR Base 4x4 24.6k 94.2M 9.88
THOR Tiny 4x4 6.1k 7.6M 11.5
THOR Small 4x4 12.3k 25.8M 10.9
THOR Large 4x4 32.8k 0.3G 10.3


Figure S.7. Comparison of NRMSE, R², and normalized bias for 17 ERA5 variables predicted from satellite embeddings. Color scale
ranges from green (good performance) to red (poor performance), with bias colored to highlight deviations from zero.



Kalogirou, Sophie Bontemps, Pierre Defourny, and
Eric Van Bogaert. Global Land Cover Map for 2009
(GlobCover 2009), 2012.

[S3] Mark A. Friedl and Damien Sulla-Menashe. MODIS/Terra+Aqua Land Cover Type Yearly L3 Global
500 m SIN Grid V061 (MCD12Q1). NASA LP
DAAC, 2022.

[S4] Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, et al. Terramind: Largescale generative multimodality for earth observation.
_arXiv preprint arXiv:2504.11171_, 2025.

[S5] Oren Kraus, Kian Kenyon-Dean, Saber Saberian,
Maryam Fallah, Peter McLean, Jess Leung, Vasudev
Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik,
Dominique Beaini, Maciej Sypetkowski, Chi Vicky
Cheng, Kristen Morse, Maureen Makes, Ben Mabey,
and Berton Earnshaw. Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology,



2024. arXiv:2404.10242 [cs].

[S6] Jeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam
Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni
Agarwal, Fayaz Jamil, Yossi Matias, Greg S Corrado, et al. Domain-specific optimization and diverse
evaluation of self-supervised models for histopathology. _arXiv preprint arXiv:2310.13259_, 2023.

[S7] Valerio Marsocci, Yuru Jia, Georges Le Bellier,
David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli,
et al. Pangaea: A global and inclusive benchmark
for geospatial foundation models. _arXiv preprint_
_arXiv:2412.04204_, 2024.

[S8] Sari Metsämäki, Jouni Pulliainen, Miia Salminen,
Kari Luojus, Andreas Wiesmann, Rune Solberg,
Kristin Böttcher, Mwaba Hiltunen, and Elisabeth
Ripper. Introduction to GlobSnow Snow Extent
products with considerations for accuracy assessment. _Remote Sensing of Environment_, 156:96–108,
2015.



12


Figure S.8. Comparison of inter-variable correlations for ground truth (left) and model-predicted (right) ERA5 variables.


[S9] Chris Oliver and Shaun Quegan. _Understanding syn-_
_thetic aperture radar images_ . SciTech Publishing,
2004.

[S10] A Ordonez, D Wade, C Ravaut, and AU Waldeland.
Towards a foundation model for seismic interpretation. In _85th EAGE Annual Conference & Exhi-_
_bition (including the Workshop Programme)_, pages
1–5. European Association of Geoscientists & Engineers, 2024.

[S11] Daniele Zanaga, Ruben Van De Kerchove, Dirk
Daems, Wanda De Keersmaecker, Carsten Brockmann, Grit Kirches, Jan Wevers, Oliver Cartus, Maurizio Santoro, Steffen Fritz, Myroslava Lesiv, Martin Herold, Nandin-Erdene Tsendbazar, Panpan Xu,
Fabrizio Ramoino, and Olivier Arino. Esa worldcover 10 m 2021 v200, 2022.


13


Figure S.9. Model predictions plotted against ground truth observations for 17 ERA5 variables. Red dashed lines indicate perfect predictions.


14


