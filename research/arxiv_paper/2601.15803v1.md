## Optimal stochastic impulse control problem with delay with actions decided at the execution time

Said Hamadene [∗] Ibtissam Hdhiri [†]


January 23, 2026


Abstract


In this paper, we consider a class of stochastic impulse control problem when there is
a fixed delay ∆between the decision and execution times. The dynamics of the controlled
system between two impulses is an arbitrary adapted stochastic process. Unlike the most
existing literature, we consider the problem when the impulse sizes are decided at the
execution time in both risk-neutral and risk-sensitive cases. This model fits more, in the
real life, for some problems such as the pricing of swing options. The horizon T of the
problem can be finite or infinite. In each case we show the existence of an optimal impulse
strategy. The main tools we use are the notions of reflected Backward Stochastic Differential
Equations (BSDEs for short) and the Snell envelope of processes.


AMS subject Classifications: 60G40; 60H10; 93E20.


Key words : Impulse control; Execution delay; Snell envelope; Stochastic control; Backward
stochastic differential equations; Optimal stopping time; Swing options.

### 1 Introduction



The impulse control problem consists of acting on a given system whose dynamics is described
by a stochastic process (Xt)t∈T, at discrete times τk with impulses βk, k ≥ 1 (τk ≤ τk+1). The
actions are not free and then need to be implemented in such a way to achieve a certain
level of profitability, or even optimal profitability. The sequences (τk)k≥1 and (βk)k≥1 are the
parameters of interventions of the controller on the system and δ := (τk, βk)k≥1 is called a
strategy of impulse control. For k ≥ 1, let ψ(βk) be the cost for the controller when she/he
intervenes on the system at time τk with the action βk. The expected total payoff is then given
by:




 -  - [�]
E Φ




   g(s, Xs [δ][)][ds][ −]

[0,T )




- ��

ψ(ξk)1[τk<T ], (1.1)
k≥1



where X [δ] stands for the controlled dynamics when the strategy δ is implemented, g is the
instantaneous reward function, Φ the utility function of the controller. The constant T is finite
(resp. infinite) when the horizon is finite (resp. infinite). The objective is then to find an
optimal strategy, i.e., a strategy δ [∗] which maximizes (1.1).
The case Φ(x) = x (resp. Φ(x) = e [θx] ) is called risk-neutral (resp. risk-sensitive).
In the case when X [δ] is Markovian in between two successive impulses (i.e. on the time
interval [τk, τk+1[ for any k ≥ 0 (τ0 = 0)), the stochastic impulse control problem is solved


[∗LMM, Le Mans University. e-mail: said.hamadene@univ-lemans.fr. The author research is part of the ANR](mailto: said.hamadene@univ-lemans.fr)
project DREAMeS (ANR-21-CE46-0002).
[†Faculty of sciences of Gab`es & LR17ES11, University of Gab`es. e-mail: : Ibtissem.Hdhiri@fsg.rnu.tn](mailto: Ibtissem.Hdhiri@fsg.rnu.tn)


1


mainly by using deterministic techniques (one can see e.g. [3, 25, 15, 22, 27, 26, 24], etc. the
list is far from being exhaustive). Now, when the dynamics is not necessarily Markovian, the
impulse control problem was considered for the first time by Djehiche et al. [10] who proposed
a new approach based on the notions of Backward Stochastic Differential Equations and Snell
envelope of processes. Later on, several works have been carried out on the subject including

[18, 30, 1, 20] to quote a few.
When the execution of impulses requires a preparatory work, a time delay ∆is integrated
into the model. The decisions are taken at τk and executed at τk + ∆, see [19, 11, 2, 4, 27],
etc. for more details. Except in [2], in those works, the amplitude of the impulse βk is decided
at the decision time τk. However, in several realistic situations, although the controller takes
decision at τk, she/he may choose or be required to wait until the execution time τk + ∆before
to determine the size of the intervention.
This feature appears in the pricing of swing options which are options in markets of commodities (see e.g. [28, 21, 13, 9, 32] and the references therein). They consist of a contract
which delineates the least and most energy that the option holder can buy (or ”take”) per day
and per month, how much that energy will cost (known as its strike price), and how many times
Nc during the month she/he can change or ”swing” the daily quantity of energy purchased.
Swing options are most commonly used for the purchase of oil, natural gas, and electricity.
They may be used as hedging instruments by the option holder, to protect against price changes
in these commodities. For example, a power company might use a swing option to manage
changes in customer demand for electricity that occur throughout the month as temperatures
rise and fall. These contracts are more intricate than they appear to be. Consequently, they
tend to make valuation challenging. An oil company might also do the same with fuel for
customer demand for heat during winter months.
The price paid for the commodity can either be fixed or floating. A floating or ”indexed”
price essentially means that it is linked to the price in the market. In contrast to a fixed price
contract, an indexed price contract is less flexible.
There are several works on swing options including [9, 6, 13, 21, 28, 32] to quote a few. This
outstanding feature of the embedded options is reminiscent of American contingent claims with
multiple exercises. This point of view has been well documented in the paper by CarmonaTouzi [7], in the case when maturity is finite or infinite, aiming mainly to find optimal times
of interventions (τi [∗][)][i][=1][,N] c [. In [7], the authors assume a refraction period which is a condition]
which looks like to our delay, i.e. τi [∗] [−] [τ] i [ ∗] −1 [≥] [∆in order to prevent (][τ] i [∗][)][i][=1][,N] c [from bunching]
up. On the other hand, as mentionned previously, the authors assume that the dynamics of
the price of the commodity is not affected by the exercises. Namely we are in the framework
of a small investor. However one can think of that the option integrates prices by layers, in
which case the effective price depends on the amount of the commend. For example, the option
can stipulate that the price increases with the amount of the commend. If moreover there is
a refraction period and the amount of the commend is made when it is executed, then pricing
this option falls exactly in the model we study in this paper.
The novelty of this paper is to consider the impulse control problem for general stochastic
processes in the case when there is a fixed delay ∆in the interventions and the amplitudes of
the impulses are decided at the moment when those latter occur and not when the decisions to
impulse are made. We study the cases of finite and infinite horizons and risk-neutral and risksensitive utility functions. Note that, in [2] the authors deal with the so-called Markovian setting,
by using deterministic methods, since between two impulses the dynamics of the controlled
system is solution of a standard Markovian stochastic differential equation. On the other hand,
the model we consider differs from the one treated in [11] because in this latter the sizes ξn of
the impulses are decided at τn, when the decision to make an impulse is made.
The rest of the paper is organized as follows. In section 2, we consider the problem of


2


stochastic impulse control with finite horizon T and fixed delay ∆when the sizes of the impulses
are fixed at the execution times, in both the risk-neutral and risk-sensitive settings. The delay
∆makes that it is only possible to implement X = [ ∆ [T] [] impulses at most before the horizon]

T . In both cases we prove the existence of an optimal strategy which we exhibit. Mainly we
use reflected BSDEs (resp. Snell envelope of processes) to deal with the risk-neutral (resp.
risk-sensitive) framework. In Section 3, we consider the infinite horizon setting of the problem
in the risk-neutral case. Once more, by using reflected BSDEs in infinite horizon, we show the
existence of an optimal strategy which we exhibit. The risk-sensitive case can be deduced in
a similar way (see Remark 3.4). Finally, at the end of the paper we gather, as an Appendix,
some results, on the projections of stochastic processes and on the Snell envelope of processes,
to make as much as possible the paper self-contained.

### 2 The finite horizon setting


2.1 Preliminaries and description of the model


Let T > 0 be a real number which is the horizon of the problem. Let (Ω, F, P) be a complete
probability space on which is defined a standard d-dimensional Brownian motion B = (Bt)t≤T .
Let (Ft [0] [:=][ σ][{][B][s][, s][ ≤] [t][}][)][t][≤][T][ be the natural filtration of][ B][ and (][F][t][)][t][≤][T][ its completion with]
the P-null sets of F, therefore (Ft)t≤T satisfies the usual conditions. The Euclidean norm of an
element x ∈ R [n] is denoted by |x|. We also need the following notations (p > 1 and n ≥ 1):


  - P is the σ-algebra on [0, T ] × Ωof Ft-progressively measurable processes;

 - H [p,n] = {(vt)t≤T : P-measurable, R [n] -valued process s.t. E[ [�] 0 [T] [|][v][s][|][p][ ds][]][ <][ ∞}][;]

  - S [2] = {Y, P-measurable c`adl`ag (or rcll) process, such that E[sup0≤t≤T |Yt| [2] ] < ∞};

  - Sc [2] [(resp.][ S] i [2][) is the subset of][ S][2][ of continuous processes (resp. continuous non-decreasing]
processes (kt)t≤T such that k0 = 0);


  - Tt0 = {ν : (Ft)t≤T -stopping time, such that P-a.s, t0 ≤ ν ≤ T } (t0 ≤ T ).


We consider a system, whose dynamics is described by a stochastic process (Xt)t≤T of H [2][,ℓ]

(ℓ ≥ 1), which is subject to impulses which are implemented with some fixed delay ∆ < T .
Precisely when a decision to impulse the system at τ is made, it will be executed with a fixed
lag ∆, i.e., at time τ + ∆. On the other hand, unlike the most of existing works, we assume
that the size of the impulse is not determined at the time of decision τ but at the execution one
τ + ∆. Consequently an impulse strategy δ := (τn, ξn)n≥1 of the controller consists of a pair of
sequences (τn)n≥1 ∈T0 and (ξn)n≥1 such that for any n ≥ 1:


(a) min{T, τn + ∆} ≤ τn+1 ≤ T .


(b) ξn are F(τn+∆)∧T -measurable random variables taking values in a finite subset
U = {a1, . . ., ap} of R [ℓ] .


The strategy δ = (τn, ξn)n≥1 is called admissible if it satisfies the conditions (a)-(b).


In this definition of admissible strategies, point (a) means that as far as the horizon of the
problem is not reached, there is a delay ∆between two impulses while point (b) tells us that
the size of the impulse is decided when it occurs.


Remark 2.1. The condition (a) above implies also that for any n ≥ 1, τn ≤ τn+1. Actually it
is enough to consider n0 = inf{n ≥ 0, τn + ∆ - T } and to show the property for n ≤ n0 and for
n ≥ n0 + 1.


3


The set of admissible strategies will be denoted by A and for m ≥ 0, Am will be the set of
admissible strategies δ = (τn, ξn)n≥1 such that for all n > m, τn = T, i.e., they are strategies
which allow for m impulses at most. For every δ ∈Am or A, when mentioned, we assume that
τ0 = 0 and ξ0 = 0. Finally let us point out that in our model the values of ξn are irrelevant
when τn ≥ T − ∆(see the definition of the payoff in (2.4)).


If an admissible strategy or control policy δ = (τn, ξn)n≥1 is implemented, then the dynamics
X [δ] of the controlled process is given by:

      ∀t < T, Xt [δ] [=][ X][t][ +] ξn1[τn+∆≤t] and XT [δ] [= lim] s [.] (2.1)

sրT [X] [δ]
n≥1


This definition of the process X [δ] means that the controller does not make an impulse at the
terminal time T . The associated total expected reward for the controller is given by:




     - T
J(δ) := E[Φ{



ψ(ξn)1[τn<T −∆]}], (2.2)
n≥1



T 
g(s, Xs [δ][)][ ds][ −]
0



where g(.) stands for the instantaneous reward function and ψ(a) is the cost paid by the agent
when exerting a control of magnitude a which we assume non-negative. The function Φ is the
utility function of the controller. In our frameworks, it will be first chosen as the identity (resp.
exponential) function in Subsection 2.2 (resp. 2.3).
This work aims to find the optimal admissible strategy δ [∗] = (τn [∗][, ξ] n [∗][)] n≥1 [of the controller,]
i.e., which maximizes the expected reward (2.2) and then satisfies


J(δ [∗] ) = sup J(δ).
δ∈A


Since every two consecutive interventions are separated by at least a fixed lag ∆, then we
have:


Proposition 2.1. Let X = [ ∆ [T] []][. We then have:]


sup J(δ) = sup J(δ). (2.3)
δ∈A δ∈AX


Proof. Actually sup J(δ) ≥ sup J(δ) since AX ⊂A. On the other hand for any δ ∈A, δ
δ∈A δ∈AX

belongs obviously to AX, which implies the reverse inequality and then the equality between
the two hand-sides of (2.3).


Next on the functions g and ψ we assume:


Assumption 2.1.
i) The mapping (t, ω, x) ∈ [0, T ] × Ω × R [ℓ] �→ g(t, ω, x) ∈ R is P ⊗B(R [ℓ] )/B(R)-measurable.
Moreover there exists a non-negative process (γt)t≤T of H [2][,][1] such that:


P − a.s. for any (t, x) ∈ [0, T ] × R [ℓ], |g(t, ω, x)| ≤ γt(ω).


ii) For any a ∈ R [ℓ], ψ(a) ≥ 0.


2.2 The risk neutral impulse control problem


In this part, we consider the standard risk neutral case of our problem i.e. Φ(x) = x in (2.2).
The total expected reward associated with an admissible strategy δ = (τn, ξn)n≥1 is then given


4


by:




      - T
J(δ) := E[



T 
g(s, Xs [δ][)][ ds][ −]
0




   - (τ1+∆)∧T
= E[



ψ(ξn)1[τn<T −∆]] (2.4)
n≥1




- - (τn+1+∆)∧T

{
n≥1 (τn+∆)∧T



(τ1+∆)∧T 
g(s, Xs) ds +
0



g(s, Xs + ξ1 + . . . + ξn) ds − ψ(ξn)1[τn<T −∆]}].
(τn+∆)∧T



We are going to show that an optimal admissible strategy exists which is exhibited indeed.


2.2.1 Construction of the optimal strategy. An iterative scheme via reflected
BSDEs.


For any a ∈ R [ℓ], let Y [0] (a) be the process defined by:




          - [�] [T]
∀t ≤ T, Yt [0][(][a][) =][ E]




[T]  -  - t

g(s, Xs + a)ds|Ft  0 0




[T]  -  - [�] [T]

g(s, Xs + a)ds|Ft = E
t 0



g(s, Xs + a)ds.
0



(2.5)
Note that since g verifies Assumption 2.1-i), then by Doob’s maximal inequality (see [31], pp.54),
the process Y [0] (a) belongs to Sc [2][.]
Next for any n ≥ 1, let (Y [n] (a), K [n] (a), Z [n] (a)) be the solution of the following reflected
BSDE:

 Y [n] (a) ∈S�c [2] T [, Z] [n][(][a][)][ ∈H][2][,d][ and][ K] [n][(][a][)][ ∈S] i [2][;]        - T







Y [n] (a) ∈Sc [2][, Z] [n][(][a][)][ ∈H][2][,d][ and][ K] [n][(][a][)][ ∈S] i [2][;]
Yt [n][(][a][) =] �tT [g][(][s, X][s][ +][ a][)][ ds][ +][ K] T [n][(][a][)][ −] [K] t [n][(][a][)][ −] �tT [Z] s [n][(][a][)][ dB][s][, t][ ≤] [T][ ;]



t t  - T

Yt [n][(][a][)][ ≥] [O] t [n][(][a][)][,][ ∀][t][ ≤] [T][ and]



TT t t s (2.6)

(Yt [n][(][a][)][ −] [O] t [n][(][a][))][ dK] t [n][(][a][) = 0][.]
0



The barrier (Ot [n][(][a][))][t][≤][T] [, which depends on][ Y][ n][−][1][(][a][), is given by:][ ∀][t][ ≤] [T][,]




      - [�] [(][t][+∆)][∧][T]
Ot [n][(][a][) : =][ E]




[(][t][+∆)][∧][T]  
g(s, Xs + a) ds|Ft + 1[t<T −∆]E[max t+∆ [(][a][ +][ β][)][}|F][t][]]
t β∈U [{−][ψ][(][β][) +][ Y][ n][−][1]




  - (t+∆)∧T
= E[



|Ft]



g(s, Xs + a) ds + 1[t<T −∆] max t+∆ [(][a][ +][ β][)][}]

- t �� β∈U [{−][ψ][(][β][) +][ Y][ n][−][1] L [n] t [(][a][)]



Let us notice that the process (Ot [n][(][a][))][t][≤][T] [is the predictable projection of the measurable process]
(L [n] t [(][a][))][t][≤][T] [(see Appendix 4.1).] According to Theorem 1.3 in [16], if (Ot [n][(][a][))][t][≤][T] [is rcll with]
positive jumps only and uniformly square integrable then the solution of the BSDE (2.6) exists
and is unique and the first component Y [n] (a) of the solution has the following representation:

                   - τ
∀t ≤ T, Yt [n][(][a][) = esssup] τ ∈Tt [E][[] g(s, Xs + a) ds + Oτ [n][(][a][)][|F][t][]][.] (2.7)

t


Proposition 2.2. For any n ≥ 1 and a ∈ R [ℓ], the triple (Y [n] (a), Z [n] (a), K [n] (a)) is well-posed.


Proof. We proceed by induction. First note that, as mentionned previously, for any a ∈ R [ℓ], the
process Y [0] (a) belongs to Sc [2][. Now recall that for any][ t][ ≤] [T][,]

       - [�] [(][t][+∆)][∧][T]       Ot [1][(][a][) :=][ E] g(s, Xs + a) ds|Ft + 1[t<T −∆]E[max t+∆ [(][a][ +][ β][)][}|F][t][]][.]

t β∈U [{−][ψ][(][β][) +][ Y][ 0]



Therefore this process is obviously uniformly d P-square integrable. Next the processes




  - [�] [(][t][+∆)][∧][T]
(E




[(][t][+∆)][∧][T]  
g(s, Xs + a) ds|Ft )t≤T and (E[maxβ∈U {−ψ(β) + Yt [0] +∆ [(][a][ +][ β][)][}|F][t][])][t][≤][T][ are con-]
t



tinuous as they are the predictable projections of continuous processes respectively since the



5


filtration (Ft)t≤T is Brownian. Consequently the process (Ot [1][(][a][))][t][≤][T][ is uniformly][ d][P][-square]
integrable and continuous on [0, T ] −{T − ∆}. So let us analyze the discontinuity of (Ot [1][(][a][))][t][≤][T]
in T − ∆. Actually




         - [�] [T]
tցlimT −∆ [O] t [1][(][a][) =][ E]




[T]  
g(s, Xs + a) ds|FT −∆
T −∆



and




         - [�] [T]          lim t [(][a][) =][ E] g(s, Xs + a) ds|FT −∆ + max
tրT −∆ [O][1] T −∆ β∈U [(][−][ψ][(][β][))]



since YT [0][(][a][ +][ β][) = 0 and the filtration (][F][t][)][t][≤][T][ is Brownian (see e.g [8] VI, Theorem 90). As the]
function ψ ≥ 0 then the jump of (Ot [1][(][a][))][t][≤][T][ at][ T][ −][∆is non negative which implies, by Theorem]
1.3 in [16], that the processes (Y [1] (a), Z [1] (a), K [1] (a)) which verify (2.6) exist. In particular the
processes Y [1] (a) and K [1] (a) are continuous. Next, in the same way, if for some n ≥ 1, for any
a ∈ R [ℓ], the triple (Y [n] (a), Z [n] (a), K [n] (a)) which satisfies (2.6) exists then for any a ∈ R [ℓ], the
triple (Y [n][+1] (a), Z [n][+1] (a), K [n][+1] (a)) satisfying (2.6) exits. This completes the proof of the wellposedness of the processes (Y [n] (a), Z [n] (a), K [n] (a)) solution of (2.6). Finally let us notice that
the representation (2.7) holds true for Y [n] (a).


Proposition 2.3. For any n ≥ 0 and a ∈ R [ℓ], we have:

                  - T
∀t ∈ [T − ∆, T ], Yt [n][(][a][) =][ E][[] g(s, Xs + a)ds|Ft] = Ot [n][(][a][)][.]

t



Proof. If n = 0, the equality holds true by definition of Y [0] (a). So let us assume that for
some n ≥ 1, for any a ∈ R [ℓ], the relation holds true. By its definition in (2.6), the triple
(Y [n] (a), Z [n] (a), K [n] (a)) verifies on [T − ∆, T ]:

 Yt [n][(][a][) =]  - T [g][(][s, X][s][ +][ a][)][ ds][ +][ K] [n][(][a][)][ −] [K] t [n][(][a][)][ −]  - T [Z] s [n][(][a][)][ dB][s][, T][ −] [∆] [≤] [t][ ≤] [T][ ;]



Yt [n][(][a][) =] �tT [g][(][s, X][s][ +][ a][)][ ds][ +][ K] T [n][(][a][)][ −] [K] t [n][(][a][)][ −] �tT [Z] s [n][(][a][)][ dB][s][, T][ −] [∆] [≤] [t][ ≤] [T][ ;]







t t T  - tT

Yt [n][(][a][)][ ≥] [O] t [n][(][a][) for][ t][ ∈] [[][T][ −] [∆][, T][ ] and]



T (2.8)

(Yt [n][(][a][)][ −] [O] t [n][(][a][))][ dK] t [n][(][a][) = 0][.]
T −∆



But for any t ∈ [T − ∆, T ],




     - T
Ot [n][(][a][) =][ E][[] g(s, Xs + a)ds|Ft].

t



Next, once more for t ∈ [T − ∆, T ], let us set:

                 - T
Y˜t [n][(][a][) =][ E][[] g(s, Xs + a)ds|Ft].

t


Therefore, by the representation property of square integrable Brownian martingales, there
exists a P-measurable process ( Z [˜] t [n][(][a][))][T][ −][∆][≤][t][≤][T] [which is][ dt][ ⊗] [d][P][-square integrable on [][T][ −]
∆, T ] × Ωsuch that for any t ∈ [T − ∆, T ],




    - T
Y˜t [n][(][a][) =]



T - T

g(s, Xs + a)ds −
t t



Z˜s [n][(][a][)][dB][s][.]
t



It means that ( Y [˜] t [n][(][a][)][,][ ˜][Z] t [n][(][a][)][,][ 0)][T][ −][∆][≤][t][≤][T][ is also a solution for the reflected BSDE (2.8). But]
the solution of this latter is unique, therefore for any t ∈ [T − ∆, T ],

             - T
Y˜t [n][(][a][) =][ E][[] g(s, Xs + a)ds|Ft] = Yt [n][(][a][) =][ O] t [n][(][a][)]

t


which is the desired result.


6


Remark 2.2. By the uniqueness of the solution of (2.8), for any n ≥ 1 and any a ∈ R [ℓ], it
holds that dKt [n][(][a][) = 0][, for any][ t][ ∈] [[][T][ −] [∆][, T][ ]][.]

Next recall that X = [ ∆ [T] [] is the maximal amount of impulses that the controller is allowed]

to make due to the delay ∆and finite horizon T . So for k = 1, ..., X, let us define the finite set
Uk as follows:



Next recall that X = [ [T]



Uk := {a =



�k



aj, where (a1, ..., ak) ∈ U [k] }.
j=1



whileFor a fixed [�][k] j=1 [a] k [j][ is the sum of the cumulative impulses up to the], the k-tuple (a1, ..., ak) stands for the impulses implemented by the controller [ k][-th one.]

Next let V = [�] k=1,X [U][k][,][ τ][ be a stopping time and][ ξ][ an][ F][τ] [-random variable which takes its]

values in the finite subset V of R [ℓ] . We set, for any t ≥ τ,




  Yt [n][(][ξ][) :=]




- 
Yt [n][(][a][)][1][[][ξ][=][a][]][, Z] t [n][(][ξ][) :=]
a∈V a∈V




- 
Zt [n][(][a][)][1][[][ξ][=][a][]][, K] t [n][(][ξ][) :=]
a∈V a∈V



Kt [n][(][a][)][1][[][ξ][=][a][]][,] (2.9)
a∈V



and finally

       Ot [n][(][ξ][) :=] Ot [n][(][a][)][1] [ξ=a] [.] (2.10)

a∈V



From the definition (2.6), the triple (Yt [n][(][ξ][)][, Z] t [n][(][ξ][)][, K] t [n][(][ξ][))] t∈[τ,T ] [verifies:][ P][-a.s. for any][ τ][ ≤] [t][ ≤] [T][,]
    - T
 E[supt∈[τ,T ] |Yt [n][(][ξ][)][|][2][ +][ K] T [n][(][ξ][)][2][] +][ E][[] τ [|][Z] t [n][(][ξ][)][|][2][]][dt][]][ <][ ∞][;]

[T] [�] [T]



(2.11)








                 - T
E[supt∈[τ,T ] |Yt [n][(][ξ][)][|][2][ +][ K] T [n][(][ξ][)][2][] +][ E][[] τ [|][Z] t [n][(][ξ][)][|][2][]][dt][]][ <][ ∞][;]
Yt [n][(][ξ][) =][ �] t [T] [g][(][s, X][s][ +][ ξ][)][ ds][ +][ K] T [n][(][ξ][)][ −] [K] t [n][(][ξ][)][ −] [�] t [T] [Z] s [n][(][ξ][)][ dB][s][;]



t t  - T

Yt [n][(][ξ][)][ ≥] [O] t [n][(][ξ][) and]



(Yt [n][(][ξ][)][ −] [O] t [n][(][ξ][))][ dK] t [n][(][ξ][) = 0][.]
τ



Remark 2.3. As in Proposition 2.3, for any t ∈ [(T −∆)∨τ, T ], Yt [n][(][ξ][) =][ O] t [n][(][ξ][) =][ E][[] �tT [g][(][s, X][s][+]
ξ)ds|Ft] and dKt [n][(][ξ][) = 0][.]


2.2.2 The optimal strategy


We are going now to show that an optimal strategy exists. It is constructed with the help
of processes Y [n] (a), a ∈ R [ℓ], for some appropriate n.


We first show:


Proposition 2.4. For every n ∈ N [∗], there exists a strategy δ [n] in An which satisfies:


sup J(δ) = J(δ [n] ) = Y0 [n][(0)][.]
δ∈An


Thus δ [n] is optimal for the impulse control problem when only n impulses at most are allowed.


Proof. The proof will be divided into two steps.

Step 1: Let us show that Y0 [n][(0) =][ J][(][δ][n][) where][ δ][n][ will be exhibited later on, as we progress]
through the proof. First recall that the triple (Y [n] (0), Z [n] (0), K [n] (0)) verifies:

 Y [n] (0) ∈S�c [2] T [, Z] [n][(0)][ ∈H][2][,d][ and][ K] [n][(0)][ ∈S] i [2][;]          - T







Y [n] (0) ∈Sc [2][, Z] [n][(0)][ ∈H][2][,d][ and][ K] [n][(0)][ ∈S] i [2][;]
Yt [n][(0) =] �tT [g][(][s, X][s][)][ ds][ +][ K] T [n][(0)][ −] [K] t [n][(0)][ −] �tT [Z] s [n][(0)][ dB][s][, t][ ≤] [T][ ;]



t t �T T

Yt [n][(0)][ ≥] [O] t [n][(0)][, t][ ≤] [T][ and]



T t t s [s] (2.12)

(Yt [n][(0)][ −] [O] t [n][(0))][ dK] t [n][(0) = 0][.]
0



Next let τ1 [n] [be the following stopping time:]


τ1 [n] [= inf][{][s][ ≥] [0][, Y] s [ n][(0) =][ O] s [n][(0)][}][.]


7


By Proposition 2.3, P − a.s., τ1 [n] [≤] [T][ −] [∆. On the other hand since the process (][O] t [n][(0))][t][≤][T][ is]
continuous on [0, T ] −{T − ∆} and cannot have a negative jump at T − ∆, then we have:




      - τ1 [n]
Y0 [n][(0)] = E[



g(s, Xs) ds + Yτ [n] 1 [n] [(0)]] (2.13)
0




[τ][ n] 1    - T

g(s, Xs) ds + 1[τ1 [n][<T][ −][∆]][O] τ [n] 1 [n] [(0) +][ 1][[][τ][ n] 1 [=][T][ −][∆]][E][[]
0 τ [n]




   - [�] [τ][ n] 1
= E



T 
g(s, Xs)ds|Fτ1 [n] []] .
τ1 [n]



But

       - [�] [(][τ][ n] 1 [+∆)][∧][T]        Oτ [n] 1 [n] [(0) =][ E] τ1 [n] g(s, Xs) ds|Fτ1 [n] + 1[τ1 [n][<T][ −][∆]][E][[max] β∈U [{−][ψ][(][β][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][β][)][}|F][τ][ n] 1 []][.]


So for l ∈I = {1, . . ., p}, let Γ [1] l [,n] be the following set:

     Γ [1] l [,n] = {−ψ(al) + Yτ [n] 1 [n][−][+∆][1] [(][a][l][)][ ≥−][ψ][(][a][j][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][a][j][)][}][.]

1≤j≤p, j=l


Next we define Γ [˜][1] l [,n], l ∈I, by:

˜Γ [1] 1 [,n] = Γ [1] 1 [,n] and for l ≥ 2, Γ [˜] [1] l [,n] = Γ [1] l [,n] \ [Γ [1] 1 [,n] ∪ . . . Γ [1] l− [,n] 1 []][.] (2.14)

Those sets Γ [˜][1] l [,n], l ∈I, belong to Fτ1 [n][+∆][, are pairwise disjoint and on ˜Γ] l [1][,n], if non empty, al ∈ U
realizes the maximum of
maxa∈U [(][−][ψ][(][a][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][a][))][.]

Finally let us set

       β1 [n] [=] al1˜Γ1l,n . (2.15)

1≤l≤p

The random variable β1 [n] [is][ F][τ][ n] 1 [+∆][−][measurable and verifies:]

maxa∈U [(][−][ψ][(][a][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][a][)) =][ −][ψ][(][β] 1 [n][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][)][.]


Therefore

        - [�] [(][τ][ n] 1 [+∆)][∧][T]        Oτ [n] 1 [n][(0) =][ E] g(s, Xs) ds|Fτ1 [n] + 1[τ1 [n][<T][ −][∆]][E][[(][−][ψ][(][β] 1 [n][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][))][|F][τ][ n] 1 []]

τ1 [n]


and then




      - τ n1
Y0 [n][(0)] = E{



τ n1  - [�] [(][τ][ n] 1 [+∆)][∧][T]

g(s, Xs) ds + 1[τ1 [n][<T][ −][∆]][E]
0 τ [n]




[(][τ][ n]
1 [+∆)][∧][T]  
g(s, Xs) ds|Fτ1 [n]
τ1 [n]




                       - T
+1[τ1 [n][<T][ −][∆]][E][[(][−][ψ][(][β] 1 [n][) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][))][|F][τ][ n] 1 [] +][ 1][[][τ][ n] 1 [=][T][ −][∆]][E][[]

[n]




   - τ1 [n][+∆]
= E{



g(s, Xs)ds|Fτ1 [n][]][}]
τ1 [n]



g(s, Xs) ds + 1[τ1 [n][<T][ −][∆]][(][−][ψ][(][β] 1 [n][)) +][ Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][)][}] (2.16)
0



since Yτ [n] 1 [n][−][+∆][1] [(][β] 1 [n][) =][ 1][[][τ][ n] 1 [<T][ −][∆]][.Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][) because][ Y] T [ n][−][1] (β1 [n][) = 0,][ P][-a.s. This is for the first]
step.
Next we deal with the quantity Yτ [n] 1 [n][−][+∆][1] [(][β] 1 [n][). First, let us point out that by (2.11), the triple]
(Yt [n][−][1] (β1 [n][)][, Z] t [n][−][1] (β1 [n][)][, K] t [n][−][1] (β1 [n][))] t∈[τ [ n] 1 [+∆][,T][ ]][ verifies:][ P][-a.s. for any][ τ] 1 [ n] [+ ∆] [≤] [t][ ≤] [T][,]
 Yt [n][−][1] (β1 [n][) =]   - T [g][(][s, X][s][ +][ β] 1 [n][)][ ds][ +][ K] T [n][−][1] (β1 [n][)][ −] [K] t [n][−][1] (β1 [n][)][ −]   - T [Z] s [n][−][1] (β1 [n][)][ dB][s][;]



Yt [n][−][1] (β1 [n][) =] �tT [g][(][s, X][s][ +][ β] 1 [n][)][ ds][ +][ K] T [n][−][1] (β1 [n][)][ −] [K] t [n][−][1] (β1 [n][)][ −] �tT [Z] s [n][−][1] (β1 [n][)][ dB][s][;]







t 1 t 1� T

Yt [n][−][1] (β1 [n][)][ ≥] [O] t [n][−][1] (β1 [n][) and]



(2.17)
(Yt [n][−][1] (β1 [n][)][ −] [O] t [n][−][1] (β1 [n][))][ dK] t [n][−][1] (β1 [n][) = 0][.]
τ1 [n][+∆]



8


Let D1 [n] [=][ {][τ] 1 [ n] [≤] [T][ −] [2∆][}][ and ¯][D] 1 [n] [its complement, which belong to][ F][τ][ n] 1 [, and let][ γ] 2 [n] [and][ τ] 2 [ n] [be]
the following stopping times:


γ2 [n] [= inf][{][s][ ≥] [(][τ] 1 [ n] [+ ∆)][, Y] s [ n][−][1] (β1 [n][) =][ O] s [n][−][1] (β1 [n][)][}][,]


and
τ2 [n] [=][ γ] 2 [n][1][D] 1 [n] [+][ T][ 1][ ¯] D1 [n][.]

Since on D [¯] 1 [n][,][ τ] 1 [ n] [+ ∆] [> T][ −] [∆, then by virtue of Proposition 2.3 and Remark 2.3, we have:]

                  - T
1 ¯D1 [n] [Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][) =][ 1][ ¯] D1 [n][E][[] g(s, Xs + β1 [n][)][ds][|F][τ][ n] 1 [+∆][]][.]

τ1 [n][+∆]

Now, once more by Proposition 2.3, P-a.s. γ2 [n][1][D] 1 [n] [≤] [T][ −] [∆since][ Y] T [ n] - [−] ∆ [1] [(][β] 1 [n][) =][ O] T [n] - [−][1] ∆ [(][β] 1 [n][). On]
the other hand, since K [n][−][1] (β1 [n][−][1] ) is constant on [τ1 [n] [+ ∆][, τ] 2 [ n][]][ ∩] [D] 1 [n][, we have:]



1D1 [n] [Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][)] = E{1D1 [n]


         = E 1D1 [n]




- τ n2



g(s, Xs + β1 [n][)][ds][ +][ 1][D] 1 [n] [Y] τ [ n] 2 [n][−][1] (β1 [n][)][|F][τ][ n] 1 [+∆][}]
τ1 [n][+∆]



g(s, Xs + β1 [n][)][ds][ +][ 1] D1 [n][∩][[][τ][ n] 2 [<T][ −][∆]][O] τ [n] 2 [n][−][1] (β1 [n][)]
τ1 [n][+∆]




- τ2 [n]




        - T
+1D1 [n][∩][[][τ][ n] 2 [=][T][ −][∆]][E][{]



T 
g(s, Xs + β1 [n][)][ds][|F][τ][ n] 2 [}|F][τ][ n] 1 [+∆] . (2.18)
τ2 [n]



Next we focus on 1D1 [n][∩][[][τ][ n] 2 [<T][ −][∆]][O] τ [n] 2 [n][−][1] (β1 [n][). Recall that]

       Oτ [n] 2 [n][−][1] (β1 [n][) =] 1{β1 [n][=][a][}][O] τ [n] 2 [n][−][1] (a)

a∈U


and for any a ∈ R [ℓ],

       - [�] [(][τ][ n] 2 [+∆)][∧][T]        Oτ [n] 2 [n][−][1] (a) := E τ2 [n] g(s, Xs + a) ds|Fτ2 [n] + 1[τ2 [n][<T][ −][∆]][E][[max] β∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][a][ +][ β][)][}|F][τ][ n] 2 []][.]

Therefore, since on D1 [n][,][ τ] 1 [ n] [+ ∆] [≤] [τ] 2 [n] [≤] [T][ −] [∆and][ β] 1 [n] [is][ F][τ][ n] 1 [+∆][-measurable, we have:]




              - [�] [(][τ][ n] 2 [+∆)]
1D1 [n][O] τ [n] 2 [n][−][1] (β1 [n][)] = 1D1 [n] [E]




[(][τ][ n]
2 [+∆)]  
g(s, Xs + β1 [n][)][ ds][|F][τ][ n] 2
τ2 [n]



2         
+ 1D1 [n][∩][[][τ][ n] 2 [<T][ −][∆]][E][[]



1{β1 [n][=][a][}][{][max] β∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][a][ +][ β][)][}}|F][τ][ n] 2 []][.]
a∈U



But,




1{β1 [n][=][a][}][(][−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][a][ +][ β][))][}]
a∈U




- 
1{β1 [n][=][a][}][{][max] β∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][a][ +][ β][)][}}] = maxβ∈U [{]
a∈U a∈U



Then



= maxβ∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β][)][}][.]


              - [�] [(][τ][ n] 2 [+∆)]              1D1 [n] [O] τ [n] 2 [n][−][1] (β1 [n][)] = 1D1 [n] [E] g(s, Xs + β1 [n][)][ ds][|F][τ][ n] 2

τ2 [n]



+ 1D1 [n][∩][[][τ][ n] 2 [<T][ −][∆]][E][[max] β∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β][)][}|F][τ][ n] 2 []][.] (2.19)


Now, for every l ∈I, let

    Γ [2] l [,n] = {−ψ(al) + Yτ [n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ a][l][)][ ≥−][ψ][(][a][j][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ a][j][)][}][,]

1≤j≤p


9


˜Γ [2] 1 [,n] = Γ [2] 1 [,n][, for][ l][ ≥] [2, ˜Γ] l [2][,n] = Γ [2] l [,n] \ [Γ [2] 1 [,n] ∪ . . . Γ [2] l− [,n] 1 [], and]

                   - [�]                    β2 [n] [=][ 1][D] 1 [n] al1˜Γ2l,n + a11D1n [.]

1≤l≤p


The last random variable is Fτ2 [n][+∆][−][measurable since the sets ˜Γ] l [2][,n] belong to Fτ2 [n][+∆] [and its]
value on D1 [n] [is irrelevant since, taking into account the required delay, the interventions after]
T are not allowed. For that reason, we take a1, but we can consider any other element of U .
Now, if Γ [˜][2] l [,n] is not empty, we have

maxβ∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β][)][}][ =][ −][ψ][(][a][l][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ a][l][) on ˜Γ] l [2][,n],


which means that


maxβ∈U [{−][ψ][(][β][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β][)][}][ =][ −][ψ][(][β] 2 [n][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β] 2 [n][)][.] (2.20)


Therefore by (2.18), (2.19) and (2.20),



1D1 [n] [Y] τ [ n] 1 [n][−][+∆][1] [(][β] 1 [n][)] = E[1D1 [n]




- τ2 [n][+∆]



g(s, Xs + β1 [n][)][ ds]
τ1 [n][+∆]



+1D1 [n][∩][[][τ][ n] 21 [<T][ −][∆]][{−][ψ][(][β] 2 [n][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β] 2 [n][)][}] ��Fτ n1 [+∆][]]



which implies,


Yτ [n] 1 [n][−][+∆][1] [(][β] 1 [n][)] = E[1 ¯D1 [n]



g(s, Xs + β1 [n][)][ds][ +][ 1][D] 1 [n]
τ1 [n][+∆]




- T




- τ2 [n][+∆]



g(s, Xs + β1 [n][)][ ds]
τ1 [n][+∆]



+ 1D1 [n][∩][[][τ][ n] 2 [<T] 1 [ −][∆]][{−][ψ][(][β] 2 [n][) +][ Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β] 1 2 [n][)][}] ��Fτ n1 [+∆][]][.]



By plugging the last equality in (2.16) and taking into account the fact that D1 [n] [⊂{][τ] 1 [ n] [< T][ −][∆][}][,]
1D1 [n] [+][ 1] D1 [n] [= 1 and][ Y] T [ n][−][2] (β1 [n] [+][ β] 2 [n][) = 0, we obtain]



τ1 [n][+∆] - T

g(s, Xs) ds +
0 τ [n]




        -        - τ1 [n][+∆]
Y0 [n][(0)] = E 1 ¯D1 [n] [{]



g(s, Xs + β1 [n][)][ ds][ −] [1][[][τ][ n] 1 [<T][ −][∆]][ψ][(][β] 1 [n][)][}]
τ1 [n][+∆]



τ1 [n][+∆] - (τ2 [n][+∆)]

g(s, Xs) ds +
0 τ [n][+∆]




    - τ1 [n][+∆]
+ 1D1 [n] [{]



g(s, Xs + β1 [n][)][ ds][ −] [ψ][(][β] 1 [n][)][}]
τ1 [n][+∆]



1                      
+ 1D1 [n][∩][[][τ][ n] 2 [<T][ −][∆]][(][−][ψ][(][β] 2 [n][)) +][ 1][D] 1 [n][Y] τ [ n] 2 [n][−][+∆][2] [(][β] 1 [n] [+][ β] 2 [n][)] .



Now, for any k ∈{2, · · ·, n}, let Dk [n] −1 [=][ {][τ] k [ n] −1 [≤] [T][ −] [2∆][}][,]


γk [n] [= inf][{][s][ ≥] [(][τ] k [ n] −1 [+ ∆)][, Y] s [ n][−][k][+1] (β1 [n] [+][ · · ·][ +][ β] k [n] −1 [) =][ O] s [n][−][k][+1] (β1 [n] [+][ · · ·][ +][ β] k [n] −1 [)][}]


and
τk [n] [=][ γ] k [n][1][D] k [n] −1 [+][ T][ 1][D] k [n] −1 [.]

We can easily see that the sequence of sets (Dk [n][)] k=1,...,n−1 [is decreasing, the set][ D] k [n] [is]
Fτk [n][−][measurable and the family]


{D1 [n][, D] n [n] −1 [, D] i [n] [∩] [D] i [n] +1 [; 1][ ≤] [i][ ≤] [n][ −] [2][}]


forms a complete system. So let k ∈{3, · · ·, n}, l ∈I, and let us set:


Γ [k,n] l = {−ψ(al)+Yτ [n] k [n][−][+∆][k] [(][β] 1 [n][+][. . .][+][β] k [n] −1 [+][a][l][)][ ≥−][ψ][(][a][j][)+][Y] τ [ n] k [n][−][+∆][k] [(][β] 1 [n][+][. . .][+][β] k [n] −1 [+][a][j][)][,][ ∀][j][ = 1][, . . ., p][}]


10


and Γ [˜][k,n] 1 = Γ [k,n] 1 and for l ≥ 2, Γ [˜] [k,n] l = Γ [k,n] l \ [�][l] m [−] =1 [1] [Γ] m [k,n][. Those sets ˜Γ] l [k][,][ l][ ∈I][, belong to][ F][τ][ n] k [+∆][,]
are pairwise disjoint and on Γ [˜][k] l [, if non empty,][ a][l][ realizes the maximum over][ U][ of]

(−ψ(a) + Yτ [n] k [n][−][+∆][k] [(][β] 1 [n] [+][ . . .][ +][ β] k [n] −1 [+][ a][))][.]


So let

      βk [n] [=][ {] al1˜Γk,nl }1Dk [n] −1 [+][ a][1][1][D] k [n] −1 [.]

1≤l≤p

On Dk [n] −1 [, we have:]

maxa∈U [(][−][ψ][(][a][) +][ Y] τ [ n] k [n][−][+∆][k] [(][β] 1 [n] [+][ . . .][ +][ β] k [n] −1 [+][ a][)) =][ −][ψ][(][β] k [n][) +][ Y] τ [ n] k [n][−][+∆][k] [(][β] 1 [n] [+][ . . .][ +][ β] k [n] −1 [+][ β] k [n][)][.]


By iterating the same reasoning as above, we obtain:




       -       - τ1n [+∆]
Y0 [n][(0)] = E 1D1n [{]



τ1n [+∆]  - T

g(s, Xs) ds +
0 τ [n]



g(s, Xs + β1 [n][)][ ds][ +][ 1] {τ1 [n][<T][ −][∆][}][(][−][ψ][(][β] 1 [n][))][}]
τ1 [n][+∆]



�i+1

ψ(βj [n][)][1] [τj [n][<T][ −][∆]][}]
j=1



k�−1



τin+1 [+∆] - T

g(s, Xs [δ][n] [)][ ds][ +]
0 τ [n]



g(s, Xs + β1 [n] [+][ . . .][ +][ β] i [n] +1 [)][ ds][ −]
τi [n] +1 [+∆]



�i+1



+



k�−1 - τin+1 [+∆]

1Din [∩][D] i [n] +1 [{]
i=1 0




    - τkn+1 [+∆]
+ 1Dk [n] [{] g(s, Xs [δ][n] [)][ ds][ −]

0



k�+1 
ψ(βi)1[τi [n][<T][ −][∆]][ +][ Y][ n] τk [n][−] +1 [k][+∆][−][1] [(][β] 1 [n] [+][ . . .][ +][ β] k [n] +1 [)][}]
i=1



where for any s < τk [n] +1 [+ ∆,]


Xs [δ][n] = Xs1{0≤s<τ1 [n][+∆][}][ +]



�k

(Xs + β1 [n] [+][ β] 2 [n] [+][ · · ·][ +][ β] i [n][)1] {τ [ n] i [+∆][≤][s<τ][ n] i+1 [+∆][}][.]
i=1



In particular, for k = n − 1, we have




       -       - τ1n [+∆]
Y0 [n][(0)] = E 1D1n [{]



τ1n [+∆]  - T

g(s, Xs) ds +
0 τ [n]



g(s, Xs + β1 [n][)][ ds][ +][ 1] {τ1 [n][<T][ −][∆][}][(][−][ψ][(][β] 1 [n][))][}]
τ1 [n][+∆]



�i+1

ψ(βj [n][)][1] [τj [n][<T][ −][∆]][}]
j=1



n�−2



τin+1 [+∆]  - T

g(s, Xs [δ][n] [)][ ds][ +]
0 τ [n]



g(s, Xs + β1 [n] [+][ . . .][ +][ β] i [n] +1 [)][ ds][ −]
τi [n] +1 [+∆]



�i+1



+



n�−2 - τin+1 [+∆]

1Din [∩][D] i [n] +1 [{]
i=1 0




     - τnn [+∆]
+ 1Dn [n] −1 [{]



g(s, Xs [δ][n] [)][ ds][ −]
0



�n 
ψ(βi)1[τi [n][<T][ −][∆]][ +][ Y][ 0] τn [n][+∆][(][β] 1 [n] [+][ . . .][ +][ β] n [n][)][}] .
i=1



However,




           - T
Yτ [0] n [n][+∆][(][β] 1 [n] [+][ . . .][ +][ β] n [n][) =][ E][[] g(s, Xs + β1 [n] [+][ . . .][ +][ β] n [n][)][ds][|F][τ][ n] n [+∆][]]

τn [n][+∆]



which implies that




       -       - τ1n [+∆]
Y0 [n][(0)] = E 1D1n [{]



τ1n [+∆]  - T

g(s, Xs) ds +
0 τ [n]



g(s, Xs + β1 [n][)][ ds][ +][ 1] {τ1 [n][<T][ −][∆][}][(][−][ψ][(][β] 1 [n][))][}]
τ1 [n][+∆]



�i+1

ψ(βj [n][)][1] [τj [n][<T][ −][∆]][}]
j=1



n�−2



τin+1 [+∆]  - T

g(s, Xs [δ][n] [)][ ds][ +]
0 τ [n]



g(s, Xs + β1 [n] [+][ . . .][ +][ β] i [n] +1 [)][ ds][ −]
τi [n] +1 [+∆]



�i+1



+



n�−2 - τin+1 [+∆]

1Din [∩][D] i [n] +1 [{]
i=1 0




     - T
+ 1Dn [n] −1 [{]



g(s, Xs [δ][n] [)][ ds][ −]
0



�n 
ψ(βi [n][)][1] [τi [n][<T][ −][∆]][}] (2.21)
i=1



where δ [n] = (τk [n][, β] k [n][)][k][=1][,n] [and][ X] [δ][n][ is defined as in (2.1). Let us now show that][ δ][n][ is admissible. First]
note that τ1 [n] [≤] [T][ −] [∆. Next for][ k][ ∈{][1][, ..., n][ −] [1][}][, on][ D] k [n][,][ γ] k [n] +1 [≤] [T][ −] [∆, then it follows from its]
definition that τk [n] +1 [=][ γ] k [n] +1 [≤] [T][ . Now if][ ω][ ∈] [D][¯] k [n][,][ τ][ n] k+1 [(][ω][) =][ T][ . Thus][ τ][ n] k+1 [(][ω][)][ ≤] [T][,][ P][-a.s. Next on]
Dk [n][,][ τ][ n] k [+ ∆] [≤] [τ][ n] k+1 [=][ γ] k [n] +1 [≤] [T][ −] [∆which implies that on][ D] k [n][,][ τ][ n] k+1 [≥] [min][{][τ][ n] k [+ ∆][, T][ }][ while on ¯][D] k [n][,]


11


τk [n] +1 [=][ T][ . It follows that][ τ][ n] k+1 [≥] [min][{][τ][ n] k [+ ∆][, T][ }][,][ P][-a.s. for any][ k][ ∈{][1][, ..., n][ −] [1][}][. Finally for any]
k ∈{1, . . ., n}, βk [n] [∈F][τ] k [n][+∆][, then][ δ][n][ is an admissible strategy. Next]



ψ(βk [n][)][1] {τk [n][<T][ −][∆][}][}][]]
k=1,n



T 
g(s, Xs [δ][n] [)][ ds][ −]
0



J(δ [n] ) = E[{1D1n [+]



n�−2 - T

i=1 1Din [∩][D] i [n] +1 [+][ 1][D] n [n] −1 [}{] 0



n�−2



= the right-hand side of (2.21),


thus J(δ [n] ) = Y0 [n][(0)][.]

Step 2: J(δ [n] ) ≥ J(δ [¯] n) for any δ [¯] n ∈An.

Let δ [¯] n = (¯τk [n][,][ ¯][β] k [n][)][k][≥][1] [be an admissible strategy of][ A][n][. Therefore ¯][τ][ n] k [=][ T][, for any][ k][ ≥] [n][ + 1, ¯][τ][ n] k [≤] [τ][¯][ n] k+1
and for any k ≥ 1, T ≥ τ¯k [n] +1 [≥] [min][{][T,][ ¯][τ][ n] k [+ ∆][}][. On the other hand ¯][β] k [n] [is][ F] (¯τk [n][+∆)][∧][T][ -measurable.]

The characterization (2.7) of Y [n] (0) implies that:




      - τ¯1n
Y0 [n][(0)] ≥ E[



g(s, Xs) ds + Oτ [n] ¯1 [n] [(0)]]
0




   - [�] [τ][¯] 1 [n]
≥ E



g(s, Xs) ds|Fτ¯1 [n] []]
τ¯1 [n]




[τ][¯] 1 [n]      - τ¯1n [+∆]

g(s, Xs) ds + 1[¯τ1 [n][<T][ −][∆]][E][[]
0 τ¯ [n]




       -        -        - T
+1[¯τ1 [n][<T][ −][∆]][E][[] - ψ(β [¯] 1 [n][) +][ Y][ n] τ¯1 [n][−][+∆][1] [(¯][β] 1 [n][)] |Fτ¯1 ] + 1[T −∆≤τ¯1 [n][≤][T][ ]][E][[]




   - [�] [(¯][τ] 1 [n][+∆)][∧][T]
≥ E



T 
g(s, Xs) ds|Fτ¯1 [n][]]
τ¯1 [n]




[(¯][τ] 1 [+∆)][∧][T]    - ��

g(s, Xs) ds + 1[¯τ1 [n][<T][ −][∆]]       - ψ(β [¯] 1 [n][) +][ Y][ n] τ¯1 [n][−][+∆][1] [(¯][β] 1 [n][)] . (2.22)
0



Next by (2.11) and the equation satisfied by (Yt [n][−][1] (β [¯] 1 [n][))] t≥τ¯1 [n][+∆][, we have:]




                      - [�] [τ][¯] 2 [n]
1[¯τ1 [n][<T][ −][∆]][ Y][ n] τ¯1 [n][−][+∆][1] [(¯][β] 1 [n][)] ≥ 1[¯τ1 [n][<T][ −][∆]][ E]




[τ][¯] 2     
τ¯1 [n][+∆] g(s, Xs + β [¯] 1 [n][)][ ds][ +][ O] τ [n] ¯2 [n][−][1] (β [¯] 1 [n][)][|F] τ¯1 [n][+∆]




   = E 1[¯τ1 [n][<T][ −][∆]]


   ≥ E 1[¯τ1 [n][<T][ −][∆]]




- τ¯2n



τ¯1 [n][+∆] g(s, Xs + β [¯] 1 [n][)][ ds][ +][ 1] [¯τ2 [n][<T][ −][∆]][O] τ [n] ¯2 [n][−][1] (β [¯] 1 [n][)]



τ¯2  
τ¯1 [n][+∆] g(s, Xs + β [¯] 1 [n][)][ ds][ +][ 1] [¯τ1 [n][<T][ −][∆]][O] τ [n] ¯2 [n][−][1] (β [¯] 1 [n][)][|F] τ¯1 [n][+∆]




- τ¯2n




               - [�] [T]
+1[¯τ1 [n][<T][ −][∆]][∩][[][T][ −][∆][≤][τ][¯] 2 [n][≤][T][ ]][E]




[T]  
g(s, Xs + β [¯] 1 [n][)][ds][|F] τ¯2 [n][]][|F][τ][¯] 1 [n][+∆] .
τ¯2 [n]



The first inequality stems from the fact that the process (Kt [n][−][1] (β [¯] 1 [n][))] t≥τ¯1+∆ [is non-decreasing. Once]
more β [¯] 1 [n] [is an][ F] T ∧(¯τ1 [n][+∆)][−] [measurable random variable and ¯][β] 1 [n] [=][ �] a∈U [a][1][[ ¯][β] 1 [n][=][a][]][, then we have:]




    1[¯τ2 [n][<T][ −][∆]][O] τ [n] ¯2 [n][−][1] (β [¯] 1 [n][)] =



1[¯τ2 [n][<T][ −][∆]][O] τ [n] ¯2 [n][−][1] (a)1[ ¯β1n [=][a][]]
a∈U




 = {E[1[¯τ2 [n][<T][ −][∆]]

a∈U




 =




- (¯τ2n [+∆)]



g(s, Xs + a)ds
τ¯2 [n]



+1[¯τ2 [n][<T][ −][∆]][E][[max] β∈U [(][−][ψ][(][β][) +][ Y][ n] τ¯2 [n][−][+∆][2] [(][a][ +][ β][))][|F][τ][¯] 2 [n] []][}][1][[ ¯][β] 1 [n][=][a][]]




 ≥ E[{1[¯τ2 [n][<T][ −][∆]]

a∈U




- (¯τ2n [+∆)]



g(s, Xs + a)ds
τ¯2 [n]



+1[¯τ2 [n][<T][ −][∆]][(][−][ψ][(¯][β] 2 [n][) +][ Y][ n] τ¯2 [n][−][+∆][2] [(][a][ + ¯][β] 2 [n][))][}|F] τ¯2 [n] []][1][[ ¯][β] 1 [n][=][a][]]



≥ E[{1[¯τ2 [n][<T][ −][∆]]




- (¯τ2n [+∆)]



g(s, Xs + β [¯] 1 [n][)][ds]
τ¯2 [n]



+1[¯τ2 [n][<T][ −][∆]][(][−][ψ][(¯][β] 2 [n][)) +][ Y][ n] τ¯2 [n][−][+∆][2] [(¯][β] 1 [n] [+ ¯][β] 2 [n][)][}|F] τ¯2 [n] []][.]


12


Then


                 1[¯τ1 [n][<T][ −][∆]][ Y][ n] τ¯1 [n][−][+∆][1] [(¯][β] 1 [n][)][ ≥] [E] 1[¯τ1 [n][<T][ −][∆]]



τ¯2n  - [�] [T]

g(s, Xs + β [¯] 1 [n][)][ ds][ +][ 1] [¯τ1 [n][<T][ −][∆][≤][τ][¯] 2 [n][≤][T][ ]][E]
τ¯1 [n][+∆] τ¯2 [n]




- τ¯2n



g(s, Xs + β [¯] 1 [n][)][ds][|F] τ¯2 [n] []]
τ¯2 [n]



+E[1[¯τ2 [n][<T][ −][∆]]




- (¯τ2n [+∆)]



(¯τ2 [+∆)]  
g(s, Xs + β [¯] 1 [n][)][ds][ +][ 1] [¯τ2 [n][<T][ −][∆]][{−][ψ][(¯][β] 2 [n][) +][ Y][ n] τ¯2 [n][−][+∆][2] [(¯][β] 1 [n] [+ ¯][β] 2 [n][)][}|F] τ¯2 [n] []][|F][τ][¯] 1 [n][+∆]
τ¯2 [n]




   ≥ E 1[¯τ1 [n][<T][ −][∆]]




- (¯τ2n [+∆)][∧][T]



(¯τ2 [+∆)][∧][T]  
g(s, Xs + β [¯] 1 [n][)][ ds][ +][ 1] [¯τ2 [n][<T][ −][∆]][{−][ψ][(¯][β] 2 [n][) +][ Y][ n] τ¯2 [n][−][+∆][2] [(¯][β] 1 [n] [+ ¯][β] 2 [n][)][}|F] τ¯1 [n][+∆] .
τ¯1 [n][+∆]



Therefore going back to (2.22) to obtain,




       - [�] τ¯1 [n][+∆]
Y0 [n][(0)] ≥ E



g(s, Xs + β [¯] 1 [n][)][ ds][ +][ 1] [¯τ1 [n][<T][ −][∆]][(][−][ψ][(¯][β] 1 [n][))]
(¯τ1 [n][+∆)][∧][T] 


τ¯1 [n][+∆]  - (¯τ2n [+∆)][∧][T]

g(s, Xs) ds +
0 (¯τ [n][+∆)][∧][T]



0 (¯τ1 [+∆)][∧][T]     
+ 1[¯τ2 [n][<T][ −][∆]][(][−][ψ][(¯][β] 2 [n][)) +][ 1] [¯τ2 [n][<T][ −][∆]][Y][ n] τ¯2 [n][−][+∆][2] [(¯][β] 1 [n] [+ ¯][β] 2 [n][)] .



Repeating now this procedure as many times as necessary to obtain: For any n ≥ 1,




      - τ¯1n [+∆]
Y0 [n][(0)] ≥ E[



τ¯1n [+∆]  
g(s, Xs) ds +
0




- (¯τkn+1 [+∆)][∧][T]

g(s, Xs + β [¯] 1 [n] [+][ . . .][ + ¯][β] k [n][)][ ds]
(¯τk [n][+∆)][∧][T]




 +



1≤k≤n−1



(−ψ(β [¯] k [n][))][1] [¯τk [n][<T][ −][∆]][ +][ 1][[¯][τ] n [n][<T][ −][∆]][Y][ 0] τ¯n [n] [(¯][β] 1 [n] [+][ . . .][ + ¯][β] n [n][)]][.]
1≤k≤n



Finally, taking into account (2.5) and (2.9), we get




      - τ¯1n [+∆]
Y0 [n][(0)] ≥ E[



τ¯1n [+∆]  
g(s, Xs) ds +
0




- (¯τkn+1 [+∆)][∧][T]

g(s, Xs + β [¯] 1 [n] [+][ . . .][ + ¯][β] k [n][)][ ds]
(¯τk [n][+∆)][∧][T]




 +



1≤k≤n



(−ψ(β [¯] k [n][))][1] [¯τk [n][<T][ −][∆]][]]
1≤k≤n



= J(δ [¯] n).

Henceforth, Y0 [n][(0) =][ J][(][δ][n][)][ ≥] [J][(¯][δ][n][) which implies that the strategy][ δ][n][ is optimal over the set][ A][n][.]


By using the last result and taking account of Proposition 2.1, we obtain:


Theorem 2.1. The strategy δ [X] is optimal over the set A, that is,


Y0 [X][(0) =][ J][(][δ][X][) = sup] J(δ) = sup J(δ).
δ∈AX δ∈A


2.3 The risk sensitive case



In this section, we suppose that the controller is sensitive with respect to risk, then he/she
aims to make safe decisions by balancing both the expected yield and the possible risk. We use
an exponential function to model his/her utility which means that the total expected reward is
given by:




     -     - T
J(δ) = E exp θ{




- 
ψ(ξn)1[τn<T −∆]} . (2.23)
n≥0



T 
g(s, Xs [δ][)][ds][ −]
0



The role of the parameter θ is to adjust the sensitivity of the controller with respect to risk.
It can be positive in risk seeking scenario and negative when the controller is averse to risk.
Without loss of generality and for the sake of simplicity, we suppose that θ = 1. On the other
hand on g(.) and ψ(.) we assume:


Assumption 2.2.
i) The mapping (t, ω, x) ∈ [0, T ] × Ω × R [ℓ] �→ g(t, ω, x) ∈ R is P ⊗B(R [ℓ] )/B(R)-measurable.
Moreover there exists an R-valued non-negative P-measurable process (γt)t≤T such that:

      -      - T
E e 0 [γ][s][ds][�] < ∞ and P − a.s. for any (t, x) ∈ [0, T ] × R [ℓ], |g(t, ω, x)| ≤ γt(ω).


ii) For any a ∈ R [ℓ], ψ(a) ≥ 0.


13


2.3.1 The iterative scheme



Our scheme will be defined via iterative Snell envelopes in the following way: ∀t ≤ T,
∀a ∈ R [ℓ],




      - T
Yt [0][(][a][) =][ E][[exp][{]



g(s, Xs + a)ds}|Ft]. (2.24)
t



For every n ≥ 1, (Yt [n][(][a][) exp][{][�] 0 [t] [g][(][s, X][s][ +][ a][))] t≤T [is the Snell envelope of the process]




   - t
(exp{ 0 [g][(][s, X][s][ +][ a][)][ds][}][O] t [n][(][a][))] t≤T [, that is,]




           - τ
Yt [n][(][a][) = esssup] τ ∈Tt [E][[exp][{] g(s, Xs + a)ds}Oτ [n][(][a][)][|F][t][]][,] (2.25)

t



where for any t ≤ T,




             - t+∆
Ot [n][(][a][)] = 1[t<T −∆]E[max
β∈U [{][exp[]



g(s, Xs + a)ds − ψ(β)1[t<T −∆]] Yt [n] +∆ [−][1][(][a][ +][ β][)][}|F][t][]]
t




        - T
+1[T −∆≤t≤T ]E[exp{



g(s, Xs + a)ds}|Ft]. (2.26)
t



The process O [n] (a) is the predictable projection of




           - t+∆
L¯ [n] t [(][a][) =][ 1][[][t<T][ −][∆]] [max] β∈U [{][exp[]



g(s, Xs + a)ds − ψ(β)1[t<T −∆]]Yt [n] +∆ [−][1][(][a][ +][ β][)][}]
t




        - T
+1[T −∆≤t≤T ]E[exp{



g(s, Xs + a)ds}, t ≤ T.
t



We then have:


Proposition 2.5. For any n ∈ N and any a ∈ R [ℓ],


(i) The processes Y [n] (a) is well defined, continuous and verifies:

∀t ≤ T, 0 ≤ Yt [n][(][a][)][ ≤] [E][[][e] �tT γsds|Ft], P − a.s. and YT n [(][a][) = 1][.]


(ii) ∀t ∈ [T − ∆, T ],
Yt [n][+1] (a) = Ot [n][+1] (a) = E[e�tT g(s,Xs+a)ds|Ft].


Proof. We first focus on point (i). We proceed by induction. Let us show the property for
n = 0. Let a be an arbitrary element of R [ℓ] . Since g(.) verifies Assumption 2.2-i), then the
process Y [0] (a) is obviously well defined and continuous since, for any t ≤ T,




      - T
Yt [0][(][a][) =][ E][[exp][{] g(s, Xs + a)ds}|Ft]

     - 0 ��     Θt




   - t
exp{− g(s, Xs + a)ds}.

0



But (Θt)t≤T is a continuous martingale as the filtration (Ft)t≤T is Brownian, thus Y [0] (a) is
continuous. In addition, by (2.24) and Assumption 2.2, i) we have: ∀t ≤ T,


                         - T
0 ≤ Yt [0][(][a][)][ ≤] [E][[][e] t [γ][s][ds] |Ft], P − a.s.


and finally YT [0][(][a][) = 1. Thus the property is valid for][ n][ = 0. Assume now that the property is]
valid for some n ≥ 0, i.e., for any a ∈ R [ℓ], the processes Y [n] (a) is well defined, continuous and
verifies: - T
∀t ≤ T, 0 ≤ Yt [n][(][a][)][ ≤] [E][[][e] t [γ][s][ds] |Ft], P − a.s. and YT [n][(][a][) = 1][.]


14


The process (Ot [n][+1] (a))t≤T is c`adl`ag. It is continuous except possibly at T − ∆, since it may
have a non negative jump at that point and it satisfies: ∀t ≤ T,




                 - t+∆
0 ≤ Ot [n][+1] (a) = 1[t<T −∆]E[max
β∈U [{][exp(]



g(s, Xs + a)ds − ψ(β)1[t<T −∆])Yt [n] +∆ [(][a][ +][ β][)][}|F][t][]]
t




        - T
+1[T −∆≤t≤T ]E[exp{



g(s, Xs + a)ds}|Ft]
t



≤ 1[t<T −∆]E[e�tt+∆ γsdsE[e�tT+∆ [γ][s][ds] |Ft+∆]|Ft] + 1[T −∆≤t≤T ]E[e�tT γsds|Ft]



= E[e�tT γsds|Ft].




           - t
Next the process (exp� T{ 0 [g][(][s, X][s][ +][ a][)][ds][}][O] t [n][+1] (a))t≤T is c`adl`ag, positive and upper bounded by

the martingale (E[e 0 [γ][s][ds] |Ft])t≤T then it is of class [D]. Therefore, the process Y [n][+1] (a) defined
by




             - τ
Yt [n][+1] (a) = esssupτ ∈T0E[exp{ g(s, Xs + a)ds}Oτ [n][+1] (a)|Ft], t ≤ T,

t



is well posed, c`adl`ag, belongs to class [D] and satisfies YT [n][+1] (a) = OT [n][+1] (a) = 1. It is also
continuous since the process (Ot [n][+1] (a))t≤T is c`adl`ag and has possibly a positive jump at T − ∆
and exp(.) ≥ 0 (see Proposition 4.11). Finally for any t ≤ T, we have:




             - τ
Yt [n][+1] (a) = esssupτ ∈T0E[exp{



g(s, Xs + a)ds}Oτ [n][+1] (a)|Ft]
t



≤ E[e�tτ [γ][s][ds] E[e�τT [γ][s][ds] |Fτ ]|Ft] = E[e�tT γsds|Ft].



Thus the property is valid for n + 1 and then it is valid for any n ≥ 0.
We now focus on point (ii). For any t ∈ [T − ∆, T ] and any a ∈ R [ℓ],

                    - T
Ot [n][(][a][) =][ E][[exp][{] t [g][(][s, X][s][ +][ a][)][ds][}|F][t][]]


and

Yt [n][(][a][) = esssup] τ ∈Tt [E][[exp][{] �tτ [g][(][s, X][s][ +][ a][)][ds][}][O] τ [n][(][a][)][|F][t][] =][ E][[exp][{] �tT [g][(][s, X][s][ +][ a][)][ds][}|F][t][] =][ O] t [n][(][a][)][.]


Next for every n ∈ N, and ξ a random variable which takes its values in V (once more which
is finite), let us set:




  Yt [n][(][ξ][) =]




- 
Yt [n][(][a][)][1][[][ξ][=][a][]] [and][ O] t [n][(][ξ][) =]
a∈V a∈V



Ot [n][(][a][)][1][[][ξ][=][a][]][, t][ ≤] [T.] (2.27)
a∈V



As a by-product of Proposition 2.5, we have:


Corollary 2.1. Let τ be a stopping time and ξ an Fτ -random variable valued in V, then for
any n ≥ 0 and t ∈ [τ, T ], on has:


                             - T
0 ≤ Yt [0][(][ξ][)][ ≤] [Y] t [ n][(][ξ][)][ ≤] [E][[][e] τ [γ][s][ds] |Ft], P − a.s.


Moreover if τ ≥ T − ∆, then Yt [n][(][ξ][) =][ O] t [n][(][ξ][)][ for any][ t][ ∈] [[][τ, T][ ]][ and][ n][ ≥] [1][.]


15


2.3.2 The optimal strategy

Let n ≥ 1 be fixed. We will construct a strategy δ [˜][n] = (˜τk [n][,][ ˜][β] k [n][)] k=1,...,n [that will be proved]
in the sequel, to be optimal in An. So first let


τ˜1 [n] [= inf][{][s][ ≥] [0][, Y] s [ n][(0) =][ O] s [n][(0)][}][.]


For l ∈I let,




       -        - τ˜ n1 [+∆]
Λ [1] l [,n] = [�] 1≤j≤p, j=l exp( τ˜1 [n] g(s, Xs)ds − ψ(al)1[˜τ1 [n][<T][ −][∆]][)][Y] τ [ n] ˜1 [n][−][+∆][1] [(][a][l][)]




   - τ˜1 [n][+∆]
≥ exp(



τ˜1 [n][+∆]  
g(s, Xs)ds − ψ(aj)1[˜τ1 [n][<T][ −][∆]][)][Y] τ [ n] ˜1 [n][−][+∆][1] [(][a][j] [)] ;
τ˜1 [n]



˜Λ [1] 1 [,n] = Λ [1] 1 [,n] and for l ∈{2, . . ., p}, Λ [˜] [1] l [,n] = Λ [1] l [,n] \ [Λ [1] 1 [,n] ∪ . . . Λ [1] l− [,n] 1 []][,] (2.28)


and finally

       β˜1 [n] [=] al1˜Λ1l,n .

1≤l≤p

The random variable β [˜] 1 [n] [is][ F][τ][˜][ n] 1 [+∆][−][measurable and verifies:]




    - τ˜1n [+∆]
max
a∈U [{][exp(] [n]



g(s, Xs)ds − ψ(β [˜] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]][)][Y][ n] τ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)][.]
τ˜1 [n]



τ˜1n [+∆]  - τ˜1n [+∆]

g(s, Xs)ds − ψ(a)1[˜τ1 [n][<T][ −][∆]][)][Y][ n] τ˜1 [n][−][+∆][1] [(][a][)][}][ = exp(]
τ˜1 [n] τ˜1 [n]



Next for k ∈{2, ..., n}, let G [n] k−1 [=][ {][τ] k [ n] −1 [≤] [T][ −] [2∆][} ∈F][τ][ n] k−1 [and,]


γ [n] k [= inf][{][s][ ≥] [τ][˜] k [ n] −1 [+ ∆][, Y] s [ n][−][k] (β [˜] 1 [n] [+][ . . .][ + ˜][β] k [n] −1 [) =][ O] s [n][−][k] (β [˜] 1 [n] [+][ . . .][ + ˜][β] k [n] −1 [)][}][,]


τ˜k [n] [= ¯][γ] k [n][1][G][n] k−1 [+][ T][ 1][G][n] k−1

and finally

                   - [�]                    β˜k [n] [=][ 1][G][n] k−1 al1˜Λk,nl + a11Gnk−1 [,]

1≤l≤p


with



Λ [k,n] l = [�] 1≤j≤p - exp(� τ˜k [n][+∆]



g(s, Xs + β [˜] 1 [n] [+][ . . .][ + ˜][β] k [n] −1 [)][ds][ −] [ψ][(][a][l][)][1] [˜τ [ n] k [<T][ −][∆]][)][Y] τ [ n] ˜k [n][−][+∆][1] [(][a][l][)]
τ˜k [n]




   - τ˜ nk [+∆]
≥ exp(



τ˜ nk [+∆]  
g(s, Xs + β [˜] 1 [n] [+][ . . .][ + ˜][β] k [n] −1 [)][ds][ −] [ψ][(][a][j] [)][1][[˜][τ][ n] k [<T][ −][∆]][)][Y] τ [ n] ˜1 [n][−][+∆][1] [(][a][j][)] ;
τ˜k [n]



˜Λ [k,n] 1 = Λ [k,n] 1 and for l ∈{2, ..., p}, Λ [˜] [k,n] l = Λ [k,n] l \ [Λ [k,n] 1 ∪ . . . Λ [k,n] l−1 []][.]

Theorem 2.2. The strategy δ [˜][n] = (˜τk [n][,][ ˜][β] k [n][)] k=1,...,n [is optimal in][ A][n][ for the risk-sensitive impulse]
control problem.


Proof. To begin with, let us show that δ [˜][n] = (˜τk [n][,][ ˜][β] k [n][)] k=1,...,n [is admissible. First note that]
τ˜1 ≤ T − ∆. Next for any k = 1, ..., n − 1, on G [n] k [, ¯][γ] k [n] +1 [≤] [T][ −] [∆, then it follows from its]
definition that ˜τk [n] +1 [= ¯][γ] k [n] +1 [≤] [T][ . Now if][ ω][ ∈] [G][¯] k [n][, ˜][τ] k [ n] +1 [(][ω][) =][ T][ . Thus ˜][τ] k [ n] +1 [(][ω][)][ ≤] [T][,][ P][-a.s. Next]
on G [n] k [, ˜][τ] k [ n] [+ ∆] [≤] [τ][˜] k [ n] +1 [= ¯][γ] k [n] +1 [≤] [T][ −] [∆which implies that on][ G] k [n][, ˜][τ] k [n] +1 [≥] [min][{][τ][˜] k [ n] [+ ∆][, T][ }]
while on G [¯][n] k [, ˜][τ] k [ n] +1 [=][ T][ . It follows that][ T][ ≥] [τ][˜] k [n] +1 [≥] [min][{][τ][˜] k [ n] +1 [+ ∆][, T][ }][,][ P][-a.s. Finally for]
any k ∈{1, . . ., n}, the sets (Λ [˜] [k,n] l )l=1,...,p ∈Fτ˜k [n][+∆][, then ˜][β] k [n] [is][ F][τ][˜][ n] k [+∆][, thus the strategy][ δ][n][ is]
admissible.


16


Now, since (Yt [1][(0)exp][{][�] 0 [t] [g][(][s, X][s][))] t≤T [is the Snell envelope of the process]

   - t
(exp{ 0 [g][(][s, X][s][)][ds][}][O] t [1][(0))] t≤T [and the latter is continuous with a possible non-negative jump]
at T − ∆, then ˜τ1 [n] [is an optimal stopping time after 0 and we have:]




        - τ˜ n1
Y0 [n][(0)] = E[exp{



g(s, Xs)ds}Oτ [n] ˜1 [n] [(0)]]
0



τ˜ n1  - τ˜ n1 [+∆]

g(s, Xs)ds}{1[˜τ1 [n][<T][ −][∆]][E][[exp(]
0 τ˜ [n]




     - τ˜ n1
= E[exp{




              - T
Yτ˜ [n] 1 [n][−][+∆][1] [(˜][β] 1 [n][)][|F][τ][˜][ n] 1 [] +][ 1][[˜][τ][ n] 1 [=][T][ −][∆]][E][[exp[]

[n]



g(s, Xs)ds − ψ(β [˜] 1 [n][)][1][[˜][τ][ n] 1 [<T][ −][∆]][)]
τ˜1 [n]




         - τ˜1 [n][+∆]
= E[1[˜τ1 [n][<T][ −][∆]][exp][{]



g(s, Xs)ds]|Fτ˜1 [n] []][}][]]
τ˜1 [n]




        - T
+ 1[˜τ1 [n][=][T][ −][∆]][exp][{]



g(s, Xs)ds − ψ(β [˜] 1 [n][)][1] [˜τ [ n] 1 [<T][ −][∆]][}][Y] τ [ n] ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)]
0




     - τ˜ n1 [+∆]
= E[exp{



g(s, Xs)ds}]
0



g(s, Xs)ds − ψ(β [˜] 1 [n][)][1][[˜][τ][ n] 1 [<T][ −][∆]][}][Y] τ [ n] ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)]][.] (2.29)
0



Now, notice that according to Corollary 2.1, for t ∈ [T − ∆, T ],

                   - T
1Gn1 [Y] τ [ n] ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)] = 1Gn1 [E][[exp][{] τ˜1 [n][+∆] g(s, Xs + β [˜] 1 [n][)][ds][}|F][τ][˜][ n] 1 [+∆][]][.]


On the other hand,




                  - τ
1G [n] 1 [Y] τ [ n] ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)] = 1G [n] 1 [esssup][τ] [∈T] τ˜1 [n][+∆] [E][[exp][{]




      - γ¯2n
= 1G [n] 1 [E][[exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][}][O] τ [n][−][1] (β [˜] 1 [n][)][|F][τ][˜][ n] 1 [+∆][]]
τ˜1 [n][+∆]



τ˜1 [n][+∆] g(s, Xs + β [˜] 1 [n][)][ds][}][O] γ [n] ¯2 [n][−][1] (β [˜] 1 [n][)][|F][τ][˜][ n] 1 [+∆][]][.]



But on G [n] 1 [, ˜][τ] 2 [ n] [= ¯][γ] 2 [n] [≤] [T][ −] [∆, then]




             - τ˜2n
1G [n] 1 [Y][ n] τ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)] = 1G [n] 1 [E][[exp][{]



τ˜2n  
g(s, Xs + β [˜] 1 [n][)][ds][}] 1[˜τ2 [n][<T][ −][∆]][.]
τ˜1 [n][+∆]




     - τ˜2n [+∆]
E[max
β∈U [{][exp[] [n]



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(][β][)][1] [˜τ2 [n][<T][ −][∆]][]][ Y][ n] τ˜2 [n][−][+∆][2] [(˜][β] 1 [n] [+][ β][)][}|F][τ][˜] 2 [n] []]
τ˜2 [n]




        - T
+1[˜τ2 [n][=][T][ −][∆]][E][[exp][{]



T 
g(s, Xs + β [˜] 1 [n][)][ds][}|F] τ˜2 [n] []] |Fτ˜1 [n][+∆][]][.]
τ˜2 [n]



Since G [n] 1 [is][ F][τ][˜][ n] 1 [+∆][-measurable, we obtain]




                 - τ˜ n2 [+∆]
1G [n] 1 [Y] τ [ n] ˜1 [n][−][+∆][1] [(˜][β] 1 [n][)] = E[1G [n] 1 [∩][[˜][τ][ n] 2 [<T][ −][∆]][exp][{]

[n]




         - T
+ 1G [n] 1 [∩][[˜][τ][ n] 2 [=][T][ −][∆]][exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(˜][β] 2 [n][)][1][[˜][τ][ n] 2 [<T][ −][∆]][}][ Y] τ [ n] ˜2 [n][−][+∆][2] [(˜][β] 1 [n] [+ ˜][β] 2 [n][)]
τ˜1 [n][+∆]



g(s, Xs + β [˜] 1 [n][)][ds][}|F][τ][˜][ n] 1 [+∆][]]
τ˜1 [n][+∆]



Therefore




                - τ˜2n [+∆]
Yτ˜ [n] 1 [n][−][+∆][1] [(˜][β] 1 [n][)] = E[1G [n] 1 [∩][[˜][τ] 2 [n][<T][ −][∆]][exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(˜][β] 2 [n][)][1] [˜τ2 [n][<T][ −][∆]][}][ Y][ n] τ˜2 [n][−][+∆][2] [(˜][β] 1 [n] [+ ˜][β] 2 [n][)]
τ˜1 [n][+∆]




         - T
+ 1G [n] 1 [∩][[˜][τ] 2 [n][=][T][ −][∆]][exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][}|F] τ˜1 [n][+∆][]][.]
τ˜1 [n][+∆]



T - T

τ˜1 [n][+∆] g(s, Xs + β [˜] 1 [n][)][ds][}][ +][ 1] G [ ¯][n] 1 [exp][{] τ˜1 [n]



17


Replace now Yτ˜ [n] 1 [n][−][+∆][1] [(˜][β] 1 [n][) with its last expression in (2.29) to obtain]




              - τ˜1n [+∆]
Y0 [n][(0)] = E[1G [n] 1 [∩][[˜][τ] 2 [n][<T][ −][∆]][exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(˜][β] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]]
τ˜1 [n][+∆]



τ˜1n [+∆]  - τ˜2n [+∆]

g(s, Xs)ds +
0 τ˜ [n][+∆]




                        - τ˜1n [+∆]

- ψ(β [˜] 2 [n][)][1] [˜τ2 [n][<T][ −][∆]][}][Y][ n] τ˜2 [n][−][+∆][2] [(˜][β] 1 [n] [+ ˜][β] 2 [n][) +][ 1] G [n] 1 [∩][[˜][τ] 2 [n][=][T][ −][∆]][exp][{]




  - T
+



g(s, Xs)ds
0



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(˜][β] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]][}]
τ˜1 [n][+∆]




     - τ˜1n [+∆]
+ 1Gn1 [exp][{]



τ˜1n [+∆]  - T

g(s, Xs)ds +
0 τ˜ [n]




      - τ˜2n [+∆]
= E[1G [n] 1 [exp][{]



g(s, Xs + β [˜] 1 [n][)][ds][ −] [ψ][(˜][β] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]][}][]]
τ˜1 [n][+∆]



δ˜ [n]
g(s, Xs [)][ds][ −] [ψ][(˜][β] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]][ −] [ψ][(˜][β] 2 [n][)][1] [˜τ2 [n][<T][ −][∆]][}][Y][ n] τ˜2 [n][−][+∆][2] [(˜][β] 1 [n] [+ ˜][β] 2 [n][)]
0




     - T
+ 1Gn1 [exp][{]



δ˜ [n]
g(s, Xs [)][ds][ −] [ψ][(˜][β] 1 [n][)][1] [˜τ1 [n][<T][ −][∆]][}][]]
0



By iterating the same reasoning as many times as necessary, we obtain




          - τ˜n [n][+∆]
Y0 [n][(0)] = E[1G [n] n−1 [exp][{] g(s, Xsδ˜ [n][)][ds][ −]

0



�n

ψ(β [˜] k [n][)][1] [˜τ [ n] k [<T][ −][∆]][}][Y] τ [ 0] ˜n [n][+∆][(˜][β] 1 [n] [+][ . . .][ + ˜][β] n [n][)]
k=1



�n



n�−2



δ˜ [n]
g(s, Xs [)][ds][ −]
0



k�+1

ψ(β [˜] i [n][)][1] [˜τ [ n] i [<T][ −][∆]][}]
i=1



+



n�−2 - T

k=1 1Gnk [∩][G][n] k+1 [exp][{] 0




     - T
+ 1Gn1 [exp][{]



δ˜ [n]
g(s, Xs [)][ds][ −] [ψ][(˜][β] 1 [n][)][1][[˜][τ][ n] 1 [<T][ −][∆]][}][]][.] (2.30)
0



Next, let us set β [˜][n] = β [˜] 1 [n][+][. . .][+ ˜][β] n [n][. By virtue of (2.27) and (2.24) and since ˜][β][n][ is][ F][τ][˜][ n] n [+∆][−][measurable,]
we have




   Yτ˜ [0] n [n][+∆][(˜][β][n][)] =



Yτ˜ [0] n [n][+∆][(˜][β] 1 [n] [+][ . . .][ ˜][β] n [n] −1 [+][ a][)][1] [ β [˜] n [n][=][a][]]
a∈U




- - T

E[exp{
a∈U τ˜n [n]




 =



g(Xs + β [˜] 1 [n] [+][ . . .][ ˜][β] n [n] −1 [+][ a][)][ds][}|F][τ][˜][ n] n [+∆][]][1] [ β [˜] n [n][=][a][]]
τ˜n [n][+∆]




- - T

a∈U E[1[ ˜βnn−1 [=][a][]][exp][{] τ˜n [n]




 =




     - T
= E[exp{



g(Xs + β [˜] 1 [n] [+][ . . .][ ˜][β] n [n] −1 [+][ a][)][ds][}|F][τ][˜][ n] n [+∆][]]
τ˜n [n][+∆]



g(Xs + β [˜][n] )ds}|Fτ˜ nn [+∆][]][.]
τ˜n [n][+∆]



The last inequality plugged in (2.30) yields




       - T
Y0 [n][(0) =][ E][[exp][{]



ψ(β [˜] k [n][)][1] [¯τ [ n] k [<T][ −][∆]][}][] =][ J][( ˜][δ][n][)][.]
k≤n



T 
δ˜ [n]
g(s, Xs [)][ds][ −]
0



Now, let δ [¯][′] = (¯τk [′] [,][ ¯][β] k [′] [)] k=1,...,n [be an arbitrary strategy of][ A][n][. We then have:]




            - τ
Y0 [n][(0)] = esssupτ ∈T0E[exp{




     - τ¯ ′1
≥ E[exp{



g(s, Xs)ds}Oτ [n][(0)]]
0



g(s, Xs)ds}Oτ [n] ¯1 [′] [(0)]][.]
0



18


But




              - τ¯1 [′] [+∆]
Oτ [n] ¯1 [′] [(0)] = 1[¯τ1 [′] [<T][ −][∆]][E][[max] β∈U [{][exp(] [′]




        - T
+ 1[T −∆≤τ¯1 [′][]][E][[exp][{]



τ¯1 [′] g(s, Xs)ds − ψ(β)1[¯τ1 [′] [<T][ −][∆]][)][Y] τ [ n] ¯1 [′][−][+∆][1] [(][β][)][}|F][τ][¯][ ′] 1 []]



g(s, Xs)ds}|Fτ¯1 [′] []]
τ¯1 [′]




         - τ¯1 [′] [+∆]
≥ 1[¯τ1 [′] [<T][ −][∆]][E][[][{][exp(]



τ¯1 [′] g(s, Xs)ds − ψ(β [¯] 1 [′] [)][1] [¯τ [ ′] 1 [<T][ −][∆]][)][Y] τ [ n] ¯1 [′][−][+∆][1] [(¯][β] 1 [′] [)][}|F][τ][¯][ ′] 1 []]




        - T
+ 1[T −∆≤τ¯1 [′][]][E][[exp][{]



g(s, Xs)ds}|Fτ¯1 [′] []][.]
τ¯1 [′]



It follows that




        - τ¯1′
Y0 [n][(0)] ≥ E[exp{



τ¯1′  - τ¯1′ [+∆]

g(s, Xs)ds}{1[¯τ1′ [<T][ −][∆]][E][[][{][exp(]
0 τ¯ [′]




         - T
+ 1[T −∆≤τ¯1′ []][E][[exp][{]



g(s, Xs)ds − ψ(β [¯] 1 [′] [)][1] [¯τ1 [′] [<T][ −][∆]][)][Y][ n] τ¯1 [′][−][+∆][1] [(¯][β] 1 [′] [)][}|F] τ¯1 [′] []]
τ¯1 [′]



g(s, Xs)ds}|Fτ¯1′ []][}][]]
τ¯1 [′]




         - τ¯1′ [+∆]
≥ E[1[¯τ1′ [<T][ −][∆]][exp][{]



g(s, Xs)ds}]
0




     - (¯τ1′ [+∆)][∧][T]
≥ E[exp{



τ¯1′ [+∆]  - T

g(s, Xs)ds − ψ(β [¯] 1 [′] [)][1] [¯τ1 [′] [<T][ −][∆]][)][}][Y][ n] τ¯1 [′][−][+∆][1] [(¯][β] 1 [′] [) +][ 1] [T −∆≤τ¯1 [′] []][exp][{]
0 0



g(s, Xs)ds − ψ(β [¯] 1 [′] [)][1] [¯τ1 [′] [<T][ −][∆]][)][}][Y][ n] (¯τ1 [−][′] [+∆)][1] [∧][T][ (¯][β] 1 [′] [)]]
0



since YT [n][−][1] (β [¯] 1 [′] [) = 1. By reasoning many times in the same way, we obtain:]



Y0 [n][(0)] ≥ E[1[¯τ ′n<T −∆]exp{� (¯τn [′] [+∆)][∧][T] g(s, Xsδ¯′ [)][ds][ −]

0


Finally, using (2.24) to get



�n

ψ(β [¯] k [′] [)][1] [¯τ [ ′] k [<T][ −][∆]][)][}][Y] (¯ [ 0] τn [′] +∆)∧T [(¯][β] 1 [′] [+][ . . .][ + ¯][β] n [′] [)][.]
k=1



�n




       - T
Y0 [n][(0)][ ≥] [E][[exp][{]



ψ(β [¯] k [′] [)][1][[¯][τ][ ′] k [<T][ −][∆]][}][] =][ J][(¯][δ][′][)]
k≤n



T g(s, Xsδ¯′ [)][ds][ −] 
0



and then the strategy δ [˜][n] is optimal in An.


As a by-product, in taking n = X = [ ∆ [T] [], we obtain:]


Corollary 2.2. The risk-sensitive impulse control problem with delay ∆(2.23) has an optimal
strategy.

### 3 The infinite horizon case


In this section B = (Bt)t≥0 is a d-dimensional Brownian motion on (Ω, F, P), (Ft [0] :=
σ{Bs, s ≤ t})t≥0 is the natural filtration of B, (Ft)t≥0 its completion with the P-null sets
of F and finally F∞ = [�] t≥0 [F][t][. On the other hand,]


  - P is the σ-algebra on [0, +∞[×Ωof (Ft)t≥0-progressively measurable processes;

 - H∞ [p,n] [=][ {][(][v][t][)][t][≥][0][ :][ P][-measurable,][ R][n][-valued process s.t.][ E][[][�] 0 [∞] |vs| [p] ds] < ∞} (n ≥ 1 and
p > 1);


  - S∞ [2] [=][ {][(][Y][t][)][0][≤][t][≤∞][,][ P][-measurable c`adl`ag (or rcll) process, s.t.][ E][[sup] 0≤t≤∞ [|][Y][t][|][2][]][ <][ ∞}][;]


  - Sc, [2] ∞ [(resp.][ S] i, [2] ∞ [) is the subset of][ S] ∞ [2] [of continuous processes (resp. continuous non-]
decreasing processes (kt)0≤t≤∞ such that k0 = 0);


19


  - L [2] ∞ [=][ {][η][ :][ F][∞][−] [measurable random variable, s.t.][ E][[][|][η][|] 2 ] < ∞};


  - Tt0 = {ν : (Ft)t≥0-stopping time, s.t. P-a.s, t0 ≤ ν} (t0 ≥ 0).


Next, for any stochastic process y := (yt)t∈[0,∞), we use the following notation: y∞ =
lim supt→∞ yt. In addition, the process y is said to be continuous at +∞ if limt→∞ yt exists.
Finally, if y is a c`adl`ag Ft-supermartingale, which is non-negative or bounded from below, then
according to ([23], pp.18), it is continuous at +∞.
In this section we are considering the impulse control problem in infinite horizon. An impulse
strategy δ is (τn, ξn)n≥1 where for any n ≥ 1, τn is an (Ft)t≥0-stopping time such that τn +∆ ≤
τn+1 and ξn a random variable with values in U a finite subset of R [ℓ] and Fτn+∆-measurable.
We denote by A∞ the set of admissible strategies.
When a strategy δ is implemented by the controller his/her payoff is given by:




     - +∞
J(δ) := E[Φ{



e [−][rτ][n] ψ(ξn)1[τn<+∞]}],
n≥1



+∞ 
e [−][rs] g(s, Xs [δ][)][ ds][ −]
0



with:
i) the controlled process X [δ], taking its values in R [ℓ], is given by:

        ∀t ≥ 0, Xt [δ] [=][ X][t][ +] ξn1[τn+∆≤t]

n≥1


where (Xt)t≥0 is a P-measurable process with values in R [ℓ] which stands for the dynamics of
the uncontrolled system.
ii) Φ(.) is the utility function and r is a discount factor.


We aim to find an optimal strategy δ [∗], i.e., satisfying:


sup J(δ) = J(δ [∗] ).
δ∈A∞


An example of the stochastic process above we have in mind is:

                    - t                    - t
∀t ≥ 0, Xt = x + 0 [b][(][s, ω][)][ds][ +] 0 [σ][(][s, ω][)][dB][s]


where (bt)t≥0 (resp. (σt)t≥0) is a process of H∞ [1][,ℓ] [(resp.][ H] ∞ [2][,ℓ][×][d] ). For general processes b and σ,
the Itˆo process X is no longer a Markov process and then the deterministic methods cannot be
applied to deal with this problem.


We are going to consider the risk-neutral case, i.e., Φ(x) = x. The risk-sensitive case can
be considered in the same way. On the other hand, we assume that the functions g(.) and ψ(.)
verify:


Assumption 3.3.
i) The mapping (t, ω, x) ∈ [0, ∞[×Ω × R [ℓ] �→ g(t, ω, x) ∈ R is P ⊗B(R [ℓ] )/B(R)-measurable.
Moreover there exists a non-negative process (γt)t≥0 such that:

                           - ∞
P − a.s. for any (t, x) ∈ [0, +∞[×R [ℓ], |g(t, ω, x)| ≤ γt(ω) and E[( e [−][rs] γsds) [2] ] < ∞.

0


ii) For any a ∈ R [ℓ], ψ(a) ≥ 0.


20


3.1 Construction of the iterative scheme


In the sequel, a will stand for an element of R [ℓ] . So let us consider the following scheme:
The pair of processes (Y [0] (a), Z [0] (a)) is a solution of the following standard BSDE in infinite
horizon which exists according to the result by Chen [5], pp.96:

 (Y [0] (a), Z [0] (a)) ∈Sc, [2] ∞ [× H] ∞ [2][,d][;]







(3.31)
Yt [0][(][a][) =] �t∞ e [−][rs] g(s, Xs + a) ds − �t∞ Zs [0][(][a][)][dB][s][, t][ ∈] [[0][,][ +][∞][]][.]



First let us point out that Y∞ [0] [(][a][) = 0. Next for any][ t][ ≥] [0 we have,]

Yt [0][(][a][) =][ E][[] �t∞ e [−][rs] g(s, Xs + a) ds|Ft] (3.32)


which implies that for any a ∈ R [ℓ], and for any t ≥ 0,

                  - ∞
|Yt [0][(][a][)][| ≤] [E][[] e [−][rs] γsds|Ft]. (3.33)

t

                 - ∞
Now for t ∈ [0, +∞[, let Mt := E[ 0 e [−][rs] γs ds | Ft]. Then by Assumption 3.3-i), (Mt)t≥0
is a uniformly integrable martingale. Next using the martingale convergence theorem, there

                                  - ∞
exists M∞ ∈ L [1] (dP) such that� ∞ Mt −→ M∞ a.s. and in L [2] (dP). Since 0 e [−][rs] γs ds is F∞measurable, we have M∞ = 0 e [−][rs] γs ds (see [31], Chapter 2, Theorem 3.1 and Corollary 2.4).
Consequently,



�� ∞
tlim→∞ [E] t e [−][rs] γs ds ���� Ft





    -     - t     = limt→∞ Mt − 0 [e][−][rs][γ][s][ ds] = 0 P-a.s. and in L [2] (dP). (3.34)



As g(.) verifies Assumption 3.3-i), then from (3.33) we deduce that limt→∞ |Yt [0][(][a][)][|][ = 0 uniformly]
w.r.t a ∈ R [ℓ] .
Next for any n ≥ 1, let (Y [n] (a), Z [n] (a), K [n] (a)) be a triple of processes which satisfies the
following reflected BSDE in infinite horizon:

 Y(Yt [n][n][(][a] (a [) =] ), Z� [n] t∞(ae), K [−][rs][n] g((as, X)) ∈Ss +c, [2] a∞) ds [× H] + K∞ [2][,d] ∞ [n][× S][(][a][)] i, [2][ −] ∞ [;][K] t [n][(][a][)][ −] �t∞ Zs [n][(][a][)][ dB][s][, t][ ∈] [[0][,][ +][∞][];]

 Y� t∞ [n][(][a][)][ ≥] [O] t [n][(][a][) :=][ E] ��tt+∆ eE [−] - [rs] maxβg∈(Us, X� −s +e [−] a [r][(] ) [t] ds [+∆)] |Fψt(�β+) + Yt [n] +∆ [−][1][(][a][ +][ β][)] �|Ft�, t ∈ [0, +∞[;

0 [(][Y] t [ n][(][a][)][ −] [O] t [n][(][a][))][ dK] t [n][(][a][) = 0][.]
(3.35)
For any n ≥ 1, the process (Ot [n][(][a][))][t][≥][0][ is defined as the (][F][t][)][t][≥][0][-predictable projection of]



(Y [n] (a), Z [n] (a), K [n] (a)) ∈Sc, [2] ∞ [× H] ∞ [2][,d] [× S] i, [2] ∞ [;]



Yt [n][(][a][) =] �t∞ e [−][rs] g(s, Xs + a) ds + K∞ [n] [(][a][)][ −] [K] t [n][(][a][)][ −] �t∞ Zs [n][(][a][)][ dB][s][, t][ ∈] [[0][,][ +][∞][];]



Yt [n][(][a][)][ ≥] [O] t [n][(][a][) :=][ E] ��tt+∆ e [−]    - [rs] g(s, X� s + a) ds|Ft�+    -    E max            - e [−][r][(][t][+∆)] ψ(β) + Yt [n] +∆ [−][1][(][a][ +][ β][)] |Ft, t ∈ [0, +∞[;

   - ∞ β∈U
0 [(][Y] t [ n][(][a][)][ −] [O] t [n][(][a][))][ dK] t [n][(][a][) = 0][.]
(3.35)
For any n ≥ 1, the process (Ot [n][(][a][))][t][≥][0][ is defined as the (][F][t][)][t][≥][0][-predictable projection of]




    - t+∆    -    L [n] t [(][a][) :=] t e [−][rs] g(s, Xs + a)ds + maxβ∈U - e [−][r][(][t][+∆)] ψ(β) + Yt [n] +∆ [−][1][(][a][ +][ β][)], t ≥ 0.



As mentioned in the very beginning, its value for t = +∞ is limt→+∞ Ot [n][(][a][) which we show]
that it exists and equal to 0.


Proposition 3.6. For any n ≥ 1, for any a ∈ R [ℓ], we have:
a) The process (Ot [n][(][a][))][t][≥][0] [is continuous,][ E][[sup] t≥0 [|][O] t [n][(][a][)][|][2][]][ <][ ∞] [and][ O] ∞ [n] [(][a][) := lim][t][→][+][∞] [O] t [n][(][a][) = 0][.]
b) The triple of processes (Y [n] (a), Z [n] (a), K [n] (a)) is well-posed.
c) For any t ∈ [0, +∞],

                     - [�] [τ]                     Yt [n][(][a][) =][ esssup] τ ∈Tt [E] e [−][rs] g(s, Xs + a) ds + Oτ [n][(][a][)][|F][t] . (3.36)

t


21


d) For any t ≥ 0,




        - ∞
Yt [0][(][a][)][ ≤] [Y] t [ n][(][a][)][ ≤] [E][[] e [−][rs] γsds|Ft]. (3.37)

t



e) For any a ∈ R [ℓ], Y [n] (a) ≤ Y [n][+1] (a).


Proof. We first deal with points a), b), c) and d). We use induction and first we focus on the
case when n = 1.
As mentioned previously, for any a ∈ R [ℓ], the process Y [0] (a) exists and belongs to Sc, [2] ∞ [.]
As U is finite then the process (L [1] t [(][a][))][t][≥][0] [is continuous and consequently (][O] t [1][(][a][))][t][≥][0] [is so (see]
Theorem 4.5 in Appendix). On the other hand for any t ≥ 0,




     - t+∆
|Ot [1][(][a][)][| ≤] [E][[]



sup |Yt [0][(][a][ +][ β][)][||F][t][]]
t≥0
β∈U



t+∆ 
e [−][rs] |g(s, Xs + a)|ds + Ce [−][r][(][t][+∆)] +
t




  - ∞
≤ E[



sup |Yt [0][(][a][ +][ β][)][||F][t][]]
t≥0
β∈U



∞ 
e [−][rs] γsds + Ce [−][r][(][t][+∆)] +
0



Using now Doob’s maximal inequality to deduce




           - ∞
E[sup |Ot [1][(][a][)][|][2][]][ ≤] [C][(1 +][ E][[][{]
t≥0 0



sup |Yt [0][(][a][ +][ β][)][|][2][]][ <][ ∞][.]
t≥0
β∈U



∞ 
e [−][rs] γsds} [2] +
0



Note that the constant C may change from line to line. Next, once again since U is finite and
using the inequality (3.33), we have:




[t][+∆]  - ∞

e [−][rs] γsds + Ce [−][r][(][t][+∆)] + E[
t t




      - [�] [t][+∆]
|Ot [1][(][a][)][| ≤] [E]



∞ 
e [−][rs] γsds|Ft+∆]|Ft
t+∆




  - [�] [∞]
= E




[∞]  
e [−][rs] γsds + Ce [−][r][(][t][+∆)] |Ft → 0 as t → +∞
t



and then limt→∞ Ot [1][(][a][) = 0 =][ O] ∞ [1] [(][a][) for any][ a][ ∈] [R][ℓ][. Therefore by Theorem 3.2 in [17], the]
triple (Y [1] (a), Z [1] (a), K [1] (a)) satisying (3.35) (with n = 1) exists and is unique. Additionally
Y [1] (a) verifies:

                   - [�] [τ]                    ∀t ∈ [0, ∞], Yt [1][(][a][) = esssup] τ ∈Tt [E] e [−][rs] g(s, Xs + a) ds + Oτ [1][(][a][)][|F][t] (Y∞ [1] [(][a][) = 0)][.] (3.38)

t


Finally taking τ = +∞ in (3.38) to obtain: For any t ≥ 0,

                  - [�] [+][∞]                  Yt [1][(][a][)][ ≥] [E] e [−][rs] g(s, Xs + a) ds|Ft = Yt [0][(][a][)] (3.39)

t


since, once more for any a ∈ R [ℓ], O∞ [1] [(][a][) = 0][.][ On the other hand, for any][ t][ ∈] [[0][,][ ∞][[,]

         - [�] [t][+∆]         -         -         Ot [1][(][a][) :=][ E] e [−][rs] g(s, Xs + a) ds + max   - e [−][r][(][t][+∆)] ψ(β) + Yt [0] +∆ [(][a][ +][ β][)] |Ft .

t β∈U


But ψ(.) ≥ 0, Y [0] (a) verifies (3.33) and finally using Assumption 3.3-i) to obtain: ∀t ≥ 0,




[t][+∆]  - ∞

e [−][rs] γsds + E[
t t




[∞]  
e [−][rs] γsds|Ft . (3.40)
t




      - [�] [t][+∆]
Ot [1][(][a][)][ ≤] [E]



∞ - - [�] [∞]

e [−][rs] γsds|Ft+∆]|Ft = E
t+∆ t



Going back to (3.38) and using the inequality (3.40) to obtain: ∀t ≥ 0,




           - [�] [τ]
Yt [1][(][a][)][ ≤] [esssup] τ ∈Tt [E]




[τ]   - [�] [∞]

e [−][rs] γsds + E
t τ




[∞]  -  -  - [�] [∞]

e [−][rs] γsds|Fτ Ft = E
τ t




[∞]  
e [−][rs] γsds|Ft . (3.41)
t



22


Thus for n = 1, a), b), c) and d) are satisfied. Next assume that for some n ≥ 1, a), b),
c) and d) are satisfied. As for O [1] (a), the process O [n][+1] (a) is continuous, uniformly dP-square
integrable and limt→∞ Ot [n][+1] (a) = 0 = O∞ [n][+1][(][a][).] Thus, once more by Theorem 3.2 in [17],
for any a ∈ R [ℓ], the triple (Y [n][+1] (a), Z [n][+1] (a), K [n][+1] (a)) verifying (3.32) exists and is unique.
Moreover the process Y [n][+1] (a) verifies (3.36) with O [n][+1] (a). Now since Y [n] (a) verifies (3.37) for
any a ∈ R [ℓ] and ψ(.) ≥ 0, then O [n][+1] (a) verifies the inequality (3.40) above and then for any
a ∈ R [ℓ], Y [n][+1] (a) verifies (3.41). The fact that Y [n][+1] (a) ≥ Y [0] (a) is easily deduced from (3.36)
in taking τ = +∞. Thus Y [n][+1] (a) verifies (3.37).
As a result, points a), b), c) and d) are verified for n + 1 and then they are valid for any
n ≥ 1.
Let us now focus on point e). For n = 1, this is just (3.39). To conclude it is enough to use
an induction argument and once more the representation (3.36).


Next we will focus on the limit of Y [n] (a) and we have the following:


Proposition 3.7. For any a ∈ R [ℓ], the sequence (Y [n] (a))n≥0 converges to a process Y (a) ∈S∞ [2]
satisfying:
i) ∀t ∈ [0, +∞[, Yt [0][(][a][)][ ≤] [Y][t][(][a][)][ ≤] [E][[] �t∞ γse [−][rs] ds|Ft].
ii) ∀t ∈ [0, +∞],

                - τ
Yt(a) = esssupτ ∈TtE[ e [−][rs] g(s, Xs + a)ds + Oτ (a)|Ft], (3.42)

t


where (Ot(a))t≥0 is c`adl`ag and the pointwise limit of (Ot [n][(][a][))][t][≥][0][, given by:][ ∀][t][ ∈] [[0][,][ ∞][[][,]

       - [�] [t][+∆]       -       -       -       -       Ot(a) := E e [−][rs] g(s, Xs + a) ds|Ft + E max - e [−][r][(][t][+∆)] ψ(β)+ Yt+∆(a+ β) |Ft (3.43)

t β∈U


and O∞(a) = 0.


Proof. The sequence (Y [n] (a))n≥0 is increasing and taking into account of (3.37), the limit
Yt(a) := limn→+∞ Yt [n][(][a][),][ t][ ∈] [[0][,][ ∞][], exists. Next the property (i) is obviously obtained by]
taking the limit in (3.37). Now, taking into account (3.36), the process
(Yt [n][(][a][) +] �0t [g][(][X][s][ +][ a][)][ds][)] t∈[0,+∞] [can be seen as the Snell envelope, and then an]

                          - t
(Ft)t∈[0,+∞]-supermatingale, of the continuous process ( 0 [g][(][X][s][ +][ a][)][ds][) +][ O] t [n][(][a][))] t∈[0,+∞] [which]

implies that its increasing limit (Yt(a) + [�] 0 [t] [g][(][X][s][ +][ a][)][ds][)] t∈[0,+∞] [is c`adl`ag in [0][,][ +][∞][] (see e.g.]

[8], pp. 86). Therefore the process (� Yt(a))t∈[0,+∞] is c`adl`ag for any� a ∈ R [ℓ] . It follows that, since
U is finite, the process (max - e [−][r][(][t][+∆)] ψ(β)+ Yt+∆(a + β) )t∈[0,∞] is also c`adl`ag and then its
β∈U

(Ft)t≥0-predictable projection is so. Thus the process (Ot(a))t≥0 is c`adl`ag and verifies the first
equation of (3.43) by taking the limit w.r.t n in each hand-sides of the definition of O [n] (a) in
(3.35). Finally note that (3.43) and point i) imply that O∞(a) := limt→∞ Ot(a) = 0. Thus for
any a ∈ R [ℓ], for any t ∈ [0, +∞], limn→∞ ր Ot [n][(][a][) =][ O][t][(][a][) pointwise. Using now Proposition]
4.10 in Appendix and taking the limit in both hand-sides of (3.36) one deduces that for any
a ∈ R [ℓ], (3.42) is satisfied.


We now focus on the continuity of the process Y (a). So for a stopping time τ and ξ a random
variable Fτ -measurable which takes its values in a finite set U ⊂ R [ℓ] we define (Πt(ξ))t≥τ by:

       ∀t ≥ τ, Πt(ξ) = Πt(a)1[ξ=a], (3.44)

a∈U


with Π(a) is one of the processes Y [n] (a), Y (a), Z [n] (a), K [n] (a).


23


Proposition 3.8. For any a ∈ R [ℓ], the processes (Yt(a))t∈[0,∞] and (Ot(a))t∈[0,∞] are continuous.


Proof. We will show that the process (Yt(a))t∈[0,∞] is continuous and then continuity of (Ot(a))t∈[0,∞]
will follow by Theorem 4.5 in Appendix. First recall that for any a ∈ R [ℓ], O∞(a) = limt→∞ Ot(a) =
0.
Next for any predictable stopping time τ and any finite random variable η which is Fτ  measurable with values in R [ℓ], let us set Aτ (η) := {Yτ (η) − Yτ −(η) < 0}.
So, let T be a predictable stopping time for which P[AT (a)] > 0. According to Proposition
4.9 in Appendix, we have 1AT (a)OT − (a) = 1AT (a)YT − (a) and


(OT − (a) − OT (a))1AT (a) ≥ (YT − (a) − YT (a))1AT (a) > 0.


Therefore P[{T = ∞} ∩ AT (a)] = 0 since O∞(a) = limt→∞ Ot(a) = 0. On the other hand,




   -    -    -    -    1AT (a) OT − (a) − OT (a) = 1AT (a) E max
β∈U [{−][e][−][r][(][T][ +∆)][ψ][(][β][) +][ Y][(][T][ +∆)][−][(][a][ +][ β][)][}|F][T]




  - ��
−E max
β∈U [{−][e][−][r][(][T][ +∆)][ψ][(][β][) +][ Y][T][ +∆][(][a][ +][ β][)][}|F][T]




       -       -       -       ≤ 1AT (a)E max Y(T +∆)−(a + β) − YT +∆(a + β) |FT
β∈U




       -       -       -       -       ≤ 1AT (a)E max 1AT +∆(a+β) Y(T +∆)−(a + β) − YT +∆(a + β) |FT .
β∈U



Since AT (a) ∈FT, we get:

                  -                   -                   - ��                   0 < 1AT (a){OT −(a) − OT (a)} ≤ E 1AT (a) max 1AT +∆(a+β) Y(T +∆)−(a + β) − YT +∆(a + β) |FT .
β∈U


It implies that the right-hand side is non negative, which means that there exists a random

                  variable β [¯] such that β [¯] (Ω) ⊂ U and 1AT +∆(a+ ¯β) Y(T +∆)−(a + β [¯] ) - YT +∆(a + β [¯] ) > 0, P-a.s. Now,
since YT +∆(a+ β [¯] ) ≥ OT +∆(a+ β [¯] ) and 1AT +∆(a+ ¯β)Y(T +∆)−(a+ β [¯] ) = 1AT +∆(a+ ¯β)O(T +∆)−(a+ β [¯] ),
then

                -                -                -                -                1AT (a){OT −(a) − OT (a)} ≤ E 1AT (a) max 1AT +∆(a+β) O(T +∆)− (a + β) − OT +∆(a + β) |FT .
β∈U


So let β0 = 0 and β1 the FT +∆-measurable random variable valued in U (which exists since U
is finite) such that

  -  - ��  maxβ∈U 1AT +∆(a+β) O(T +∆)−(a+β)−OT +∆(a+β) = 1AT +∆(a+β1) O(T +∆)− (a+β1)−OT +∆(a+β1)}.


It follows that

                  -                  -                  -                  -                  -                  1AT (a){OT −(a) − OT (a)} ≤ E 1AT (a)E 1AT +∆(a+β1) O(T +∆)−(a + β1) − OT +∆(a + β1) |FT +∆ |FT

                  -                  -                  -                  ≤ E 1AT (a) × 1AT +∆(a+β1) O(T +∆)−(a + β1) − OT +∆(a + β1) |FT .


Repeating this reasoning as many times as necessary to obtain random variables βk valued in
U and FT +k∆-measurable, k = 1, ..., n, such that:



�� �n
0 < 1AT (a){OT −(a) - OT (a)} ≤ E



1AT +k∆(a+β1+···+βk)
k=0




  - ��   × O(T +n∆)−(a + β1 + · · · + βn) − OT +n∆(a + β1 + · · · + βn) |FT




         - ∞
≤ 2E[Ce [−][(][n][+1)∆][−][T] +



e [−][rs] γsds|FT ] → 0 as n →∞.
T +n∆



24


The last inequality stems from the definition of the process (Ot(a))t∈[0,∞[ in (3.43) and then for
and a ∈ R [ℓ], for any t ≥ 0 we have,

                - ∞
|Ot(a)| ≤ E[ e [−][rs] γsds + Ce [−][t][−][∆] |Ft]

t


by using Assumption 3.3-i), i) of Proposition 3.7 and the finitness of U . Therefore,


0 < 1AT (a){OT −(a) − OT (a)} = 0,


which is absurd. Thus for any a ∈ R [ℓ], the process (Yt(a))t∈[0,∞] is continuous. Finally the
process (Ot(a))t∈[0,∞] is then so.


Theorem 3.3. There exists an optimal strategy δ [∗][,][∞] = (τn [∗][, β] n [∗][)][n][≥][0][ for the infinite horizon]
stochastic impulse control problem, i.e.,


Y0(0) = sup J(δ) = J(δ [∗][,][∞] ).
δ∈A


Proof. We first exhibit the strategy δ [∗][,][∞] . Let


τ1 [∗] [= inf][{][s][ ∈] [[0][,][ +][∞][];][ O][s][(0) =][ Y][s][(0)][}]


and β1 [∗] [an][ U] [-valued random variable,][ F][τ][ ∗] 1 [+∆][−][measurable such that]






.



Oτ1 [∗][(0) :=][ E]


For any n ≥ 2



�� τ ∗1 [+∆]

e [−][rs] g(s, Xs)ds − e [−][r][(][τ][ ∗] 1 [+∆)] ψ(β1 [∗][) +][ Y][τ][ ∗] 1 [+∆][(][β] 1 [∗][)][|F][τ][ ∗] 1
τ1 [∗]




      -      τn [∗] [= inf] s ∈ [τn [∗] −1 [+ ∆][,][ +][∞][]][, O][s][(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗] −1 [) =][ Y][s][(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗] −1 [)]


and βn [∗] [an][ F][τ][ ∗] n [+∆][−][measurable,][ U] [-valued random variable satisfying]



�� τ ∗n [+∆]
Oτ ∗n [(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗] −1 [)] = E e [−][rs] g(s, Xs + β1 [∗] [+][ · · ·][ +][ β] n [∗] −1 [)][ds]

τn [∗]

       - e [−][r][(][τ][ ∗] n [+∆)] ψ(βn [∗][) +][ Y][τ][ ∗] n [+∆][(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗][)][|F][τ][ ∗] n




.



Let us highlight first that the stopping times τn [∗][,][ n][ ≥] [1, are optimal. Indeed, for any][ a][ ∈] [R][ℓ][,]
(Yt(a))t∈[0,∞] and (Ot(a))t∈[0,∞] are continuous. Moreover, taking (3.42) into account and using
the fact that Y∞(a) = limt→∞ Ot(a) = 0, the optimality follows.
We now divide the proof into two steps:
Step 1: First let us show that Y0(0) = J(δ [∗][,][∞] ). Note that 0 in the index is related to the initial
time while 0 in the parentheses is related to the initial state where an impulse is not made yet.


Since τ1 [∗] [is optimal and][ β] 1 [∗] [is the optimal impulse then]



�� τ
Y0(0) = esssupτ ∈T0E



�� τ ∗1



τ 
e [−][rs] g(s, Xs)ds + Oτ (0)
0











= E


= E



e [−][rs] g(s, Xs)ds + Oτ1 [∗][(0)]
0



e [−][rs] g(s, Xs)ds − e [−][r][(][τ][ ∗] 1 [+∆)] ψ(β1 [∗][) +][ Y][τ][ ∗] 1 [+∆][(][β] 1 [∗][)]
0



�� τ ∗1 [+∆]



.



25


On the other hand, we have


Yτ1 [∗][+∆][(][β] 1 [∗][)] = esssupτ ∈Tτ1 ∗ [+∆][E]



e [−][rs] g(s, Xs + β1 [∗][)][ds][ +][ O][τ] [(][β] 1 [∗][)][|F][τ][ ∗] 1 [+∆]
τ1 [∗][+∆]

           


�� τ







�� τ ∗2







= E


= E



τ1 [∗][+∆] e [−][rs] g(s, Xs + β1 [∗][)][ds][ +][ O][τ][ ∗] 2 [(][β] 1 [∗][)][|F][τ][ ∗] 1 [+∆]



e [−][rs] g(s, Xs + β1 [∗][)][ds][ −] [e][−][r][(][τ][ ∗] 2 [+∆)] ψ(β2 [∗][) +][ Y][τ][ ∗] 2 [+∆][(][β] 1 [∗] [+][ β] 2 [∗][)][|F][τ][ ∗] 1 [+∆]
τ1 [∗][+∆]



�� τ ∗2 [+∆]



.



Therefore,



�� τ ∗1 [+∆]
Y0(0) = E



e [−][rs] g(s, Xs + β1 [∗][)][ds]
τ1 [∗][+∆]



τ ∗1 [+∆]  - τ ∗2 [+∆]

e [−][rs] g(s, Xs)ds +
0 τ [∗][+∆]



1              
−e [−][r][(][τ][ ∗] 1 [+∆)] ψ(β1 [∗][)][ −] [e][−][r][(][τ][ ∗] 2 [+∆)] ψ(β2 [∗][) +][ Y][τ][ ∗] 2 [+∆][(][β] 1 [∗] [+][ β] 2 [∗][)] .



Repeating n times this reasoning to obtain:



�� τ ∗1 [+∆]
Y0(0) = E



τ ∗1 [+∆]  
e [−][rs] g(s, Xs)ds +
0




- τ ∗k+1 [+∆]



1≤k≤n−1



e [−][rs] g(s, Xs + β1 [∗] [+][ · · ·][ +][ β] k [∗][)][ds]
τk [∗][+∆]



�n







�n 
e [−][r][(][τ][ ∗] k [+∆)] ψ(βk [∗][) +][ Y][τ][ ∗] n [+∆][(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗][)] .
k=1



It implies that

��
|Y0(0) − J(δ [∗][,][∞] )| ≤ E


k≥n




- τ ∗k+1 [+∆]



e [−][rs] |g(s, Ls + β1 [∗] [+][ · · ·][ +][ β] k [∗][)][|][ds][+]
τk [∗][+∆]








   - ∞
≤ 2E[




                    

e [−][r][(][τ][ ∗] k [+∆)] ψ(βk [∗][) +][ |][Y][τ][ ∗] n [+∆][(][β] 1 [∗] [+][ · · ·][ +][ β] n [∗][)][|]
k≥n+1



e [−][rs] γsds + Ce [−][n][∆] ]
n∆



where C is a constant which does not depend on n. This latter inequality stems from Assumption
3.3-i), the inequality of Proposition 3.7-i) and finally the delay ∆between two successif stopping
times. Finally take the limit w.r.t n to obtain that Y0(0) = J(δ [∗][,][∞] ).


Step 2: Y0(0) ≥ J(δ [′] ) for any δ [′] ∈A∞.
By virtue of Proposition 3.7-ii), we have:



�� τ ′1







Y0(0) ≥ E



e [−][rs] g(s, Xs)ds + Oτ1 [′] [(0)]
0



.



However,







Oτ1 [′] [(0)][ ≥] [E]



�� τ ′1 [+∆]

e [−][rs] g(s, Xs)ds − e [−][r][(][τ][ ′] 1 [+∆)] ψ(β1 [′] [) +][ Y][τ][ ∗] 1 [+∆][(][β] 1 [′] [)][|F][τ][ ′] 1
τ1 [′]



.



Combining the last inequality with the previous one, we obtain:



�� τ ′1 [+∆]







Y0(0) ≥ E



e [−][rs] g(s, Xs)ds − e [−][r][(][τ][ ′] 1 [+∆)] ψ(β1 [′] [) +][ Y][τ][ ′] 1 [+∆][(][β] 1 [′] [)]
0



. (3.45)



26


On the other hand, we have that


Yτ1 [′] [+∆][(][β] 1 [′] [)] = esssupτ ∈Tτ1′ [+∆][E]



�� τ

e [−][rs] g(s, Xs + β1 [′] [)][ds][ +][ O][τ] [(][β] 1 [′] [)][|F][τ][ ′] 1 [+∆]
τ1 [′] [+∆]

            






�� τ ′2







≥ E


≥ E



e [−][rs] g(s, Xs + β1 [′] [)][ds][ +][ O][τ][ ′] 2 [(][β] 1 [′] [)][|F][τ][ ′] 1 [+∆]
τ1 [′] [+∆]



e [−][rs] g(s, Xs + β1 [′] [)][ds][ −] [e][−][r][(][τ][ ′] 2 [+∆)] ψ(β2 [′] [) +][ Y][τ][ ′] 2 [+∆][(][β] 1 [′] [+][ β] 2 [′] [)][|F][τ][ ′] 1 [+∆]
τ1 [′] [+∆]



�� τ ′2 [+∆]



.



Therefore,



�� τ ′1 [+∆]
Y0(0) ≥ E



e [−][rs] g(s, Xs + β1 [′] [)][ds]
τ1 [′] [+∆]



τ ′1 [+∆]  - τ2 [′] [+∆]

e [−][rs] g(s, Xs)ds +
0 τ [′] [+∆]



1                       
- e [−][r][(][τ][ ′] 1 [+∆)] ψ(β1 [′] [)][ −] [e][−][r][(][τ][ ′] 2 [+∆)] ψ(β2 [′] [) +][ Y][τ][ ′] 2 [+∆][(][β] 1 [′] [+][ β] 2 [′] [)] .



In repeating this reasoning as many times as necessary, we obtain:



�� τ ′1 [+∆]
Y0(0) ≥ E



τ ′1 [+∆]  
e [−][rs] g(s, Xs)ds +
0



1≤k≤n−1




- τ ′k+1 [+∆]

e [−][rs] g(s, Xs + β1 [′] [+][ · · ·][ +][ β] k [′] [)][ds]
τk [′] [+∆]



�n







�n 
e [−][r][(][τ][ ′] k [+∆)] ψ(βk [′] [) +][ Y][τ][ ′] n [+∆][(][β] 1 [′] [+][ · · ·][ +][ β] n [′] [)] .
k=1



Finally, by taking the limit as n → +∞, we get







e [−][r][(][τ][ ′] n [+∆)] ψ(βn [′] [)]
n≥1







 = J(δ [′] )



Y0(0) ≥ E




 - +∞




+∞ 
e [−][rs] g(s, Xs [δ][′][)][ds][ −]
0



since by Proposition 3.7-i), (3.33), (3.34) and (3.44), limn→∞ Yτ ′n+∆(β1 [′] [+][ · · ·][ +][ β] n [′] [) = 0][ P][-a.s.]
and in L [1] (dP). Hence, the strategy δ [∗][,][∞] is optimal


Remark 3.4. It is also possible to consider the infinite horizon risk-sensitive case. As this
setting does not rise major difficulties w.r.t the risk neutral framework we considered, then it is
left to the care of the reader. Only small adaptations are needed especially the integrability of
the function g(.) and its majorizing process (γt)t≥0 (see Assumption 2.2).

### References


[1] Agram, N., Pucci, G. and Oksendal, B. (2024): Impulse Control of Conditional
McKean-Vlasov Jump Diffusions. Optim. Theory Appl. 200, 1100-1130.


[2] Bayraktar, E. and Egami, M. (2007): The effects of implementation delay on decision
making under uncertainty. Stochastic Processes and their Applications, 117, 333-358.


[3] Bensoussan, A. and Lions, J.L. (1984): Impulse Control and Quasivariational inequalities. Gauthier-Villars, Paris.


[4] Bruder, B. and Pham, H. (2009): Impulse control problem on finite horizon with
execution delay. Stochastic Processes and their Applications, 119(5), 1436-1469.


[5] Chen Zengjing(1998): Existence and uniqueness for BSDE’s with stopping time. Chinese Science Bulletin 43 pp.96-99.


27


[6] Carmona, R., & Dayanik, S. (2008): Optimal multiple stopping of linear diffusions.
Mathematics of Operations Research, 33(2), 446-460.


[7] Carmona, R., & Touzi, N. (2008): Optimal multiple stopping and valuation of swing
options. Mathematical Finance: An International Journal of Mathematics, Statistics
and Financial Economics, 18(2), 239-268.


[8] Dellacherie, C. and Meyer, P. A. (1982): Probabilit´es et potentiel. Chap. V-VIII.
Hermann, Paris.


[9] Diener, N. (2009): Mathematical Models For Swing Options And Subprime Mortgage
Derivatives (Doctoral dissertation, Cornell University).


[10] Djehiche, B., Hamad`ene, S. and Hdhiri, I. (2010): Stochastic Impulse Control of NonMarkovian Processes. Appl. Math. Optim., 61(1), 1-26.


[11] Djehiche, B., Hamad`ene, S., Hdhiri, I., Zaatra, H. (2021): Infinite Horizon Stochastic
Impulse Control with Delay and Random Coefficients. Mathematics of Operations
Research 47(1):665-689.


[12] Djehiche, B., Hamad`ene, S. and Popier, A. (2009): A Finite Horizon Optimal Multiple
Switching Problem. SIAM Journal on Control and Optimization, 48(4), 2751-2770.


[13] Eydeland, A., & Wolyniec, K. (2002): Energy and power risk management: New
developments in modeling, pricing, and hedging. John Wiley & Sons.


[14] El-Karoui, N. (1981): Les aspects probabilistes du contrˆole stochastique. Ecole d’´et´e
de Saint-Flour, Lecture Notes in Mathematics 876, (Springer, Berlin), 73-238.


[15] Harrison, J. M., Sellke, T. M. and Taylor. A. J. (1983): Impulse control for Brownian
motion. Mathematics of operation research, 8, 454-466.


[16] Hamad`ene, S. (2002): Reflected BSDEs with discontinuous barrier and applications.
Stochastics and Stochastic Reports, 74 (3-4), 571-596.


[17] Hamad`ene S, Lepeltier JP, Wu Z.(1999): Infinite horizon reflected backward stochastic differential equations and applications in mixed control and game problems.
Prob.Math. Stat. vol 19, 211-234.


[18] Hdhiri, I. and Karouf, M. (2011): Risk Sensitive Impulse Control of Non-Markovian
Processes. Math. Meth. Oper Res., 74(1), 1-20.


[19] Hdhiri, I. and Karouf, M. (2017): Optimal stochastic impulse control with random
coefficients and execution delay. Stochastics, 90(2), 151-164.


[20] Hdhiri, I. and Zaatra, H. (2026): Risk-sensitive impulse control problems with jumps
and execution delays. MCRF, 16: 329-343.


[21] Jaillet, P., Ronn, E. I., & Tompaidis, S. (2003): On the Existence of a Unique Optimal
Threshold Value for the Early Exercise of Call Options.


[22] Jeanblanc-Picqu´e, M. (2003): Impulse control method and exchange rate. Math. Finance 3(2), pp. 161-177.


[23] Karatzas, I. and Shreve, S. (1987): Brownian Motion and Stochastic Calculus. Springer
(Graduate Texts in Mathematics, Volume 113).


28


[24] Keppo, J. and Peura, S. (2005): Optimal bank capital with costly recapitalization.
Journal of Business.


[25] Korn, R. (1999): Some applications of impulse control in mathematical finance. Math.
Meth. Oper. Res. 50, pp. 493-518.


[26] Lepeltier, J. P. and Marchal, B. (1984): Th´eorie g´en´erale du contrˆole impulsionnel
markovien. SIAM J. Cont. Optim., vol 22 (4), pp 645-665.


[27] Oksendal, B. and Sulem, A. (2008) : Optimal stochastic impulse control with delayed
reaction. Appl. Math. Optim., 58(2), 243-255.


[28] Pflug, G. C., & Broussev, N. (2009): Electricity swing options: Behavioral models and
pricing. European journal of operational research, 197(3), 1041-1050.


[29] Pardoux, E., & Peng, S. (1990): Adapted solution of a backward stochastic differential
equation. Systems & control letters, 14(1), 55-61.


[30] Jonsson, J., Perninge, M. (2023): Finite horizon impulse control of stochastic functional differential equations. SIAM J. Control Optim. 61, Iss. 2


[31] Revuz, D. and Yor, M. (2013): Continuous Martingales and Brownian Motion. Vol.
293. Springer Science & Business Media.


[32] Yeo, C. (2025): Numerical Methods for the Pricing of Swing and Storage Contracts
(Doctoral dissertation, Sorbonne University).

### 4 Appendix


4.1 Optional and predictable projections


The acronymes rcll and lcrl refer respectively to right continuous left limited and left continuous right limited. Next let (Ω, F, P) be a complete probability space which carries a filtration
(Ft)t≥0 which satifies the usual conditions.


Theorem 4.4. ([8], pp. 116) Let X := (Xt)t≥0 be a measurable process verifying
E[supt≥0 |Xt|] < ∞. Then there exists a unique (up to indistinguishability) optional process [o] X
such that for any (Ft)t≥0-stopping time τ,


E(Xτ 1{τ<∞}|Fτ ) = [o] Xτ 1{τ<∞}.


Likewise, there exist a unique predictable projection [p] X such that: for any (Ft)t≥0-predictable
stopping time τ,
E(Xτ 1{τ<∞}|Fτ −) = [p] Xτ 1{τ<∞}.


The process [o] X (resp. [p] X) is called the optional (resp. predictable) projection of X.


Theorem 4.5. ([8], pp.119, 125) Let X be a measurable process such that E[supt≥0 |Xt|] < ∞
and [o] X, [p] X are respectively its optional and predictable projections.


i) If X is right continuous (resp. rcll), [o] X is right continuous (resp. rcll).


ii) If X is left continuous (resp. lcrl), [p] X is left continuous(resp. lcrl).


Lemma 4.1. For the Brownian filtration, the optional and predictable σ−fields coincide which
means that the optional projection of a continuous process X such that E[supt≥0 |Xt|] < ∞ is
also continuous.


29


For more details, we refer to [31], Theorem 3.5, pp.201 and Corollary 5.7, pp.174.


Remark 4.5. Let T > 0 be a real number. The predictable and optional projections of a
stochastic measurable process X = (Xt)t∈[0,T ] are the ones of the process X [T] = (Xt∧T )t≥0 on
the filtered probability space (Ω, F, P, (Ft∧T )t≥0).


4.2 Snell envelope of a process


Let T ∈ [0, +∞]. An (Ft)t≤T -adapted rcll process U = (Ut)0≤t≤T, is called of class [D], if
the family {Uτ }τ ∈T0 is uniformly integrable.


Proposition 4.9. (see [8], pp.432) Let U = (Ut)0≤t≤T be a process of class [D], then there
exists an (Ft)t≤T -adapted R-valued rcll process N := (Nt)t≤T such that:


i) N is the smallest c`adl`ag supermartingale of class [D], called the Snell envelope of U, which
dominates U, i.e., if (Nt [′][)][0][≤][t][≤][T] [is another c`adl`ag supermartingale of class][ [][D][]][ such that,]
Nt [′] [≥] [U][t][ then][ N][ ′] t [≥] [N][t][ for any][ 0][ ≤] [t][ ≤] [T][ .]


ii) For any t ≤ T, it holds:


Nt = esssupτ ∈TtE[Uτ |Ft] (NT = UT ). (4.46)


iii) The jumping times of (Nt)t≤T are predictable and satisfy:


{∆tN < 0} ⊂{Nt− = Ut−} ∩{∆tU < 0}.


Proposition 4.10. ([12], Proposition 2) Let (Un)n≥0 and U be c`adl`ag processes of class [D]
such that the sequence of processes (Un)n≥0 converges increasingly and pointwisely to U, then
the snell envelope of (Un)n≥0 converges increasingly and pointwisely to the one of U .


Proposition 4.11. ([14], Proposition II.2.7) If U is right upper semi-continuous, then the
process N is continuous. Furthermore, if γ is an (Ft)t≤T -stopping time and
τγ [∗] [= inf][{][s][ ≥] [γ, N][s][ =][ U][s][} ∧] [T][, then][ τ] γ [ ∗] [is optimal after][ γ][, i.e.,]


Nγ = esssupτ ∈Tγ E[Uτ |Fγ] = E[Uτ ∗γ [|F][γ][] =][ E][[][N][τ][ ∗] γ [|F][γ][]][.] (4.47)


30


