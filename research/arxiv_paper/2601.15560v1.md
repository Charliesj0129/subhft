## **Relative Classification Accuracy: A Calibrated Metric for Identity** **Consistency in Fine-Grained K-pop Face Generation**



**Sylvey Lin**
MS in Information Management
UIUC
yuhsinl2@illinois.edu


**Abstract**


Denoising Diffusion Probabilistic Models
(DDPMs) have achieved remarkable success
in high-fidelity image generation. However, evaluating their semantic controllability—specifically for fine-grained, singledomain tasks—remains challenging. Standard
metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such
specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol
face generation (32 _×_ 32), a domain characterized by high inter-class similarity. We propose a calibrated metric, **Relative Classifica-**
**tion Accuracy (RCA)**, which normalizes generative performance against an oracle classifier’s baseline. Our evaluation reveals a critical
trade-off: while the model achieves high visual
quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly
for visually ambiguous identities. We analyze
these failure modes through confusion matrices
and attribute them to resolution constraints and
intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity
consistency in conditional generative models.


**1** **Introduction**


Denoising Diffusion Probabilistic Models
(DDPMs) have recently emerged as the state-ofthe-art framework for generative tasks, surpassing
Generative Adversarial Networks (GANs) in terms
of training stability and mode coverage. Among
various domains, human face generation remains a
cornerstone problem in computer vision due to its
profound implications for downstream applications.
High-fidelity, controllable face synthesis holds
transformative potential for industries ranging
from digital entertainment (e.g., virtual idols
and avatars) to privacy-preserving synthetic data
generation for facial recognition systems. However,
as the visual quality of generated images reaches
near-photorealistic levels, the research focus must



**Eranki Vasistha**
MS in Information Management
UIUC
veranki2@illinois.edu


shift from mere "visual fidelity" to "semantic
controllability"—specifically, the model’s ability
to adhere precisely to class-conditional constraints.

Despite these advancements, evaluating the semantic consistency of conditional generative models remains an open challenge. Standard metrics
such as Fréchet Inception Distance (FID) and Inception Score (IS) primarily assess the distributional distance and perceptual quality of generated samples. While effective for general-purpose
datasets like ImageNet, these metrics often fail in
specialized, single-domain tasks. Crucially, Inception Score (IS) becomes unreliable when applied
to face-only datasets, as the underlying Inception
classifier—pretrained on general objects—cannot
meaningfully discriminate between distinct human
identities. This creates a "blind spot": a model
might generate high-quality faces that look realistic
(low FID) but fail to preserve the specific identity
required by the condition (poor class consistency).

To rigorously test the limits of conditional identity preservation, we focus on a specific and challenging domain: K-pop idol face generation. Unlike generic face generation tasks (e.g., distinguishing gender or age), generating specific K-pop idols
represents a fine-grained generation task. The subjects in this domain share highly similar demographic features, makeup styles, and lighting conditions, resulting in significantly lower inter-class
variance compared to general datasets. This high
degree of similarity poses a rigorous test for conditional DDPMs, compelling the model to learn
subtle, high-frequency features to distinguish between Identity A and Identity B, rather than relying
on coarse global structures.

To address these evaluation gaps, we propose a comprehensive study on Class-Conditional
DDPMs tailored for fine-grained face generation.
Beyond implementing the generative pipeline, our
primary contribution is a robust evaluation framework designed to quantify identity preservation.


We introduce the Relative Classification Accuracy
(RCA) metric, which utilizes an "Oracle" classifier
trained on real data to assess generated samples.
By normalizing the model’s accuracy against the
classifier’s performance on real test data, RCA provides an interpretable and calibrated measure of
semantic consistency. This approach disentangles
generation quality from the intrinsic difficulty of
the classification task. By combining this semantic
metric with standard visual metrics (FID, LPIPS),
we offer a holistic view of the trade-offs between
image quality, diversity, and identity consistency in
diffusion-based generation.


**2** **Related Work**


Traditional evaluation metrics in Natural Language
Processing (NLP), such as BLEU and ROUGE,
rely on n-gram overlap, which captures surfacelevel similarity but often fails to assess deep semantic alignment. To address this, BERTScore was
introduced as a semantic-oriented metric (Zhang
et al., 2020). Instead of exact token matching,
BERTScore computes similarity in a contextual
embedding space generated by pre-trained Transformers (e.g., BERT). This shift allows for evaluating whether a generated sentence preserves the
meaning and intent of the reference, even if the
phrasing differs. We draw direct inspiration from
this paradigm: just as NLP metrics moved from surface statistics to embedding-based semantic evaluation, image generation metrics must move from
distributional statistics (FID) to identity-aware semantic evaluation.
In the visual domain, deep convolutional neural networks (CNNs), such as ResNet, serve a role
analogous to BERT in NLP. While BERT extracts
rich semantic text embeddings, a ResNet classifier
trained on a specific domain (e.g., human faces) extracts high-level discriminative features that encode
identity, gender, and attributes. Standard metrics
like FID (Heusel et al., 2018) operate on the distribution of Inception features (trained on ImageNet),
which—similar to n-gram matching—captures general visual quality but lacks the granularity to verify specific class conditions. Our work leverages
a domain-specific ResNet as a "semantic oracle,"
ensuring that the generated images are not only realistic but also semantically aligned with the target
identity condition.
The use of classifiers for evaluation has been
explored in prior works, most notably the Classifi


cation Accuracy Score (CAS) (Ravuri and Vinyals,
2019). Ravuri et al. proposed using a classifier
trained on real data to evaluate the class-conditional
consistency of GANs. While CAS provides a raw
accuracy metric, it does not inherently account for
the difficulty of the classification task itself. In
fine-grained domains like K-pop idol generation,
even ground-truth data may not achieve 100% accuracy due to visual similarities. To mitigate this, we
extend the concept of CAS by introducing a normalized ratio—Relative Classification Accuracy
(RCA). By benchmarking the generative model’s
accuracy against the oracle classifier’s performance
on real data, we provide a calibrated measure of
identity preservation that is robust to the intrinsic
difficulty of the dataset.


**3** **Data (Resources)**


**3.1** **Source & Characteristics**


We utilize the **KoIn** dataset, a large-scale face identity classification benchmark released by (Seo et al.,
2023). The dataset consists of high-quality images
of Korean celebrities collected from social media
platforms (e.g., Instagram) and web sources.

Unlike generic face datasets (such as CelebAHQ), KoIn presents a unique challenge for generative modeling due to its **fine-grained** nature.
The subjects share highly similar demographic features, makeup styles, and lighting conditions, resulting in low inter-class variance. Figure 1 illustrates representative samples from the training set,
highlighting the visual similarity that makes identity preservation particularly difficult.


**3.2** **Composition & Subsets**


The complete KoIn dataset comprises over 100
identities and 100,000+ face images, with explicit
identity labels (one celebrity per class). To facilitate benchmarking at different scales, the original
authors provide three standard subsets: KoIn10,
KoIn50, and KoIn100, containing 10, 50, and 100
identities respectively.

For this project, we focus on the **KoIn10** subset. This selection allows us to rigorously evaluate
the model’s class-conditional consistency in a controlled setting, ensuring that the evaluation metrics
(such as our proposed RCA) can focus on subtle
identity differences without being overwhelmed by
a massive label space.


Figure 1: Representative samples from 3 of the 10 identity classes in the KoIn10 training set. Each row corresponds
to a unique celebrity identity. The high visual similarity across classes illustrates the difficulty of the fine-grained
generation task.



**4** **Methods**


Our methodology centers on the generation of
identity-preserving face images using Denoising
Diffusion Probabilistic Models (DDPMs). We approach this problem by first training resolutionspecific generative models and subsequently evaluating their semantic consistency using a pre-trained
classifier. This section details the data processing
pipeline, the generative architecture, and the evaluation framework.


**4.1** **Data Preprocessing and Partitioning**


To facilitate the training of resolution-specific diffusion models, we curated a dataset of high-quality
K-Pop idol faces, categorized into distinct identity classes. The raw images underwent a rigorous preprocessing pipeline where they were first
center-cropped to isolate facial features and subsequently resized to two target resolutions, 32 _×_ 32
and 64 _×_ 64 pixels, to support our multi-resolution
experiments. Pixel values were then normalized
from the standard integer range [0 _,_ 255] to the tensor range [ _−_ 1 _,_ 1] to align with the input requirements of the Denoising Diffusion Probabilistic
Models (DDPM) scheduler. The processed dataset
was stratified into a training set, used to optimize
both the generative models and the ResNet classi


fiers, and an evaluation set, reserved for calculating
validation accuracy and Fréchet Inception Distance
(FID) scores.
**MTCNN (de Paz Centeno, 2024)**  - A multistage cascaded network that detects faces and five
facial landmarks for alignment. It provides robust
performance on various poses and lighting conditions and will serve as our primary detector for
automatic cropping.
After detection, each face will be cropped with
a small margin (25%) to preserve contextual cues
such as hair and face contour. All cropped images
will be resized to a fixed resolution before being
used for DDPM training.


**4.2** **Generative Framework: Denoising**
**Diffusion Probabilistic Models (DDPM)**


We adopt a **class-conditional DDPM** (Ho et al.,

2020) to synthesize face images for different K-pop
idols, treating each idol as a separate class. The
model is trained on cropped and aligned faces with
identity labels, enabling generation that reflects
both intra-class variation (e.g., makeup, expression) and inter-class differences across identities.
To keep the method modular and implementationagnostic, we avoid committing to architectural
specifics here and focus on the evaluation setting:


  - **Conditioning:** each training sample carries
an identity label; the diffusion model is conditioned on this label during training and sampling.


  - **Training data:** only preprocessed real images and their corresponding identity labels
are used for training.


  - **Sampling:** we primarily use DDPM sampling; deterministic/DDIM-style steps may be
used for efficiency if needed, without affecting the evaluation protocol.


  - **Outputs:** generated images are stored per
identity and forwarded to the classifier for
automatic identity-preservation assessment.


To build this model, we will adapt the architecture from a minimal PyTorch implementation of
a class-conditional DDPM (Birdal, 2023). This
repository provides a clear baseline that directly
aligns with our goals. Our primary task will involve integrating this implementation with our preprocessed Koln10 data pipeline (from Sec. 3.1) and
connecting its output to our trained ResNet classifier (from Sec. 3.3) for the final evaluation. This
setup isolates our core question while providing a
concrete starting point for implementation and the
ablations discussed in Section 4.3.


**4.2.1** **The Forward Diffusion Process**


The forward process is defined as a fixed Markov
chain that gradually destroys the structure of the
data distribution _q_ ( _x_ 0) by adding Gaussian noise
over _T_ timesteps. We define a variance schedule
_β_ 1 _, . . ., βT_, where the transition kernel _q_ ( _xt|xt−_ 1)
is a Gaussian distribution:


_q_ ( _xt|xt−_ 1) = _N_ ( _xt_ ; ~~�~~ 1 _−_ _βtxt−_ 1 _, βt_ **I** ) (1)


A key property of this formulation is that we can
sample _xt_ at any arbitrary timestep _t_ directly from
_x_ 0 using the closed-form expression:



**4.2.2** **The Reverse Generative Process**


The generative process serves as the reverse of the
forward diffusion, aiming to recover the original
data _x_ 0 by approximating the intractable posterior
_q_ ( _xt−_ 1 _|xt_ ). This is modeled as a Markov chain
with learned Gaussian transitions:


_pθ_ ( _xt−_ 1 _|xt_ ) = _N_ ( _xt−_ 1; _µθ_ ( _xt, t_ ) _,_ Σ _θ_ ( _xt, t_ )) (3)


Instead of predicting the mean _µθ_ directly, we
train a function approximator _ϵθ_ ( _xt, t_ ) to predict
the noise component _ϵ_ that was added to _x_ 0 to
produce _xt_ . The training objective is simplified to a
weighted Mean Squared Error (MSE) loss between
the true noise and the predicted noise:



_√_
_L_ simple( _θ_ ) = E _t,x_ 0 _,ϵ_ - _∥ϵ −_ _ϵθ_ ( _[√]_ _α_ ¯ _tx_ 0 +


**4.2.3** **Network Architecture and**
**Implementation**



1 _−_ _α_ ¯ _tϵ, t_ ) _∥_ [2][�]

(4)



_√_
_xt_ = _[√]_ _α_ ¯ _tx_ 0 +



1 _−_ _α_ ¯ _tϵ_ (2)



To implement the noise predictor _ϵθ_, we utilized a
UNet2DModel architecture featuring a contracting
path for context capture and an expanding path for
precise localization. For the 32 _×_ 32 resolution experiments, the model comprises three block levels
with channel multipliers of (1 _,_ 2 _,_ 4) corresponding
to (64 _,_ 128 _,_ 256) channels, while the 64 _×_ 64 model
extends this to four levels (64 _,_ 128 _,_ 256 _,_ 512) to
resolve higher-frequency spatial details. Both architectures employ sinusoidal position embeddings
to encode the timestep _t_ into the network, ensuring the model adapts its denoising behavior across
different stages of the diffusion process. The network is regularized using Group Normalization
and utilizes the DDPMScheduler with _T_ = 1000
timesteps, optimized via AdamW with a learning
rate of 1 _×_ 10 _[−]_ [4] .


**4.3** **Evaluation Methodology: Semantic**
**Consistency and Visual Metrics**


To check if our model truly preserves identity—rather than just generating good-looking
faces—we used a metric called **Relative Classi-**
**fication Accuracy (RCA)** . Standard metrics often
fail to catch when a model generates the wrong
person. RCA fixes this by directly measuring if the
generated image looks like the specific person we
asked for.



where _αt_ = 1 _−_ _βt_, ¯ _αt_ = [�] _s_ _[t]_ =1 _[α][s]_ [, and]
_ϵ ∼N_ (0 _,_ **I** ). As _T →∞_, the data _xT_ asymptotically approaches an isotropic Gaussian distribution, providing a tractable starting point for the
generative process.


Figure 2: **Overview of the Proposed Framework.** The architecture illustrates the forward diffusion process (top)
and the reverse generative process (bottom) using a U-Net with attention mechanisms.



We used our pre-trained **ResNet-34** and **ResNet-**
**18** classifiers as "judges." The process works as follows: for every identity class _c_, we generated 5,000
images. We then passed these images through the
classifier to see if it recognized them correctly.
The RCA score is simply the percentage of generated images that the classifier labels correctly:


_RCA_ = _[Accuracy][gen]_ (5)

_Accuracyreal_


Where the terms are defined as follows:


  - **Accuracygen** : The raw classification accuracy
on the synthetic dataset. It measures the proportion of generated images that the oracle
correctly assigns to their intended class label.


  - **Accuracyreal** : The validation accuracy of
the oracle classifier on the real, ground-truth
dataset. This serves as a baseline, representing the maximum distinguishable information
available to the classifier for a given resolution.


By normalizing by Accuracyreal, RCA provides
a fairer measure of "semantic controllability," quantifying how effectively the diffusion model captures the specific features that define an identity. A
distinction must be made between aggregate and
class-specific metrics. On a macro level, accuracy
adequately captures the model’s overall efficacy.
However, at the class level, Recall is the preferred
metric as it is mathematically equivalent to ’classconditional accuracy’—the probability that an image generated for a specific class is correctly identified as such. Accordingly, we present Relative



Figure 3: **Contextual Artifacts.** Due to the 25%
crop margin, training samples frequently include microphones, hand gestures (V-signs), and smartphones.


Recall when detailing class-specific performance
in our confusion matrices, while using Relative
Accuracy as the primary global metric.


**5** **Results**


**5.1** **Data Preprocessing**


The MTCNN-based preprocessing pipeline demonstrated robust performance across the majority of
the KoIn10 dataset. However, visual inspection
revealed specific characteristics and challenges introduced by the preprocessing strategy.


**Contextual Artifacts from Cropping.** To capture the holistic visual identity of the subjects, we
applied a crop with a 25% additional margin. This
decision was driven by the need to retain stylistic
attributes—particularly distinctive hairstyles and
headwear—which are integral to K-pop idol recognition. However, this wider field of view inevitably
introduced non-facial artifacts common in K-pop
photography. As shown in Figure 3, samples frequently contain characteristic hand gestures (e.g.,
**V-signs** ), microphones, and smartphones. These
artifacts pose a challenge for the generative model,
which must learn to disentangle facial features from
these transient objects.


Figure 4: **Expression Variance and Hard Cases.** Examples of exaggerated expressions (winking) and heavy
occlusions (masks, hats) that act as distributional outliers for the generative model.


**Expression Variance and Hard Cases.** While
the MTCNN detector successfully retrieved these
samples, they represent significant distributional
outliers that challenge the generative model. As
illustrated in Figure 4, K-pop idols frequently perform exaggerated facial expressions or are captured at extreme angles. Additionally, consistent
with the dataset’s definition of "hard cases" (Seo
et al., 2023), many samples feature heavy occlusions such as masks, hats, or sunglasses. These
high-variance samples introduce complex geometric deformations that deviate from standard facial
priors.


**Target Ambiguity and False Positives.** A critical challenge arose from the "in-the-wild" nature
of the dataset, particularly regarding stage backgrounds and group shots (Seo et al., 2023). We
observed two distinct failure modes in the preprocessing stage:


1. **Identity Mismatch:** In scenes featuring multiple subjects, the MTCNN detector occasionally captured a non-target face. Common instances included fellow group members, show
hosts, or collaborating artists (Figure 5). This
results in the generative model receiving a
valid face signal but the _wrong_ identity label.


2. **Non-facial False Positives:** The detector
occasionally flagged non-human objects as
faces (Figure 6). We observed three main
sub-categories of such errors: (i) **Anthro-**
**pomorphic representations**, including character statues, mascots, or 2D illustrations
(e.g., drawings of faces); (ii) **Pareidolia ef-**
**fects**, where geometric arrangements in clothing folds or random background patterns accidentally resembled facial landmarks; and
(iii) **Spurious detections**, where the model
cropped unrecognizable background textures
or clutter that held no visual resemblance to a
face.



Figure 5: **Identity Mismatch.** Left: Source image
(Ground Truth: Class "Karina"). Right: The detector inadvertently prioritized the adjacent co-star ("Heechul"),
resulting in a training sample labeled "Karina" that visually depicts a male face.


Figure 6: **Non-facial False Positives.** The detector erroneously flags non-human regions as faces, including
anthropomorphic props, pareidolia effects (patterns resembling faces), and spurious background noise.


Both scenarios introduce explicit label noise into
the training data, forcing the DDPM to occasionally
learn irrelevant objects or incorrect identities under
a specific class condition.


**5.2** **Classifier Benchmarking and Selection**


To validate the semantic consistency of our generative models, we required a robust "ground truth"
classifier capable of distinguishing between the 10
K-Pop identities. Initially, we considered employing the deeper ResNet-50 architecture, anticipating that its high parameter count would be necessary for capturing fine-grained facial features.
However, given that our generative experiments
were constrained to a scaled-down resolution of
32 _×_ 32 pixels, the massive capacity of ResNet-50
proved unnecessary and risked overfitting on such
low-dimensional inputs. Consequently, we pivoted
to implementing and benchmarking the more efficient ResNet-18 and ResNet-34 architectures.Both
models were adapted for the 32 _×_ 32 input size
by modifying the initial convolutional layer and
removing the aggressive max-pooling operations
to preserve spatial information. All models were
trained under identical protocols, consisting of a
frozen-backbone warm-up phase followed by fullnetwork fine-tuning.


Table 1: **Performance Comparison of Candidate Classifiers (** 32 _×_ 32 **Resolution).** The ResNet-34 model
demonstrates superior accuracy and loss metrics compared to the ResNet-18 baseline.


**Metric** **ResNet-18 (Baseline)** **ResNet-34 (Selected Oracle)**


Test Accuracy 74.4% **76.8%**
Test Loss 0.9438 **0.8834**
Macro F1-Score 0.74 **0.77**
Best Class F1 0.86 (Class 1) 0.85 (Class 1)
Worst Class F1 0.61 (Class 5) **0.69 (Class 5)**



**5.2.1** **Quantitative Analysis**

Our experiments demonstrated that **ResNet-34** offered the optimal balance between model complexity and feature extraction capability for this specific resolution. As detailed in Table 1, ResNet-34
achieved a **Top-1 Accuracy of 76.8%**, outperforming the shallower ResNet-18 (74.4%) by a margin
of 2.4%. Furthermore, ResNet-34 exhibited a significantly lower test loss (0.8834 vs. 0.9438), indicating superior calibration and generalization on
the unseen test set. Based on these quantitative
results, ResNet-34 was selected as the oracle for
evaluating the semantic consistency of the DDPM
outputs.


**5.2.2** **Class-Specific Performance (ResNet-34)**

The selected ResNet-34 oracle demonstrated robust
performance across most identities, achieving a
weighted average F1-score of 0.77. As shown in
the confusion matrix (Figure 7), the model’s errors
were not uniformly distributed.


  - **Best Performing Class:** Class 0001 was identified with high reliability (Precision: 0.81,
Recall: 0.90), suggesting this subject possesses distinct facial markers that are resilient
to low-resolution artifacts.


  - **Challenging Classes:** The model struggled
most with Class 0005 (F1-score: 0.69) and
Class 0007 (Recall: 0.67). The confusion matrix reveals that misclassifications for these
classes were often distributed among visually
similar peers rather than a single confusing
class, highlighting the inherent difficulty of
fine-grained recognition at 32 _×_ 32 resolution.


**5.2.3** **Confusion Matrix**

The confusion matrix confirms the model’s stability,
showing a strong diagonal. However, specific error
modes are visible; for instance, Class 0003 was



frequently confused with Class 0006 (13 misclassified instances). This specific ambiguity provides
critical context for our generative evaluation: if the
DDPM generates images for Class 0003 that look
like Class 0006, it may be reflecting this underlying dataset ambiguity rather than a failure of the
generative process itself.


**5.3** **Image Generation Analysis**


**5.3.1** **Visual Quality and Diversity**

We first evaluate the generated samples using standard metrics to assess visual fidelity and diversity.


**Fréchet Inception Distance (FID).** Our model
achieved a global FID score of **8.9294** . In the context of 32 _×_ 32 face generation, this low score indicates that the DDPM has successfully learned the
data distribution and generates images with high
perceptual quality that are statistically similar to
the real dataset.


**Inception Score (IS).** Conversely, the model
yielded an Inception Score of **1.0652** . As hypothesized in our introduction, this score is near the
theoretical minimum ( _≈_ 1 _._ 0). This confirms that
IS is an unsuitable metric for fine-grained, singledomain datasets (such as K-pop faces), as the
ImageNet-pretrained Inception network cannot discriminate between specific identities, treating all
samples as a single "face" class.


**LPIPS Diversity.** To ensure the model is not simply memorizing training data (mode collapse), we
calculated the Per-Class LPIPS diversity. For each
identity class, we randomly sampled 1,000 pairs
of generated images and computed their pairwise
perceptual distances. The resulting average score
of **0.4719** indicates a healthy degree of variation
within each class. As detailed in Table 3, diversity
is relatively consistent across identities, ranging
from a minimum of 0.4219 (Class 2) to a maximum of 0.5232 (Class 5), suggesting that the model


Table 2: **Confusion Matrix for ResNet-34 Oracle.** The rows represent the actual identity classes, and the columns
represent the predicted classes. Strong diagonal values indicate correct classifications.


**Actual** **Predicted Class**
**Class** **0** **1** **2** **3** **4** **5** **6** **7** **8** **9**


**0** **77** 2 4 3 0 2 2 7 2 1
**1** 1 **87** 1 0 0 2 0 4 2 0
**2** 7 6 **68** 2 0 5 0 3 7 0
**3** 2 3 2 **70** 1 2 13 2 0 3
**4** 4 1 0 9 **68** 7 6 0 1 4
**5** 0 0 0 2 10 **66** 4 1 2 8
**6** 1 0 0 3 0 6 **84** 0 1 3
**7** 9 6 7 3 0 0 0 **67** 6 2
**8** 3 3 0 1 0 2 1 2 **84** 2
**9** 1 0 0 3 1 5 2 4 0 **82**



avoids generating identical samples even for difficult classes.


Table 3: Per-Class LPIPS Diversity Scores (Pairwise).
Higher values indicate greater diversity within the generated samples of that class. The consistent scores across
classes suggest the model avoids partial mode collapse.


**Class ID** **LPIPS Diversity** _↑_


0 0.4428
1 0.4713
2 0.4219
3 0.5008
4 0.4993
5 **0.5232**
6 0.4697
7 0.4485
8 0.4373
9 0.5039


**Average** **0.4719**


**5.4** **Semantic Consistency Analysis (RCA)**


While visual metrics confirm image fidelity, they
do not verify identity retention. To rigorously quantify this, we utilize a ResNet-34 Oracle Classifier
to compare the semantic discriminability of real
versus generated faces. We extend the evaluation
beyond simple accuracy to include **Precision**, **Re-**
**call**, and **F1-score**, providing a granular view of
the model’s failure modes.


**Oracle Performance on Real Data (Baseline).**
As shown in Table 4 (Real columns), the Oracle
classifier achieves a robust performance on the test



set with an Overall Accuracy of 0.77. Classes are
recognized with high consistency, with F1-scores
ranging from 0.69 (Class 5) to 0.85 (Class 1), establishing a reliable upper bound for identity verification.


**Generative Performance and RCA.** We evaluated the model on 50,000 generated samples (5,000
per class). The model achieved an Overall Accuracy of 0.2077. Based on this, the global Relative
Classification Accuracy is 0.27. This indicates that
the diffusion model preserves approximately 27%
of the semantic identity information relative to the
real data distribution.


**Class-wise Performance.** Breaking down the
performance by class (Table 4) reveals significant
variance in identity preservation. **Class 1** achieved
the highest consistency with a Class RCA of **0.55**,
indicating that the model successfully recovered
55% of the semantic features relative to the real
baseline. Conversely, **Class 2** demonstrated the
lowest performance, with a Class RCA of **0.02**
(Generated Recall: 1.5%), indicating a significant
gap in identity retention for this specific class.


**Error Topology and Class Confusion.** To further investigate the nature of these failures, we visualize the Confusion Matrix of the 50,000 generated
samples in Figure 7. The matrix reveals that the
errors are not uniformly distributed; instead, they
exhibit a strong **directional collapse** .


**6** **Discussion**


Our analysis of the DDPM’s performance reveals
a complex landscape where quantitative metrics


Table 4: Detailed Class-wise Semantic Consistency


**Real Data Baseline** **Generated Data** **Normalized Metric**
**Class** Precision **Recall** F1 Precision **Recall** F1 **Class RCA**


0 0.73 0.77 0.75 0.18 0.38 0.25 0.50
1 0.81 0.90 0.85 0.20 0.50 0.29 **0.55**
2 0.83 0.69 0.76 0.31 0.02 0.03 0.02
3 0.73 0.71 0.72 0.16 0.14 0.15 0.19
4 0.85 0.68 0.76 0.32 0.36 0.34 0.53
5 0.68 0.71 0.69 0.14 0.21 0.17 0.30
6 0.75 0.86 0.80 0.24 0.14 0.18 0.17
7 0.74 0.67 0.71 0.48 0.15 0.23 0.23
8 0.80 0.86 0.83 0.13 0.07 0.09 0.08
9 0.78 0.84 0.81 0.22 0.11 0.15 0.13


**Global Metric** **Accuracy: 0.770** **Accuracy: 0.208** **Global RCA: 0.27**


often diverge from qualitative expectations. While
the Relative Classification Accuracy (RCA) of
20.77% indicates a general difficulty in preserving semantic identity, the error modes are not random. We identify three primary factors driving
these results: resolution constraints, intra-gender
ambiguity, and class-specific bias.


(a) Loss of Fine Detail


**6.1** **The Bottleneck of Low Resolution**
**(** 32 _×_ 32 **)**


(b) Generic Features



The most significant constraint on semantic consistency was the input resolution. At 32 _×_ 32 pixels,
the distinct high-frequency features required to differentiate between K-Pop idols—such as precise
eye shape, skin texture, or subtle facial landmarks—
are severely compressed or lost entirely. Consequently, the generative model tends to learn a
"mean face" distribution that captures the general
global structure of a human face but fails to resolve
the fine-grained details necessary for identity verification. This observation aligns with the behavior
of our ResNet-34 oracle, which, even on real data,
struggled most with classes lacking distinct global
features (e.g., Class 5), suggesting that at this resolution, identity often becomes indistinguishable
from generic facial attributes.



Figure 9: **Visualizing the Resolution Bottleneck.** At
32 _×_ 32 resolution, critical high-frequency identity markers are lost.


**6.2** **Intra-Gender Misclassification**


A close examination of the confusion matrix reveals that misclassifications were largely clustered
within specific subgroups, primarily driven by
same-gender similarity. The model errors occurred
between subjects sharing similar hairstyles, face
shapes, or gender presentation. For example, the
misclassification of Class 2 as Class 1 suggests that
the DDPM successfully captured the _attributes_ of
the demographic (e.g., "young female with long
dark hair") but lacked the capacity to disentangle


Figure 7: **Confusion Matrix of Generated Samples.**


the specific _identity_ within that demographic. This
implies that the model’s latent space is organized
hierarchically, learning broad semantic categories
(gender/style) first, while fine-grained identity separation remains a higher-order challenge that requires greater capacity or resolution to solve.


(a) Generated Image (Misclassified)


(b) Training Sample (Target Class)


Figure 10: **Intra-Gender Ambiguity.** The generated
image (a) shares significant visual attributes—such as
hair length and facial structure—with the ground truth
class (b), leading to confusion despite belonging to a
different identity.


**6.3** **Class Bias and Mode Dominance**


The evaluation highlighted a strong bias toward
specific classes, particularly **Class 0** and **Class 1**,
which exhibited significantly higher recall (38.3%
and 49.8% respectively) compared to the population mean. This indicates a form of "partial mode
collapse," where the generative process preferentially converges onto the manifolds of these specific identities. It is hypothesized that these classes
possess highly distinct visual markers—such as a
unique pose, background consistency, or distinct
accessory—that are easier for the U-Net to reconstruct than subtle facial features. As a result, when
the model is uncertain during the reverse diffusion process, it gravitates toward these "stronger"
modes, leading to a disproportionate number of
generated images being classified as Class 0 or 1,
even when conditioned on other labels.


**7** **Conclusion and Future Work**


In this work, we presented a framework for evaluating the semantic consistency of Class-Conditional
DDPMs on fine-grained K-pop face generation.
Our experiments demonstrated a critical divergence
between visual fidelity and identity preservation:
while the model achieved a competitive FID score
of 8.93, our proposed **Relative Classification Ac-**
**curacy (RCA)** revealed that only 27% of the generated samples retained their intended semantic
identity. This finding underscores the inadequacy
of standard metrics like Inception Score for singledomain tasks and validates RCA as a more rigorous,
calibrated standard for conditional generation.
Our analysis attributes the current performance
bottlenecks primarily to resolution constraints and
intra-gender ambiguity. To address these limitations, future research will focus on two key directions. First, to mitigate the observed confusion between visually similar subjects (e.g., intra-gender
misclassification), we propose integrating metriclearning objectives such as **ArcFace loss** . This
would enforce larger angular margins between identity embeddings, encouraging the model to learn
more discriminative features beyond generic attributes. Second, to overcome the information
loss at 32 _×_ 32 resolution, we aim to implement
a **Cascaded Diffusion Model**, employing superresolution stages to upscale outputs to higher resolutions. This multi-stage approach is essential to recover the high-frequency details—such as precise
eye shape and skin texture—required for robust



identity verification.


**References**


[Berk Birdal. 2023. conditional-ddpm.](https://github.com/byrkbrk/conditional-ddpm)


[Iván de Paz Centeno. 2024. ipazc/mtcnn: v1.0.0.](https://doi.org/10.5281/zenodo.13901378)


Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
[Bernhard Nessler, and Sepp Hochreiter. 2018. Gans](http://arxiv.org/abs/1706.08500)
[trained by a two time-scale update rule converge to a](http://arxiv.org/abs/1706.08500)
[local nash equilibrium.](http://arxiv.org/abs/1706.08500)


Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In _Proceed-_
_ings of the 34th International Conference on Neu-_
_ral Information Processing Systems_, NIPS ’20, Red
Hook, NY, USA. Curran Associates Inc.


[Suman Ravuri and Oriol Vinyals. 2019. Classification](http://arxiv.org/abs/1905.10887)
[accuracy score for conditional generative models.](http://arxiv.org/abs/1905.10887)


Jinwoo Seo, Soora Choi, Eungyeom Ha, Beomjune
Kim, and Dongbin Na. 2023. New benchmarks
for asian facial recognition tasks: Face classification with large foundation models. _arXiv preprint_
_arXiv:2310.09756_ .


Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
[Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-](http://arxiv.org/abs/1904.09675)
[ating text generation with bert.](http://arxiv.org/abs/1904.09675)


