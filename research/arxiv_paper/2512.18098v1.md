## A Games-in-Games Paradigm for Strategic Hybrid Jump-Diffusions: Hamilton-Jacobi-Isaacs Hierarchy and Spectral Structure

Yunian Pan & Quanyan Zhu
ECE Department
New York University
Brooklyn, NY
_{_ `yp1170, qz494` _}_ `@nyu.edu`


**ABSTRACT**


This paper develops a hierarchical games-in-games control architecture for hybrid stochastic systems
governed by regime-switching jump-diffusions. We model the interplay between continuous state
dynamics and discrete mode transitions as a bilevel differential game: an inner layer solves a robust
stochastic control problem within each regime, while a strategic outer layer modulates the transition
intensities of the underlying Markov chain. A Dynkin-based analysis yields a system of coupled
Hamilton-Jacobi-Isaacs (HJI) equations. We prove that for the class of Linear-Quadratic games and
Exponential-Affine games, this hierarchy admits tractable semi-closed form solutions via coupled
matrix differential equations. The framework is demonstrated through a case study on adversarial
market microstructure, showing how the outer layer’s strategic switching pre-emptively adjusts
inventory spreads against latent regime risks, which leads to a hyper-alert equilibrium.


**1** **Introduction**


Hybrid systems, characterized by the interplay between continuous state dynamics and discrete event transitions,
constitute a fundamental modeling paradigm for complex socio-economic engineering systems. Within this broad class,
_regime-switching jump-diffusions_ occupy a central role, capturing systems where continuous stochastic trajectories
are modulated by a hidden or observable Markov chain. Applications range from fault-tolerant control in networked
systems [21, 17, 18] and cyber-physical security [22, 19, 16] to economic systems [14].


In these settings, decision-making is rarely monolithic. It operates hierarchically: fast-timescale controllers regulate the
continuous state (e.g., stabilizing a plant or hedging a portfolio), while slow-timescale policies influence the discrete
operating modes (e.g., system reconfiguration or regime induction). However, classical literature typically decouples
these layers. The theory of _Piecewise-Deterministic Markov Processes_ (PDMPs) [11, 9] and switching diffusions

[21] generally treats the regime transition mechanism as either exogenous (governed by nature) or subject to a single
controller’s optimization (optimal switching) [12].


A critical gap exists in modeling _adversarial hybrid interactions_, where the regime transitions themselves are the
outcome of a strategic game. For instance, in cyber-physical systems, an attacker may seek to destabilize the system
by inducing transitions to vulnerable modes [8], while a defender attempts to harden the transition logic. Existing
differential game theory [6, 7] provides robust tools for the continuous layer but typically assumes a fixed or purely
stochastic discrete structure. Conversely, impulse games [1] focus on discrete interventions but often abstract away the
continuous-time feedback loops.


This paper bridges this gap by developing a hierarchical _Games-in-Games_ control architecture for regime-switching
jump-diffusions. We construct a bilevel system where a fast inner layer solves a robust stochastic differential game
within each mode, while a strategic outer layer actively modulates the _transition intensity kernel_ of the underlying
Markov chain. This structure, illustrated in Figure 1, formalizes the problem of _strategic regime control_, applicable to
scenarios ranging from adversarial market microstructure to multi-modal resilient control.


Our contributions are threefold:


1. We provide a unified formulation for the games-in-games architecture on jump-diffusion spaces. By leveraging
a unified Dynkin formula for switching diffusions [21], we decompose the bilevel problem into a hierarchy of


1


Figure 1: Compact Games-in-Games diagram: macro players shape regime switching; micro controls act on continuous
dynamics under the active regime.


coupled Hamilton-Jacobi-Isaacs (HJI) equations. This separates the continuous control (inner Isaacs equation)
from the strategic switching (outer Hamilton-Jacobi equation) without circular dependencies.


2. While general HJI equations are computationally demanding, we prove that for the class of Linear-Quadratic
(LQ) and CARA-type exponential transformations (as in the market microstructure case study), the hierarchy
collapses into a system of _coupled matrix differential equations_ under certain conditions. This extends the
classical coupled Riccati theory [13, 10] to the game-theoretic setting with endogenous transition rates.


3. We demonstrate the framework’s efficacy through an inventory game case study on adversarial market making.
We derive a risk isomorphism principle that allows us to characterize equilibrium policies that pre-emptively
adjust control gains (inventory spreads) based on the stability gap between regimes.


The remainder of this paper is organized as follows. Section 2 formulates the two-layer hybrid game. Section 3 derives
the viscosity solution hierarchy and the Dynkin transformation. Section 4 presents the spectral solution for the LQ case.
Section 5 details the market microstructure application, and Section 6 concludes.


**2** **Problem Formulation**


We consider a hybrid decision architecture in which a continuous state evolves according to mode-dependent stochastic
dynamics, while a discrete mode process switches between a finite collection of regimes. Two layers of strategic
decision-making interact: a fast-timescale controller/disturbance pair regulating the continuous state, and a slowtimescale pair of agents whose actions influence the transition rates among the discrete modes. This structure captures
a wide range of multi-layer hybrid systems, including resilient infrastructure networks, multi-agent cyber-physical
systems, and robust control under regime uncertainty.


Let (Ω _, F,_ P) be a complete probability space equipped with a filtration F = ( _Ft_ ) _t≥_ 0 satisfying the usual conditions of
right-continuity and completeness. The time horizon is finite, _T < ∞_ . We define the following primitive stochastic
processes adapted to F:


1. _W_ = ( _Wt_ ) _t≥_ 0 is a standard _d_ -dimensional Brownian motion.


2


2. _N_ ( _dt, dz_ ) is a Poisson random measure on [0 _, T_ ] _× Z_ with intensity measure _λ_ ( _dz_ ) _dt_, where _Z ⊆_ R _[k]_ . We
denote the compensated measure by _N_ [˜] ( _dt, dz_ ) = _N_ ( _dt, dz_ ) _−_ _λ_ ( _dz_ ) _dt_ .


**Definition 1** (Two-layer hybrid decision system) **.** _Let I_ = _{_ 1 _, . . ., N_ _} be a finite set of regimes (modes), U and_
_W compact convex sets representing continuous-layer control and disturbance actions, and AD, AA finite sets of_
_actions available to the two agents governing the mode transitions. A_ two-layer hybrid system _is a tuple_ Γ =

- _X_ _, I, V, W, F, G_ - _, where:_


_1. X ⊆_ R _[n]_ _is the continuous state space and I is the finite mode space;_


_2. The continuous-layer policies_


_ν_ : [0 _, T_ ] _× X × I →_ _U,_ _ω_ : [0 _, T_ ] _× X × I →_ _W_


_are Borel measurable and_ F _-progressively measurable; the associated admissible policy classes are denoted V_
_and W;_


_3. The mode-selection policies_


_f_ : [0 _, T_ ] _× X × I →_ ∆( _AD_ ) _,_ _g_ : [0 _, T_ ] _× X × I →_ ∆( _AA_ )


_assign mixed actions from AD and AA. The admissible classes are denoted F and G._


_Given_ ( _f, g_ ) _and_ ( _ν, ω_ ) _, the hybrid state process_ ( _Xt, It_ ) _t∈_ [0 _,T_ ] _evolves as a_ Regime-Switching Jump-Diffusion _defined_
_by:_


_(a) Continuous dynamics: Between jumps of the mode It, the continuous state Xt evolves according to the_
_stochastic differential equation (SDE):_


_dXt_ = _fp_               - _t, Xt, νt, ωt_ ; _It_               - _dt_ + _σ_               - _t, Xt_ ; _It_               - _dWt_


                  + _ρ_ ( _t, Xt, z_ ; _It_ ) _N_ [˜] ( _dt, dz_ ) _,_ (1)

_Z_


_with X_ 0 = _x_ 0 _∈X_ _. Here, νt_ = _ν_ ( _t, Xt, It_ ) _and ωt_ = _ω_ ( _t, Xt, It_ ) _._


_(b) Discrete dynamics: The mode process It ∈I is a controlled continuous-time Markov chain with generator_
_matrix_ Π _t_ = [ _µij_ ( _t_ )] _i,j∈I modulated by the outer policies:_


P� _It_ + _dt_ = _j | It_ = _i, Xt_ = _x_           - = _µij_           - _f_ ( _t, x, i_ ) _, g_ ( _t, x, i_ )� _dt_ + _o_ ( _dt_ ) _,_ (2)


_where µii_ = _−_ [�] _j_ = _i_ _[µ][ij][. The mode-selection policies]_ [ (] _[f, g]_ [)] _[ determine the regime transition intensities]_

_µij_ : ∆( _AD_ ) _×_ ∆( _AA_ ) _→_ [0 _, ∞_ ) _, yielding the instantaneous rate µij_     - _f_ ( _t, Xt, It_ ) _, g_ ( _t, Xt, It_ )� _at each t._


_Above defines a measure_ P _[ν,ω,f,g]_ _on the path space D_ ([0 _, T_ ]; _X × I_ ) _._


The performance of a policy tuple is evaluated through the cost functional:




      
_J_ ( _f, g_ ; _ν, ω_ ) = E




      - _T_
_qf_ - _XT, IT_ - +



_c_  - _t, Xt, νt, ωt, It_  - _dt_
0



_._ (3)







The two-layer decision architecture is expressed as the bi-level optimization problem


min
_f_ _∈F_ [max] _g∈G_ [Φ (] _[f, g]_ [;] _[ ν][∗][, ω][∗]_ [)]

(4)
s.t. _ν_ _[∗]_ _, ω_ _[∗]_ _∈_ inf _J_ ( _f, g_ ; _ν, ω_ )
_ν∈V_ _ω_ [sup] _∈W_


where the inner minimax determines the continuous-layer value and the outer minimax governs mode manipulation.
The functional Φ ( _·, ·_ ; _ν_ _[∗]_ _, ω_ _[∗]_ ) : ∆( _AD_ ) _×_ ∆( _AA_ ) _→_ R represents the outer-layer preference and will be specified
later.


**Assumption 1** (Standing assumptions) **.**


3


_(i) (_ Generalized Isaacs Condition _) For each fixed mode i ∈I and any smooth test function ϕ ∈_ _C_ [2] ( _X_ ) _, we define_
_the generalized Hamiltonian Hi_ [ _ϕ_ ] _acting on the state x, gradient p_ = _∇ϕ_ ( _x_ ) _, Hessian M_ = _∇_ [2] _ϕ_ ( _x_ ) _, and_
_controls_ ( _u, w_ ) _:_


_Hi_ [ _ϕ_ ]( _t, x, p, M, u, w_ ) = _c_ ( _t, x, u, w, i_ ) + _p_ _[⊤]_ _fp_ ( _t, x, u, w_ ; _i_ )




 +

_Z_




            -             
+ [1] _σ_ ( _t, x_ ; _i_ ) _σ_ ( _t, x_ ; _i_ ) _[⊤]_ _M_

2 [Tr]


- _ϕ_ ( _x_ + _ρ_ ( _t, x, z_ ; _i_ )) _−_ _ϕ_ ( _x_ ) _−_ _p_ _[⊤]_ _ρ_ ( _t, x, z_ ; _i_ ) _ν_ ( _dz_ ; _u, w_ ) _._



_We assume that the minimax condition holds for all valid inputs:_


min
_u∈U_ _w_ [max] _∈W_ _[H][i]_ [[] _[ϕ]_ [](] _[t, x, p, M, u, w]_ [) = max] _w∈W_ [min] _u∈U_ _[H][i]_ [[] _[ϕ]_ [](] _[t, x, p, M, u, w]_ [)] _[.]_


_(ii) The coefficients fp, σ, ρ satisfy the standard Lipschitz and linear growth conditions in x, uniformly in_ ( _u, w_ ) _._
_The measure ν_ ( _dz_ ; _u, w_ ) _is a bounded kernel satisfying appropriate integrability conditions. The costs c, qf_
_satisfy quadratic growth conditions._


**Lemma 1** (Existence and Estimates) **.** _Under Assumption 1, for any admissible policies, the hybrid system_ (1) _–_ (2)
_admits a unique strong solution_ ( _Xt, It_ ) _t∈_ [0 _,T_ ] _. Furthermore, for any p ≥_ 1 _, there exists a constant Cp >_ 0 _such that:_








     sup _∥Xt∥_ _[p]_
_t∈_ [0 _,T_ ]



E



_≤_ _Cp_ �1 + _∥x_ 0 _∥_ _[p]_ [�] _._



_Proof._ Since the transition rates are bounded, the mode process _It_ undergoes finitely many jumps in [0 _, T_ ] almost
surely. Between any two jump times, the system evolves as a standard SDE with jumps. Under the Lipschitz and linear
growth conditions (Assumption 1), a unique strong solution exists for each interval (see e.g., [2, 15]). We construct the
global solution ( _Xt_ ) _t≥_ 0 by concatenating these trajectory segments at the jump times of _It_ .

We apply Ito’s formula to the functionˆ _ϕ_ ( _x_ ) = _∥x∥_ _[p]_ . The linear growth assumption on the drift _fp_ and jump intensity
implies that the generator is bounded by _Lϕ_ ( _x_ ) _≤_ _C_ (1 + _∥x∥_ _[p]_ ). To bound the expectation of the supremum, we
handle the martingale terms (diffusion and compensated jumps) using the Burkholder-Davis-Gundy (BDG) inequalities,
which control the maximum of the stochastic integrals. Combining these bounds yields an integral inequality for
_g_ ( _t_ ) = E[sup _s≤t ∥Xs∥_ _[p]_ ]. The final result follows immediately from Gr¨onwall’s inequality.


**3** **Cross-Layer Viscosity Solution**


Once the hybrid architecture Γ in Definition 1 is equipped with Assumption 1, the coupled state process ( _Xt, It_ ) is
a Regime-Switching Jump-Diffusion. Between jumps of the discrete mode _It_, the continuous state _Xt_ follows the
stochastic evolution generated by the drift _fp_, diffusion _σ_, and jump measure defined in (1). For any probe function


_ϕ_ : _X × I ×_ [0 _, T_ ] _→_ R _,_ _ϕ_ ( _·, i, ·_ ) _∈_ _C_ [2] _[,]_ [1] ( _X ×_ [0 _, T_ ]) _,_


the infinitesimal generator of ( _Xt, It_ ) under continuous-layer policies ( _ν, ω_ ) and mode-selection policies ( _f, g_ ) is given
by the sum of the diffusion generator, the inner jump generator, and the regime-switching operator:


( _L_ _[f,g,ν,ω]_ _ϕ_ )( _t, x, i_ ) = _[∂ϕ]_          - _t, x, ν, ω_ ; _i_          
_∂t_ [(] _[t, x, i]_ [) +] _[ ∇][x][ϕ]_ [(] _[t, x, i]_ [)] _[⊤][f][p]_




 +

_Z_



+ [1]  - _σ_ ( _t, x_ ; _i_ ) _σ_ ( _t, x_ ; _i_ ) _[⊤]_ _∇_ [2] _xx_ _[ϕ]_ [(] _[t, x, i]_ [)]  
2 [Tr]


- _ϕ_ ( _t, x_ + _ρ_ ( _t, x, z_ ; _i_ ) _, i_ ) _−_ _ϕ_ ( _t, x, i_ ) _−∇xϕ_ _[⊤]_ _ρ_ ( _t, x, z_ ; _i_ ) _ν_ ( _dz_ )



(5)



+           - _µij_           - _f_ ( _t, x, i_ ) _, g_ ( _t, x, i_ )�� _ϕ_ ( _t, x, j_ ) _−_ _ϕ_ ( _t, x, i_ )� _._


_j_ = _i_


Because ( _f, g_ ) may depend explicitly on time and on the instantaneous continuous state, the generator (5) captures the
full state-time dependence of the mode-switching rates.


4


**Lemma 2** (Dynkin identity) **.** _Under Assumption 1 and fixed admissible policies_ ( _f, g, ν, ω_ ) _, the process_




              - _τ_
_Mϕ_ ( _τ_ ) = _ϕ_ - _τ, Xτ_ _, Iτ_ - _−_ _ϕ_ ( _t, x, i_ ) _−_

_t_




- _L_ _[f,g,ν,ω]_ _ϕ_ �( _s, Xs, Is_ ) _ds_



_is a local martingale. If ϕ and its derivatives are bounded, it is a martingale for any bounded stopping time τ ≤_ _T_ _._
_Consequently,_




            - _τ_
E� _ϕ_ ( _τ, Xτ_ _, Iτ_ )� = _ϕ_ ( _t, x, i_ ) + E




- _L_ _[f,g,ν,ω]_ _ϕ_ �( _s, Xs, Is_ ) _ds._ (6)



_t_


_Proof._ Let ( _Tk_ ) _k≥_ 0 denote the jump times of the mode process _It_ with _T_ 0 = _t_ . On each random interval [ _Tk, Tk_ +1),
the mode _It_ = _i_ remains constant. The evolution of _ϕ_ ( _t, Xt, i_ ) is governed by Ito’s formula for semimartingales withˆ
jumps [2, Thm 4.4.7]:


                       - _Tk_ +1
_ϕ_ ( _Tk_ _[−]_ +1 _[, X]_ _Tk_ _[−]_ +1 _[, i]_ [)] _[ −]_ _[ϕ]_ [(] _[T][k][, X][T][k]_ _[, i]_ [) =] _Linnerϕ ds_ + _Mk,k_ +1 _,_

_Tk_

where _Linner_ represents the continuous drift, diffusion, and inner jump parts of the generator, and _Mk,k_ +1 collects
the stochastic integrals with respect to _dWt_ and _N_ [˜] ( _dt, dz_ ), which are zero-mean martingales under the boundedness
assumptions.


At the jump time _Tk_ +1, the mode switches from _i_ to _j_ with intensity _µij_ . The compensator for this discrete transition is
exactly the regime-switching sum in (5). Summing these contributions over all intervals up to _τ_ and taking expectations
eliminates the martingale terms, yielding the claimed identity.


Lemma 2 connects the stochastic dynamics in Definition 1 to the value functions developed below. It justifies
infinitesimal expansions of probe functions and supports the viscosity-solution formulations of the coupled HamiltonJacobi-Isaacs (HJI) equations [20].


**3.1** **Inner-Layer Hamilton-Jacobi-Isaacs Equation**


Definition 1 shows that once the mode-selection policies ( _f, g_ ) are frozen, the continuous-layer control and disturbance
interact through a zero-sum stochastic differential game. The state evolves as a controlled Jump-Diffusion, coupled
with mode-dependent switching intensities.


For fixed ( _f, g_ ), define the inner-layer value functions by



�� _T_
_Vi_ ( _x, t_ ; _f, g_ ) = inf E _c_ - _s, Xs, νs, ωs, Is_ - _ds_ + _cT_ - _XT, IT_ ��� _Ft_
_ν∈V_ _ω_ [sup] _∈W_ _t_




_._ (7)



**Lemma 3** (Inner-layer HJI) **.** _For fixed mode-selection policies_ ( _f, g_ ) _, the family of inner-layer value functions_
_{Vi_ ( _·, ·_ ; _f, g_ ) _}i∈I is the unique viscosity solution (with appropriate growth conditions) of the following system of_
_Partial Integro-Differential Equations (PIDE):_



_−∂tVi_ ( _t, x_ ) = min
_u∈U_ _w_ [max] _∈W_




_c_ ( _t, x, u, w, i_ ) + _L_ _[u,w]_ _i_ _Vi_ ( _t, x_ )




 - - �� - [�]
+ _µij_ _f_ ( _t, x, i_ ) _, g_ ( _t, x, i_ ) _Vj_ ( _t, x_ ) _−_ _Vi_ ( _t, x_ ) _,_


_j_ = _i_



(8)



_Vi_ ( _T, x_ ) = _cT_ ( _x, i_ ) _,_

_for all_ ( _t, x_ ) _∈_ [0 _, T_ ] _× X_ _. Here, L_ _[u,w]_ _i_ _is the local integro-differential operator:_

_L_ _[u,w]_ _i_ _ϕ_ ( _x_ ) = _∇xϕ_ ( _x_ ) _[⊤]_ _fp_ ( _t, x, u, w_ ; _i_ ) + 2 [1] [Tr]       - _σ_ ( _t, x_ ; _i_ ) _σ_ ( _t, x_ ; _i_ ) _[⊤]_ _∇_ [2] _xx_ _[ϕ]_ [(] _[x]_ [)]       



 +

_Z_




- _ϕ_ ( _x_ + _ρ_ ( _t, x, z_ ; _i_ )) _−_ _ϕ_ ( _x_ ) _−∇xϕ_ ( _x_ ) _[⊤]_ _ρ_ ( _t, x, z_ ; _i_ ) _ν_ ( _dz_ ) _._



_Proof._ We proceed in two steps: first establishing that the value function _Vi_ is a viscosity solution (satisfying the
subsolution and supersolution properties), and second invoking a comparison principle for uniqueness.


We only demonstrate the subsolution property as the supersolution argument is symmetric. Let _ϕ_ ( _t, x_ ) _∈_ _C_ [1] _[,]_ [2] ([0 _, T_ ] _×_
_X_ ) be a smooth test function such that _Vi_ ( _t, x_ ) _−_ _ϕ_ ( _t, x_ ) achieves a local maximum at ( _t,_ [ˆ] ˆ _x_ ) with _Vi_ ( _t,_ [ˆ] ˆ _x_ ) = _ϕ_ ( _t,_ [ˆ] ˆ _x_ ).


5


From the Dynamic Programming Principle (DPP), for small _h >_ 0:



�� _t_ ˆ+ _h_







_Vi_ ( _t,_ [ˆ] ˆ _x_ ) _≤_ inf
_ν_ [sup] _ω_ [E]



_t_ ˆ _c_ ( _s, Xs, νs, ωs, i_ ) _ds_ + _VIt_ ˆ+ _h_ ( _t_ [ˆ] + _h, Xt_ ˆ+ _h_ )



_._



We decompose the expectation based on whether the mode _Is_ jumps during [ _t,_ [ˆ] _t_ [ˆ] + _h_ ]. With probability 1 _−_ _O_ ( _h_ ), no
jump occurs. In this case, _It_ ˆ+ _h_ = _i_ . Using _Vi ≤_ _ϕ_ and applying Ito’s formula toˆ _ϕ_ : E[ _ϕ_ ( _t_ [ˆ] + _h, Xt_ ˆ+ _h_ ) _−_ _ϕ_ ( _t,_ [ˆ] ˆ _x_ )] =

E - _t_ ˆ _t_ ˆ+ _h_ ( _∂tϕ_ + _L_ _[u,w]_ _i_ _ϕ_ ) _ds._ With probability rate _µij_ ( _f, g_ ), the mode switches to _j_ . The contribution to the expected value



change is dominated by the difference _Vj −Vi ≈_ _Vj −ϕ_, the jump contribution is then:� _t_ ˆ _t_ ˆ+ _h_ 


change is dominated by the difference _Vj −Vi ≈_ _Vj −ϕ_, the jump contribution is then:� _t_ ˆ - _j_ = _i_ _[µ][ij]_ [(] _[f, g]_ [)[] _[V][j]_ [(] _[s, X][s]_ [)] _[−]_

_ϕ_ ( _s, Xs_ )] _ds_ + _o_ ( _h_ ) _._


Substituting these expansions back into the DPP inequality and using _Vi_ ( _t,_ [ˆ] ˆ _x_ ) = _ϕ_ ( _t,_ [ˆ] ˆ _x_ ) to cancel the zero-order terms:



















 _c_ + _∂tϕ_ + _L_ _[u,w]_ _i_ _ϕ_ + 


_µij_ ( _f, g_ )[ _Vj −_ _ϕ_ ]

_j_ = _i_



0 _≤_ inf
_ν_ [sup] _ω_ [E]




 - _t_ ˆ+ _h_





_t_ ˆ



 _ds_



 + _o_ ( _h_ ) _._



Dividing by _h_ and letting _h ↓_ 0, the mean value theorem applies. Since the inequality holds for all controls, we obtain:

_−∂tϕ_ ( _t,_ [ˆ] ˆ _x_ ) _−_ inf _i_ _ϕ} −_        - _µij_ ( _f, g_ )[ _Vj_ ( _t,_ [ˆ] ˆ _x_ ) _−_ _Vi_ ( _t,_ [ˆ] ˆ _x_ )] _≤_ 0 _._
_u_ [sup] _w_ _[{][c]_ [ +] _[ L][u,w]_

_j_ = _i_


This confirms the viscosity subsolution condition. The supersolution argument is symmetric using a local minimum.


The system (8) is a system of coupled non-linear PIDEs. Under Assumption 1 (Lipschitz coefficients, quadratic growth),
the comparison principle for viscosity solutions of such systems holds [5, Thm 3.4]. Specifically, if _U_ is a subsolution
and _V_ is a supersolution with _U_ ( _T_ ) _≤_ _V_ ( _T_ ), then _U ≤_ _V_ on [0 _, T_ ]. Since our value function is both, it is the unique
solution.


The family _{Vi}_ encodes the inner-layer response to any fixed choice of mode-selection strategies ( _f, g_ ). In particular,
_Vi_ ( _x, t_ ; _f, g_ ) can be regarded as the effective performance index associated with starting in mode _i_ at state _x_ and time _t_,
factoring in the optimal continuous-time response to the induced regime uncertainty.


**3.2** **Outer-Layer Hamilton-Jacobi-Isaacs System**


We now return to the outer-level problem (4), in which the two mode-selection agents choose strategies ( _f, g_ ) that
influence the mode-transition dynamics while anticipating the optimal inner-layer responses captured by Lemma 3. We
interpret the outer-layer interaction as a _committed_ (Stackelberg-type) Markov game: at time _t_, the macro-agents choose
Markov (state-feedback) policies ( _f, g_ ) on [ _t, T_ ], anticipating that the micro-layer subsequently plays the induced inner
saddle-point feedback ( _ν_ _[⋆,f,g]_ _, ω_ _[⋆,f,g]_ ) associated with ( _f, g_ ).


For each fixed pair of mode-selection policies ( _f, g_ ), the inner-layer value functions _Vi_ ( _·, ·_ ; _f, g_ ) are determined by the
system (8). Let the optimal inner feedback policies be denoted by:
( _u_ _[∗][,f,g]_ _, w_ _[∗][,f,g]_ )( _t, x, i_ ) _∈_ arg min
_u∈U_ _w_ [max] _∈W_ _[H][i]_ [[] _[V][i]_ [](] _[t, x,][ ∇][x][V][i][,][ ∇]_ [2] _[V][i][, u, w]_ [)]

Define the corresponding coefficients: _f_ ¯ _p_ ( _t, x, i_ ; _f, g_ ) := _fp_ - _t, x, u_ _[∗][,f,g]_ _, w_ _[∗][,f,g]_ ; _i_ �, ¯ _σ_ ( _t, x, i_ ; _f, g_ ) := _σ_ - _t, x_ ; _i_ �,
_ν_ ¯( _dz_ ; _t, x, i, f, g_ ) := _ν_ ( _dz_ ; _u_ _[∗][,f,g]_ _, w_ _[∗][,f,g]_ ) _._ Under ( _f, g_ ), the hybrid state ( _Xt, It_ ) therefore evolves as a Jump-Diffusion
driven by these effective coefficients, coupled with the regime transition rates _µij_ ( _f, g_ ).


We model the outer-layer objective as a path integral whose running cost depends on the inner-layer value functions.
Let _φ_ : [0 _, T_ ] _× X × I ×_ R _→_ R denote a bounded, continuous outer-layer running cost, where the final argument will
be instantiated with the scalar quantity _Vi_ ( _t, x_ ; _f, g_ ).


For an initial condition ( _t, x, i_ ) and mode-selection policies ( _f, g_ ), define the outer-layer performance functional


_T_
_J_ out( _t, x, i_ ; _f, g_ ) := E _[t,x,i]_ _f,g_       - [�] _φ_       - _s, Xs, Is, VIs_ ( _s, Xs_ ; _f, g_ )� _ds_ + _cT_       - _XT, IT_       - [�] _,_ (9)

_t_

where E _[t,x,i]_ _f,g_ denotes expectation with respect to the probability law induced by the closed-loop Jump-Diffusion
dynamics and the transition rates _µij_ ( _f, g_ ).


The outer-layer value functions are then
_Ui_ ( _t, x_ ) := inf _J_ out( _t, x, i_ ; _f, g_ ) _._ (10)
_f_ _∈F_ [sup] _g∈G_


6


**Outer-layer Isaacs condition and HJI system**


We impose an Isaacs condition at the outer layer.

**Assumption 2** (Outer-layer Isaacs condition) **.** _For each tuple_ ( _t, x, i_ ) _and test function ψ, consider the outer-layer_
_Hamiltonian_

_H_ out[ _ψ_ ]( _t, x, i, α, β_ ) := _φ_       - _t, x, i, Vi_ _[α,β]_ ( _t, x_ )� + _L_ _[α,β]_ _eff_ _[ψ]_ [(] _[x]_ [)]

+              - _µij_ ( _α, β_ )� _ψ_ ( _t, x, j_ ) _−_ _ψ_ ( _t, x, i_ )� _,_ (11)


_j_ = _i_


_where L_ _[α,β]_ _eff_ _[, similar to what is defined in]_ [ (5)] _[, is the generator associated with the closed-loop coefficients]_ [ ¯] _[f][p][,]_ [ ¯] _[σ,]_ [ ¯] _[ν]_
_(evaluated at mixed actions α, β). We assume that the minimax condition holds:_


inf sup _H_ out = sup inf (12)
_α∈_ ∆( _AD_ ) _β∈_ ∆( _AA_ ) _β∈_ ∆( _AA_ ) _α∈_ ∆( _AD_ ) _[H]_ [out] _[.]_


Under Assumption 2 and the regularity inherited from Assumption 1, the outer-layer value functions satisfy the
following system of HJI equations.
**Lemma 4** (Outer-layer HJI) **.** _For each i ∈I, the outer-layer value function Ui is the unique viscosity solution of_



_−∂tUi_ ( _t, x_ ) = inf sup
_α∈_ ∆( _AD_ ) _β∈_ ∆( _AA_ )




- _φ_ - _t, x, i, Vi_ _[α,β]_ ( _t, x_ )�




   -   -   - [�]
+ _L_ _[α,β]_ _eff_ _[U][i]_ [(] _[t, x]_ [) +] _µij_ ( _α, β_ ) _Uj_ ( _t, x_ ) _−_ _Ui_ ( _t, x_ ) _,_

_j_ = _i_



(13)



_Ui_ ( _T, x_ ) = _cT_ ( _x, i_ ) _,_


_for all_ ( _t, x_ ) _∈_ [0 _, T_ ] _× X_ _._


The proof follows the same argument as Lemma 3. We observe that the outer optimization problem (10) is a generalized
Bolza problem for a Regime-Switching Jump-Diffusion, where the drift, diffusion, and jump measure are determined
by the closed-loop coefficients _f_ [¯] _p,_ ¯ _σ,_ ¯ _ν_ .


Under Assumption 1(iii) (strict convex-concavity of the inner Hamiltonian), the inner feedback maps ( _u_ _[∗][,f,g]_ _, w_ _[∗][,f,g]_ )
are continuous with respect to the gradient _∇xVi_ [4]. Although _∇xVi_ exists only in the generalized viscosity sense,
the effective outer dynamics satisfy the necessary growth and continuity conditions to apply the standard dynamic
programming principle. Consequently, _Ui_ is characterized as the unique viscosity solution to (13) via the same
expansion of the Dynkin identity employed in Lemma 3. Thus the outer-layer HJI system (13) mirrors the “generatorplus-minimax” structure of the inner-layer HJI (8), but with effective dynamics that incorporate the optimal inner-layer
response.


Having established the necessary conditions for the inner layer (Lemma 3) and the outer layer (Lemma 4) independently,
we now characterize the solution to the full bi-level problem (4).

**Theorem 1** (Feedback Stackelberg Equilibrium) **.** _Let V and U be the spaces of admissible feedback strategies for_
_the inner and outer layers, respectively. A strategy profile pair_ �( _u_ _[∗]_ _, w_ _[∗]_ ) _∈V × W,_ ( _f_ _[∗]_ _, g_ _[∗]_ ) _∈F × G_ - _constitutes a_
_**Feedback Stackelberg Equilibrium**_ _for the bi-level problem_ (4) _if there exist value functions V_ ( _t, x, i_ ) _and U_ ( _t, x, i_ )
_that simultaneously satisfy the coupled HJI system:_


_1. Given the outer strategy_ ( _f_ _[∗]_ _, g_ _[∗]_ ) _, the inner value V satisfies the inner Isaacs equation_ (8) _, with_ ( _u_ _[∗]_ _, w_ _[∗]_ )
_attaining the minimax of the local Hamiltonian Hi. This guarantees that the inner layer is optimal for the_
_current topology._


_2. Given the inner value field V, the outer value U satisfies the outer Isaacs equation_ (13) _, where_ ( _f_ _[∗]_ _, g_ _[∗]_ ) _attains_
_the saddle point of the switching Hamiltonian. This guarantees that the outer layer optimizes the system_
_objective subject to the inner layer’s best response._


_Since the value functions V, U satisfy the dynamic programming equations over the entire domain_ [0 _, T_ ] _×_ R _[n]_ _, the_
_equilibrium strategy is time-consistent and constitutes a Subgame Perfect Equilibrium._


_Proof sketch._ Fix ( _f_ _[∗]_ _, g_ _[∗]_ ). By the dynamic programming principle for the inner layer and the verification theorem for
Isaacs equations, if _V_ is a (sufficiently regular) solution of (8) and ( _u_ _[∗]_ _, w_ _[∗]_ ) attains the saddle condition (8), then the


7


associated controlled state-regime process achieves the inner game value and no admissible deviation of ( _u, w_ ) can
improve the follower’s objective; hence ( _u_ _[∗]_ _, w_ _[∗]_ ) is a best response to ( _f_ _[∗]_ _, g_ _[∗]_ ). Next, treat the resulting inner value field
_V_ as the induced continuation payoff entering the outer running cost. By the dynamic programming principle for the
outer layer and the corresponding verification theorem, if _U_ solves (13) and ( _f_ _[∗]_ _, g_ _[∗]_ ) attains (13), then no admissible
deviation of ( _f, g_ ) can improve the leader’s objective given the follower’s best-response mapping encoded by _V_ ; hence
( _f_ _[∗]_ _, g_ _[∗]_ ) is optimal at the outer layer. Because both layers are characterized by HJI equations posed on [0 _, T_ ] _×_ R _[n]_ _× I_
and the equilibrium strategies are feedback (Markov) and obtained from pointwise saddle conditions, the resulting
policy is time-consistent on every subgame starting at any ( _t, x, i_ ), i.e., it is subgame-perfect.


**4** **Case Study: Mode-Controlled Markov Jump Linear System**


We now examine the important case in which the inner-layer differential game admits closed-form solutions. We focus
on the _Linear-Quadratic-Gaussian (LQG)_ setting without stochastic inner-layer jump, which yields a coupled family of
matrix Riccati equations.

Fix a mode _i ∈I_, and assume the continuous state _Xt ∈_ R _[n]_ evolves as a linear SDE controlled by affine drifts, with
the control and disturbance action space being _U ⊆_ R _[d]_ [1] _, W ⊆_ R _[d]_ [2] :


_dXt_ = ( _AiXt_ + _Biu_ + _Diw_ ) _dt_ + Σ _idWt,_

where _Ai, Bi,_ and _Di_ are system matrices of proper dimensions, Σ _i_ Σ _[⊤]_ _i_ _[≻]_ [0][ captures the regime-dependent volatility.]
Throughout this section, we assume that, The running and terminal costs are quadratic: _c_ ( _t, x, u, w, i_ ) = _x_ _[⊤]_ _Qix_ +
_u_ _[⊤]_ _Riu −_ _w_ _[⊤]_ _Siw, cT_ ( _x, i_ ) = _x_ _[⊤]_ _QT,ix,_ with _Qi ∈_ R _[n][×][n]_, _Ri ∈_ R _[d]_ [1] _[×][d]_ [1], and _Si ∈_ R _[d]_ [2] _[×][d]_ [2] _, Si ≻_ 0. Under the
conditions that _Qi ⪰_ 0 _, Ri ≻_ 0, and _Si ≻_ 0, the generalized Isaacs condition (Assumption 1) holds. Although the
diffusion adds a constant trace term to the Hamiltonian, the saddle-point feedback ( _u_ _[∗]_ _i_ _[, w]_ _i_ _[∗]_ [)][ remains unique and affine]
in the gradient _∇xVi_ ( _Vi_ ( _t, x_ ) = 2 [1] _[x][⊤][P][i]_ [(] _[t]_ [)] _[x]_ [ is the value function):]

_u_ _[∗]_ _i_ [(] _[t, x]_ [) =] _[ −][R]_ _i_ _[−]_ [1] _[B]_ _i_ _[⊤][∇][x][V][i]_ [(] _[x, t]_ [;] _[ f, g]_ [)] _[, w]_ _i_ _[∗]_ [(] _[t, x]_ [) =] _[ S]_ _i_ _[−]_ [1] _Di_ _[⊤][∇][x][V][i]_ [(] _[x, t]_ [;] _[ f, g]_ [)] _[.]_ (14)


Substituting these into the inner-layer HJI (8) yields the Stochastic LQ-specialized HJI:

_−∂tVi_ = _x_ _[⊤]_ _Qix −∇xVi_ _[⊤][B][i][R]_ _i_ _[−]_ [1] _[B]_ _i_ _[⊤][∇][x][V][i]_ [+] _[ ∇][x][V][ ⊤]_ _i_ _[D][i][S]_ _i_ _[−]_ [1] _Di_ _[⊤][∇][x][V][i]_



+ _∇xVi_ _[⊤][A][i][x]_ [ + ][1]




- _µij_ ( _f, g_ )� _Vj −_ _Vi_ - _._ (15)


_j_ = _i_




   _i_ _[∇]_ _xx_ [2] _[V][i]_ [) +]
2 [Tr(Σ] _[i]_ [Σ] _[⊤]_



We adopt the quadratic ansatz _Vi_ ( _t, x_ ) = _x_ _[⊤]_ _Pi_ ( _t_ ) _x_ + _ri_ ( _t_ ). The second-order term _∇_ [2] _xx_ _[V][i]_ [= 2] _[P][i]_ [(] _[t]_ [)][ results in a trace]
term that decouples from the quadratic optimization. Consequently, the quadratic weight matrices _{Pi}i∈I_ satisfy the
_Coupled Riccati Differential Equations_ :


_−P_ [˙] _i_ = _Qi_ + _A_ _[⊤]_ _i_ _[P][i]_ [+] _[ P][i][A][i]_ _[−]_ _[P][i]_ [Σ] _[ctrl]_ _i_ _Pi_ +        - _µij_ ( _f, g_ )� _Pj −_ _Pi_        - _,_ (16)

_j_ = _i_


with boundary conditions _Pi_ ( _T_ ) = _QT,i_ for _i ∈I_, where the control matrces Σ _[ctrl]_ _i_ = - _BiRi_ _[−]_ [1] _[B]_ _i_ _[⊤]_ _[−]_ _[D][i][S]_ _i_ _[−]_ [1] _Di_ _[⊤]_ �.
Note that while the stochastic noise affects the scalar offset _−r_ ˙ _i_ ( _t_ ) = Tr(Σ _i_ Σ _[⊤]_ _i_ _[P][i]_ [(] _[t]_ [)) +][ �] _j_ = _i_ _[µ][ij]_ [(] _[f, g]_ [)(] _[r][j]_ [(] _[t]_ [)] _[ −]_ _[r][i]_ [(] _[t]_ [))][,]

with _ri_ ( _T_ ) = 0. We define the Metzler operator _M_, trace operator _T_ and Ricatti operator _R_, the coupled Ricatti flow
can be written as:

_−_ **P** [˙] = _R_ ( **P** ) + _M_ **P** _,_ **P** ( _T_ ) = **Q** _T_

_−_ **r** ˙ = _T_ ( **P** ) + _M_ **r** _,_ **r** ( _T_ ) = 0

where **r** _,_ **P** are stacked vector and matrix.


For outer players, we let there be discrete action sets _AA_ and _AD_, and each action pair ( _aA, aD_ ) _∈AA × AD_
corresponds to one jump rate matrix (Λ [(] _ij_ _[a][A][,a][D]_ [)] ) _ij∈I_, let _f, g_ be chosen from simplices ∆( _AA_ ) _,_ ∆( _AD_ ), hence the
resulting controlled transition rates are given by the expected jump intensities _µij_ ( _f, g_ ) = ¯ _µij_ + _f_ _[⊤]_ Λ _ijg_, where ¯ _µij_
denotes the nominal baseline rate and Λ _ij_ collects the action-pair-dependent perturbations.


**4.1** **Hierarchical Solution and Outer-Layer Structure**


To render the hierarchical game tractable, we define the outer running cost as the function of the instantaneous value
matrices _φ_ ( _t, x, i, Vi_ ) = _φ_ ( _Pi_ ), e.g., the trace _φ_ ( _t, x, i_ ) = Tr( _Pi_ ( _t_ )) _,_ This cost captures the magnitude of the risk


8


exposure in regime _i_, incorporating both the Hessian of the value function and the covariance of the noise. Crucially, by
using the trace, we project the matrix-valued risk into a scalar cost, allowing us to adopt a state-independent ansatz for
the outer value function: _Ui_ ( _t, x_ ) = _ki_ ( _t_ ) _._ Under this ansatz, the outer HJI equation reduces to a scalar differential
equation driven by the inner risk source:











 _,_ (17)



_−k_ [˙] _i_ ( _t_ ) = min max
_f_ _g_


with boundary condition _ki_ ( _T_ ) = 0.



Tr( _Pi_ ( _t_ )) + 


_µij_ ( _f, g_ )( _kj_ ( _t_ ) _−_ _ki_ ( _t_ ))

_j_ = _i_



Crucially, while the term Tr( _Pi_ ( _t_ )) evolves dynamically based on the switching history, it is _exogenous_ to the
instantaneous optimization at time _t_ (i.e., it does not depend explicitly on _f, g_ at that instant). Under the bilinear
transition model _µij_ ( _f, g_ ) = ¯ _µij_ + _f_ _[⊤]_ Λ _ijg_, the optimization becomes the saddle-point solution of the state-independent
game matrix **M** _i_ ( _t_ ) = [�] _j_ = _i_ [Λ] _[ij]_ - _kj_ ( _t_ ) _−_ _ki_ ( _t_ )� _._ The equilibrium policies ( _f_ _[∗]_ ( _t_ ) _, g_ _[∗]_ ( _t_ )) are the mixed-strategy

saddle-point of this matrix game.


**Theorem 2.** _Consider the hierarchical LQG case study with mode-only outer mixed strategies_ ( _f_ ( _t, i_ ) _, g_ ( _t, i_ )) _inducing_
_bilinear switching rates µij_ ( _f, g_ ) = ¯ _µij_ + _f_ _[⊤]_ Λ _ijg, and assume the inner Isaacs regularity conditions so that the_
_dynamic programming principle applies. A feedback equilibrium is characterized by the coupled backward system on_

[0 _, T_ ] _:_


_1._ _**Outer value and equilibrium switching.**_ _Let_ **k** ( _t_ ) = ( _ki_ ( _t_ )) _i∈I and define the (forced) outer flow_


_−_ **k** [˙] ( _t_ ) = _**φ**_ ( _t_ ) + _M_ ( _µ_ _[∗]_ ( _t_ )) **k** ( _t_ ) _,_ **k** ( _T_ ) = 0 _,_ (18)


_where_ ( _M_ ( _µ_ ) **z** ) _i_ := [�] _j_ = _i_ _[µ][ij]_ [(] _[z][j][ −]_ _[z][i]_ [)] _[. At each]_ [ (] _[t, i]_ [)] _[, define the local matrix game payoff]_


**M** _i_ ( _t_ ) :=         - Λ _ij_         - _kj_ ( _t_ ) _−_ _ki_ ( _t_ )� _,_ (19)


_j_ = _i_


_and let_ ( _fi_ _[∗]_ [(] _[t]_ [)] _[, g]_ _i_ _[∗]_ [(] _[t]_ [))] _[ be any mixed saddle point of]_ **[ M]** _[i]_ [(] _[t]_ [)] _[. Then the equilibrium rates][ µ][∗]_ [(] _[t]_ [)] _[ are obtained]_
_row-wise by_


             _µ_ _[∗]_ _ij_ [(] _[t]_ [) = ¯] _[µ][ij]_ [+] _[ f]_ _i_ _[ ∗]_ [(] _[t]_ [)] _[⊤]_ [Λ] _[ij]_ _[g]_ _i_ _[∗]_ [(] _[t]_ [)] _[,]_ _j ̸_ = _i,_ _with µ_ _[∗]_ _ii_ [(] _[t]_ [) =] _[ −]_ _µ_ _[∗]_ _ij_ [(] _[t]_ [)] _[.]_ (20)

_j_ = _i_


_2._ _**Inner LQG (Riccati) equilibrium.**_ _Given µ_ _[∗]_ ( _t_ ) _, the inner value admits the quadratic form Vi_ ( _t, x_ ) =
_x_ _[⊤]_ _Pi_ ( _t_ ) _x_ + _ri_ ( _t_ ) _and the matrices_ **P** ( _t_ ) = ( _Pi_ ( _t_ )) _i∈I satisfy the coupled Riccati flow_


_−_ **P** [˙] ( _t_ ) = _R_ ( **P** ( _t_ )) + _M_ ( _µ_ _[∗]_ ( _t_ )) **P** ( _t_ ) _,_ **P** ( _T_ ) = **Q** _T,_


_with the standard LQG saddle feedback laws u_ _[∗]_ _i_ [(] _[t, x]_ [) =] _[ −][R]_ _i_ _[−]_ [1] _[B]_ _i_ _[⊤]_ [(] _[P][i]_ [(] _[t]_ [)] _[x]_ [)] _[ and][ w]_ _i_ _[∗]_ [(] _[t, x]_ [) =] _[ S]_ _i_ _[−]_ [1] _Di_ _[⊤]_ [(] _[P][i]_ [(] _[t]_ [)] _[x]_ [)] _[.]_


_Proof sketch._ With mode-only outer strategies, the outer layer is a finite-state zero-sum switching game; dynamic
programming yields (18). Under the bilinear rate model, the outer Hamiltonian separates row-wise and depends on
( _fi, gi_ ) only through the matrix game (19); any mixed saddle point induces the equilibrium rates (20). Fixing _µ_ _[∗]_ ( _t_ )
reduces the inner layer to a Markov jump LQG Isaacs problem; the quadratic ansatz closes and the usual verification
argument yields the coupled Riccati flow and feedback saddle controls.


**4.2** **Spectral Structure and Adaptive Filtering**


The coupled system defined in Theorem 2 describes a nonlinear feedback loop where the switching topology adapts to
the magnitude of the underlying risk. We can interpret this physically using spectral operator theory, viewing the outer
layer as an _adaptive filter_ acting on the risk signals generated by the inner layer.

Let _L_ Π( _t_ ) denote the graph Laplacian associated with the equilibrium switching rates _µ_ _[∗]_ ( _t_ ). The outer flow equation
can be rewritten as a forced heat equation on an evolving graph. The inner-layer **P** ( _t_ ) acts as the exogenous heat source,
generating risk cost based on the local control authority and noise covariance. The outer-layer acts as the thermal
medium, diffusing this risk across the network via the strategic coupling _M_ ( _µ_ _[∗]_ ).


9


**4.2.1** **Adaptive Spectral Gap Modulation**


Unlike a passive diffusion process, the strategic coupling creates an active feedback mechanism. The equilibrium
strategy effectively modulates the _spectral gap_ of the switching graph to match the heterogeneity of the inner risk
source: (i) When the risk difference between regimes is large ( _kj ≫_ _ki_ ), the game solver drives the transition rates _µ_ _[∗]_ _ij_
high. This increases the _algebraic connectivity_ (the second eigenvalue _λ_ 2) of the Laplacian _L_ Π _∗_, where Π _[∗]_ = ( _µ_ _[∗]_ _ij_ [)] _[ij][∈I]_
is the equilibrium rate matrix and _L_ Π _∗_ = _−_ Π _[∗]_ . causing rapid diffusion of the risk cost from expensive to cheap regimes.
(ii) When the risks are balanced ( _kj ≈_ _ki_ ), the switching incentives vanish. The rates _µ_ _[∗]_ drop, the Laplacian spectral
gap closes, and the regimes effectively decouple, isolating the local risks to prevent unnecessary transition costs.

**Proposition 1** (Two-Scale Turnpike Property) **.** _The system exhibits a dual Turnpike behavior characterized by two_
_distinct time scales:_


_1. Established by the local control authority, the convergence rate is governed by the_ Hamiltonian Spectral Gap



_ρH_ := min _i |_ Re( _λ_ ( **H** _i_ )) _|, where_ **H** _i_ = - _Ai_ _−_ Σ _[ctrl]_ _i_
_−Qi_ _−A_ _[⊤]_ _i_




_∈_ R [2] _[n][×]_ [2] _[n]_ _are the Ricatti Hamiltonian._



_2. Established by the strategic switching, the convergence rate is governed by the_ Laplacian spectral gap
_λ_ 2(Π _[∗]_ ( _t_ )) := min _{ℜ_ ( _λ_ ) _>_ 0 : _λ ∈_ _σ_ ( _L_ Π( _t_ )) _}._


_The global equilibrium is reached only when both the risk generation (inner) and the risk distribution (outer) have_
_settled to their algebraic limits._


_Proof Sketch._ We analyze the convergence in backward time _τ_ = _T −_ _t_ . Let _P_ [˜] _i_ ( _τ_ ) = _Pi_ ( _τ_ ) _−_ _Pi,_ ss denote the deviation
from the algebraic Riccati solution. Linearizing the Riccati flow around _Pi,_ ss yields _dτd_ _[P]_ [˜] _[i]_ [ =] _[ A]_ _cl,i_ _[⊤]_ _[P]_ [˜] _[i]_ [ + ˜] _[P][i][A][cl,i][, A][cl,i]_ [ =]
_Ai −_ Σ [ctrl] _i_ _Pi,_ ss _._ It is well known that the eigenvalues of _Acl,i_ coincide with the stable spectrum of the Hamiltonian
matrix **H** _i_ . Hence, _∥P_ [˜] _i_ ( _τ_ ) _∥≤_ _Ce_ _[−]_ [2] _[ρ][H][τ]_, where _ρH_ is the Hamiltonian spectral gap.

The homogeneous part of the outer value dynamics satisfies _dτd_ **[k]** [(] _[τ]_ [) =] _[ −]_ [Π] _[∗]_ [(] _[τ]_ [)] **[k]** [(] _[τ]_ [)] _[.]_ [ Interpreting] _[ −]_ [Π] _[∗]_ [as the Laplacian]
_L_ Π _∗_ of the switching graph, standard spectral graph theory implies exponential contraction of the disagreement subspace
at rate _λ_ 2(Π _[∗]_ ), i.e., _∥_ **k** ( _τ_ ) _−_ **k** avg _∥≤_ _Ce_ _[−][λ]_ [2][(Π] _[∗]_ [)] _[τ]_ _._ Since the full equilibrium dynamics are the Cartesian product
of these two layers, the global turnpike behavior emerges only once both contraction mechanisms have taken effect,
establishing the stated two-scale property.


**5** **Application: Cross-layer Avellaneda-Stoikov Game**


To demonstrate the efficacy of the games-in-games architecture, we apply the framework to a high-frequency market
making problem under regime uncertainty. We extend the classical Avellaneda–Stoikov (AS) inventory management
model [3] to a hierarchical hybrid-systems framework with two coupled decision layers. At the inner layer, a zero-sum
differential game is played between a _Market Maker_ (MM), who controls inventory and quoting decisions, and a
_Strategic Predator_ (SP), who acts adversarially by perturbing the short-term price drift.


At the outer layer, a separate strategic game governs the evolution of market regimes. The outer players, the _macro-_
_attacker_ and the _macro-stabilizer_, select discrete actions that parameterize the generator of a controlled Markov jump
process over market regimes. To simplify the exposition, we assume binary actions for _{_ `off` _,_ `stab` _}_ macro-stabilizer
and _{_ `off` _,_ `att` _}_ for macro-attacker. Hence the mixed strategies can be each parameterized by a single parameter within

[0 _,_ 1]. The macro strategy pair ( _ft, gt_ ) thus induces a transition-rate matrix governing switches between calm, volatile,
and stressed market conditions. Through these rate controls, the outer game shapes the stochastic environment faced by
the inner inventory game.


**5.1** **Market Dynamics & Game Formulation**


Let the market operate in one of _N_ regimes, _It ∈I_ . The mid-price _St ∈_ R+ follows a controlled diffusion process:


_dSt_ = _wtdt_ + _σ_ ( _It_ ) _dWt,_ (21)


where _σ_ ( _i_ ) is the regime-dependent volatility, and _wt_ is the drift controlled by the inner adversary SP.

The MM holds inventory _qt ∈Q_ = _{−Q_ max _, . . ., Q_ max _}_ and cash _mt ∈_ R. The inventory dynamics are pure jump
processes driven by the execution of limit orders:

_dqt_ = _dNt_ _[b]_ _[−]_ _[dN]_ _t_ _[ a][,]_ (22)


10


where _Nt_ _[b]_ [and] _[ N]_ _t_ _[ a]_ [are Poisson processes with intensities][ Λ] _[b]_ [(] _[u][b]_ [) =] _[ Ae][−][ku][b]_ [ and][ Λ] _[a]_ [(] _[u][a]_ [) =] _[ Ae][−][ku][a]_ [, controlled by the]
MM’s spreads _u_ _[b]_ _, u_ _[a]_ _∈_ R _≥_ 0 and the market depth parameters _A, k_ . Mapping to our general framework (Def. 1), we
have the physical state: _Xt_ = ( _St, qt, mt_ ), _x_ 0 = ( _S_ 0 _, q_ 0 _, m_ 0). Note that _St_ and _mt_ are continuous, while _q_ is discrete.
The MM is the _micro-player_ who chooses spread _ut_ = ( _u_ _[a]_ _t_ _[, u]_ _t_ _[b]_ [)][; The predator is the] _[ micro-adversary]_ [ chooses] _[ w][t]_ [(price]
drift). The jump rate is state-independent, with magnitude _ρ_ ( _z_ ) = 1.


At the outer layer, the regime transition rates _µij_ ( _f, g_ ) are controlled by a _Macro-Attacker_ ( _ft_ ) who seeks to maximize
the MM’s disutility (or induce a “Crisis” regime where _σ_ is high); and a _Stabilizer_, ( _gt_ ) who Seeks to maintain the
“Calm” regime. The outer value function _Ui_ ( _t, q_ ) is computed by substituting the inner value _vi,q_ into the outer objective.


We formulate the inner layer as a zero-sum differential game. The MM maximizes the Constant Absolute Risk Aversion
(CARA) utility of terminal wealth, _U_ ( _x_ 0) = _−_ exp( _−γ_ ( _mT_ + _qT ST_ )), where _γ ≥_ 0 is the risk aversion parameter.
The SP observes the MM’s inventory _qt_ and exerts price pressure _wt_ to minimize the MM’s Certainty Equivalent,
subject to a quadratic cost 21 _ξ_ _[w]_ _t_ [2] [representing the capital cost or risk of manipulation.]


**5.2** **Hierarchical Solution: Matrix Exponential and Approximate Equilibrium**


Using the Ansatz _Vi_ ( _t, S, q, m_ ) = _−_ exp( _−γ_ ( _m_ + _qS_ + _θi_ ( _t, q_ ))), the inner Hamiltonian _Hi_ decomposes additively
due to the separation of drift (price) and jump (execution) controls. The SP minimizes the Hamiltonian component
associated with the price drift: min _w_ - _w∂SV −_ _V_ 2 [1] _ξ_ _[w]_ [2][�] _._ Using _∂SV_ = _−γqV_ and factoring out _−V >_ 0, the

optimization yields a closed-form structural reaction function:


_w_ _[∗]_ ( _t, q_ ) = _−ξγq._ (23)


This strategy reveals a _mean-averting_ behavior: if the MM is long ( _q >_ 0), the SP pushes the price down ( _w_ _[∗]_ _<_ 0) to
devalue the position; if short, the SP pushes the price up.


Simultaneously, the MM maximizes the trading component:



max
_u_ _[a]_ _,u_ _[b]_ _∈_ R _≥_ 0




 - Λ _[side]_ ( _u_ _[side]_ ) �1 _−_ _e_ _[−][γ]_ [(] _[u][side]_ [+∆] _[θ][side]_ [)][�] _,_ (24)


_side∈{a,b}_



where ∆ _θa_ = _θ_ ( _q −_ 1) _−_ _θ_ ( _q_ ) and ∆ _θb_ = _θ_ ( _q_ + 1) _−_ _θ_ ( _q_ ). This recovers the standard AS spread formula adjusted for
inventory shadow cost. Substituting the optimal strategies back into the HJB equation, the predatory term contributes a
quadratic penalty scaled by inventory size: _w_ _[∗]_ ( _−γq_ ) _−_ [1] [(] _[w][∗]_ [)][2][ =] [1] _[ξγ]_ [2] _[q]_ [2] _[.]_ [ Consequently, the inventory value function]



2 [1] _ξ_ [(] _[w][∗]_ [)][2][ =] [1] 2



quadratic penalty scaled by inventory size: _w_ _[∗]_ ( _−γq_ ) _−_ 2 [1] _ξ_ [(] _[w][∗]_ [)][2][ =] [1] 2 _[ξγ]_ [2] _[q]_ [2] _[.]_ [ Consequently, the inventory value function]

_θi_ ( _t, q_ ) satisfies the coupled system of ODEs:




- _−_ _γ_ _[k]_ _e−γ_ ∆ _θside_



_−θ_ [˙] _i_ ( _t, q_ ) = [1] _i_ _[q]_ [2]

2 _[γσ]_ [2]

~~�~~       - ~~��~~
Volatility Risk



+ [1]

2 _[ξγ]_ [2] _[q]_ [2]

~~��~~  - ~~�~~
Predatory Risk




1 + _[γ]_

_k_



+ 

_side∈{a,b}_



_A_


_γ_




- _µij_ ( _f, g_ ) [1]

_γ_

_j_ = _i_



+ 


_γ_




1 _−_ _e_ _[−][γ]_ [(] _[θ][j]_ _[−][θ][i]_ [)][�] _._ (25)



Under CARA utility, we define the value function and its exponential transformation as:


_Vi_ ( _t, S, q, m_ ) = _−_ exp� _−_ _γ_ [ _m_ + _qS_ + _θi_ ( _t, q_ )]� _,_ _vi,q_ ( _t_ ) := _e_ _[−][γθ][i]_ [(] _[t,q]_ [)] _._


Substituting optimal quotes into the HJB equation reduces the system to a linear ODE:


_v_ ˙( _t_ ) = _M_ ( _t_ ) _v_ ( _t_ ) _,_ _v_ ( _T_ ) = **1** _._


The generator _M_ acts on the state space stacked by regimes _i_ = 1 _, . . ., N_ and inventory _q ∈{−Q_ max _, . . ., Q_ max _}_ . It
decomposes into micro-structure and macro-switching blocks: _M_ = _D_ + - _Q ⊗_ _I|Q|_ - _._ Here, _Q_ is the regime transition
matrix ( _Qij_ = _µij_ for _i ̸_ = _j_, row-sums zero). The block-diagonal matrix _D_ = diag( _A_ 1 _, . . ., AN_ ) captures the
micro-dynamics. Each block _Ai_ is tridiagonal in the inventory dimension _q_, containing the diagonal risk & outflow:
12 _[γ]_ [2][(] _[σ]_ _i_ [2] [+] _[ ξγ]_ [)] _[q]_ [2] _[ −]_ [(Λ] _i_ _[a]_ [+ Λ] _i_ _[b]_ [)][, and off-diagonal entries][ Λ][side] _i_ _e_ _[−][γu]_ _i_ _[∗][,]_ [side] at ( _q, q ±_ 1).


For outer parameters piecewise-constant on [ _t, T_ ], the solution is explicit:


_v_ ( _t_ ) = exp( _−Mτ_ ) **1** _,_ _τ_ := _T −_ _t._ (26)


11


**Regime mixing and expected variance.** For small horizons _τ_, we expand the matrix exponential to characterize
how regime uncertainty affects pricing. Using the Feynman-Kac representation, the inventory cost _θi_ ( _t, q_ ) is driven by
the expected accumulated variance. Expanding the solution to second order in _τ_ yields (when _q_ is at the boundaries,
remove the coefficient 2 in the _monopoly rent_ term):



_θi_ ( _t, q_ ) = _[q]_ [2]

2




- _γ wi_ ( _τ_ ) + _γ_ [2] _ξ τ_ _−_ [2] _[A]_

_γ_




1 + _[γ]_

_k_




- _γ wi_ ( _τ_ ) + _γ_ [2] _ξ τ_ _−_ [2] _[A]_




1 + _[γ]_




- _−k/γτ_ + _O_ ( _τ_ [3] ) _._ (27)



Here, _wi_ ( _τ_ ) represents the _expected integrated variance_ starting from regime _i_ . By expanding the generator _e_ _[Qu]_ _≈_
_I_ + _Qu_, we explicitly capture the regime mixing effect. Letting _si_ := _σi_ [2][:]




    - _τ_
_w_ ( _τ_ ) :=



_τ_ - _τ_

[ _e_ _[Qu]_ _s_ ] _i du ≈_
0 0




[( _I_ + _Qu_ ) _s_ ] _i du_
0



= _σi_ [2] _[τ]_ [ + ][1]

2




- _µij_ ( _σj_ [2] _[−]_ _[σ]_ _i_ [2][)] _[ τ]_ [ 2][ +] _[ O]_ [(] _[τ]_ [ 3][)] _[.]_ (28)

_j_ = _i_



The _O_ ( _τ_ [2] ) term captures the _Regime Risk_ : the probability-weighted drift into different volatility states. Thus, for very
short horizons, the MM prices using the current regime’s volatility. As _τ_ increases, the pricing formula “bends” to
incorporate the volatilities of connected regimes.


**Remark 1** (Risk Isomorphism) **.** _The Hamiltonian separability preserves the tractability of the solution while providing_
_a key insight. The presence of a strategic predator (ξ >_ 0 _) is mathematically isomorphic to an increase in market_
_volatility. The MM perceives an_ effective volatility _σeff_ [2] [(] _[i]_ [) =] _[ w][i]_ [(] _[τ]_ [)][2][ +] _[ ξγ][. This implies that in the presence of]_
_predatory order flow, the optimal policy is to widen spreads and liquidate inventory faster, exactly as one would in a_
_high-volatility environment._


**Optimal quotes and time-varying spreads.** Defining the effective risk factor _Ci_ ( _τ_ ) := _γwi_ ( _τ_ )+ _γ_ [2] _ξτ_, the inventory

indifference pricing implies: ∆ _θa ≈_ - 21 _[−]_ _[q]_ - _Ci_ ( _τ_ ) _,_ ∆ _θb ≈_ - _q_ + [1] 2 - _Ci_ ( _τ_ ) _._ The resulting optimal spreads _u_ _[∗]_ is:



_u_ _[∗]_ _i_ [(] _[t, q]_ [) =] _[ u]_ _i_ _[∗][,a]_ + _u_ _[∗]_ _i_ _[,b]_ = [1]




[1] �1 + _[γ]_

_γ_ [ln] _k_



_k_




- + [1]

2 _[C][i]_ [(] _[τ]_ [)] _[,]_



This demonstrates that spreads widen pre-emptively based on future expected volatility _wi_ ( _τ_ ), pricing in the macroattacker’s threat before the regime shift occurs.


**5.3** **Macro Equilibrium (Outer HJI)**


Let _ft_ (Attacker) and _gt_ (Stabilizer) control the regime generator _µij_ ( _f, g_ ) and micro-parameters. We posit that the
macro-agents optimize against the _anticipated_ inventory cost priced in by the Market Maker. Thus, the outer-layer
running cost _φi_ ( _q_ ; _f, g_ ) is defined directly by the risk function _θi_ over the horizon _τ_ :



_φi_ ( _q_ ; _f, g_ ) := _θi_ ( _t, q_ ) _≈_ _[q]_ [2]

2




- _γwi_ ( _τ_ ; _f, g_ ) + _γ_ [2] _ξτ_ _−_ [2] _[A]_

_γ_




- _γwi_ ( _τ_ ; _f, g_ ) + _γ_ [2] _ξτ_ _−_ [2] _[A]_




1 + _[γ]_

_k_




- _−k/γ_
_τ._



This modeling choice implies a _sentiment-driven interaction_ : the macro-attacker seeks to maximize the Market Maker’s
forward-looking risk assessment (which drives liquidity drying), rather than merely the instantaneous volatility.


The macro value function _Ui_ ( _t, q_ ), representing the cumulative market stress, satisfies the Isaacs equation:










- _µij_ ( _f, g_ )� _Uj_ ( _t, q_ ) _−_ _Ui_ ( _t, q_ )�

_j_ = _i_ 




   _[φ][i]_ [(] _[q]_ [;] _[ f, g]_ [) +] _j_ = _i_



(29)
 _[.]_



_−∂tUi_ ( _t, q_ ) = min max
_g∈_ ∆( _AD_ ) _f_ _∈_ ∆( _AA_ )



**Remark 2** (Behavioral Interpretation) **.** _By utilizing the integrated variance wi_ ( _τ_ ) _within the running cost, this_
_formulation creates a feedback loop where the macro-agents are highly sensitive to_ future _regime risks. The switching_
_probability enters twice: once in the MM’s pricing (wi) and again in the outer value dynamics (_ [�] _µij_ ∆ _U_ _). This_
_creates a “Hyper-Alert” equilibrium where attackers preemptively strike as soon as the_ expectation _of future volatility_
_rises, mirroring the self-fulfilling nature of liquidity crises._


Let ∆ _ij_ ( _t, q_ ) := _Uj_ ( _t, q_ ) _−_ _Ui_ ( _t, q_ ) be the stability gap (the cost impact of switching from _i_ to _j_ ).


12


1. _Affine Control:_ If the transition rates _µij_ are controllable within [ _µ_ ~~_i_~~ _j,_ ~~_µ_~~ _ij_ ], _µij_ ( _f, g_ ) = _µ_ [0] _ij_ [+] _[f][ ·]_ _[λ]_ _ij_ [att] _[−]_ _[g][ ·]_ _[λ]_ _ij_ [stab]
the optimization decouples into pointwise Bang-Bang switches. The Attacker ( _f_ ) maximizes the drift toward
higher cost regimes, while the Stabilizer ( _g_ ) minimizes it:



_f_ _[∗]_ ( _t_ ) = I _{_ 


_j_ = _i_ _[λ]_ _ij_ [att] [∆] _[ij]_ _[<]_ [0] _[}][,]_ _g_ _[∗]_ ( _t_ ) = I _{_ - _j_ = _i_ _[λ]_ _ij_ [stab][∆] _[ij]_ _[<]_ [0] _[}][.]_



This highlights the conflict: when a regime switch is dangerous (∆ _ij >_ 0), the Attacker pushes the accelerator
~~(~~ ~~_µ_~~ ) while the Stabilizer slams the brake ( _µ_ ).



2. _Quadratic Costs:_ We relax the bounded control assumption and instead impose quadratic effort penalties
_ρf_ _[ρ][g]_ [2]



_ρf_ _[ρ][g]_

2 _[f]_ [ 2][ and] 2 _[g]_ [2][ in the outer Hamiltonian. Assuming the transition rates remain affine in effort (] _[µ][ij]_ [ =]

_µ_ [0] _ij_ [+] _[ fλ]_ _ij_ [att] _[−]_ _[gλ]_ _ij_ [stab][), the control-dependent part of the Hamiltonian is:]



_f_ _[ρ][g]_

2 _[f]_ [ 2][ and] 2





 [�] _λ_ [stab] _ij_ [∆] _[ij]_

_j_ = _i_







 _−_ _[ρ][f]_




_[ρ][f]_

2 _[f]_ [ 2] _[ −]_ _[ρ]_ 2 _[g]_




_[g]_

2 _[g]_ [2] _[.]_



_H_ ( _f, g_ ) _∝_ _f_





 [�] _λ_ [att] _ij_ [∆] _[ij]_


_j_ = _i_






 _−_ _g_



The first-order optimality conditions ( _∂f_ _H_ = 0 _, ∂gH_ = 0) yield explicit proportional feedback rules:









[�] _λ_ [att] _ij_ - _Uj_ ( _t, q_ ) _−_ _Ui_ ( _t, q_ )�


_j_ = _i_



+



_f_ _[∗]_ ( _t_ ) = [1]

_ρf_



 [�]







_,_









[�] _λ_ [stab] _ij_ - _Ui_ ( _t, q_ ) _−_ _Uj_ ( _t, q_ )�


_j_ = _i_



+



_g_ _[∗]_ ( _t_ ) = [1]

_ρg_



 [�]







_,_



where [ _x_ ] [+] = max(0 _, x_ ). This result characterizes the macro-agents as _variable-gain controllers_ : the intensity
of their intervention scales linearly with the severity of the stability gap. For instance, the Attacker exerts
minimal effort when the system is robust (∆ _ij ≈_ 0) but surges activity proportionally as the MM’s inventory
vulnerability increases (∆ _ij ≫_ 0).


**5.4** **Numerical Illustration**


We demonstrate the equilibrium strategies and risk isomorphism principle using calibrated Bitcoin (BTC) market data.
The experiment compares two market making strategies facing a strategic predatory trader: 1. _vanilla AS_ : standard AS
strategy, unaware of predatory drift; 2. _equilibrium AS_ : modified strategy using effective volatility _σ_ eff [2] [=] _[ w][i]_ [(] _[τ]_ [)][2][ +] _[ ξγ]_
to account for predatory risk.


We calibrate regime-switching parameters from Kraken ticker “BTC-USD” 30-minute OHLCV data (December 7-14,
2025). Using rolling volatility with _K_ -means clustering, we identify two distinct regimes: 1. _stable regime_ (regime 0):
_σ_ 0 = 0 _._ 2253 (22.53% annualized); 2. _volatile regime_ (regime 1): _σ_ 1 = 0 _._ 5305 (53.05% annualized). The volatility ratio
is _σ_ 1 _/σ_ 0 = 2 _._ 35.


Empirical transition matrix estimation yields a base transition rate of _µ_ 0 = 30 per day, corresponding to an average
holding time of 48 minutes per regime. Figure 2 shows the calibrated regime evolution with price dynamics colored by
regime state (green for stable, red for volatile).


**5.4.1** **Counterfactual Simulation Design**


We simulate 12 hours of market making activity (December 12, 2025, 15:00–03:00) with the following setup: starting
in stable regime ( _I_ 0 = 0) with intial price _S_ 0 = $90 _,_ 863 _._ 90, the market making process lasts for 2,880 total steps, with
each step counting for ∆ _t_ = 15 seconds. We set the price drift to be 0 so that only the predator affects the drift, whose
optimal drift control is _w_ _[∗]_ ( _q_ ) = _−ξγq_ with cost coefficient _ξ_ = 10 _._ 0. MM’s risk aversion parameter and inventory
constraint are set to be _γ_ = 0 _._ 02, _q ∈_ [ _−_ 10 _,_ 10]. We fit the order arrival to follow Poisson intensity Λ( _u_ ) = _λ_ 0 _e_ _[−][ku]_
with market depth _A_ = 250 _,_ 000 per year, spread sensitivity _k_ = 10. We simulate 1000 Monte-Carlo paths for statistical
significance.


Both strategies face the same strategic predator who observes their inventory in real-time and applies adversarial drift.
The key difference is that Vanilla AS uses the actual volatility _σi_ in the spread formula, while Equilibrium AS uses the
effective volatility _σ_ eff _,i_ = - _wi_ ( _τ_ ) [2] + _ξγ_ derived from Remark 1 (Risk Isomorphism).


13


Figure 2:


Figure 3: Sample BTC price evolution with equilibrium AS spread bands, (10 times the actual spread for clearer
visualization.)


14


**5.4.2** **Results and Behavioral Analysis**


Figure 3 presents one sample price evolution for the counterfactual simulation, with equilibrium AS spread bands,
colored by regime state (green for stable, red for volatile). The simulation exhibits realistic regime dynamics with
6 regime switches over 12 hours, spending approximately equal time in each regime (54.5% volatile, 22.5% stable,
calibrated from Figure 2).


Figure 4: Sample optimal spread strategy evolution and inventory dynamics of MM under SP adverse selection.


Figure 4 shows the dynamic spread adjustment and inventory management. The equilibrium strategy adaptively widens
spreads in volatile regimes, incorporating both the heightened market volatility _σi_ and the predatory risk _ξγ_ into the
reservation price calculation.


We plot the Profit & Loss (PnL) histogram across 1,000 Monte Carlo paths, from which we summarize: The equilibrium
strategy achieves 111% higher mean PnL and 58% better Sharpe ratio despite facing a stronger predatory attack (16.4%
higher average drift magnitude).


This seemingly counterintuitive result validates the risk isomorphism principle, in that wider spreads compensate
for predatory risk, (equilibrium AS quotes 27% wider spreads against Vanilla AS), capturing the effective volatility
increase from _σ_ eff [2] [=] _[ w][i]_ [(] _[τ]_ [)][2][ +] _[ ξγ]_ [; inventory accumulation is strategic according to equilibrium AS as it intentionally]
accumulates more inventory because the wider spreads not only provide adequate compensation for predatory drift
exposure, and dominate fill rate loss: although wider spreads reduce order arrival rates by approximately 0 _._ 27% per
basis point (via Λ( _u_ ) = _λ_ 0 _e_ _[−][ku]_ ), the increased spread revenue more than compensates for reduced volume.


**6** **Conclusion**


This paper presents a hierarchical “games-in-games” control framework for systems governed by _regime-switching_
_jump-diffusions_ . By decomposing the problem into a fast inner game and a strategic outer game, we derived a coupled
system of Hamilton-Jacobi-Isaacs (HJI) equations via a unified Dynkin identity. A key theoretical contribution is the
proof that for Exponential-Affine and Linear-Quadratic games, this hierarchy admits tractable _spectral solutions_ via the
matrix exponential, bridging the gap between hybrid modeling and computational feasibility.


The framework’s practical value is demonstrated through an adversarial market microstructure case study. The results
reveal a _Risk Isomorphism_ principle, where the hierarchical controller naturally interprets strategic predation as effective
volatility, inducing a “hyper-alert” equilibrium that pre-emptively widens spreads. Future research will extend this
architecture to partially observable regimes and integrate data-driven learning for empirical transition kernels.


**References**


[1] R. A¨ıd, M. Basei, G. Callegaro, L. Campi, and T. Vargiolu. Nonzero-sum stochastic differential games with
impulse controls: A verification theorem with applications. _Mathematics of Operations Research_, 45(1):205–232,
2020.

[2] D. Applebaum. _L´evy processes and stochastic calculus_, volume 116. Cambridge university press, 2009.


15


Figure 5: Distribution of terminal PnL over 1,000 simulated 12-hour market-making episodes (vanilla AS vs equilibrium
AS with predator).


[3] M. Avellaneda and S. Stoikov. High-frequency trading in a limit order book. _Quantitative Finance_, 8(3):217–224,
2008.


[4] M. Bardi and I. Capuzzo-Dolcetta. _Optimal Control and Viscosity Solutions of Hamilton–Jacobi–Bellman_
_Equations_ . Birkh¨auser, 2008.


[5] G. Barles, R. Buckdahn, and E. Pardoux. Backward stochastic differential equations and integral-partial differential
equations. _Stochastics: An International Journal of Probability and Stochastic Processes_, 60(1-2):57–83, 1997.


[6] T. Bas¸ar and G. J. Olsder. _Dynamic Noncooperative Game Theory_ . SIAM, Philadelphia, PA, 2nd edition, 1999.


[7] R. Buckdahn and J. Li. Stochastic differential games and viscosity solutions of Hamilton-Jacobi-Bellman-Isaacs
equations. _SIAM Journal on Control and Optimization_, 47(1):444–475, 2008.


[8] Y. Chen and U. Vaidya. Deception attacks on networked control systems: A game-theoretic approach. _IEEE_
_Transactions on Automatic Control_, 63(9):2905–2912, 2018.


[9] O. L. V. Costa and F. Dufour. Stability of piecewise-deterministic Markov processes. _SIAM Journal on Control_
_and Optimization_, 37(5):1483–1502, 1999.


[10] O. L. V. Costa, M. D. Fragoso, and R. P. Marques. _Discrete-Time Markov Jump Linear Systems_ . Springer, London,
2006.


[11] M. H. A. Davis. Piecewise-deterministic Markov processes: A general class of non-diffusion stochastic models.
_Journal of the Royal Statistical Society, Series B (Methodological)_, 46(3):353–388, 1984. With discussion.


[12] B. Djehiche, S. Hamadene, and A. Popier. Optimal switching in finite horizon.` _SIAM Journal on Control and_
_Optimization_, 48(4):2663–2688, 2009. Foundational reference for optimal switching.


[13] V. Dragan and T. Morozan. Game-theoretic coupled Riccati equations associated to controlled linear differential
systems with jump Markov perturbations. _Stochastic Analysis and Applications_, 19(5):715–751, 2001.


[14] X. Mao and C. Yuan. _Stochastic Differential Equations with Markovian Switching_ . Imperial College Press,
London, 2006.


16


[15] B. Øksendal. _Stochastic differential equations: an introduction with applications_ . Springer, 6 edition, 2005.

[16] Y. Pan, T. Li, and Q. Zhu. Is stochastic mirror descent vulnerable to adversarial delay attacks? a traffic assignment
resilience study. In _2023 IEEE 62nd Conference on Decision and Control (CDC)_, pages 8328–8333. IEEE, 2023.

[17] Y. Pan, T. Li, and Q. Zhu. On the resilience of traffic networks under non-equilibrium learning. In _2023 American_
_Control Conference (ACC)_, pages 3484–3489. IEEE, 2023.

[18] Y. Pan and Q. Zhu. On poisoned wardrop equilibrium in congestion games. In _Decision and Game Theory for_
_Security: 13th International Conference, GameSec 2022_, volume 13727 of _Lecture Notes in Computer Science_,
pages 191–211, Cham, 2023. Springer.

[19] F. Pasqualetti, F. Dorfler, and F. Bullo. Control-theoretic methods for cyberphysical security: Geometric principles¨
for optimal cross-layer resilient control systems. _IEEE Control Systems Magazine_, 35(1):110–127, 2015.

[20] H. Pham. _Continuous-time stochastic control and optimization with financial applications_, volume 61. Springer
Science & Business Media, 2009.

[21] G. Yin and C. Zhu. _Hybrid Switching Diffusions: Properties and Applications_, volume 63 of _Stochastic Modelling_
_and Applied Probability_ . Springer New York, 2010.

[22] Q. Zhu and T. Bas¸ar. Game-theoretic methods for robustness, security, and resilience of cyberphysical control
systems: Games-in-games principle for optimal cross-layer resilient control systems. _IEEE Control Systems_
_Magazine_, 35(1):46–65, 2015.


17


