# Recursive Flow: A Generative Framework for MIMO Channel Estimation

[Zehua Jiang, Fenghao Zhu, Chongwen Huang, Richeng Jin, Zhaohui Yang,](https://orcid.org/0009-0003-2550-3368)

[Xiaoming Chen,](https://orcid.org/0000-0001-7747-6646) _Senior Member, IEEE_ [, Zhaoyang Zhang,](https://orcid.org/0000-0003-2346-6228) _Senior Member, IEEE_, and

[M¬¥erouane Debbah,](https://orcid.org/0000-0001-8941-8080) _Fellow, IEEE_



_**Abstract**_ **‚ÄîChannel estimation is a fundamental challenge in**
**massive multiple-input multiple-output systems, where estimation**
**accuracy governs the spectral efficiency and link reliability. In**
**this work, we introduce Recursive Flow (RC-Flow), a novel**
**solver that leverages pre-trained flow matching priors to robustly**
**recover channel state information from noisy, under-determined**
**measurements. Different from conventional open-loop genera-**
**tive models, our approach establishes a closed-loop refinement**
**framework via a serial restart mechanism and anchored trajec-**
**tory rectification. By synergizing flow-consistent prior directions**
**with data-fidelity proximal projections, the proposed RC-Flow**
**achieves robust channel reconstruction and delivers state-of-the-**
**art performance across diverse noise levels, particularly in noise-**
**dominated scenarios. The framework is further augmented by an**
**adaptive dual-scheduling strategy, offering flexible management**
**of the trade-off between convergence speed and reconstruction**
**accuracy. Theoretically, we analyze the Jacobian spectral radius**
**of the recursive operator to prove its global asymptotic stability.**
**Numerical results demonstrate that RC-Flow reduces inference**
**latency by two orders of magnitude while achieving a 2.7 dB**
**performance gain in low signal-to-noise ratio regimes compared**
**to the score-based baseline.**

_**Index Terms**_ **‚ÄîDeep learning, fixed-point iteration, flow match-**
**ing, generative models, MIMO channel estimation.**


I. INTRODUCTION

The emergence of sixth-generation (6G) wireless networks
promises to deliver hyper-connectivity and terabits-per-second
data rates [1, 2], with massive multiple-input multiple-output
(MIMO) serving as a fundamental enabler to unlock spectral
efficiency gains [3, 4]. By deploying large-scale antenna
arrays, future base stations can exploit high-resolution spatial
multiplexing to serve massive numbers of users simultaneously

[5]. However, the theoretical benefits of such massive arrays
are fundamentally predicated on the availability of precise
channel state information (CSI) at the transmitter [6]. As the
number of antennas scales to hundreds or even thousands, the
conventional strategy of orthogonal pilot transmission incurs a
prohibitive signaling overhead that consumes valuable coherence resources, thereby severely degrading the effective system
throughput [7, 8]. Consequently, acquiring high-fidelity CSI
from a limited number of pilot measurements has evolved into
a severely ill-posed linear inverse problem [9], representing a


Z. Jiang, F. Zhu, C. Huang, R. Jin, Z. Yang, X. Chen and Z. Zhang are with
the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (E-mails: _{_ [jiangzehua, zjuzfh, chongwen-](mailto:jiangzehua@zju.edu.cn)
[huang, richengjin, yang zhaohui, chen xiaoming, ning ming](mailto:chongwenhuang@zju.edu.cn) _}_ [@zju.edu.cn).](mailto:ning_ming@zju.edu.cn)
M. Debbah is with KU 6G Research Center, Department of Computer and
Information Engineering, Khalifa University, Abu Dhabi 127788, UAE (E[mail: merouane.debbah@ku.ac.ae).](mailto:merouane.debbah@ku.ac.ae)



critical bottleneck that necessitates algorithmic breakthroughs
beyond traditional estimation paradigms.
To mitigate the ill-posedness of channel estimation, conventional approaches resort to statistical or structural regularization to constrain the solution space. The linear minimum
mean square error (LMMSE) estimator, widely regarded as the
performance benchmark, optimally minimizes the estimation
error for Gaussian channels by leveraging second-order statistics [10]. However, its practical deployment is often hindered
by the difficulty of acquiring accurate channel covariance
matrices, particularly in highly dynamic environments where
channel statistics vary rapidly. Alternatively, compressed sensing (CS) techniques have emerged as a powerful paradigm

[9], exploiting the sparsity of millimeter-wave channels in the
angular domain to recover signals from undersampled measurements . Algorithms such as approximate message passing
(AMP) [8] and atomic norm minimization [11] have demonstrated theoretical guarantees under strict sparsity conditions.
Nevertheless, these model-based methods fundamentally hinge
on ideal assumptions. In realistic scenarios characterized by
grid mismatch, energy leakage, or rich scattering, the sparsity assumption is frequently violated, leading to significant
performance degradation and reconstruction artifacts.
In order to circumvent the reliance on rigid analytical priors,
the research community has pivoted towards data-driven deep
learning paradigms, which learn the complex inverse mapping
directly from training data. Seminal works have employed
convolutional neural networks to estimate channel matrices
by exploiting spatial correlations [12, 13]. Furthermore, deep
unfolding networks, such as Learned D-AMP [14], unfold iterative algorithms into trainable layers, aiming to combine the
interpretability of model-based methods with the expressivity
of neural networks. Despite their computational efficiency,
these discriminative approaches typically minimize a pixelwise mean squared error (MSE) loss. Mathematically, such objectives drive the network to approximate the conditional mean
of the posterior distribution. While minimizing error variance,
this ‚Äòregression to the mean‚Äô behavior inevitably leads to oversmoothed reconstructions [15], resulting in the loss of critical
high-frequency spatial details and texture information that are
essential for high-precision beamforming.
Recently, generative artificial intelligence has empowered
physical layer modeling to transcend the constraints of deterministic regression, enabling a more faithful capture of
the inherent stochastic complexity of wireless channels [16‚Äì
19]. Unlike discriminative models that collapse the posterior


into a single point estimate, generative approaches explicitly
learn the underlying data distribution by reversing a gradual
corruption process. Pioneering frameworks, such as denoising
diffusion probabilistic models [20] and score-based generative
models [21], estimate the score function to iteratively refine
Gaussian noise into high-fidelity channel realizations. These
methods have been applied in channel synthesis [22] and
semantic communications [23], demonstrating superior capability in modeling high-frequency spatial features. Building
upon these diffusion foundations, flow matching (FM) has
emerged as a cutting-edge generative paradigm [24‚Äì27]. By
regressing a time-dependent vector field approximating the optimal transport path between prior and data distributions [28],
FM rectifies generative trajectories, enabling faster and more
deterministic sampling than stochastic diffusion. In the broader
context for linear inverse problems, general-purpose solvers
such as FLOWER [29] and PnP-Flow [30] have been proposed. Treating the pre-trained flow as a generative prior, these
algorithms alternate flow-based denoising and measurementconsistency projections to steer trajectories toward the data
manifold and measurement subspace intersection.

Despite the theoretical potential of generative models, their
practical deployment for high-dimensional MIMO channel
estimation is hindered by specific implementation constraints.
A prominent branch of recent flow-based estimators focuses on
architectural innovations to accelerate inference. Specifically,
the approach in [31] applies standard flow matching to model
the conditional channel distribution, while [32] introduces a
signal-to-noise ratio (SNR) aware truncation strategy for the
reverse diffusion process to reduce computational complexity.
Furthermore, [33] leverages the average velocity field to collapse the integration trajectory, enabling channel estimation in
a single step. While algorithmically distinct, these methods
typically rely on the conjugate transpose of pilot matrices,
implicitly assuming pilot orthogonality. In under-determined
massive MIMO systems where pilots are limited, this assumption fails, causing severe aliasing artifacts.
Alternatively, the foundational score-based method [34]
provides a theoretically rigorous solution for such underdetermined systems by employing annealed Langevin dynamics to sample from the posterior. However, the stochastic nature
of the dynamics necessitates the continuous injection of noise
to correct trajectory errors, requiring thousands of iterative
steps that result in prohibitive computational latency. Aiming
to reduce this latency, subsequent accelerated deterministic
solvers have been proposed. For instance, [35] derives a
closed-form approximation of the likelihood score to guide
the diffusion reverse process, while [36, 37] utilize Tweedie‚Äôs
formula [38, 39] or energy-based variational inference to
approximate the posterior mean in fewer steps. Nevertheless,
critical limitations persist across these deterministic modifications. Their data-consistency updates typically rely on explicit
gradient descent steps, where determining the optimal step size
presents a significant challenge. Besides, despite the reduction
in sampling steps compared to score-based methods [34], these
solvers still require hundreds of iterations, which remains
computationally prohibitive for low-latency systems. More
fundamentally, they operate in an open-loop manner, where the



generative trajectory is initialized once from noise and refined
sequentially. In low SNR regimes, the guidance provided by
the likelihood term is dominated by heavy measurement noise
rather than the true signal residual, causing the solver to drift
towards a biased solution.
To resolve the systematic bias inherent in open-loop solvers
and the instability of gradient-based physical updates, a
paradigm shift is required from ‚Äòtrajectory sampling‚Äô to ‚Äòequilibrium finding‚Äô. Drawing inspiration from deep equilibrium
models [40], which define the output of a network as the
fixed point of a non-linear transformation, we propose to
reformulate the generative channel estimation process as a
closed-loop iteration. The main contributions of this paper can
be summarized as follows:




_‚Ä¢_ We introduce Recursive Flow (RC-Flow), a novel generative framework for high-dimensional MIMO channel
estimation. By reformulating the inference process as a
closed-loop fixed-point iteration and leveraging anchored
trajectory rectification, we theoretically guarantee global
asymptotic stability towards a fixed point, providing a
robust mathematical foundation for generative channel
recovery.

_‚Ä¢_ We propose a closed-form proximal projection mechanism to replace heuristic gradient-based updates. This approach ensures unconditional data consistency throughout
the iterative process and eliminates the need for tedious
step-size tuning, thereby enhancing the solver‚Äôs robustness against measurement noise compared to standard
diffusion baselines.

_‚Ä¢_ We design an adaptive dual-scheduling strategy ( _Œª, Œ≤_ ) that
dynamically reallocates the computational budget during
inference. This mechanism allows the solver to adaptively
trade off between convergence speed and restoration precision, enabling the system to meet stringent low-latency
requirements essential for real-time 6G applications.

_‚Ä¢_ Extensive simulations demonstrate that RC-Flow achieves
SOTA performance across diverse scenarios. In low-SNR
regimes, it effectively suppresses noise-induced hallucinations, significantly outperforming existing baselines. In
high-SNR regimes, it matches the accuracy of computationally expensive methods while reducing inference
latency by orders of magnitude, validating its practical
superiority. The code for this paper is available online in

[41].
The paper is organized as follows: Section II presents an
introduction to the system model and flow matching. Section
III introduces the proposed RC-Flow framework, followed by
the presentation of simulation results in Section IV. Finally,
Section V concludes the paper.
_Notation_ : Fonts _a_, **a**, and **A** denote scalars, vectors, and
matrices, respectively. _‚à•_ **a** _‚à•_ 2 represent the 2-norm of **a** . **A** _[T]_,
**A** _[H]_, **A** _[‚àí]_ [1], _‚à•_ **A** _‚à•_ 2 and _‚à•_ **A** _‚à•F_ represent the transpose, conjugate transpose, inverse, 2-norm and Frobenius norm of **A**,
respectively. The ( _m, n_ )-th entry of **A** is denoted by _amn_,
and _|¬∑|_ denotes the modulus. _O_ represents the asymptotic time
complexity. Finally, we can represent the diagonal matrix and
trace of matrix **A** using the notations diag( **A** ) and Tr( **A** ),
respectively.


II. PRELIMINARIES
_A. Wireless System Model_
We consider a narrowband, point-to-point MIMO communication system consisting of _Nt_ transmit antennas and
_Nr_ receive antennas. The wireless propagation environment
is characterized by the complex-valued CSI matrixC _[N][r][√ó][N][t]_ . To estimate this channel, the transmitter sends a **H** _‚àà_
pilot symbol andsequence of _Np_ pilot symbols. Let **W** _‚àà_ C _[N][r][√ó][N][r]_ be the receive beamforming **p** _i ‚àà_ C _[N][t]_ denote the _i_ -th
matrix. The received signal vector for the _i_ -th pilot **y** _i_ is given
by:
**y** _i_ = **W** _[H]_ ( **Hp** _is_ + **n** _i_ ) _,_ (1)

where **n** _i_ is the complex additive white Gaussian noise and _s_
is a complex-valued scalar. For the remainder of the paper, we
assume _s_ = 1 and a fully digital receiver where **W** = **I** . The
pilot **p** _i_ is constrained to have unit-amplitude entries with lowresolution phase, where each element is a randomly selected
quadrature phase shift keying symbol that remains fixed for all
test samples. Assuming the channel remains constant over the
duration of the pilot transmission, the received signal matrix
**Y** is modeled as:
**Y** = **HP** + **N** _,_ (2)

where **Y** = [ **y** 1 _,_ **y** 2 _, ..._ **y** _Np_ ] C _[N][r][√ó][N][p]_ is the received signal
matrix, **P** = [ **p** 1 _,_ **p** 2 _, ..._ **p** _Np ‚àà_ ] C _[N][t][√ó][N][p]_ is the pilot matrix,
**N** = [ **n** 1 _,_ **n** 2 _, ..._ **n** _Np_ ] C _[N][r][√ó]_ _‚àà_ _[N][p]_ is the additive noise matrix.
_‚àà_
The entries of **N** are typically assumed to be independent and
identically distributed complex white Gaussian noise with zero
mean and variance _œÉ_ pilot [2] [.]
The objective of channel estimation is to recover the highdimensional matrix **H** from the observed measurements **Y**,
given the knowledge of the pilot matrix **P** . We define the
pilot density as _Œ±_ = _Np/Nt_ . In practical mmWave scenarios,
pilot resources are often severely constrained ( _Œ± <_ 1), thereby
rendering the recovery of an inherently under-determined
inverse problem.


_B. Continuous Normalizing Flows_
Continuous normalizing flows represent a class of generative models that transform a simple prior distribution _p_ 0( **H** )
(e.g., a standard Gaussian distribution) into a complex data
distribution _p_ 1( **H** ) through a continuous-time generative process [25, 26]. This process is characterized by a flow, denoted
as:
_œït_ : [0 _,_ 1] _√ó_ R _[d]_ _‚àí‚Üí_ R _[d]_ _,_ (3)

where _d_ represents the dimension of the data, the symbol _√ó_
and _‚àí‚Üí_ denote the Cartesian Product and the mapping between
data domains, respectively. Intuitively, _œït_ moves a sample
from its initial state at _t_ = 0 to a target state at _t_ = 1.
Mathematically, for any time _t_ and any channel matrix **H** in
the signal space, the flow is defined by an ordinary differential
equation (ODE):
_d_
_dt_ _[œï][t]_ [(] **[H]** [) =] **[ V]** _[t]_ [(] _[œï][t]_ [(] **[H]** [))] _[,]_ _œï_ 0( **H** ) = **H** 0 _,_ (4)

where _œï_ 0( **H** ) = **H** 0 defines the boundary condition, **V** _t_ is
the time-dependent velocity field that determines the instantaneous direction and speed of the evolution. This sample-wise



transport induces a time-dependent probability density path _pt_ .
The coupling between the density _pt_ and the velocity field **V** _t_
is formally described by the continuity equation:


_‚àÇpt_ ( **H** )

_‚àÇt_ + div( _pt_ ( **H** ) **V** _t_ ( **H** )) = 0 _,_ (5)

which ensures that the velocity field correctly describes the
evolution of the probability density over time.


_C. Flow Matching_

The objective of flow matching (FM) is to train a deep
neural network, parameterized by _Œ∏_, to approximate the optimal marginal velocity field **V** _t_ [24, 30, 31]. The network
parameterizes a time-dependent vector field **V** _t_ _[Œ∏]_ [, which is]
optimized to minimize the MSE-based FM loss:

_L_ FM( _Œ∏_ ) = E _t‚àºU_ [0 _,_ 1] _,_ **H** _‚àºpt_ ( **H** )  - _‚à•_ **V** _t_ _[Œ∏]_ [(] **[H]** [)] _[ ‚àí]_ **[V]** _[t]_ [(] **[H]** [)] _[‚à•]_ _F_ [2]  - _._ (6)

However, directly minimizing _L_ FM is typically infeasible in
practice as both the marginal density _pt_ ( **H** ) and the target field
**V** _t_ are generally inaccessible. To circumvent this, conditional
flow matching (CFM) is utilized [42], which simplifies the
optimization by conditioning on individual data samples **H** 1
drawn from the empirical distribution _p_ 1( **H** ). By defining a
conditional probability path _pt_ ( **H** **H** 1) and a corresponding
_|_
conditional velocity field **V** _t_ ( **H** **H** 1) that transports probabil_|_
ity mass toward **H** 1, the CFM objective is formulated as:

_L_ CFM( _Œ∏_ ) = E _t,_ **H** 1 _‚àºp_ 1 _,_ **H** _‚àºpt_ ( **H** _|_ **H** 1) - _‚à•_ **V** _t_ _[Œ∏]_ [(] **[H]** [)] _[ ‚àí]_ **[V]** _[t]_ [(] **[H]** _[|]_ **[H]** [1][)] _[‚à•]_ _F_ [2] - _._
(7)
Notably, the gradients of _L_ CFM and _L_ FM with respect to _Œ∏_
are equivalent, provided that the marginal velocity field is the
aggregation of its conditional counterparts:


       **V** _t_ ( **H** ) = **V** _t_ ( **H** **H** 1) _[p][t]_ [(] **[H]** _[|]_ **[H]** [1][)] _[p]_ [1][(] **[H]** [1][)] _d_ **H** 1 _._ (8)
_|_ _pt_ ( **H** )

This framework enables ‚Äòsimulation-free‚Äô training. Specifically, for Gaussian conditional paths, samples at any time _t_ can
be generated via a closed-form reparameterization, eliminating
the need for expensive ODE solvers during the training phase.
In our implementation, we adopt a Gaussian conditional
probability path _pt_ ( **H** _|_ **H** 1) = _N_ ( **H** ; _¬µt_ ( **H** 1) _, œÉt_ [2] **[I]** [)][ and employ]
a linear interpolation strategy to construct the flow. This
trajectory is defined by the displacement map:

_œàt_ ( **H** 0 _,_ **H** 1) = _t_ **H** 1 + (1 _‚àí_ _t_ ) **H** 0 _,_ (9)

where **H** 0 ( **0** _,_ **I** ) denotes the initial noise distribution.
_‚àºN_
Under this formulation, the mean and variance evolve as
_¬µt_ ( **H** 1) = _t_ **H** 1 and _œÉt_ [2] [= (1] _[ ‚àí]_ _[t]_ [)][2][, respectively. Such a]
construction induces a time-independent conditional velocity
field:

**V** _t_ ( **H** _|_ **H** 1) = _dt_ _[d]_ _[œà][t]_ [(] **[H]** [0] _[,]_ **[ H]** [1][) =] **[ H]** [1] _[ ‚àí]_ **[H]** [0] _[.]_ (10)

Compared to curved diffusion-based paths, this rectified linear
trajectory significantly accelerates training convergence and
enhances sampling efficiency during inference. Finally, to
align with the estimation task, we define the generative process
via a reverse-time flow, evolving from _t_ = 1 back to _t_ = 0
(ground-truth).


**Outer Loop: Recursive Refinement**



Recursive Estimation



Final Channel
Estimate



Anchor Anchor

‚ãØ Inner Loop ùëòùëò : ‚ãØ

ùêáùêá [(ùëòùëò+1,0)] ùêáùêá [(ùëÅùëÅ][1][,0)]



Randomly
Initialize ùêáùêá [(0,0)]



Anchor
Inner Loop 0:

[(1,0)]



Anchor Anchor

‚ãØ Inner Loop ùëòùëò :

ùêáùêá [(1,0)] ùêáùêá [(ùëòùëò+1,0)]



**Inner Loop: Flow-based Refinement**



Repeat from ùëñùëñ= 0 ùë° ùëÅùëÅ2 ‚àí1



ùúÜùúÜ



Start, Initialize
ùêáùêá [ùëòùëò,0] = ùêáùêá [ùëòùëò‚àí1,ùëÅùëÅ][2][‚àí1]


**Core Component Operators**



ùúÜùúÜ No

Time ùë°ùë°= 1 ‚àí ùëÅùëÅ [ùëñùëñ] 2
Step ùõΩùõΩ Execute Core Updated Check
Setup ùë°ùë° [‚Ä≤] = 1 ‚àí [ùëñùëñ+ 1] ùë°ùë°‚â§0



ùë°ùë°= 1 ‚àí [ùëñùëñ]
ùëÅùëÅ2



Output
ùêáùêá [(ùëòùëò,ùëÅùëÅ][2][‚àí1)]



Updated
Channel



Yes



ùë°ùë° [‚Ä≤] = 1 ‚àí [ùëñùëñ+ 1]
ùëÅùëÅ2



ùõΩùõΩ Execute Core
Operators



Check
ùë°ùë°‚â§0



**1** : Flow-Consistent Prior Estimation



**2** : Physics-Aware Proximal Projection **3** : Anchored Trajectory Rectification



ùêáùêáproj = ùêëùêë+ ùë§ùë§ [‚àí1][ ÔøΩ] ùêáùêá ùêåùêå+ ùë§ùë§ [‚àí1] ùêàùêà [‚àí1]



ùêáùêá [ùëòùëò,ùëñùëñ+1] = t [‚Ä≤] ‚ãÖùùêùùê+ 1 ‚àít [‚Ä≤] ‚ãÖùêáùêáproj

Interpolation
Block



Input ùêáùêá [ùëòùëò,ùëñùëñ]



Proximal \epsilon
Projection



Anchor



ùùêùùê= ùêáùêá [ùëòùëò,0]



Pilot ùêèùêè
Measurement ùêòùêò



Physics-Guided
Refinement ùêáùêáproj



U-Net Denoiser Coarse Channel Pilot ùêèùêè Physics-Guided Output ùêáùêá [ùëòùëò,ùëñùëñ+1]

Time ùë°ùë° Estimation ùêáùêá [ÔøΩ] Refinement ùêáùêáproj [ùëòùëò,0]



ùë°ùë°



U-Net Denoiser Coarse Channel
Estimation ùêáùêá [ÔøΩ]



Fig. 1. Schematic of the RC-Flow algorithm. The method employs a nested loop structure: the Outer Loop recursively resets the anchor to correct trajectory
drift, while the Inner Loop refines the estimate. The Core Component Operators combine a deep flow prior, physics-aware proximal projection, and anchored
rectification to achieve robust reconstruction.



III. RECURSIVE FLOW FRAMEWORK
In this section, we present the RC-Flow framework, as illustrated in Fig. 1 and summarized in Algorithm 1. We structure
the presentation hierarchically: first, we define the core component operators that serve as the fundamental building blocks;
second, we detail the inner loop mechanism driven by an
adaptive dual-scheduling strategy; third, we introduce the outer
loop recursive anchor refinement for trajectory rectification;
finally, we provide a rigorous theoretical analysis establishing
the existence and local asymptotic stability of the solver‚Äôs fixed
point.


_A. Core Component Operators_
_a) Flow-Consistent Prior Estimation:_ To establish an
informative generative prior for channel estimation, we utilize the CFM framework pre-trained on empirical channel
realizations. In the training phase, the network, parameterized by _Œ∏_, learns to approximate a time-dependent velocity
field. For each training instance, an SNR is sampled from
a predefined discrete set _dB_ . Based on the selected SNR,
_S_
complex Gaussian noise is generated and superimposed on
the ground-truth channel **H** . Here, **H** is globally normalized
such that E[ _hij_ ] = 1. We define the probability density
_|_ _|_ [2]
path through linear interpolation between the noise-free source
**H** 0 = **H** and the noisy target **H** 1 = **H** + **N**, expressed as
**H** _t_ = (1 _‚àí_ _t_ ) **H** 0 + _t_ **H** 1. To optimize the learning efficiency
across the flow trajectory, the temporal variable _t ‚àà_ [0 _,_ 1]
follows a logit-normal schedule. The parameters _Œ∏_ are optimized by minimizing the MSE between the predicted and
target velocity fields:

( _Œ∏_ ) = E _t,_ **H** _,_ **N**    - **V** _t_ _[Œ∏]_ [(] **[H]** _[t][, t]_ [)] _[ ‚àí]_ [(] **[H]** [1] _F_    - _._ (11)
_L_ _‚à•_ _[‚àí]_ **[H]** [0][)] _[‚à•]_ [2]



During inference, the objective is to recover the clean channel from a noisy intermediate state by leveraging the learned
flow dynamics. Specifically, given a current state **H** [(] _[k,i]_ [)] and
a time step _t_, the pre-trained network predicts the velocity
**V** = Model( **H** [(] _[k,i]_ [)] _, t_ ) tangent to the probability flow. A flowconsistent prior estimate **H** [Àú] is then obtained by projecting
the state along the trajectory back to _t_ = 0. This yields the
following deterministic single-step estimation formula:

**H** Àú = **H** [(] _[k,i]_ [)] _‚àí_ _t ¬∑_ **V** _._ (12)

This operation effectively utilizes the learned generative prior
to extract the intrinsic, noise-free channel structure from the
current observation.
_b) Physics-Aware Proximal Projection:_ While the prior
estimate **H** [Àú] is consistent with the learned distribution, it may
not strictly adhere to the physical constraints imposed by the
pilot observations. To enforce data consistency, we employ a
proximal operator that reconciles the generative prior with the
measurement likelihood. This is formulated as a regularized
least-squares problem:


1
**H** proj = arg min **H** 2 _œÉ_ pilot [2] _‚à•_ **Y** _‚àí_ **HP** _‚à•F_ [2] [+] 2 [1] _w_ _[‚à•]_ **[H]** _[ ‚àí]_ **[H]** [Àú] _[‚à•]_ _F_ [2] _[,]_ [ (13)]

where _w_ is a time-dependent regularization parameter. We
adopt a variance annealing schedule defined as _w_ = _t_ [2] _/_ ( _t_ [2] +
(1 _‚àí_ _t_ ) [2] ), which dynamically balances the data fidelity of the
pilot measurements with the flow-consistent prior.
As a quadratic optimization problem, (13) admits a unique
closed-form solution:

**H** proj = ( **R** + _w_ _[‚àí]_ [1][ Àú] **H** )( **M** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] _,_ (14)


where **M** = _œÉ_ pilot _[‚àí]_ [2] **[PP]** _[H]_ [ and] **[ R]** [ =] _[ œÉ]_ pilot _[‚àí]_ [2] **[YP]** _[H]_ [. To circumvent]
the prohibitive computational complexity associated with direct matrix inversion, we leverage eigenvalue decomposition
(EVD). Given that **M** is a Hermitian matrix, its EVD is
expressed as:
**M** = **UŒõU** _[H]_ _,_ (15)


where **U** is a unitary matrix and **Œõ** is a diagonal matrix of
eigenvalues. Consequently, the regularized inversion term in
(14) can be efficiently reformulated as:


( **M** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] = **U** ( **Œõ** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] **U** _[H]_ _._ (16)


Since ( **Œõ** + _w_ _[‚àí]_ [1] **I** ) remains diagonal, the inversion is reduced to
an element-wise reciprocal of its diagonal entries, significantly
reducing the per-iteration computational overhead.
_c) Anchored Trajectory Rectification:_ Although the proximal projection enforces data consistency, directly adopting
the refined estimate **H** proj may cause the state to deviate
from the learned probability flow trajectory. To mitigate this
trajectory drift and maintain structural integrity, we implement
an anchored rectification strategy designed to preserve the
generative path. Specifically, we define the latent anchor
_**œµ**_ = **H** [(] _[k,]_ [0)] to represent the source of the flow. The updated
state for the subsequent time step _t_ _[‚Ä≤]_ is then reconstructed by
re-interpolating between the anchor and the physically refined
estimate **H** proj, expressed as:

**H** [(] _[k,i]_ [+1)] = _t_ _[‚Ä≤]_ _¬∑_ _**œµ**_ + (1 _‚àí_ _t_ _[‚Ä≤]_ ) _¬∑_ **H** proj _._ (17)

This formulation ensures that the transition from noise to data
adheres to the optimal straight-line path, while effectively
steering the generative evolution toward the physically consistent manifold.


_B. Inner Loop: Flow-based Refinement_

The aforementioned operators form the core processing unit
of the proposed framework, integrated into an inner loop
designed to progressively refine the channel estimate from a
noisy observation toward a ground-truth realization. The flowbased refinement process is structured as follows:
In the _k_ -th outer iteration, the inner loop is initialized with
the anchor **H** [(] _[k,]_ [0)] = **H** [(] _[k][‚àí]_ [1] _[,N]_ [2] _[‚àí]_ [1)] as dictated by the recursive
anchor logic. The loop then executes _N_ 2 steps ( _i ‚àà_ [0 _, N_ 2 _‚àí_ 1])
to traverse the probability flow trajectory.
To balance trajectory stability with the capture of rapid
velocity dynamics near the data manifold, we employ an
adaptive dual-scheduling strategy. This approach decouples the
time coordinate for feature extraction from the temporal step
used for noise removal. Specifically, for the _i_ -th inner step, the
current time _t_ and target time _t_ _[‚Ä≤]_ are governed by two distinct
polynomial schedules:


_t_ = (1 _i/N_ 2) _[Œª]_ _,_ (18)
_‚àí_

_t_ _[‚Ä≤]_ = (1 ( _i_ + 1) _/N_ 2) _[Œ≤]_ _._ (19)
_‚àí_

Where _Œª_ modulates the time embedding of the network to
govern the granularity of prior estimation, while _Œ≤_ regulates
the rectification step size and the rate of anchor removal.
This mechanism facilitates rapid progression in high-noise



~~**Algorithm 1**~~ Recursive Flow Algorithm



**Input:** Observations **Y**, Pilots **P**, _œÉ_ pilot, _N_ 1 _, N_ 2, _Œª, Œ≤_ .
**Output:** Estimated Channel **H** est.



1: Initialize **M** = _œÉ_ pilot _[‚àí]_ [2] **[PP]** _[H]_ [ and] **[ R]** [ =] _[ œÉ]_ pilot _[‚àí]_ [2] **[YP]** _[H]_ [.]



2: EVD: **M** = **UŒõU** _[H]_ .
3: Initialize **H** [(0] _[,]_ [0)] _‚àºCN_ ( **0** _,_ **I** ).
4: **for** _k ‚Üê_ 0 _, . . ., N_ 1 _‚àí_ 1 **do**
5: **for** _i ‚Üê_ 0 _, . . ., N_ 2 _‚àí_ 1 **do**
6: _t_ = (1 _i/N_ 2) _[Œª]_
_‚àí_

_[‚Ä≤]_



7: _t_ _[‚Ä≤]_ = (1 ( _i_ + 1) _/N_ 2) _[Œ≤]_
_‚àí_

[(] _[k,i]_ [)]



8: **V** = Model( **H** [(] _[k,i]_ [)] _, t_ )
9: **H** Àú = **H** [(] _[k,i]_ [)] _‚àí_ _t ¬∑_ **V** _‚ñ∑_ Denoising
10: _w_ = _t_ [2] _/_ ( _t_ [2] + (1 _‚àí_ _t_ ) [2] )
11: **H** proj = ( **R** + _w_ _[‚àí]_ [1][ Àú] **H** ) **U** ( **Œõ** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] **U** _[H]_

12: _‚ñ∑_ Projection
13: **if** _k_ = 0 **then**
14: _**œµ**_ _‚àºCN_ ( **0** _,_ **I** ) _‚ñ∑_ Initialize Anchor
15: **else**
16: _**œµ**_ = **H** [(] _[k][‚àí]_ [1] _[,N]_ [2] _[‚àí]_ [1)] _‚ñ∑_ Reset Anchor
17: **end if**
18: **H** [(] _[k,i]_ [+1)] = _t_ _[‚Ä≤]_ _¬∑_ _**œµ**_ + (1 _‚àí_ _t_ _[‚Ä≤]_ ) _¬∑_ **H** proj _‚ñ∑_ Interpolation
19: **end for**
20: **H** [(] _[k]_ [+1] _[,]_ [0)] = **H** [(] _[k,N]_ [2] _[‚àí]_ [1)]



21: **end for**
22: **H** est = **H** [(] _[N]_ [1] _[‚àí]_ [1] _[,N]_ [2] _[‚àí]_ [1)]



23: **return H** est



regimes while ensuring fine-grained adjustments as the channel structure emerges. The impact of _Œª_ and _Œ≤_ configurations
on convergence speed and estimation performance is further
analyzed in Section IV.
Given the time indices _t_ and _t_ _[‚Ä≤]_, the loop updates the channel
state **H** [(] _[k,i]_ [)] by sequentially applying the operators defined
in (12)‚Äì(17). This sequence integrates flow-consistent prior
extraction, physics-aware projection, and trajectory rectification to generate the subsequent state **H** [(] _[k,i]_ [+1)] . This procedure
repeats until _i_ = _N_ 2 1 (where _t_ _[‚Ä≤]_ = 0). The final state
**H** [(] _[k,N]_ [2] _[‚àí]_ [1)] serves as both the refined channel estimate for the _‚àí_
current outer iteration and the anchor for the recursive update
in the subsequent stage.


_C. Outer Loop: Recursive Anchor Refinement_
While the inner loop integrates the core operators, a singular
traversal of the probability flow trajectory often fails to obtain
an optimal channel estimate. The substantial gap between the
Gaussian prior and the observed posterior, coupled with nonlinear corrections from proximal projections, typically results
in suboptimal convergence. To address this, we propose a
recursive anchor refinement strategy. In contrast to standard
single-pass inference, this approach leverages the output of
the current stage to re-initialize the anchor for the subsequent
stage, effectively moving the generative flow source closer to
the target data manifold.
Formally, the anchor serves as the source state of the probability path at _t_ = 1, functioning as the fixed reference point
for the trajectory rectification step in (17). In this recursive
scheme, the final estimate from the _k_ -th inner loop, denoted


as **H** [(] _[k,N]_ [2] _[‚àí]_ [1)], is assigned as the anchor for the subsequent
iteration. This update rule is formulated as:


**H** [(] _[k]_ [+1] _[,]_ [0)] = **H** [(] _[k,N]_ [2] _[‚àí]_ [1)] _,_ (20)

**H** [(0] _[,]_ [0)] _‚àºCN_ ( **0** _,_ **I** ) _._ (21)

where **H** [(0] _[,]_ [0)] denotes the initial Gaussian noise. This recursion
proceeds for _N_ 1 outer iterations, after which the final state
**H** [(] _[N]_ [1] _[,]_ [0)] is output as the estimated channel matrix.
This anchor-based refinement functions as a critical selfcorrection mechanism. By re-initializing the flow source with
the most recent estimate, the algorithm effectively compensates for trajectory drift induced by the finite discretization
steps of the inner loop. Consequently, this recursive process
progressively steers the estimate toward a high-fidelity solution that is consistent with both learned priors and physical
measurements.


_D. Theoretical Analysis: Global Asymptotic Stability_

To establish a rigorous theoretical footing for the proposed
RC-Flow algorithm, we model the algorithm as a discrete
dynamical system governed by the composite operatorC _[N][r][√ó][N][t]_ _‚Üí_ C _[N][r][√ó][N][t]_ . Specifically, a single inner-loop iteration _T_ :
can be expressed as:

**H** [(] _[k]_ [+1] _[,]_ [0)] = _T_ ( **H** [(] _[k,]_ [0)] ) ‚âú _A ‚ó¶P ‚ó¶D_ ( **H** [(] _[k,]_ [0)] ) _,_ (22)

where _D_ represents the flow-matching denoiser, _P_ denotes
the physics-aware proximal projection, and _A_ is the anchored
rectification step.
To prove the existence of a fixed point **H** _[‚ãÜ]_ = _T_ ( **H** _[‚ãÜ]_ ), we
adopt the standard assumption of _Bounded Denoisers_, which is
widely used in the analysis of Plug-and-Play algorithms [43].

**Assumption 1** (Bounded Denoiser Hypothesis) **.** _The pre-_
_trained flow matching model acts as a bounded denoiser. This_
_indicates that for any input state_ **H** _‚àà_ C _[N][r][√ó][N][t]_ _, the output_
_magnitude predicted by the network is bounded by a constant_
_Bflow < ‚àû:_
_‚à•D_ ( **H** ) _‚à•F ‚â§_ _Bflow._ (23)

_This assumption implies that the generative prior imposes_
_finite local updates and does not introduce infinite energy into_
_the system._

Under this mild assumption, we can utilize the properties
of the physical projection operator to guarantee the existence
of equilibrium states.

**Theorem 1** (Existence of Fixed Point) **.** _Consider the com-_
_posite operator_ _with anchor coefficient t_ _[‚Ä≤]_ _i_
_regularization weight T_ _w >_ 0 _. Under Assumption 1, there exists_ _[‚àà]_ [(0] _[,]_ [ 1)] _[ and]_
_a compact convex set K ‚äÇ_ C _[N][r][√ó][N][t]_ _such that T maps K to_
_itself (T_ ( _K_ ) _‚äÜK). Consequently, by Brouwer‚Äôs Fixed-Point_
_Theorem [44], the algorithm admits at least one fixed point_
**H** _[‚ãÜ]_ _‚ààK._

_Proof._ See Appendix A.


Having established the existence of a fixed point, we now
analyze the stability of the RC-Flow solver. We define the total
Jacobian of the outer-loop operator _T_ at the fixed point **H** _[‚ãÜ]_ as



**J** _T_ ‚âú _[‚àÇ]_ _‚àÇ_ [vec(] vec( **[H]** **H** [(] _[k]_ [(][+1)] _[k]_ [)] ) [)]

generative priors often exhibit local expansion (i.e., Lipschitz _[|]_ **[H]** _[‚ãÜ]_ [. In high-dimensional inverse problems,]
constant _>_ 1) to recover high-frequency textures. However,
our framework relies on the geometric interplay between the
prior and the physical constraints. We encapsulate this via the
following hypothesis.

**Assumption 2** (Spectral Contraction of Composite Operator) **.**
_Let_ **J** _D,i and_ **J** _P,i be the Jacobians of the flow denoiser and the_
_physics-aware projection at the i-th inner step, respectively._
_We assume that the projection operator imposes sufficient_
_structural constraints to suppress the expansive modes of_
_the denoiser, such that the spectral radius of the composite_
_Jacobian_ **T** _i_ ‚âú **J** _P,i_ **J** _D,i satisfies:_

_œÅ_ ( **T** _i_ ) _Œ≥ <_ 1 _._ (24)
_‚â§_

_This condition implies that the error components orthogonal_
_to the physical manifold (e.g., hallucinations) are effectively_
_nullified, ensuring strict contraction in the eigen space of the_
_operator._

In fact, the above assumption always holds in our numerical
experiments, more details can be found in Appendix B of the
Supplementary Material. Under this assumption, we have the
following theorem:

**Theorem 2** (Global Asymptotic Stability in Generalized Metric Space) **.** _Suppose the inner-loop operators satisfy Assump-_
_tion 2, and the anchor coefficients_ _t_ _[‚Ä≤]_ _i_ _[}][N]_ _i_ =0 [2] _[‚àí]_ [1] _are time-varying_
_{_
_within_ [0 _,_ 1) _. Then there exists a generalized vector norm_ _‚àó_
_‚à•¬∑‚à•_
_under which the total Jacobian_ **J** _T is strictly contractive, i.e.,_
**J** _T_ _‚àó_ _<_ 1 _. Consequently, the recursive iteration converges_
_‚à•linearly to the fixed point ‚à•_ **H** _[‚ãÜ]_ _for any initialization within the_
_basin of attraction._

_Proof._ See Appendix C.

Therefore, due to the equivalence of norms in finitedimensional spaces, convergence in _‚àó_ implies global
_‚à•¬∑ ‚à•_
asymptotic stability in the standard Euclidean norm 2.
_‚à•¬∑ ‚à•_


IV. NUMERICAL RESULTS
In this section, we provide a comprehensive numerical
evaluation of the proposed RC-Flow framework across a wide
range of operational conditions. Following the algorithmic
established in Section III, the performance and robustness of
the scheme are systematically evaluated, with the resulting
data and comparative analyses detailed in the following subsections.


_A. System Configurations_

_a) Dataset Generation:_ We evaluate the performance of
the proposed RC-Flow using the 3GPP clustered delay line
(CDL) channel models, which represent realistic propagation
environments for 5G/6G MIMO systems [45]. The dataset
comprises four distinct channel profiles: CDL-A, CDL-B,
CDL-C, and CDL-D. Specifically, CDL-D represents a lineof-sight (LOS) environment, whereas CDL-B and CDL-C represent non-line-of-sight (NLOS) scenarios. CDL-A contains a
mixture of both LOS and NLOS components. For each channel


Table I
SYSTEM CONFIGURATIONS


**Parameter** **Value**
_Training Confgurations_
Optimizer Adam
Learning Rate 1 _√ó_ 10 _[‚àí]_ [4]

Batch Size 256
Total Epochs 1600
Training SNR Range _{‚àí_ 10 _, ‚àí_ 5 _, . . .,_ 30 _}_
Time Sample _œÉ_ ( _Œæ_ )
EMA Decay Rate 0.999
Precision bf16
_Inference Confgurations_
Time Schedule Parameter ( _Œª, Œ≤_ ) 2,2
Model Selection (Epoch) 1600
Inner Loop Number (25)
_N_ max, _N_ min 50,3
_Network Architecture_
Input/Output Channels 2 (Real/Imag)
Base Model Channels 32
Channel Multipliers (1, 2, 4)
Residual Blocks per Level 2
Time Embedding Dimension 128
Group Normalization Numbers 8
Activation (ResNet, Time MLP) SiLU, GELU
Downsampling/Upsampling Steps 2


model, we generate 10,000 independent channel realizations
for training and 100 realizations for testing. Furthermore,
a mixed dataset is constructed by aggregating the training
samples from all four CDL profiles.
_b) Network Architecture:_ The CFM network employs a
time-conditioned U-Net architecture, as illustrated in Fig. 2,
where real and imaginary channel components are processed
as a two-channel real-valued input. The backbone follows an
encoder-decoder structure with multi-level ResNet blocks [46],
whose specific configurations are detailed in Table I. Temporal
information _t_ is integrated into each block via sinusoidal
positional encoding and a scale-and-shift mechanism.
_c) Training and Inference:_ The model is optimized via
the CFM objective using the Adam optimizer. To ensure
robustness, the training SNR is uniformly sampled from
_S_ dB _‚àà{‚àí_ 10 _, ‚àí_ 5 _, . . .,_ 30 _}_ for each batch. Additionally, the
time step _t_ is sampled from a logit-normal distribution, defined
as _t_ = _œÉ_ ( _Œæ_ ) where _Œæ ‚àºN_ (0 _,_ 1). Furthermore, an exponential
moving average (EMA) is maintained to stabilize the weights,
with comprehensive configurations detailed in Table I.
During inference, the inner iteration number _N_ 2 is adaptively scheduled based on the noise standard deviation to
balance estimation accuracy and computational complexity, as
follows:
_N_ 2 = _N_ min + ‚àÜ _N,_ (25)







_,_ (26)



ÔøΩ2 [ÔøΩ]



In ~~pu~~ t Output


Fig. 2. The U-net architecture for flow matching. The yellow block represents
the convolutional residual module, the red block represents down-sampling
while the blue block represents up-sampling.


_t_ = 1 _._ 00 _t_ = 0 _._ 64 _t_ = 0 _._ 36 _t_ = 0 _._ 16 _t_ = 0 _._ 04 _t_ = 0 _._ 00 GT


Fig. 3. Visual evolution of the channel estimation during an outer loop.
The process recovers the channel structure from noise ( _t_ = 1) to the target
distribution ( _t_ = 0). GT refers to ground truth channel. Rows display the
intermediate outputs of the flow denoising ( **H** [Àú] ), physical projection ( **H** proj),
and anchor interpolation ( **H** [(] _[k,i]_ [+1)] ), respectively.


_d) Hardware and Software Environment:_ All experiments were implemented in Python 3.10 using the PyTorch
2.6.0 on the Ubuntu 22.04 system. The training and inference
processes were accelerated using an NVIDIA A100 GPU.


_B. Baselines_

To validate the effectiveness of the proposed method, we
compare it against several baselines, ranging from classical
estimation theory to recent deep learning-based approaches.
The following baselines are evaluated in the simulations:

_‚Ä¢_ **Linear Minimum Mean Square Error (LMMSE)** [10]:
This estimator represents the optimal linear strategy designed to minimize the Bayesian MSE between the estimated and true channel matrices. By incorporating prior
statistical knowledge, specifically the channel spatial correlation matrix, it effectively balances the reliability of
the received observation against the prior distribution of
the channel. The formulation leverages the second-order
statistics of the channel, expressed as:


**H** LMMSE = **Y** ( **P** _[H]_ **RHP** + _œÉ_ pilot [2] **[I]** [)] _[‚àí]_ [1] **[P]** _[H]_ **[R][H]** (27)

where **RH** denotes the channel covariance matrix representing the spatial correlation.

_‚Ä¢_ **Frequency-selective** **Atomic** **Decomposition** **(fsAD)**

[11]: Representing the class of CS methods, the fsAD algorithm exploits the inherent sparsity of millimeter-wave
channels in the continuous angular domain. It formulates
channel estimation as an atomic norm minimization problem. While effective for sparse channels, its performance
may degrade in rich scattering environments where the
channel exhibits low sparsity or when the number of



‚àÜ _N_ =



( _N_ max _‚àí_ _N_ min) _¬∑_ - loglog1010(( _œÉœÉ_ maxmax _/œÉ/œÉ_ minpilot))



log10( _œÉ_ max _/œÉ_ min)



where _N_ max and _N_ min represent the predefined upper and
lower bounds of the inner loop iteration numbers, while _œÉ_ max
and _œÉ_ min denote the noise standard deviation boundaries that
delineate the dynamic range of the environment.


(a) Train = CDL-C, Test = CDL-A ( _Œ±_ = 0 _._ 6)



(b) Train = CDL-C, Test = CDL-B ( _Œ±_ = 0 _._ 6)



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35



0


~~_‚àí_~~ ~~5~~



0




_‚àí_ 8 _‚àí_ 6




_‚àí_ ~~10~~ _‚àí_ ~~5~~ ~~0~~ ~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~
SNR (dB)



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35




_‚àí_ ~~10~~ _‚àí_ ~~5~~ ~~0~~ ~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~
SNR (dB)




_‚àí_ 5

_‚àí_ 8 _‚àí_ 6



(c) Train = CDL-C, Test = CDL-C ( _Œ±_ = 0 _._ 6)



(d) Train = CDL-C, Test = CDL-D ( _Œ±_ = 0 _._ 6)



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35



0


~~_‚àí_~~ ~~5~~



~~0~~


~~_‚àí_~~ ~~5~~

~~_‚àí_~~ ~~10~~




_‚àí_ 8 _‚àí_ 6




_‚àí_ ~~10~~ _‚àí_ ~~5~~ ~~0~~ ~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~
SNR (dB)



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35




_‚àí_ ~~10~~ _‚àí_ ~~5~~ ~~0~~ ~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~
SNR (dB)




_‚àí_ 8 _‚àí_ 6



Lower Bound
Score-based (CDL-C)



RC-Flow (CDL-C)
RC-Flow (In-Dist.)



RC-Flow (Mixed)
LMMSE (CDL-C)



L-DAMP
fsAD



Fig. 4. Performance comparison of various schemes trained on CDL-C channels and evaluated across diverse channel conditions in a 16 _√ó_ 64 mmWave
MIMO system when _Œ±_ = 0 _._ 6.



propagation paths increases significantly, challenging the
fundamental CS assumptions.

_‚Ä¢_ **Learned Denoising-based Approximate Message Pass-**
**ing (L-DAMP)** [14]: A model-based deep learning approach that unfolds the Approximate Message Passing
(AMP) algorithm into a neural network architecture. By
replacing the traditional shrinkage function in AMP with
a learned denoiser (typically a DnCNN [15]), L-DAMP
effectively learns the channel prior from training data to
reconstruct sparse signals from noisy measurements.

_‚Ä¢_ **Score-based** [34]: A generative approach based on the
noise conditional score network. This method learns the
gradient of the log-prior distribution (the score function)
of the channel. During inference, it employs annealed
Langevin dynamics to iteratively denoise a random initialization, producing a single sample from the posterior
distribution _p_ ( **h** _|_ **y** ). This represents the performance of
posterior sampling.

_‚Ä¢_ **Lower Bound** [34]: An enhanced inference strategy
utilizing the same pre-trained Score-based model as above

[34]. To establish a performance lower bound, this strat


egy approximates the MMSE estimator using the same
pre-trained Score-based model. Instead of relying on a
single posterior sample, it computes the posterior mean
E[ **h** _|_ **y** ] by averaging 50 independent samples generated
from parallel Langevin dynamics chains, thereby minimizing stochastic variance and providing the theoretically
optimal error floor for the score-based approach.


_C. CSI Estimation Evaluation_
To provide an intuitive understanding of the proposed
framework, Fig. 3 illustrates the temporal evolution of the
channel estimate of a single outer iteration. The three rows
depict the outputs of the core operators _D_, _P_, and _A_, respectively. Starting from an anchor source at _t_ = 1, the
prior estimate **H** [Àú] progressively uncovers the coarse latent
structure of the channel. Subsequently, the proximal projection
**H** proj enforces data consistency by aligning the generative
trajectory with the pilot observations. Finally, the rectification
balances both components to provide a refined anchor for the
subsequent iteration, effectively steering the trajectory source
toward the ground truth.


~~Score-based~~


~~5.89 M~~




_‚àí_ 20

_‚àí_ 22

_‚àí_ 24

_‚àí_ 26

_‚àí_ 28

_‚àí_ 30

_‚àí_ 32



_Œ±_ =0.3
~~_Nr_~~ ~~= 16~~ ~~_,_~~ ~~_Nt_~~ ~~= 64, ULA~~
_Nr_ = 32 _,_ _Nt_ = 128, ULA
~~_Nr_~~ ~~= 64~~ ~~_,_~~ ~~_Nt_~~ ~~= 256, UPA~~



~~LMMSE~~



~~L-DAMP~~


~~4.81 M~~


~~fsAD~~


~~RC-Flow~~

~~3.88 M~~



10 [2]


10 [1]


10 [0]



~~10~~ ~~_[‚àí]_~~ ~~[3]~~ ~~10~~ ~~_[‚àí]_~~ ~~[1]~~ ~~10~~ ~~[1]~~ ~~10~~ ~~[3]~~ ~~10~~ [5]
FLOPs (G)


Fig. 5. Visualization of the trade-off between algorithm performance and
computational cost. The evaluation is conducted with _Œ±_ = 0 _._ 6 at an SNR
of 30 dB. The bubble size represents the scale of model parameters, and the
color corresponds to the inference latency. The models were trained on CDLC and tested on CDL-D.



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35



~~_Œ±_~~ ~~=0.6~~
_Nr_ = 16 _,_ _Nt_ = 64, ULA
~~_Nr_~~ ~~= 32~~ ~~_,_~~ ~~_Nt_~~ ~~= 128, ULA~~
_Nr_ = 64 _,_ _Nt_ = 256, UPA




_‚àí_ ~~10~~ _‚àí_ ~~5~~ ~~0~~ ~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~
SNR (dB)



0


_‚àí_ 5


_‚àí_ 10


_‚àí_ 15



LMMSE
fsAD
L-DAMP
~~Score-based~~
RC-Flow


~~0~~ _._ ~~1~~ ~~0~~ _._ ~~2~~ ~~0~~ _._ ~~3~~ ~~0~~ _._ ~~4~~ ~~0~~ _._ ~~5~~ ~~0~~ _._ ~~6~~ ~~0~~ _._ ~~7~~ ~~0~~ _._ ~~8~~ ~~0~~ _._ ~~9~~ ~~1~~ _._ ~~0~~
_Œ±_




_‚àí_ 20


_‚àí_ 25


_‚àí_ 30



Fig. 7. Performance of the proposed schemes trained and tested on CDL-C
channels with different MIMO size.

_‚àí_ 10


_‚àí_ 15



Fig. 8. Performance of the proposed scheme trained and tested on CDL-C
channels at an SNR of 30 dB with different number of inner ( _N_ 2) and outer
loop iterations ( _N_ 1), when _Œ±_ = 0 _._ 6.


power-limited environments. Although the LMMSE estimator
is competitive across all SNR regimes, it consistently lags
behind the proposed RC-Flow by a margin exceeding 2 dB.
The generalization capabilities of the generative priors
are further analyzed through the Mixed model and out-ofdistribution (OOD) evaluations. The Mixed model maintains
robustness at high SNR but suffers pronounced NMSE degradation at low SNR (excluding CDL-B) relative to its indistribution counterparts. This stems from the structural ambiguity of a generalized prior, which lacks the specialization necessary to resolve profile-specific features when measurement
guidance is weak. In OOD scenarios, the RC-Flow trained
on CDL-C exhibits an NMSE floor of approximately _‚àí_ 11 dB
when evaluated on CDL-A and CDL-B. In contrast, negligible
saturation is observed on CDL-D channels, where the NMSE
decreases consistently with SNR, maintaining a 5 dB gap
relative to the in-distribution benchmark. This discrepancy is
attributed to channel complexity: the rich scattering in CDLA and B increases estimation difficulty, whereas the dominant
LOS components in CDL-D facilitate simpler recovery.
Finally, conventional baselines such as L-DAMP and fsAD



_N_ 1 = 1
~~_N_~~ ~~1 = 2~~
_N_ 1 = 3
_N_ 1 = 4
~~_N_~~ ~~1 = 6~~


~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~ ~~35~~ ~~40~~ ~~45~~ ~~50~~
_N_ 2



Fig. 6. Performance of different schemes trained on CDL-C channels and
tested on CDL-D channels at an SNR of 10 dB with different pilot density.


Fig. 4 illustrates the robustness of the proposed RC-Flow in
CDL- _{_ A,B,C,D _}_ environments, employing a 16 _√ó_ 64 MIMO
configuration with _Œ±_ = 0 _._ 6. For comparison, a Mixed RCFlow model is trained on a composite dataset of CDL_{_ A,B,C,D _}_ comprising 40,000 channel realizations. Furthermore, the performance of in-distribution models trained specifically on each corresponding CDL model is included to characterize the estimation accuracy within matched environments.
As for performance measurement, we use normalized mean
squared error (NMSE), defined as:

NMSE = 10 log10 _‚à•_ **H** est _‚àí_ **H** _‚à•F_ [2] (28)
**H** _F_
_‚à•_ _‚à•_ [2]

In in-distribution scenarios, the proposed RC-Flow outperforms the score-based baseline and closely approaches
the lower bound. While RC-Flow exhibits nearly identical
precision to the score-based method in high-SNR regimes,
it achieves a substantial performance gain of approximately
2 dB in the low-SNR region ( _‚àí_ 10 dB to _‚àí_ 5 dB). This
superior noise resilience underscores the potential of RCFlow as a high-fidelity estimation framework, particularly for


_Œ≤_ = 1
_Œ≤_ = 2
~~_Œ≤_~~ ~~= 16~~



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35



SNR=-10 dB
~~SNR=10 dB~~
SNR=30 dB


~~0 10 20~~ ~~40~~ ~~80~~ ~~100~~ ~~200~~ ~~400~~ ~~800~~ ~~1600~~
Training Epochs



~~20~~ ~~25~~ ~~30~~ ~~35~~




_‚àí_ 35 ~~0~~ ~~10~~ ~~20~~ ~~30~~ ~~40~~ ~~50~~

_N_ 1


Fig. 11. The iterative process of the algorithm‚Äôs dynamics with different _Œ≤_,
where _Œ±_ = 0 _._ 6 and SNR = 30 dB.



0


_‚àí_ 5

_‚àí_ 10

_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30




_‚àí_ 32 _._ 0


_‚àí_ ~~32~~ _._ ~~5~~


~~_‚àí_~~ ~~33~~ ~~_._~~ ~~0~~



Fig. 9. Performance of the proposed scheme trained and tested on CDL-C
channels with different training epochs, when _Œ±_ = 0 _._ 6.


0


_‚àí_ 5

_‚àí_ 10




_‚àí_ 2 _._ 9


_‚àí_ 3 _._ 0


_‚àí_ 3 _._ 1



_Œ≤_ = 1
_Œ≤_ = 2
~~_Œ≤_~~ = ~~16~~




_‚àí_ 15

_‚àí_ 20

_‚àí_ 25

_‚àí_ 30

_‚àí_ 35



SNR=-10 dB
SNR=10 dB
SNR=30 dB


~~0.06~~ ~~0.24~~ ~~0.96~~ ~~3.88~~ ~~15.5~~
Model Params (M)



~~10~~ ~~20~~ ~~30~~ ~~40~~



~~0~~ ~~50~~ ~~100~~ ~~150~~ ~~200~~ ~~250~~ ~~300~~
_N_ 1


Fig. 12. The iterative process of the algorithm‚Äôs dynamics with different _Œ≤_,
where _Œ±_ = 0 _._ 6 and SNR = -10 dB.


coherence time and renders them impractical for real-time
deployments. Specifically, the Score-based approach involves
a prohibitive FLOPs count at the 10 [4] G level, necessitating
substantial hardware resources. In contrast, the proposed RCFlow delivers near-optimal NMSE with an inference latency
at the 10 [0] ms magnitude.


_E. System Scaling Analysis_
_a) Performance Scaling with Pilot Overhead:_ Fig. 6
illustrates the NMSE performance as a function of the pilot density _Œ± ‚àà{_ 0 _._ 1 _,_ 0 _._ 2 _, . . .,_ 1 _._ 0 _}_ . All evaluated methods
demonstrate consistent NMSE reduction as _Œ±_ increases, with
the proposed RC-Flow achieving a significant performance
advantage. Specifically, the LMMSE estimator demonstrates
robust performance across all pilot densities, showing particular resilience in the extremely low pilot density regime
( _Œ±_ = 0 _._ 1). However, it still lags significantly behind both
the RC-Flow and Score-based methods in the medium-to-high
regimes. In terms of L-DAMP, while it provides competitive
results at very low pilot densities, its performance scales
poorly with _Œ±_, resulting in marginal gains in the medium-tohigh density regimes. Conversely, the fsAD method is more



0 _._ 5


0 _._ 0


_‚àí_ 0 _._ 5

_‚àí_ 1 _._ 0

_‚àí_ 1 _._ 5

_‚àí_ 2 _._ 0

_‚àí_ 2 _._ 5

_‚àí_ 3 _._ 0



Fig. 10. Performance of the proposed scheme trained and tested on CDL-C
channels with different model size.


provide relatively reliable and consistent performance across
all scenarios, particularly for CDL-C and D. However, a
critical limitation of fsAD is its strict dependence on precise
antenna array geometry, which is often unavailable in practical
deployments. Similarly, while Compressed Sensing (CS) based
approaches align with the structural sparsity of CDL models,
their reliance on idealistic sparsity assumptions often proves
infeasible in complex real-world environments.


_D. Complexity Analysis_
Fig. 5 illustrates the comprehensive trade-off among estimation accuracy (NMSE), the number of floating-point operations
(FLOPs), and inference latency for the evaluated algorithms.
Notably, FLOPs and latency are reported as the averaged time
per sample, measured with a batch size of 100 on an NVIDIA
A100 GPU. It is evident that the proposed RC-Flow achieves a
superior balance between reconstruction fidelity and computational overhead. While the LMMSE and L-DAMP estimators
offer extremely low latencies at the 10 _[‚àí]_ [1] ms magnitude,
they fail to provide competitive NMSE results. Conversely,
the Score-based method and fsAD exhibit excessive latencies
on the order of 10 [3] ms, which exceeds the typical channel


sensitive to pilot density, showing poor performance at low _Œ±_
but improving faster as _Œ±_ increases, though a substantial gap
persists relative to the generative solvers. Notably, while RCFlow and the Score-based method achieve similar precision
for _Œ± >_ 0 _._ 4, RC-Flow demonstrates a noticeable advantage in
the low-pilot regime, outperforming the Score-based baseline
by 3 dB at _Œ±_ = 0 _._ 2. These findings underscore the remarkable
resilience of RC-Flow to limited pilot overhead.
_b) Scalability Across Massive MIMO Dimensions:_ Fig.
7 illustrates the NMSE performance of RC-Flow across three
distinct MIMO configurations with _Œ± ‚àà{_ 0 _._ 3 _,_ 0 _._ 6 _}_, while
maintaining antenna spacing at half-wavelength. For each
antenna dimension, we train a separate model individually.
As observed, the estimation accuracy improves consistently
as the MIMO dimensions scale up. This performance gain
is primarily attributed to the superior angular resolution and
the more pronounced structural sparsity inherent in larger
antenna arrays. These characteristics allow the generative prior
to provide more precise guidance within high-dimensional
channel manifolds.


_F. Hyperparameter Analysis_

_a) Impact of Inner and Outer Iteration Numbers:_ Fig.
8 illustrates the NMSE performance across varying outer and
inner iteration numbers ( _N_ 1 and _N_ 2) at an SNR of 30 dB with
_Œ±_ = 0 _._ 6. For a fixed _N_ 1, the NMSE decreases significantly as
_N_ 2 increases, typically plateauing beyond _N_ 2 = 25. Notably,
the marginal gains from increasing _N_ 2 are more pronounced
at lower _N_ 1 values, whereas this influence diminishes as
_N_ 1 becomes larger. For a fixed _N_ 2, increasing _N_ 1 leads to
significant improvements when _N_ 1 is small, with gains becoming marginal once _N_ 1 exceeds approximately 4 steps. These
observations validate the efficacy of the proposed recursive
anchor refinement mechanism. However, it is evident that both
the outer processes and the inner fine-grained steps exhibit
marginal effect, as the performance improvements gradually
plateauing.
_b) Impact of Training Epochs:_ Fig. 9 illustrates the
NMSE performance of the proposed method as a function
of training epochs across three SNR regimes ( _Œ±_ = 0 _._ 6).
Estimation accuracy consistently improves across all SNR
levels as training progresses. Notably, the performance gains
obtained through extended training are significantly more
pronounced in high-SNR scenarios compared to low-SNR
regimes. This phenomenon stems from the varying dependence
on the learned generative prior across different noise levels.
In high-SNR environments, where measurements are relatively
reliable, a well-trained network is significant to resolve finegrained manifold structures for high-precision reconstruction.
Conversely, at low SNR, performance is primarily limited by
dominant measurement noise, leading to the earlier saturation
of gains provided by the generative prior.
_c) Impact of Model Scale:_ Fig. 10 evaluates the NMSE
of RC-Flow across five model scales under three noise levels,
utilizing the CDL-C dataset with _Œ±_ = 0 _._ 6. Notably, in
low-to-medium SNR regimes, the model scale has a slight
impact on the estimation accuracy. However, at high SNR,
the performance improves significantly with model scale and



reaches saturation at 3.88M parameters. As detailed in the
preceding paragraph, this phenomenon stems from the varying
dependence on the channel prior across different noise levels.
The model scale reflects the ability to characterize the complex
channel prior. While a larger model can better capture the intricate structure of channel priors, the representational capacity
of the model becomes saturated beyond a certain scale under a
fixed volume of training data. Consequently, further expansion
of parameters provides diminishing gains and may even lead
to performance degradation due to potential overfitting.


_G. Analysis of Algorithm Dynamics_

_a) Convergence Dynamics and Œ≤ Tuning:_ Figs. 11 and 12
illustrate the convergence dynamics of the proposed algorithm
with _Œ±_ = 0 _._ 6 and _Œª_ = 2 under high and low SNR regimes,
respectively.
In the high SNR case, the NMSE exhibits a monotonic
decreasing trend with respect to _N_ 1, eventually stabilizing
at a convergence floor. Increasing _Œ≤_ significantly accelerates
convergence. For instance, at _Œ≤_ = 16, the algorithm achieves
near-optimal performance within a single iteration. However,
this acceleration incurs a marginal performance loss of approximately 0.1 dB compared to lower _Œ≤_ settings. Conversely,
at low SNR case, the dynamics follow a non-monotonic
pattern: the NMSE initially descends to a minimum ‚Äòsweet
spot‚Äô, followed by a gradual ascent toward an asymptotic
plateau. Notably, as _Œ≤_ increases, the minimum point shifts
toward the bottom-left, indicating simultaneous improvements
in inference accuracy and speed. Furthermore, larger _Œ≤_ values
accelerate the transition to the plateau phase while achieving
a lower converged NMSE.
A comparative analysis reveals that while the algorithm
shares a common initialization dynamics, its asymptotic behavior is strictly governed by the signal quality. In the initial
phase, both regimes exhibit a rapid descent in NMSE dominated by noise suppression, where the model effectively steers
the estimate toward the inherently smooth posterior mean.
However, subsequent behaviors diverge due to the perceptiondistortion trade-off [47]. At _‚àí_ 10 dB, the NMSE rebounds after
reaching a minimum ‚Äòsweet spot‚Äô as the model proceeds to
synthesize high-frequency structural details to satisfy manifold constraints. However, due to the ill-posed nature of the
problem, these generated stochastic features suffer from spatial
misalignment, effectively prioritizing perceptual consistency
at the expense of pixel-wise fidelity. Conversely, as the SNR
increases to 30 dB, reliable measurements ensure that the
synthesized fine-grained textures remain faithful to the true
channel state. Consequently, the convergence region aligns
with the sweet spot, avoiding the distortion penalty associated with hallucinatory features and resulting in the strictly
monotonic trajectory where no early stopping is required.
_b) Sensitivity analysis of Œª-Œ≤ configurations:_ Fig. 13
illustrates a comprehensive sensitivity analysis of RC-Flow
regarding hyperparameters _Œª_ and _Œ≤_ at _Œ±_ = 0 _._ 6. The heatmaps
(a) ‚Äì (c) represent the NMSE achieved at the dynamics sweet
spot, while the line graphs (d) ‚Äì (f) depict the outer iteration
numbers required to reach this point within the maximum limit
of _N_ 1 = 100.


(b) SNR = 10 dB


1.4 0.9 0.5 -0.9 -13.0 -14.6 -14.3 -14.0


1.4 0.9 0.2 -3.8 -15.9 -15.1 -14.6 -14.5


1.4 0.5 -1.0 -13.8 -15.9 -15.2 -14.9 -14.8


0.0 -1.2 -11.2 -16.2 -15.7 -15.1 -15.0 -15.0


-5.5 -14.5 -16.4 -15.9 -15.4 -15.2 -15.2 -15.2


-16.3 -16.2 -15.5 -15.1 -15.1 -15.0 -14.9 -15.0


-13.7 -13.5 -13.4 -13.4 -13.3 -13.3 -13.4 -13.3


-10.1 -9.9 -9.2 -10.6 -11.0 -11.0 -11.0 -11.0


2 _[‚àí]_ [2] 2 _[‚àí]_ [1] 2 [0] 2 [1] 2 [2] 2 [3] 2 [4] 2 [5]
_Œ≤_

(e) SNR = 10 dB


2 _[‚àí]_ ~~[2]~~ 2 _[‚àí]_ ~~[1]~~ 2 ~~[0]~~ 2 ~~[1]~~ 2 ~~[2]~~ 2 ~~[3]~~ 2 ~~[4]~~ 2 ~~[5]~~
_Œ≤_



(c) SNR = 30 dB


0.1 -0.9 -1.9 -2.7 -4.0 -31.7 -32.2 -32.1


0.1 -0.9 -1.9 -2.9 -20.1 -32.6 -32.6 -32.4


0.1 -0.9 -2.1 -13.1 -32.9 -32.7 -32.7 -32.7


0.0 -1.8 -15.3 -33.0 -32.8 -32.8 -32.8 -32.7


-10.4 -31.9 -32.8 -32.7 -32.7 -32.6 -32.6 -32.6


-32.5 -32.3 -32.2 -32.1 -32.1 -32.1 -32.1 -32.0


-28.2 -28.2 -28.1 -28.1 -28.1 -28.1 -28.1 -28.1


-27.4 -27.3 -27.3 -27.3 -27.3 -27.3 -27.4 -27.4


2 _[‚àí]_ [2] 2 _[‚àí]_ [1] 2 [0] 2 [1] 2 [2] 2 [3] 2 [4] 2 [5]
_Œ≤_

(f) SNR = 30 dB


2 _[‚àí]_ ~~[2]~~ 2 _[‚àí]_ ~~[1]~~ 2 ~~[0]~~ 2 ~~[1]~~ 2 ~~[2]~~ 2 ~~[3]~~ 2 ~~[4]~~ 2 ~~[5]~~
_Œ≤_



100


80


60


40


20


0



(a) SNR = -10 dB


-0.6 -2.0 -2.7 -2.7 -2.8 -2.8 -2.8 -2.8


-0.7 -2.1 -2.8 -2.7 -2.8 -2.7 -2.7 -2.8


-0.7 -2.2 -3.0 -3.0 -3.0 -3.0 -2.9 -3.0


-1.2 -2.5 -2.9 -3.1 -3.1 -3.1 -3.1 -3.1


-2.4 -2.9 -3.0 -3.0 -3.1 -3.1 -3.1 -3.1


-3.0 -3.0 -3.1 -3.0 -3.1 -3.0 -3.1 -3.1


-2.9 -2.9 -2.9 -2.8 -2.9 -2.8 -2.8 -2.7


-2.8 -2.8 -2.8 -2.9 -2.7 -2.7 -2.8 -2.8


2 _[‚àí]_ [2] 2 _[‚àí]_ [1] 2 [0] 2 [1] 2 [2] 2 [3] 2 [4] 2 [5]
_Œ≤_

(d) SNR = -10 dB


2 _[‚àí]_ ~~[2]~~ 2 _[‚àí]_ ~~[1]~~ 2 ~~[0]~~ 2 ~~[1]~~ 2 ~~[2]~~ 2 ~~[3]~~ 2 ~~[4]~~ 2 ~~[5]~~
_Œ≤_



0


_‚àí_ 5


_‚àí_ 10


_‚àí_ 15


_‚àí_ 20


_‚àí_ 25


_‚àí_ 30


_Œª_

2 [5]

2 [4]

2 [3]

2 [2]

2 [1]

2 [0]

2 _[‚àí]_ [1]

2 _[‚àí]_ [2]



Fig. 13. Sensitivity analysis of hyperparameters _Œª_ (Flow Schedule) and _Œ≤_ (Anchor Schedule) across different SNR regimes. The heatmap displays the NMSE
(dB), and the line graph displays the least number of outer iterations required to reach the sweet spot as show in Fig. 12.



Analysis of the heatmaps and line graphs reveal a distinct correspondence between reconstruction fidelity and convergence efficiency. Regarding NMSE performance, optimal
results are consistently achieved in the moderate _Œª_ regime,
while larger _Œª_ values significantly decelerate the convergence
process. This occurs because an excessively high _Œª_ causes
the time variable _t_ to decrease too rapidly, diminishing the
flow-predicted velocity and resulting in smaller per-step updates. Furthermore, while increasing _Œ≤_ generally accelerates
convergence, enabling RC-Flow to reach the sweet spot in
a single _N_ 1 step under medium-to-high SNR scenarios, it
induces a slight performance degradation of approximately
1 dB in medium SNR regimes. In the low-SNR regime, at
least _N_ 1 = 6 iterations are required under an optimal _Œª_ - _Œ≤_
configurations. This suggests that higher _Œ≤_ values prioritize
convergence speed, potentially at the expense of absolute
precision in intermediate noise conditions.


Finally, we highlight that certain _Œª_  - _Œ≤_ configurations, particularly in the upper-left regions of the heatmaps. The sharp
performance degradation in these regions does not necessarily
imply that such _Œª_ - _Œ≤_ combinations result in poor potential
performance, but rather that the trajectory has not yet reached
the sweet spot within the maximum _N_ 1 limitation. This underscores the significance of balanced scheduling to optimize
both reconstruction fidelity and computational efficiency.



V. CONCLUSION


In this paper, we presented RC-Flow, a unified generative
framework for high-dimensional MIMO channel estimation.
By reformulating the inference process as a closed-loop
fixed-point iteration, our method addresses the fundamental
limitations of open-loop generative samplers in ill-posed inverse problems. The core novelty lies in the serial restart
mechanism combined with anchored trajectory rectification,
which effectively suppresses error propagation, even under
extreme noise conditions. This design allows RC-Flow to
achieve SOTA accuracy across a wide range of SNRs and
pilot density, reducing inference latency by orders of magnitude compared to Score-based approaches. We believe this
framework establishes a new paradigm for solving constrained
inverse problems, paving the way for low-latency, high-fidelity
generative inference in 6G communications.


APPENDIX A
PROOF OF THEOREM 1


To invoke Brouwer‚Äôs Fixed-Point Theorem, we consider the
composite operator of the entire inner loop, denoted asC _[N][r][√ó][N][t]_ _‚Üí_ C _[N][r][√ó][N][t]_, which maps the input state **H** [(] _[k,]_ [0)] to the _T_ :
output **H** [(] _[k]_ [+1] _[,]_ [0)] after _N_ 2 recursive steps. We must identify a
compact convex ball _R‚ãÜ_ such that ( _R‚ãÜ_ ) _R‚ãÜ_ .
_K_ _T_ _K_ _‚äÜK_


_A. Continuity_

The neural network _D_ (with continuous activation functions), the linear matrix operations in the projection _P_, and
the convex combination with the anchor are all continuous
mappings. Since the matrix inverse ( **M** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] exists
is continuous onand is continuous for C _[N][r][√ó]_ _w_ _[N][t][‚àí]_ . [1] _>_ 0, the composite operator _T_


_B. Self-Mapping on a Compact Set_

We establish a global upper bound on the projected states,
independent of the iteration index. Based on the Bounded
Denoiser Hypothesis (Assumption 1), the output is bounded
by _‚à•D_ ( **H** ) _‚à•F ‚â§_ _B_ flow. Consider the projection step at any
inner index _i_ :

**H** [(] proj _[i]_ [)] [= (] **[R]** [ +] _[ w][‚àí]_ [1] _[D]_ [(] **[H]** [(] _[k,i]_ [)][))(] **[M]** [ +] _[ w][‚àí]_ [1] **[I]** [)] _[‚àí]_ [1] _[.]_ (29)

Applying the norm inequality and the spectral bound
_w_ _[‚àí]_ [1] ( **M** + _w_ _[‚àí]_ [1] **I** ) _[‚àí]_ [1] 2 1:
_‚à•_ _‚à•_ _‚â§_



_‚à•_ **H** [(] proj _[i]_ [)] _[‚à•][F][ ‚â§‚à•]_ **[R]** [(] **[M]** [ +] _[ w][‚àí]_ [1] **[I]** [)] _[‚àí]_ [1] _[‚à•][F]_ [ +] _[ ‚à•D]_ [(] _[¬∑]_ [)] _[‚à•][F][ ¬∑]_ [ 1]



_‚â§_ _C_ obs + _B_ flow ‚âú _R_ limit _._ (30)



The spectral radius of **J** _D,i_ consistently exceeds unity, yet
exhibits a clear asymptotic trend toward stability threshold
as the inner iterations progress. This behavior is analytically
tied to the generative refinement term _t ¬∑_ **V** in the update
formula; as the temporal variable _t_ monotonically decreases
toward zero across the flow trajectory, the effective influence
of the generative prior is gradually attenuated. Consequently,
the operator‚Äôs expansive tendency is suppressed, causing its
spectral radius to shrink toward the stability boundary. In
contrast, **J** _P,i_ maintains a spectral radius strictly below unity.
This stabilizing behavior can be analytically explained by the
structure of the proximal solution. Specifically, the eigenvalues
of the linear part of, denoted by _¬µi_, satisfy the relationship
_P_
_¬µi_ = 1 _/_ ( _wŒªi_ +1), where _Œªi_ represent the eigenvalues of matrix
**M** . Consequently, the spectral radius is expressed as


_œÅ_ ( **J** _P,i_ ) = max _¬µi_ = max 1 _/_ ( _wŒªi_ + 1) _._ (31)
_i_ _|_ _|_ _i_ _|_ _|_

As _t_ evolves from 1 toward 0, the variance annealing parameter
_w_ monotonically decreases, causing the denominator to approach unity. Consequently, the spectral radius increases from
a highly contractive state and asymptotically approaches the
stability boundary.
Intriguingly, _œÅ_ ( **T** _i_ ) is not only maintained below the stability boundary but is also consistently lower than _œÅ_ ( **J** _P,i_ )
alone. This phenomenon suggests a synergy rooted in the
complementary compression between the learned generative
prior and the physical manifold. The flow dynamics provide
update directions aligned with the measurement likelihood,
effectively reinforcing the proximal contraction to enhance the
stability margin and accelerate convergence.
With increasing inner loop iterations _N_ 2, all spectral radii
asymptotically converge toward unity, signifying a quasiequilibrium state where generative refinement and data consistency reach a dynamic balance. Crucially, the convergence
of _œÅ_ ( **T** _i_ ) from below the threshold ensures that the algorithm
maximizes informational utility while strictly adhering to the
stability and contractivity requirements throughout the iterative
evolution.


APPENDIX C
PROOF OF THEOREM 2

In this appendix, we provide a rigorous derivation of the
global asymptotic stability in a generalized metric space for
the RC-Flow algorithm. The objective is to bound the spectral
radius of the total Jacobian matrix **J** _T_ C _[N]_ _[√ó][N]_ (where _N_ =
_‚àà_
_NtNr_ ), which governs the sensitivity of the output of the _k_ -th
outer iteration, **H** [(] _[k]_ [+1] _[,]_ [0)], with respect to its input, **H** [(] _[k,]_ [0)] .


_A. Stability Criterion via Induced Metric Space_

We invoke Assumption 2, which postulates that the composite inner-loop operator **T** _i_ ‚âú **J** _P,i_ **J** _D,i_ satisfies the spectral
contraction condition _œÅ_ ( **T** _i_ ) _Œ≥ <_ 1. According to the
_‚â§_
Householder theorem, for any matrix **A** with _œÅ_ ( **A** ) _<_ 1 and
any given _Œ∑ >_ 0, there exists an induced vector norm _‚àó_
_‚à•¬∑ ‚à•_
such that:
**A** _‚àó_ _œÅ_ ( **A** ) + _Œ∑._ (32)
_‚à•_ _‚à•_ _‚â§_



Crucially, _R_ limit is a constant determined solely by the observations and the physical prior manifold. It does not depend on
the norm of the intermediate state **H** [(] _[k,i]_ [)] or the step index _i_ .
Define the invariant ball _R_ limit ‚âú **H** **H** _F_ _R_ limit,
_K_ _{_ _| ‚à•_ _‚à•_ _‚â§_ _}_
which is inherently a convex set. Assume the initialization
satisfies **H** [(] _[k,]_ [0)] _R_ limit, implying the anchor _**œµ**_ _R_ limit .
_‚ààK_ _‚ààK_
Since we have established that any projected state is bounded
by _R_ limit, we also have **H** [(] proj _[i]_ [)] limit [. The update rule]

_[‚ààK][R]_
**H** [(] _[k,i]_ [+1)] = _t_ _[‚Ä≤]_ _i_ _**[œµ]**_ [ + (1] _[ ‚àí]_ _[t][‚Ä≤]_ _i_ [)] **[H]** [(] proj _[i]_ [)] [constitutes a convex com-]
bination, where _t_ _[‚Ä≤]_ _i_ [= (1] _[ ‚àí]_ [(] _[i]_ [ + 1)] _[/N]_ [2][)] _[Œ≤]_ [. By the definition]
of convexity, the resulting state **H** [(] _[k,i]_ [+1)] must remain within
satisfies _KR_ limit for any **H** [(] _[k,N]_ [2] _t_ [)] _[‚Ä≤]_ _i_ _[‚àà]_ _R_ [[0] limit _[,]_ [ 1)] . [. By induction, the final output]
_‚ààK_
Therefore, we have proved that if the input **H** [(] _[k,]_ [0)] _R_ _[‚ãÜ]_,
then the output of the composite inner loop ( **H** [(] _[k,]_ [0)] ) _‚ààKR‚ãÜ_ .
Thus, is a self-mapping on the compact convex set _T_ _‚ààKR‚ãÜ_ .
_T_ _K_
By Brouwer‚Äôs Fixed-Point Theorem, there exists at least one
fixed point **H** _[‚ãÜ]_ _R‚ãÜ_ .
_‚ààK_


APPENDIX B
NUMERICAL VALIDATIONS OF ASSUMPTION 2

Fig. 14 illustrates the evolution of the spectral radius as
a function of the inner iteration numbers _N_ 2 under various
outer iteration settings _N_ 1. Specifically, the first and second
rows correspond to the initial and fully converged iterations,
respectively. The simulation parameters are configured as
_Œ≤_ = 16 _, Œª_ = 2 _, Œ±_ = 0 _._ 6 within a 16 _√ó_ 64 MIMO system
across three distinct SNR regimes. A stability threshold of 1 _._ 0
is established as the convergence criterion, where the spectral
radius consistently below this boundary signifies algorithmic
convergence. While only the initial and fully converged stages
are presented, the spectral radius evolution across the intermediate progression of _N_ 1 iterations exhibits qualitatively similar
behavior.


SNR = -10 dB, _N_ 1 = 1

1 _._ 25


1 _._ 00


0 _._ 75


0 _._ 50


0 _._ 25

~~1~~ ~~2~~ ~~3~~
_N_ 2

SNR = -10 dB, _N_ 1 = 500


1 _._ 0


0 _._ 9


0 _._ 8


0 _._ 7


~~1~~ ~~2~~ ~~3~~
_N_ 2



SNR = 10 dB, _N_ 1 = 3
1 _._ 2


1 _._ 0


0 _._ 8


0 _._ 6


0 _._ 4


~~2~~ ~~4~~ ~~6~~ ~~8~~ ~~10~~ ~~12~~ ~~14~~
_N_ 2



1 _._ 25


1 _._ 00


0 _._ 75


0 _._ 50


0 _._ 25


1 _._ 0


0 _._ 8


0 _._ 6


0 _._ 4



SNR = 30 dB, _N_ 1 = 1


~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~ ~~35~~ ~~40~~ ~~45~~ ~~50~~
_N_ 2

SNR = 30 dB, _N_ 1 = 3


~~5~~ ~~10~~ ~~15~~ ~~20~~ ~~25~~ ~~30~~ ~~35~~ ~~40~~ ~~45~~ ~~50~~
_N_ 2



1 _._ 25


1 _._ 00


0 _._ 75


0 _._ 50


0 _._ 25



SNR = 10 dB, _N_ 1 = 1


~~2~~ ~~4~~ ~~6~~ ~~8~~ ~~10~~ ~~12~~ ~~14~~
_N_ 2



Composite Spectral Radius Denoiser Spectral Radius Physics Spectral Radius Stability Threshold (1.0)


Fig. 14. The evolution of spectral radius for the composite operator _P ‚ó¶D_, the denoiser _D_, and the physics-based operator _P_ as a function of inner iteration
numbers ( _N_ 2) under various SNR and outer iteration number.



However, since the operator **T** _i_ is time-varying, strictly establishing stability requires a common metric. Given the
smooth evolution of the parameters, we analytically extend
Assumption 2 to postulate the existence of a common induced
norm for the sequence **T** _i_ _i_ =0, such that:
_‚à•¬∑ ‚à•_ _{_ _}_ _[N]_ [2] _[‚àí]_ [1]

**T** _i_ _‚àó_ _Œ≥_ _[‚Ä≤]_ _<_ 1 _,_ _i._ (33)
_‚à•_ _‚à•_ _‚â§_ _‚àÄ_

Consequently, our proof strategy is to establish the contraction
of the total Jacobian **J** _T_ within this generalized metric space
(C _[N]_ _,_ _‚àó_ ).
_‚à•¬∑ ‚à•_


_B. Differential Dynamics of the Inner Loop_

Consider the _k_ -th outer loop iteration. Let the input state be
**H** [(] _[k,]_ [0)] . According to Algorithm 1, the anchor point is reset
to the previous estimate, i.e., _**œµ**_ = **H** [(] _[k,]_ [0)] . The inner loop
generates a sequence of states _{_ **H** [(] _[k,i]_ [)] _}i_ _[N]_ =0 [2] [, where the update]
rule at the _i_ -th inner step is given by:

**H** [(] _[k,i]_ [+1)] = _t_ _[‚Ä≤]_ _i_ **[H]** [(] _[k,]_ [0)][ + (1] _[ ‚àí]_ _[t][‚Ä≤]_ _i_ [)] _[P]_ [(] _[D]_ [(] **[H]** [(] _[k,i]_ [)][))] _[,]_ (34)

where _P_ ( _¬∑_ ) and _D_ ( _¬∑_ ) denote the physics-aware projection and
flow-matching denoiser operators, respectively, and _t_ _[‚Ä≤]_ _‚àà_ [0 _,_ 1)
is the time-varying anchor coefficient.
We define the cumulative sensitivity matrix at inner step
_i_ as **J** [(] _[i]_ [)] ‚âú _‚àÇ‚àÇ_ vec(vec( **HH** [(k][(k] _[,][,]_ [0)][i)] )) [. By applying the chain rule to the]
recurrence relation, we obtain:


_‚àÇ_ vec( **H** [(k] _[,]_ [i+1)] ) _‚àÇ_ vec( **H** [(k] _[,]_ [i)] )

= _t_ _[‚Ä≤]_ **I** + (1 _t_ _[‚Ä≤]_ ) **J** _P,i_ **J** _D,i_
_‚àÇ_ vec( **H** [(k] _[,]_ [0)] ) _‚àí_ _‚àÇ_ vec( **H** [(k] _[,]_ [0)] ) _[,]_ [ (35)]

where **I** is the identity matrix resulting from differentiating the
anchor term _t_ _[‚Ä≤]_ **H** [(] _[k,]_ [0)] . Using the definition of the composite



Jacobian **T** _i_, the dynamics follow a non-homogeneous linear
recurrence:


**J** [(] _[i]_ [+1)] = _t_ _[‚Ä≤]_ _i_ **[I]** [ + (1] _[ ‚àí]_ _[t]_ _i_ _[‚Ä≤]_ [)] **[T]** _[i]_ **[J]** [(] _[i]_ [)] _[.]_ (36)


_C. Recursive Expansion and Closed-Form Solution_

The base case for the recurrence is **J** [(0)] = **I** . To reveal the
structure of the total Jacobian, let us define the path transition
operator **Œ¶** _m,j_, which represents the accumulated decay and
rotation from step _j_ to _m_ :



_D. Strict Contraction Analysis via Partition of Unity_

We now apply the induced norm _‚àó_ defined in (33) to
_‚à•¬∑ ‚à•_
the closed-form expansion (39). Using the triangle inequality



**Œ¶** _m,j_ ‚âú



ÔøΩÔøΩ _m‚àí_ 1
_n_ = _j_ [(1] _[ ‚àí]_ _[t]_ _n_ _[‚Ä≤]_ [)] **[T]** _[n][,]_ if _m > j,_
(37)
**I** _,_ if _m_ = _j._



Expanding the recurrence relation (36) iteratively:


**J** [(1)] = _t_ _[‚Ä≤]_ 0 **[Œ¶]** [1] _[,]_ [1] [+] **[ Œ¶]** [1] _[,]_ [0] _[,]_

**J** [(2)] = _t_ _[‚Ä≤]_ 1 **[Œ¶]** [2] _[,]_ [2] [+] _[ t]_ 0 _[‚Ä≤]_ **[Œ¶]** [2] _[,]_ [1] [+] **[ Œ¶]** [2] _[,]_ [0] _[.]_ (38)


By induction, the total Jacobian **J** _T_ ‚âú **J** [(] _[N]_ [2][)] at the end of the
inner loop can be expressed as a weighted sum of historical
transition paths:



_._


(39)



**J** _T_ = **Œ¶** _N_ 2 _,_ 0
Residual of Initialization ~~ÔøΩ~~ ÔøΩÔøΩÔøΩ



+



_N_ 2 _‚àí_ 1

- _t_ _[‚Ä≤]_ _i_ **[Œ¶]** _[N]_ 2 _[,i]_ [+1]

_i_ =0 Accumulated Anchor InjectionsÔøΩ ~~ÔøΩÔøΩ~~ 

and the sub-multiplicativity property of the induced norm, we
obtain:




[6] K. Venugopal, A. Alkhateeb, N. Gonz¬¥alez Prelcic _et al._, ‚ÄúChannel Estimation for Hybrid Architecture-Based Wideband Millimeter Wave Systems,‚Äù _IEEE J. Sel. Areas Commun._, vol. 35,
no. 9, pp. 1996‚Äì2009, 2017.

[7] A. Alkhateeb, O. El Ayach, G. Leus _et al._, ‚ÄúChannel Estimation
and Hybrid Precoding for Millimeter Wave Cellular Systems,‚Äù
_IEEE J. Sel. Top. Signal Process._, vol. 8, no. 5, pp. 831‚Äì846,
2014.

[8] P. Schniter and A. Sayeed, ‚ÄúChannel estimation and precoder
design for millimeter-wave communications: The sparse way,‚Äù
in _Asilomar Conf. Signals, Syst. Comput._, 2014, pp. 273‚Äì277.

[9] D. Donoho, ‚ÄúCompressed sensing,‚Äù _IEEE Trans. Inf. Theory_,
vol. 52, no. 4, pp. 1289‚Äì1306, 2006.

[10] E. Nayebi and B. D. Rao, ‚ÄúSemi-blind Channel Estimation
for Multiuser Massive MIMO Systems,‚Äù _IEEE Trans. Signal_
_Process._, vol. 66, no. 2, pp. 540‚Äì553, 2018.

[11] B. N. Bhaskar, G. Tang, and B. Recht, ‚ÄúAtomic Norm Denoising
With Applications to Line Spectral Estimation,‚Äù _IEEE Trans._
_Signal Process._, vol. 61, no. 23, pp. 5987‚Äì5999, 2013.

[12] H. He, C.-K. Wen, S. Jin _et al._, ‚ÄúDeep Learning-Based Channel
Estimation for Beamspace mmWave Massive MIMO Systems,‚Äù
_IEEE Wirel. Commun. Lett._, vol. 7, no. 5, pp. 852‚Äì855, 2018.

[13] M. Soltani, V. Pourahmadi, A. Mirzaei _et al._, ‚ÄúDeep LearningBased Channel Estimation,‚Äù _IEEE Commun. Lett._, vol. 23, no. 4,
pp. 652‚Äì655, 2019.

[14] C. Metzler, A. Mousavi, and R. Baraniuk, ‚ÄúLearned D-AMP:
Principled neural network based compressive image recovery,‚Äù
in _Adv. Neural Inf. Process. Syst._, vol. 30, 2017.

[15] K. Zhang, W. Zuo, Y. Chen _et al._, ‚ÄúBeyond a Gaussian Denoiser:
Residual Learning of Deep CNN for Image Denoising,‚Äù _IEEE_
_Trans. Image Process._, vol. 26, no. 7, pp. 3142‚Äì3155, 2017.

[16] D. Fan, R. Meng, X. Xu _et al._, ‚ÄúGenerative diffusion models
for wireless networks: Fundamental, architecture, and state-ofthe-art,‚Äù _arXiv preprint arXiv:2507.16733_, 2025.

[17] F. Khoramnejad and E. Hossain, ‚ÄúGenerative AI for the Optimization of Next-Generation Wireless Networks: Basics, Stateof-the-Art, and Open Challenges,‚Äù _IEEE Commun. Surv. Tutor._,
vol. 27, no. 6, pp. 3483‚Äì3525, 2025.

[18] A. Shahid, A. Kliks, A. Al-Tahmeesschi _et al._, ‚ÄúLargescale AI in telecom: Charting the roadmap for innovation,
scalability, and enhanced digital experiences,‚Äù _arXiv preprint_
_arXiv:2503.04184_, 2025.

[19] F. Zhu, X. Wang, S. Jiang _et al._, ‚ÄúWireless large AI model:
Shaping the AI-native future of 6G and beyond,‚Äù _arXiv preprint_
_arXiv:2504.14653_, 2025.

[20] J. Ho, A. Jain, and P. Abbeel, ‚ÄúDenoising Diffusion Probabilistic
Models,‚Äù in _Int. Conf. Learn. Represent._, H. Larochelle, M. Ranzato, R. Hadsell _et al._, Eds., vol. 33. Curran Associates, Inc.,
2020, pp. 6840‚Äì6851.

[21] Y. Song, J. Sohl-Dickstein, D. P. Kingma _et al._, ‚ÄúScoreBased Generative Modeling through Stochastic Differential
Equations,‚Äù in _Int. Conf. Learn. Represent._, 2021.

[22] T. Lee, J. Park, H. Kim _et al._, ‚ÄúGenerating high dimensional
user-specific wireless channels using diffusion models,‚Äù _IEEE_
_Trans. Wirel. Commun._, pp. 1‚Äì1, 2025.

[23] T. Wu, Z. Chen, D. He _et al._, ‚ÄúCDDM: Channel Denoising
Diffusion Models for Wireless Semantic Communications,‚Äù
_IEEE Trans. Wirel. Commun._, vol. 23, no. 9, pp. 11 168‚Äì11 183,
2024.

[24] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu _et al._, ‚ÄúFlow Matching
for Generative Modeling,‚Äù in _Int. Conf. Learn. Represent._, 2023.

[25] G. Papamakarios, E. Nalisnick, D. J. Rezende _et al._, ‚ÄúNormalizing Flows for Probabilistic Modeling and Inference,‚Äù _J. Mach._
_Learn. Res._, vol. 22, no. 57, pp. 1‚Äì64, 2021.

[26] I. Kobyzev, S. J. Prince, and M. A. Brubaker, ‚ÄúNormalizing
Flows: An Introduction and Review of Current Methods,‚Äù _IEEE_
_Trans. Pattern Anal. Mach. Intell._, vol. 43, no. 11, pp. 3964‚Äì
3979, 2021.

[27] X. Liu, C. Gong, and Q. Liu, ‚ÄúFlow Straight and Fast: Learning
to Generate and Transfer Data with Rectified Flow,‚Äù in _Int. Conf._



**J** _T_ _‚àó_ **Œ¶** _N_ 2 _,_ 0 _‚àó_ +
_‚à•_ _‚à•_ _‚â§‚à•_ _‚à•_



_N_ 2 _‚àí_ 1

- _t_ _[‚Ä≤]_ _i_ _[‚à•]_ **[Œ¶]** _[N]_ 2 _[,i]_ [+1] _[‚à•][‚àó][.]_ (40)


_i_ =0



The norm of the transition operator is bounded by the product
of individual operator norms. Using **T** _n_ _‚àó_ _Œ≥_ _[‚Ä≤]_ _<_ 1, we
_‚à•_ _‚à•_ _‚â§_
have:




- (1 _‚àí_ _t_ _[‚Ä≤]_ _n_ [)] _[.]_

_n_ = _j_



**Œ¶** _N_ 2 _,j_ _‚àó_
_‚à•_ _‚à•_ _‚â§_



_N_ 2 _‚àí_ 1




_N_ 2 _‚àí_ 1 _N_ 2 _‚àí_ 1

- (1 _‚àí_ _t_ _[‚Ä≤]_ _n_ [)] _[‚à•]_ **[T]** _[n][‚à•][‚àó]_ _[‚â§]_ [(] _[Œ≥][‚Ä≤]_ [)] _[N]_ [2] _[‚àí][j]_ 
_n_ = _j_ _n_ = _j_



(41)
For brevity, let us define the cumulative scalar decay factor
_Pj_ :



_Pj_ ‚âú



_N_ 2 _‚àí_ 1

- (1 _‚àí_ _t_ _[‚Ä≤]_ _n_ [)] _[,]_ with _PN_ 2 ‚âú 1 _._ (42)

_n_ = _j_



Substituting this bound into the inequality (40), we arrive at:



**J** _T_ _‚àó_ _P_ 0( _Œ≥_ _[‚Ä≤]_ ) _[N]_ [2] +
_‚à•_ _‚à•_ _‚â§_



_N_ 2 _‚àí_ 1

- _t_ _[‚Ä≤]_ _i_ _[P][i]_ [+1][(] _[Œ≥][‚Ä≤]_ [)] _[N]_ [2] _[‚àí]_ [(] _[i]_ [+1)] _[.]_ (43)


_i_ =0



To prove strict contraction, we utilize the algebraic property
of the weighting coefficients. Observe that the recurrence
relation _Pi_ = (1 _‚àít_ _[‚Ä≤]_ _i_ [)] _[P][i]_ [+1][ implies] _[ t][‚Ä≤]_ _i_ _[P][i]_ [+1][ =] _[ P][i]_ [+1] _[‚àí][P][i]_ [. Conse-]
quently, the sum of the coefficients (excluding the contraction
factor _Œ≥_ _[‚Ä≤]_ ) forms a telescoping series:



_N_ 2 _‚àí_ 1

- _i_ =0 ( _Pi_ +1 _‚àí_ _Pi_ ) = _PN_ 2 = 1 _._ (44)



_P_ 0 +



_N_ 2 _‚àí_ 1

- _t_ _[‚Ä≤]_ _i_ _[P][i]_ [+1] [=] _[ P]_ [0] [+]

_i_ =0



This confirms that the weights form a partition of unity. Thus,
the upper bound in (43) represents a convex combination of
the terms ( _Œ≥_ _[‚Ä≤]_ ) _[k]_ .
Since _Œ≥_ _[‚Ä≤]_ _<_ 1 and _N_ 2 1, the factor ( _Œ≥_ _[‚Ä≤]_ ) _[k]_ is strictly less
_‚â•_
than 1 for all _k ‚â•_ 1. Specifically, the exponents in (43) are
_N_ 2 _, N_ 2 _‚àí_ 1 _, . . .,_ 0. Unless _t_ _[‚Ä≤]_ _N_ 2 _‚àí_ 1 [= 1][, the sum contains terms]
scaled by _Œ≥_ _[‚Ä≤]_ _<_ 1. Thus, the strict inequality holds:



**J** _T_ _‚àó_ _< P_ 0 1 +
_‚à•_ _‚à•_ _¬∑_



_N_ 2 _‚àí_ 1

- _t_ _[‚Ä≤]_ _i_ _[P][i]_ [+1] (45)

_i_ =0 _[¬∑]_ [ 1 = 1] _[.]_



Since **J** _T_ _‚àó_ _<_ 1, the mapping is a strict contraction in the
metric space _‚à•_ _‚à•_ (C _[N]_ _,_ _‚àó_ ). Therefore, the proof is complete.
_‚à•¬∑ ‚à•_

REFERENCES

[1] X. You, C.-X. Wang, J. Huang _et al._, ‚ÄúTowards 6G wireless
communication networks: Vision, enabling technologies, and
new paradigm shifts,‚Äù _Sci. China Inf. Sci._, vol. 64, no. 1, p.
110301, 2021.

[2] C.-X. Wang, X. You, X. Gao _et al._, ‚ÄúOn the Road to 6G:
Visions, Requirements, Key Technologies, and Testbeds,‚Äù _IEEE_
_Commun. Surv. Tutor._, vol. 25, no. 2, pp. 905‚Äì974, 2023.

[3] E. G. Larsson, O. Edfors, F. Tufvesson _et al._, ‚ÄúMassive MIMO
for next generation wireless systems,‚Äù _IEEE Commun. Mag._,
vol. 52, no. 2, pp. 186‚Äì195, 2014.

[4] L. Lu, G. Y. Li, A. L. Swindlehurst _et al._, ‚ÄúAn Overview of
Massive MIMO: Benefits and Challenges,‚Äù _IEEE J. Sel. Top._
_Signal Process._, vol. 8, no. 5, pp. 742‚Äì758, 2014.

[5] J. Tian, Y. Han, S. Jin _et al._, ‚ÄúAnalytical channel modeling:
From MIMO to extra large-scale MIMO,‚Äù _Chin. J. of Electron._,
vol. 34, no. 1, pp. 1‚Äì15, 2025.


_Learn. Represent._, 2023.

[28] E. F. Montesuma, F. M. N. Mboula, and A. Souloumiac, ‚ÄúRecent
Advances in Optimal Transport for Machine Learning,‚Äù _IEEE_
_Trans. Pattern Anal. Mach. Intell._, vol. 47, no. 2, pp. 1161‚Äì
1180, 2025.

[29] M. Pourya, B. E. Rawas, and M. Unser, ‚ÄúFLOWER: A
Flow-Matching Solver for Inverse Problems,‚Äù _arXiv preprint_
_arXiv:2509.26287_, 2025.

[30] S. T. Martin, A. Gagneux, P. Hagemann _et al._, ‚ÄúPnP-Flow: Plugand-Play Image Restoration with Flow Matching,‚Äù in _Int. Conf._
_Learn. Represent._, 2025.

[31] W. Liu, N. Ma, J. Chen _et al._, ‚ÄúFlow matching-based generative models for MIMO channel estimation,‚Äù _arXiv preprint_
_arXiv:2511.10941_, 2025.

[32] B. Fesl, M. Baur, F. Strasser _et al._, ‚ÄúDiffusion-Based Generative
Prior for Low-Complexity MIMO Channel Estimation,‚Äù _IEEE_
_Wirel. Commun. Lett._, vol. 13, no. 12, pp. 3493‚Äì3497, 2024.

[33] Z. Jiang, F. Zhu, S. Jiang _et al._, ‚ÄúOne-Step Generative Channel
Estimation via Average Velocity Field,‚Äù in _IEEE Wirel. Com-_
_mun. Netw. Conf. (WCNC)_, 2026.

[34] M. Arvinte and J. I. Tamir, ‚ÄúMIMO Channel Estimation Using
Score-Based Generative Models,‚Äù _IEEE Trans. Wirel. Commun._,
vol. 22, no. 6, pp. 3698‚Äì3713, 2023.

[35] X. Zhou, L. Liang, J. Zhang _et al._, ‚ÄúGenerative Diffusion Models for High Dimensional Channel Estimation,‚Äù _IEEE Trans._
_Wirel. Commun._, vol. 24, no. 7, pp. 5840‚Äì5854, 2025.

[36] Z. Chen, H. Shin, and A. Nallanathan, ‚ÄúGenerative Diffusion
Model-Based Variational Inference for MIMO Channel Estimation,‚Äù _IEEE Trans. Commun._, vol. 73, no. 10, pp. 9254‚Äì9269,
2025.

[37] Z. Diao, X. Zhou, L. Liang _et al._, ‚ÄúRobust MIMO Channel
Estimation Using Energy-Based Generative Diffusion Models,‚Äù
_IEEE Wirel. Commun. Lett._, vol. 15, pp. 820‚Äì824, 2026.

[38] B. Efron, ‚ÄúTweedie‚Äôs formula and selection bias,‚Äù _J. Am. Stat._
_Assoc._, vol. 106, no. 496, pp. 1602‚Äì1614, 2011.

[39] K. Kim and J. C. Ye, ‚ÄúNoise2score: tweedie‚Äôs approach to selfsupervised image denoising without clean images,‚Äù _Adv. Neural_
_Inf. Process. Syst._, vol. 34, pp. 864‚Äì874, 2021.

[40] S. Bai, J. Z. Kolter, and V. Koltun, ‚ÄúDeep equilibrium models,‚Äù
_Adv. Neural Inf. Process. Syst._, vol. 32, 2019.

[41] Z. Jiang and F. Zhu, ‚ÄúRC-Flow,‚Äù _[https://github.com/jzzh123/](https://github.com/jzzh123/RC-Flow)_
_[RC-Flow](https://github.com/jzzh123/RC-Flow)_, 2025.

[42] E. Tamir, N. Laabid, M. Heinonen _et al._, ‚ÄúConditional Flow
Matching for Time Series Modelling,‚Äù in _ICML Workshop_
_Struct. Probabilistic Inference & Gener. Model._, 2024.

[43] S. H. Chan, X. Wang, and O. A. Elgendy, ‚ÄúPlug-and-Play
ADMM for Image Restoration: Fixed-Point Convergence and
Applications,‚Äù _IEEE Trans. Comput. Imaging_, vol. 3, no. 1, pp.
84‚Äì98, 2017.

[44] R. B. Kellogg, T.-Y. Li, and J. Yorke, ‚ÄúA Constructive Proof of
the Brouwer Fixed-Point Theorem and Computational Results,‚Äù
_SIAM J. Numer. Anal._, vol. 13, no. 4, pp. 473‚Äì483, 1976.

[45] 3GPP, ‚ÄúStudy on channel model for frequencies from 0.5 to
100 GHz,‚Äù 2020, 3GPP TR 38.901 version 16.1.0 Release 16.

[46] K. He, X. Zhang, S. Ren _et al._, ‚ÄúDeep residual learning
for image recognition,‚Äù in _IEEE Conf. Comput. Vis. Pattern_
_Recognit. (CVPR)_, 2016, pp. 770‚Äì778.

[47] Y. Blau and T. Michaeli, ‚ÄúThe Perception-Distortion Tradeoff,‚Äù
in _IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)_, 2018,
pp. 6228‚Äì6237.


