## DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management

**Kieran Wood**
Oxford-Man Institute &
Machine Learning Research Group
University of Oxford
```
                 kieran.wood@eng.ox.ac.uk

```


**Stephen J. Roberts**
Machine Learning Research Group
University of Oxford
```
stephen.roberts@eng.ox.ac.uk

```

January 12, 2026


**Abstract**



**Stefan Zohren**
Machine Learning Research Group
University of Oxford
```
stefan.zohren@eng.ox.ac.uk

```


We propose **DeePM** ( **Dee** p **P** ortfolio **M** anager), a structured deep-learning macro portfolio manager
trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental
challenges in financial learning: (1) it resolves the asynchronous â€œragged filtrationâ€ problem via a
_Directed Delay_ ( _Causal Sieve_ ) mechanism that prioritizes causal impulse-response learning over
information freshness; (2) it combats low signal-to-noise ratios via a _Macroeconomic Graph Prior_,
regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a
_distributionally robust objective_ where a smooth worst-window penalty serves as a differentiable
proxy for Entropic Value-at-Risk (EVaR) â€“ a _window-robust_ utility encouraging strong performance
in the most adverse historical subperiods. In large-scale backtests from 2010â€“2025 on 50 diversified
futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are
roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily
closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer
architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s
â€œCTA (Commodity Trading Advisor) Winterâ€ and the post-2020 volatility regime shift, maintaining
consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer
environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior,
principled treatment of transaction costs, and robust minimax optimization are the primary drivers of
this generalization capability.


_**Keywords**_ Systematic Macro Â· Portfolio Management Â· Deep Learning Â· Attention Â· Graph Neural Networks Â· Transaction
Costs Â· Robust Optimization Â· Risk Measures


**1** **Introduction**


The central goal of systematic portfolio management is to construct asset allocations that generalize out-of-sample
under heavy-tailed returns, regime shifts, and significant trading frictions. While classical meanâ€“variance optimization

[Markowitz, 1952] provides a foundational framework, its practical deployment is plagued by â€œerror maximizationâ€

[Michaud, 1989], where small estimation errors in covariance matrices lead to unstable, turnover-intensive portfolios.
Consequently, modern approaches have increasingly pivoted toward machine learning pipelines. However, most existing
methods adopt a disjoint two-stage approach â€“ forecasting returns first, then performing a portfolio construction step â€“


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


which misaligns the training loss (Mean Squared Error) with the investorâ€™s ultimate utility (Net Risk-Adjusted Return)

[Gu et al., 2020, Elmachtoub and Grigas, 2022].


We propose **DeePM**, a _Structured_, _Risk-Robust_ **Dee** p-learning **P** ortfolio **M** anager that learns a trading policy end-to-end.
Unlike â€œblack boxâ€ approaches that treat assets as anonymous time series, DeePM injects specific domain structures â€“
macroeconomic graphs, realizability constraints, and robust risk measures â€“ directly into the architecture. We train
DeePM with a _window-robust_ objective that emphasizes the hardest historical subperiods: a differentiable soft-min
aggregation of rolling-window Sharpe ratios (defined in Sec. 5).


Directed Delay (lag)



V-VSN


LSTM


Adapter ğ’œ


Temporal

MHA



Adjacency

(macro

prior)



Static
context


Condition

FiLM


**Shared**
**temporal**
**backbone**

across
assets
(same
weights)



V-VSN


LSTM


Adapter ğ’œ


Temporal

MHA



V-VSN


LSTM


Adapter ğ’œ


Temporal

MHA



**...**


**...**


**...**


**...**



V-VSN


LSTM


Adapter ğ’œ


Temporal

MHA



Cross Attention Block (across assets)


Macro Graph Block (GAT/GCN)



Linear +

tanh



Linear +

tanh



Linear +

tanh tanh



tanh



Linear +



Robust Objective Loss Function


Figure 1: The DeePM pipeline. (1) **Temporal:** Per-asset history is processed via a hybrid backbone. (2) **Cross-Sectional:**
Assets attend to the global state using a causal Directed Delay. (3) **Structural/Topological:** Latent embeddings are
refined via a Macro-Graph GNN. (4) **Objective:** The network minimizes a robust loss combining pooled Net Sharpe
and a worst-window SoftMin penalty.


**1.1** **Decision Problem and Implementation Constraints**


Modern systematic macro trading requires learning sequential portfolio policies from noisy, non-stationary markets
while respecting strict realizability constraints, such as asynchronous global closes and path-dependent transaction costs.
At each decision time _ğ‘¡_, the agent maps a high-dimensional information set F _ğ‘¡_ to a vector of portfolio trading signals or
weights _ğ‘ğ‘¡_ âˆˆ R _[ğ‘]_ . Unlike stylized frictionless settings, practical deployment faces three binding constraints that define
the learning challenge:


(i) **Asynchronous Information (Ragged Filtration):** global macro universes are plagued by _asynchronous_
_market closes_ (e.g., Tokyo vs. New York). Naive application of standard attention mechanisms allows models
to exploit spurious correlations between â€œstaleâ€ and â€œfreshâ€ data, leading to look-ahead bias that vanishes in
production [Lo and MacKinlay, 1990]. Concretely, under asynchronous global closes, some assetsâ€™ same-day
information is not available at the portfolio decision time, even though other markets have already closed.
If a cross-asset module mixes _same-day_ representations across all assets, it can inadvertently condition on
information that would only be known later in the day for some markets, creating a subtle look-ahead that
disappears in live trading. Our policy must ensure that any cross-asset features used at time _ğ‘¡_ are measurable
with respect to the common information set available across _all_ assets at decision time. Furthermore, learning
spurious correlations instead of true causal links can lead to backtest overfit [Bailey et al., 2016].

(ii) **Path-Dependent Frictions:** Transaction costs are not merely a post-hoc deduction but a state-dependent
penalty on turnover (| _ğ‘¤ğ‘¡_     - _ğ‘¤ğ‘¡_ âˆ’1|). To be viable, the policy must learn to internalize execution efficiency,
filtering out high-turnover signals that do not survive costs.

(iii) **Regime Fragility:** objective functions based only on pooled averages, such as Sharpe-like criteria [Sharpe,
1966], can yield seemingly strong performance while concentrating losses into a small number of adverse


2


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


windows. Risk-measure theory motivates that maximizing average performance is insufficient and we should
complement it with tail-sensitive criteria [Rockafellar and Uryasev, 2000, Ahmadi-Javid, 2012]; we require the
objective to explicitly penalize the â€œworst-caseâ€ windows (minimax), prioritizing survival over average-case
maximization.


**1.2** **Our Approach: Structured Deep Portfolio Management**


DeePM addresses these challenges via a hierarchical architecture mirroring the workflow of a discretionary macro trader.
It processes data in three stages:


1. **Temporal Encoding (Hybrid VSN-LSTM-Attention):** Inspired by the Momentum Transformer [Wood et al.,
2023], we employ a specialized per-asset encoder. A **Vectorized Variable Selection Network (V-VSN)** first
filters high-dimensional noisy features. This is followed by an **LSTM** (Long Short-Term Memory) [Hochreiter
and Schmidhuber, 1997] backbone to model local path-dependence and denoise volatility, and a **Temporal**
**Self-Attention** mechanism to capture long-range dependencies and global regime shifts.


2. **Cross-Sectional Interaction (The Causal Sieve):** To resolve the asynchrony problem, we employ a **Directed**
**Delay** protocol ( _ğ‘¡_ âˆ’1) for cross-sectional attention. This transforms the attention mechanism into a differentiable
â€œCausal Sieve,â€ filtering out confounding intraday co-movements to isolate predictive impulse-response signals
(Transfer Entropy). As a reference point, we also explore â€œcascadingâ€ filtration which maximises information
freshness.


3. **Structural Regularization (Macro Graph):** A Graph Neural Network (GNN) projects latent representations
onto a fixed **Macroeconomic Prior Graph** (e.g., linking Rates to Foreign Exchange (FX) via carry, or Energy
to Inflation). This acts as a spectral low-pass filter, forcing the learned covariance to respect economic topology,
which stabilizes performance in low-data regimes.


The entire policy is optimized against a **Robust Net Sharpe** objective. We introduce a **SoftMin** penalty on windowed
performance, which we show is mathematically equivalent to minimizing the dual form of _Entropic Value-at-Risk_
_(EVaR)_ [Ahmadi-Javid, 2012], effectively training the model against an adversarial reweighting of history ( App.D.2).


The proposed framework offers a unified end-to-end toolkit for practitioners, integrating three distinct inductive biases:
_temporal_ (learning path-dependent regimes), _cross-sectional_ (isolating causal impulse-responses), and _topological_
(regularizing via economic structure).


**Contributions.** We make the following contributions:


1. **Decision-focused, cost-aware end-to-end learning.** We train portfolio policies directly against realized
risk-adjusted net returns, aligning the gradient descent landscape with the investorâ€™s utility. This yields net
risk-adjusted performance that is approximately double that of two-stage meanâ€“variance baselines and fifty
percent higher than that of the Momentum Transformer. The model further learns to naturally dampen turnover
without requiring heuristic constraints.


2. **The Primacy of Causal Robustness:** Through ablation studies, we show that our conservative _Directed Delay_
filtration (using strictly lagged cross-sectional data) outperforms the â€œcascadingâ€ filtration that maximizes data
freshness. This suggests that in macro trading, identifying robust causal drivers (Transfer Entropy) is more
valuable than minimizing information latency.


3. **Macroeconomic Graph Regularization:** We show that injecting a sparse prior of economic linkages prevents
overfitting. The inclusion of the Graph GNN reduces Maximum Drawdown by **21%** relative to a purely
data-driven attention model, confirming the value of domain knowledge as an inductive bias.


4. **Differentiable EVaR Optimization:** We bridge the gap between deep learning and coherent risk measures by
implementing the SoftMin objective. Empirical results show this penalty is the single largest driver of stability,
enabling the strategy to maintain positive performance during the post-2020 volatility transition.


5. **Optimising Net-of-Cost Returns:** We motivate, theoretically (Sec. 5.2) and empirically, that ensembling to
aggregate the trading signals across the top _ğ¾_ seeds improves performance net-of-cost and, crucially, training
with the full transaction costs leads to suboptimal performance. Our experiments demonstrate that training with
a transaction cost scaler of _ğ›¾_ = 0 _._ 5 improves net performance out of sample by approximately fifty percent,
compared to the full cost training.


3


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**1.3** **Paper roadmap**


Section 2 reviews related work on portfolio learning under frictions, cross-sectional modelling, structured priors, and
robust objectives. Section 3 formalizes the macroeconomic graph, decision protocol, data construction, and transactioncost model. Section 4 introduces DeePMâ€™s architecture, including filtration-respecting cross-asset conditioning and
macro graph regularization. Section 5 presents the robust learning objective and optimization procedure. Section 6
details the experimental design, reports empirical results, provides ablations and discusses managerial implications.
Section 8 concludes.


**2** **Related Literature**


We situate DeePM at the intersection of deep time-series momentum, end-to-end portfolio learning, and robust
optimization. For a comprehensive study of the techniques we employ in this work, please refer to Zhang and Zohren

[2025].


**2.1** **Deep Learning for Systematic Trend**


Systematic macro strategies typically exploit two primary market phenomena: _Momentum_ (or Trend), the tendency for
asset prices to persist in their direction over time [Jegadeesh and Titman, 1993, Moskowitz et al., 2012], and _Mean_
_Reversion_, the tendency for prices to return to a long-term equilibrium after becoming over-extended [De Bondt and
Thaler, 1985].


Modern systematic trading has moved beyond linear factors toward deep sequence modelling. Lim et al. [2019]
introduced _Deep Momentum Networks (DMNs)_, demonstrating that LSTMs could capture non-linear volatility scaling
and trend behaviors that linear _Time-series Momentum_ (TSMOM) [Moskowitz et al., 2012] misses. Wood et al. [2023]
extended this with the _Momentum Transformer_, applying multi-head attention to learn regime-switching dynamics
and global temporal dependencies. Furthermore, recent advancements [Wood et al., 2022] have integrated online
changepoint detection in attempt to adapt to rapid regime shifts. Whilst explicit detection is reactive, a regime-robust
approach seeks to internalize these vulnerabilities during training.


While effective, these approaches typically treat assets as independent time series or rely on implicit data-driven
correlations. DeePM advances this lineage by explicitly modelling the _cross-sectional_ structure of the market. We
combine the temporal strengths of the DMN/Transformer architectures with a _Macroeconomic Graph Prior_, regularizing
the learning process against known economic linkages to prevent overfitting in low-signal regimes.


**2.2** **Machine Learning for Portfolio Construction under Frictions**


Classical portfolio construction typically estimates parameters ( _ğœ‡ğ‘¡_ _,_ Î£ _ğ‘¡_ ) and then optimizes a static objective. For
meanâ€“variance, one solves



_ğ‘¤_ _[â˜…]_ _ğ‘¡_ [âˆˆ] [arg max]
_ğ‘¤_ âˆˆW




- _ğœ‡_ Ë† _ğ‘¡_ [âŠ¤] _[ğ‘¤]_ [âˆ’] _[ğœ‚]_ _,_ (1)

2 _[ğ‘¤]_ [âŠ¤][Î£][Ë†] _[ğ‘¡]_ _[ğ‘¤]_



where _ğœ‚>_ 0 represents the investorâ€™s risk aversion coefficient. In the unconstrained case, the solution is _ğ‘¤_ _[â˜…]_ _ğ‘¡_ [=] _[ ğœ‚]_ [âˆ’][1][ Ë†][Î£] _ğ‘¡_ [âˆ’][1] _ğœ‡_ Ë† _ğ‘¡_ .
A central difficulty is that the optimization step can _amplify_ estimation error: because the solution depends on Î£ [Ë†] _ğ‘¡_ [âˆ’][1][, small]
perturbations in ( Ë† _ğœ‡ğ‘¡_ _,_ Î£ [Ë†] _ğ‘¡_ ) can induce large changes in _ğ‘¤_ _[â˜…]_ _ğ‘¡_ [. Michaud [1989] characterized this as â€œerror maximization,â€]
motivating shrinkage, resampling, and risk-based allocations.


To overcome this, recent works propose _end-to-end_ learning. Zhang et al. [2020] introduced _Deep Learning for Portfolio_
_Optimization_, demonstrating that optimizing the Sharpe ratio directly outperforms two-stage estimation. However,
such architectures often rely on stacking asset features. This approach has three limitations: (i) it ignores the natural
_permutation symmetry_ of the portfolio (the order of assets is arbitrary); (ii) it lacks _structural priors_, forcing the model
to learn economic relationships from scratch; and (iii) it often assumes a synchronized data grid, which introduces
look-ahead bias in global portfolios with asynchronous closes. DeePM addresses these by using set-invariant attention
mechanisms and enforcing a filtration-respecting lag structure.


4


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**2.3** **Cross-Sectional Modeling with Attention and Representation Learning**


Cross-asset dependence can be modeled with data-dependent interactions over the tradable set U _ğ‘¡_ . A generic single-head
attention layer is


âˆ‘ï¸
_â„ğ‘–,ğ‘¡_ [(][1][)] [=] _ğ›¼ğ‘–ğ‘—,ğ‘¡_ _ğ‘‰â„_ [(] _ğ‘—,ğ‘¡_ [0][)] _[,]_


_ğ‘—_ âˆˆU _ğ‘¡_

expï¿½âŸ¨ _ğ‘„â„ğ‘–,ğ‘¡_ [(][0][)] _[, ğ¾â„]_ [(] _ğ‘—,ğ‘¡_ [0][)] [âŸ©]                        - (2)
_ğ›¼ğ‘–ğ‘—,ğ‘¡_ = _._

~~ï¿½~~ ~~ï¿½~~

~~ï¿½~~ _ğ‘˜_ âˆˆU _ğ‘¡_ [exp] âŸ¨ _ğ‘„â„ğ‘–,ğ‘¡_ [(][0][)] _[, ğ¾â„]_ _ğ‘˜,ğ‘¡_ [(][0][)] [âŸ©]


where _â„ğ‘–,ğ‘¡_ [(][0][)] [denotes the input embedding for asset] _[ ğ‘–]_ [at time] _[ ğ‘¡]_ [,] _[ â„]_ _ğ‘–,ğ‘¡_ [(][1][)] [represents the output representation,] _[ ğ›¼][ğ‘–ğ‘—,ğ‘¡]_ [is the]
attention weight quantifying the influence of asset _ğ‘—_ on _ğ‘–_, _ğ‘„, ğ¾,ğ‘‰_ are the learnable projection matrices, and âŸ¨Â· _,_ Â·âŸ© signifies
the inner product similarity score. To capture heterogeneous market signals across different feature subspaces, this
mechanism is expanded to Multi-Head Attention (MHA) by calculating multiple independent attention heads in parallel
and concatenating their results into a single enriched representation.


In portfolio problems the cross-section is naturally a _set_ : the ordering of assets is arbitrary, and the traded universe
can change with missingness, contract rolls, or data availability. A desirable inductive bias is therefore _permutation_
_equivariance_ : if we permute the asset order, the output representations (and hence positions) permute in the
same way. Formally, let Î  be a permutation matrix acting on the asset dimension of the stacked representation
_ğ»ğ‘¡_ = [ _â„_ 1 [(][0] _,ğ‘¡_ [)] _[, . . ., â„]_ _ğ‘,ğ‘¡_ [(][0][)] []][âŠ¤][. Set-based layers satisfy]


Layer(Î  _ğ»ğ‘¡_ ) = Î  Layer( _ğ»ğ‘¡_ ) _,_ (3)


which is a standard requirement in set representation learning [Zaheer et al., 2017, Lee et al., 2019]. Attention of the
form (2) is permutation equivariant when applied across the asset axis, making it well suited for variable-size universes
and existence masking.


Purely permutation-equivariant layers are _anonymous_ across elements; in markets, however, assets have persistent
identities (e.g., duration-sensitive rates vs. energy futures). DeePM incorporates this by injecting a categorical ticker
embedding _ğ‘’ğ‘–_ to the per-asset state before cross-sectional interaction. This preserves equivariance in (3) (the permutation
acts jointly on ( _â„_ [Ëœ] _ğ‘–,ğ‘¡_ [(][0][)] [)] _[ğ‘–]_ [âˆˆU] _[ğ‘¡]_ [), while allowing the model to condition on asset-specific effects and long-run heterogeneity.]


Permutation-equivariant attention is also a natural mechanism for transferring structure across regimes or assets by
matching patterns in a context set. For example, Wood et al. [2024] use cross-attention to adapt trend-following
decisions to new regimes from a small number of examples, highlighting the usefulness of attention-based set reasoning
in financial time series. Instead of conditioning on a basket of historical sequences for few-shot transfer, we use a
cross-sectional context set of contemporaneous assets (or their lag-aligned histories) to transfer information across the
universe.


**2.4** **Graph Priors and Economic Structure**


While data-driven attention can capture arbitrary correlations, financial covariance matrices are notoriously noisy and
unstable. Graph Neural Networks (GNNs) offer a mechanism to inject domain knowledge as a structural prior. By
defining a sparse adjacency matrix _ğ´_ based on economic fundamentals â€“ such as supply chains, sector classifications, or
macro-correlation regimes â€“ we impose a relational inductive bias. Critically, In Graph Neural Networks, the graph
topology imposes a structural constraint on where information may flow, not what is learned.


The standard Graph Convolutional Network (GCN) [Kipf and Welling, 2017] implements a fixed spectral filter:


_ğ»ğ‘¡_ [(][2][)] = _ğœ_                    - _ğ´ğ»_ Ëœ _ğ‘¡_ [(][1][)] _ğ‘Š_ 1ï¿½ _,_ (4)


where _ğ´_ [Ëœ] is the normalized adjacency matrix, and _ğœ_ (Â·) is a non-linear activation function. This operator enforces
_isotropic_ smoothing â€“ every neighbor affects the node equally (weighted by degree). In finance, however, economic
linkages are time-varying; a â€œSafe Haven" link between Bonds and Gold may be strong in crises but weak in growth
regimes.


To address this, Graph Attention Networks (GATs) [VeliÄkoviÄ‡ et al., 2018] introduce learnable, anisotropic attention
weights _ğ›¼ğ‘–ğ‘—_ masked by the graph structure and a message passing layer then updates node _ğ‘–_ by aggregating over its


5


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


graph neighborhood:



_â„ğ‘–_ [(][2][)] = _ğœ_ [ï¿½] 
     


âˆ‘ï¸

_ğ›¼ğ‘–ğ‘—ğ‘Šâ„_ [(] _ğ‘—_ [1][)] _,_
ï¿½ï¿½
_ğ‘—_ âˆˆN( _ğ‘–_ )

       


âˆ‘ï¸




         -          exp âŸ¨ _ğ‘„ğº_ _â„ğ‘–,ğ‘¡_ [(][1][)] _[, ğ¾][ğº]_ _[â„]_ [(] _ğ‘—,ğ‘¡_ [1][)] [âŸ©]
_ğ›¼ğ‘–ğ‘—,ğ‘¡_ _[ğº]_ [=]



(5)



_,_

~~ï¿½~~ ~~ï¿½~~

~~ï¿½~~ _ğ‘˜_ âˆˆN( _ğ‘–_ ) [exp] âŸ¨ _ğ‘„ğº_ _â„ğ‘–,ğ‘¡_ [(][1][)] _[, ğ¾][ğº]_ _[â„]_ _ğ‘˜,ğ‘¡_ [(][1][)] [âŸ©]



where N ( _ğ‘–_ ) = { _ğ‘—_ : _ğ´ğ‘–ğ‘—_ = 1} is defined by the (sparse) economic adjacency _ğ´_ . Unlike fully connected attention (Section
2.3), GAT restricts attention to economically plausible pairs defined by _ğ´_, while unlike GCN, it allows the model to
dynamically upweight or downweight these priors based on the current market state. Furthermore, both GCN and
GAT layers retain the property of permutation equivariance (under simultaneous permutation of node features and the
adjacency matrix), ensuring the model remains consistent with the set-based nature of the portfolio universe discussed
in Section 2.3. DeePM utilizes this architecture to balance structural regularization with regime-dependent flexibility.


**2.5** **Robust Objectives and Risk Measures**


Standard portfolio objectives maximize average performance (e.g., pooled Sharpe ratio), which can lead to overfitting on
specific regimes or â€œlucky windows" while ignoring tail risks. To address this, we draw on the literature of coherent risk
measures. Conditional Value-at-Risk (CVaR) [Rockafellar and Uryasev, 2000] at level _ğ›¼_ admits the variational form:



CVaR _ğ›¼_ ( _ğ‘_ ) = min
_ğœ‚_ âˆˆR




- 1 _ğœ‚_ + _._ (6)
1 âˆ’ _ğ›¼_ [E][[(] _[ğ‘]_ [âˆ’] _[ğœ‚]_ [)][+][]]



Entropic Value-at-Risk (EVaR) [Ahmadi-Javid, 2012] provides a strictly tighter upper bound via exponential tilting:



1
EVaR _ğ›¼_ ( _ğ‘_ ) = inf
_ğœ†>_ 0 _ğœ†_




- log E[e _[ğœ†ğ‘]_ ] âˆ’ log _ğ›¼_ _._ (7)



Crucially, these measures link directly to _distributionally robust optimization_ (DRO) or minimax frameworks. By
convex duality, minimizing EVaR is equivalent to minimizing the expected loss against an _adversarial distribution ğ‘„_
chosen from a uncertainty set defined by the Kullback-Leibler (KL) divergence:



EVaR _ğ›¼_ ( _ğ‘_ ) = sup
_ğ‘„_ â‰ª _ğ‘ƒ_



ï¿½E _ğ‘„_ [ _ğ‘_ ] | KL( _ğ‘„_ âˆ¥ _ğ‘ƒ_ ) â‰¤ _ğ¶ğ›¼_ - _,_ (8)



where _ğ¶ğ›¼_ = âˆ’ log(1 âˆ’ _ğ›¼_ ). DeePMâ€™s training objective employs a **SoftMin** penalty, defined in Sec. 5.1, over windowed
Sharpe [1] ratios. As we detail in App. D, this is mathematically isomorphic to the dual form of EVaR. It effectively trains
the policy against an implicit adversary who reweights the training windows ( _ğ‘„_ ) to emphasize the worst-performing
regimes, thereby enforcing robustness against distributional shifts.


**3** **Problem Setup and Data**


**3.1** **Universe and Decision Protocol**


We trade a diversified universe of _ğ‘_ = 50 liquid futures and FX contracts. Each asset _ğ‘–_ is assigned to a macro group
_ğ‘”_ ( _ğ‘–_ ) (e.g., Equities, Rates, Energy), which informs the structural graph prior (details in Appendix A.2).


Systematic macro portfolios face the challenge of _asynchronous market closes_ (e.g., Nikkei closes hours before S&P
500). We evaluate two protocols to define the admissible information set F _ğ‘¡_ : **(1) Global One-Day Lag (Primary)**
which enforces a strict delay, and **(2) Cascading Filtration (Ablation)** which maximizes information freshness. These
are defined mathematically in Sec. 4.3. Unless otherwise stated, results utilize the Global One-Day Lag to prioritize
structural robustness over latency.


We define the next-period arithmetic return as:


_ğ‘Ÿğ‘–,ğ‘¡_ +1 := _[ğ‘ƒ][ğ‘–,ğ‘¡]_ [+][1]                                 - 1 _._ (9)

_ğ‘ƒğ‘–,ğ‘¡_


This convention ensures that any cross-asset operation at decision date _ğ‘¡_ uses only { _ğ‘¥ğ‘–,ğ‘¡_ } _ğ‘–_ âˆˆU computed from { _ğ‘ƒğ‘–,ğ‘¢_ } _ğ‘¢_ â‰¤ _ğ‘¡_ .


1In Sec. 5.1, we also motivate why we use Sharpe ratio


6


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Figure 2: **Visualizing the Distributionally Robust Objective.** This figure illustrates the mechanics of the SoftMin
penalty using a synthetic mixture model (85% Normal regime N (1 _._ 0 _,_ 0 _._ 9 [2] ), 15% Crisis regime N (âˆ’2 _._ 0 _,_ 1 _._ 4 [2] )). **Panel**
**A** demonstrates the implicit distribution shift: while the empirical history _ğ‘ƒ_ (grey) has a positive mean (+0 _._ 55),
the adversarial reweighting _ğ‘„_ (red) shifts probability mass to the left tail, resulting in a significantly lower robust
utility (âˆ’4 _._ 37). This effective â€œhallucinationâ€ of a harsher environment forces the optimizer to prioritize survival in
worst-case regimes. **Panel B** displays the corresponding gradient weight function _ğ‘ğ‘_ âˆ exp(âˆ’SR _ğ‘_ / _ğœ_ ). The exponential
decay ensures that â€œeasyâ€ high-Sharpe windows contribute near-zero gradient signal (Complacency), while â€œhardâ€
negative-Sharpe windows dominate the parameter updates (Panic), effectively implementing a differentiable minimax
curriculum.


**3.2** **Portfolio Returns and Frictions**


The policy network outputs raw actions Ëœ _ğ‘ğ‘–,ğ‘¡_ âˆˆ R which are squashed to bounded risk weights

_ğ‘ğ‘–,ğ‘¡_ := tanh( Ëœ _ğ‘ğ‘–,ğ‘¡_ ) âˆˆ(âˆ’1 _,_ 1) _._ (10)


Let Ë† _ğœğ‘–,ğ‘¡_ _>_ 0 be an ex-ante daily volatility estimate (typically a 63-day EWMA). We define the volatility scaling factor ( _ğœ€_
being a small regularizer)

1
_ğ‘£ğ‘–,ğ‘¡_ := (11)
_ğœ_ Ë† _ğ‘–,ğ‘¡_ + _ğœ€_ _[.]_

The corresponding (vol-targeted) notional exposure [Moskowitz et al., 2012, Harvey et al., 2018] is

_ğ‘¤ğ‘–,ğ‘¡_ := _ğ‘£ğ‘–,ğ‘¡_ _ğ‘ğ‘–,ğ‘¡_ _._ (12)


7


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


We optimize the policy directly against _Net Portfolio Return_ ( _ğ‘…_ [net] ). We define _ğ‘…_ [net] by deducting linear transaction costs
from the gross volatility-scaled performance:



(13)



_ğ‘…ğ‘¡_ [net] +1 [:][=] _ğ‘_ [1] _ğ‘¡_



_ğ‘_
âˆ‘ï¸



_ğ‘šğ‘–,ğ‘¡_ _ğ‘ğ‘–,ğ‘¡_ _ğ‘¦ğ‘–,ğ‘¡_ +1

_ğ‘–_ =1



_ğ‘_
âˆ‘ï¸



_ğ‘šğ‘–,ğ‘¡_ _ğ‘ğ‘–_ ï¿½ï¿½ _ğ‘¤ğ‘–,ğ‘¡_   - _ğ‘¤ğ‘–,ğ‘¡_ âˆ’1ï¿½ï¿½
_ğ‘–_ =1




- _[ğ›¾]_

_ğ‘ğ‘¡_



**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

- ï¿½ï¿½ Transaction Cost



**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

- ï¿½ï¿½ 


_ğ‘…ğ‘¡_ [gross] +1



where _ğ‘¦ğ‘–,ğ‘¡_ +1 is the vol-scaled asset return, _ğ‘ğ‘–_ is the asset-specific cost parameter (derived in App. A.1), _ğ‘šğ‘–,ğ‘¡_ âˆˆ{0 _,_ 1} is
an availability mask indicating whether asset _ğ‘–_ is tradable/has valid data and _ğ›¾_ is a global scaling factor. We motivate in
App. B that an intermediate value _ğ›¾_ âˆˆ(0 _,_ 1) is theoretically optimal for maximizing out-of-sample net Sharpe under
ensembling. This formulation ensures the model explicitly learns to balance signal strength against turnover constraints.


**3.3** **Features and Preprocessing**


We construct a compact, stationarized feature vector _ğ‘¥ğ‘–,ğ‘¡_ âˆˆ R _[ğ¹]_ for each asset, designed to capture multi-scale trend
dynamics and tail risks. To strictly isolate the contribution of the learned structural priors and attention mechanisms,
we restrict the input space to be derived _exclusively_ from daily closing prices. While practical systematic macro
strategies typically incorporate explicit â€œCarryâ€ signals (e.g., interest rate differentials, forward points) and fundamental
macroeconomic indicators (e.g., Consumer Price Index, Purchasing Managersâ€™ Index releases), we omit these sources in
this study. This constraint forces the model to extract latent risk premia and regime shifts solely from endogenous price
dynamics.


**Ex-ante Volatility.** We estimate daily return variance Ë† _ğ‘ ğ‘–,ğ‘¡_ [2] [via an exponentially weighted moving average (EWMA)]
with a 63-day span. This serves as the normalization factor for all inputs and notional sizing.


**1. Volatility-Normalized Returns.** To capture momentum across horizons [Moskowitz et al., 2012], we compute
returns over windows _â„_ âˆˆ{1 _,_ 21 _,_ 63 _,_ 252} days (with 252 days representing a full year). Each return is scaled by ex-ante
volatility to ensure distribution stability:



R _ğ‘–,ğ‘¡_ [(] _[â„]_ [)] := _[ğ‘ƒ][ğ‘–,ğ‘¡]_ [/] _[ğ‘ƒ]_ ~~âˆš~~ _[ğ‘–,ğ‘¡]_ [âˆ’] _[â„]_ [âˆ’] [1]



_._ (14)
_â„_ + _ğœ€_



~~âˆš~~
_ğœ_ Ë† _ğ‘–,ğ‘¡_



**2. MACD Trend Filters.** Following Appel [1979], we include Moving Average Convergence Divergence (MACD)
signals to capture trend persistence. We compute the difference between fast and slow EWMAs at multiple scales
( _ğ‘†, ğ¿_ ) âˆˆ{(8 _,_ 24) _,_ (16 _,_ 48) _,_ (32 _,_ 96)}:

MACD _ğ‘†,ğ¿_ ( _ğ‘–, ğ‘¡_ ) := [EWM] _[ğ‘†]_ [(] _[ğ‘ƒ][ğ‘–]_ [)] _[ğ‘¡]_ [âˆ’] [EWM] _[ğ¿]_ [(] _[ğ‘ƒ][ğ‘–]_ [)] _[ğ‘¡]_ _._ (15)

_ğœ_ Ë† _ğ‘–,ğ‘¡_


The resulting MACD signals are subsequently re-normalized by their own 252-day rolling standard deviation to ensure
comparability across assets and time.


**3. Mean-Reversion** To detect over-extension and regime fragility [Harvey and Siddique, 2000, Bollinger, 2002], we
include **Price Z-Scores:** _ğ‘_ -score of log-prices over rolling windows _â„“_ âˆˆ{21 _,_ 252} to signal mean-reversion potential.


**4. Robust Outlier Control.** Financial data is heavy-tailed. To prevent gradient explosions, we apply a robust,
no-lookahead clipping filter. For each feature _ğ‘“ğ‘–,ğ‘¡_, we compute a rolling median ( _ğ‘šğ‘¡_ ) and Median Absolute Deviation
(MAD _ğ‘¡_ ) over 252 days. We clip values to [ _ğ‘šğ‘¡_ Â± 5 Ã— 1 _._ 48 Ã— MAD _ğ‘¡_ ], ensuring the model trains on the core distribution
rather than artifacts. We use MAD _ğ‘¡_ (rather than the sample standard deviation) because it is far less sensitive to
heavy tails and transient price spikes, scale it by 1 _._ 48 to obtain a _ğœ_ -comparable robust dispersion estimate under a
Gaussian reference model, and, in line with the literature [Lim et al., 2019, Wood et al., 2023], use a conservative 5Ã—
(approximately â€œ5 _ğœ_ â€) band so clipping targets only extreme outliers while preserving almost all typical observations.


**5. Feature Subsets and Parsimony.** Given the high capacity of the DeePM architecture, supplying highly correlated
inputs (e.g., overlapping return windows simultaneously with MACD filters) increases the risk of noise memorization
rather than generalization. To mitigate this, we do not use all features simultaneously. Instead, we evaluate two distinct,
parsimonious subsets: (1) a _Raw Momentum_ variant, consisting of the lagged returns ( _â„_ âˆˆ{1 _, . . .,_ 252}) and Z-scores;
and (2) a _Signal-Based_ variant, consisting only of the 1-day return ( _ğ‘Ÿ_ 1 _ğ‘‘_ ), the MACD trend filters, and Z-scores. This
separation forces the model to learn from orthogonal signals and reduces the likelihood of overfitting to redundant
feature correlations.


8


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**3.4** **Philosophy of the Macro-Structural Prior**


In high-dimensional macro environments, pure data-driven correlation learning often fails due to low signal-to-noise
ratios and non-stationarity. To mitigate this, we inject domain knowledge via a fixed _Macroeconomic Graph Prior_ .
We model the universe as a graph G = (V _,_ E), constructed deterministically from economic first principles rather
than noisy return correlations. We acknowledge that any hand-specified macro topology entails some subjectivity;
accordingly, we treat G as a _structural regularizer_ rather than a claim of ground-truth causality. Our design is (i)
_deterministic and ex ante_, specified from broad economic transmission channels (not fit to the sample), and (ii) _coarse_
_and interpretable_, relying only on high-level mechanisms for which there is broad agreement in macro/markets practice.
The resulting construction is summarized here and specified in full in App. A.2.



JPY only


**COMM**
**PRECIOUS**


**COMM**
**LIVESTOCK**



**FX_USD**

**(DXY)**



**EQUITY_US**

**(ES/NQ)**


**RATES.US**
**(Treasuries)**


**COMM**
**ENERGY**


**COMM**
**GRAINS**



**EQUITY_APAC**

**(Nikkei/HSI)**


**FX_G10**


**FX_SAFE**
**(JPY/CHF)**



**FX_EM**

**(MXN)**



**EQUITY_EU**

**(CAC/DAX)**


**RATES_EU**

**(Bund)**


**COMM**

**BASE**



**Asset Class Key**


Equities


Rates


FX


Commodities



**Connection Key**


Direct Financial Link


Input / Indirect Link



**COMM**
**SOFTS**



Figure 3: The Macro-Structural Prior Graph used to regularize cross-sectional attention. Edges encode deterministic
economic linkages rather than data-driven correlations.


1. **Sectoral Homophily (Cliques):** Assets within the same macro group (e.g., `RATES_US_TREASURY` ) are
assumed to share a single latent factor. These form fully connected sub-graphs.


2. **Supply Chain & Substitution:** We encode physical dependencies, such as the input-cost relationship between
_Energy_ and _Agriculture_, or the substitution effect between _Safe Haven FX_ and _Precious Metals_ .


3. **Macro-Finance Transmission:** We explicitly link disparate asset classes based on canonical transmission
mechanisms. For instance, the â€œCarry" channel links High-Yield FX to Sovereign Rates, and the â€œInflation
Triangle" connects Energy, Breakeven-sensitive Rates, and Precious Metals.


4. **Regional Integration:** We capture local monetary and fiscal channels by forming triangular linkages between
the primary equity index, sovereign bond, and currency of the same region, enforcing consistency across local
asset classes.


This graph serves as a structural scaffold applied _after_ the cross-sectional attention mechanism: we require the
attention-enriched representations to be smooth with respect to this economic topology, effectively regularizing the
high-capacity temporal encoders.


9


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**3.5** **Philosophy of the Transaction Cost Model**


To train a policy that is deployable in practice, the training signal must deduct realistic frictions. A naive constant cost
model (e.g., 1bp flat) fails to capture the heterogeneity of global liquidity. Instead, we implement a _Structural Minimum_
_Cost Model_ that synthesizes the â€œTick Size Constraint" theory [Harris, 2003] with market impact models [Kyle, 1985].


We define the transaction cost _ğ‘ğ‘–_ for asset _ğ‘–_ as a function of its microstructure regime:


_ğ‘ğ‘–_ â‰ˆ _ğ¶_ struct _,ğ‘–_ Ã— _ğœ†ğ‘–_ (16)


    - **The Structural Floor (** _ğ¶_ **struct):** In electronic order books, the quoted spread is lower-bounded by the exchangeâ€™s
Minimum Price Variation (tick size). For liquid instruments, the cost is dominated by this quantization noise.


    - **The Liquidity Scalar (** _ğœ†ğ‘–_ **):** We apply a multiplier _ğœ†ğ‘–_ â‰¥ 1 _._ 0 to account for institutional size impact. For deep
markets (e.g., S&P 500), _ğœ†_ â‰ˆ 1. For â€œgappy" markets (where there are holes in the order book e.g., Feeder
Cattle) or â€œRoach Motel" liquidity regimes (easy to enter, hard to exit, e.g., Palladium), _ğœ†_ scales significantly
higher to reflect the effective spread paid to sweep the book.


The exact derivation of these bands and the asset-specific overrides are detailed in App. A.3.


**4** **Method: DeePM Architecture**


The architecture we used is â€œstructuredâ€, which we use to denote two inductive biases. First, the cross-sectional module
is _permutation equivariant_ over assets: relabeling assets only relabels the corresponding outputs, which is essential
when each asset has a categorical identity embedding but the model should not depend on arbitrary ordering. Second,
we incorporate a fixed macro adjacency graph that regularizes cross-asset interactions toward economically plausible
transmission channels. Together these biases reduce overfitting and improve robustness under changing cross-asset
correlation regimes.


**4.1** **Network Inputs and Context**


At each decision time _ğ‘¡_, the model processes a rolling lookback window of length _ğ¿_ . While the financial return targets
and execution logic are defined in Section 3, the neural network operates on the following specific tensor representations.


For each asset _ğ‘–_ âˆˆU, we construct a feature vector _ğ‘¥ğ‘–,ğ‘¡_ âˆˆ R _[ğ¹]_ (as defined in Section 3.3). Let _ğ‘šğ‘–,ğ‘¡_ âˆˆ{0 _,_ 1} be the
existence mask indicating if asset _ğ‘–_ is tradable at time _ğ‘¡_ . The input to the temporal backbone is the window tensor:


_ğ‘‹ğ‘¡_ := { _ğ‘¥ğ‘–,ğ‘¡_                   - _â„“_ } _ğ‘–_ =1 _,...,ğ‘_ ; _â„“_ =1 _,...,ğ¿_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ¿]_ [Ã—] _[ğ¹]_ _._ (17)


To allow the shared network weights to learn _asset-specific dynamics_ (e.g., distinguishing between mean-reverting
FX pairs and trending commodities), we initialize a learnable _Static Asset Embedding ğ‘’ğ‘–_ âˆˆ R _[ğ‘‘]_ for each ticker _ğ‘–_ .
This embedding serves as a persistent identifier that breaks parameter anonymity while maintaining _permutation_
_equivariance_ â€”since the embedding _ğ‘’ğ‘–_ is permuted alongside the assetâ€™s dynamic features _ğ‘¥ğ‘–,ğ‘¡_, the model output is
independent of the arbitrary tensor ordering.


The embedding is concatenated with the static transaction cost parameter _ğ‘ğ‘–_ and projected to form a unified _Static_
_Context Vector ğ‘ ğ‘–_ :
_ğ‘ ğ‘–_ = Linear([ _ğ‘’ğ‘–_ ; _ğ‘ğ‘–_ ]) âˆˆ R _[ğ‘‘]_ _._ (18)


As detailed below, this context _ğ‘ ğ‘–_ is injected at two specific locations: (1) to modulate feature selection in the V-VSN,
and (2) to initialize the hidden state of the LSTM.


**4.2** **Temporal Backbone: Hybrid VSN-LSTM-Attention**


Motivated by state-of-the-art results achieved by Wood et al. [2023] in the univariate setting for systematic macro,
we process each assetâ€™s time series independently to extract a latent regime embedding _â„_ [temp] _ğ‘–,ğ‘¡_ [. We process using]
_Channel-independence_, which is a deliberate regularization: by sharing the same temporal encoder weights across assets,
the model learns a reusable dynamical feature extractor while avoiding direct cross-channel mixing at the raw-input
level, which can encourage fitting to idiosyncratic cross-asset noise [Nie et al., 2023]. Cross-asset dependence is instead
injected explicitly downstream via the Macro-Graph block (Sec. 4.4). Our backbone integrates three distinct inductive
biases: sparsity (VSN), local recurrence (LSTM), and global context (Attention).


10


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**1. Vectorized Variable Selection Network (V-VSN).** _Motivation:_ Financial data is high-dimensional and noisy;
regime changes often render specific features (e.g., momentum) irrelevant while others (e.g., mean reversion) become
dominant. To address this, we employ a _Vectorized VSN_ to dynamically weight input features. Unlike sequential Gated
Residual Networks [Lim et al., 2021], we implement this efficiently using grouped _1D Convolutions_ (Conv1D). Here, a
Conv1D with kernel size 1 acts as a time-shared linear projection applied independently to each feature channel across
the sequence, enabling efficient parallelization.


We generate feature-wise gating weights via _Feature-wise Linear Modulation (FiLM)_ [Perez et al., 2018]. Conditioned
on the static context _ğ‘ ğ‘–_, FiLM applies a channel-wise affine transformation (scale _ğ›¾_ and shift _ğ›½_ ) to the inputs. For input
_ğ‘¥ğ‘¡_, we compute sparse weights _ğ‘¤ğ‘¡_ âˆˆ[0 _,_ 1] _[ğ¹]_ and transformed features _ğ‘£ğ‘¡_ :


FiLM( _ğ‘¥ğ‘¡_ _, ğ‘ ğ‘–_ ) = _ğ›¾_ ( _ğ‘ ğ‘–_ ) âŠ™ _ğ‘¥ğ‘¡_ + _ğ›½_ ( _ğ‘ ğ‘–_ ) _,_

_ğ‘¤ğ‘¡_ = Softmax(Linear(FiLM( _ğ‘¥ğ‘¡_ _, ğ‘ ğ‘–_ ))) _,_



_ğ‘£ğ‘¡_ =



_ğ¹_ (19)
âˆ‘ï¸

_ğ‘¤ğ‘¡, ğ‘“_   - Conv1D( _ğ‘¥ğ‘¡, ğ‘“_ ) _,_

_ğ‘“_ =1



where âŠ™ denotes element-wise multiplication. This allows the model to learn, for example, that â€œTrend features are less
relevant for Asset A (high mean-reversion) than Asset B,â€ and suppress them accordingly.


**2. Local Recurrence (LSTM).** _Motivation:_ Financial time series exhibit strong path dependence and heteroskedastic
noise. Standard Transformers often struggle to filter high-frequency noise without very large datasets. We pass the
weighted feature stream _ğ‘£ğ‘¡_ through an LSTM layer. This provides a strong inductive bias for sequential processing and
acts as a non-linear low-pass filter, denoising local volatility spikes that can confuse downstream attention mechanisms.

_â„ğ‘–,_ 0 _, ğ‘ğ‘–,_ 0 = tanh( _ğ‘Š_ 0 _ğ‘ ğ‘–_ ) _,_ _â„_ [lstm] _ğ‘–,ğ‘¡_ = LSTM( _ğ‘£ğ‘–,ğ‘¡_ _, â„_ [lstm] _ğ‘–,ğ‘¡_ âˆ’1 [)] _[.]_ (20)


Note that the initial state ( _â„ğ‘–,_ 0 _, ğ‘ğ‘–,_ 0) is a projection of the asset embedding _ğ‘ ğ‘–_, priming the recurrence with the assetâ€™s
specific identity.


**3. Temporal Attention with Adapter.** _Motivation:_ Regime shifts (e.g., inflation shocks) may depend on events
occurring months in the past, beyond the effective memory of an LSTM. To capture these long-range dependencies, we
refine the LSTM state using a causal _Temporal Transformer_ [Lim et al., 2021] block (Temporal MHA).


_The ResSwiGLU Adapter:_ Before entering the attention mechanism, the LSTM output is processed by a specialized
adapter module A(Â·). The adapter is necessary to project the LSTMâ€™s robust local features into a richer, non-linear
representation that is optimized for the subsequent attention mechanism to capture complex global dependencies. We
employ a _Post-Norm_ configuration with SwiGLU activation [Shazeer, 2020]:


A( _ğ‘¥_ ) = LN _ğ‘¥_ + _ğ›¿_ [ï¿½] _ğ‘Š_ 2( _ğ‘Š_ 1 _ğ‘¥_ âŠ™ SiLU( _ğ‘‰ğ‘¥_ )) [ï¿½ï¿½] _,_ (21)

[ï¿½]

where _ğ‘Š_ 1 _, ğ‘Š_ 2 _,ğ‘‰_ are learnable weight matrices, _ğ›¿_ denotes _dropout_ [Srivastava et al., 2014], and LN(Â·) is Layer
Normalization. Layer-Norm stabilizes training by re-centering and scaling the hidden states to prevent gradient
explosion, while Dropout randomly zeroes out neurons during training to force the model to learn robust, redundant
features rather than overfitting to financial noise. The residual connection facilitates gradient flow and ensures the model
can default to the robust local features extracted by the LSTM if complex non-linear adaptation is unnecessary [He et al.,
2016]. The term _ğ‘Š_ 1 _ğ‘¥_ âŠ™ SiLU( _ğ‘‰ğ‘¥_ ) represents SwiGLU activation, a non-linear, gated linear unit that improves gradient
flow depthwise [Shazeer, 2020], where SiLU: _ğ‘§_ â†¦â†’ 1+ _ğ‘§_ e ~~[âˆ’]~~ ~~_[ğ‘§]_~~ [. Unlike Pre-Norm variants [Xiong et al., 2020], common in]
modern architectures, the normalization is applied _after_ the residual connection, which we empirically found stabilizes
the high-variance financial features.


_Note on Architecture Reuse:_ This adapter A(Â·) serves as the standard non-linear building block throughout DeePM. It is
reused as the Feed-Forward Network (FFN) in both the Cross-Sectional Attention layer (Section 4.3) and the Graph
Neural Network layer (Section 4.4), ensuring consistent gradient dynamics across temporal and spatial dimensions.


While the LSTM effectively filters high-frequency noise and captures local path dependence, its recursive nature suffers
from fading memory over long horizons. To resolve this, the final temporal embedding is produced by applying a causal
_Temporal Attention_ mechanism. This allows the model to bypass the recursive bottleneck and directly attend to relevant
historical regimes (e.g., a past inflation shock similar to the current state) regardless of their temporal distance:

_â„_ [temp] _ğ‘–,ğ‘¡_ = TempMHA( _ğ‘„_ = _ğ‘§ğ‘–,ğ‘¡_ _, ğ¾_ = _ğ‘§ğ‘–,_ â‰¤ _ğ‘¡_ _,ğ‘‰_ = _ğ‘§ğ‘–,_ â‰¤ _ğ‘¡_ ) _,_ (22)


where _ğ‘§ğ‘–,ğ‘¡_ = A( _â„_ [lstm] _ğ‘–,ğ‘¡_ [)][ and][ TempMHA][(Â·] _[,]_ [ Â·] _[,]_ [ Â·)][ is masked to mitigate look-ahead bias. The output is subsequently]
processed by the adapter A(Â·), yielding _ğ»ğ‘¡_ = A( _ğ»ğ‘¡_ [temp] ).


11


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**4.3** **Cross-Sectional Interaction: Filtration-Compliant Attention**


After temporal encoding, we have a tensor of independent asset embeddings _ğ»ğ‘¡_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ . To capture cross-asset spillover
(e.g., rates driving equities), we apply a _Cross-Sectional Multi-Head Attention (MHA)_ layer.


_Post-Norm with ReZero Gating:_ We implement this block using a Post-Norm configuration augmented with _ReZero_
gating [Bachlechner et al., 2021]. Standard Transformer connections can suffer from signal degradation in deep
networks; ReZero mitigates this by introducing a learnable scalar parameter _ğ›¼_ initialized to a small value (e.g., _ğ›¼_ â‰ˆ 0).
This initializes the layer as an identity map, allowing the optimizer to gradually introduce cross-sectional interactions
only where they provide predictive gain.

_ğ»ğ‘¡_ [attn] = LN _ğ»ğ‘¡_ + _ğ›¼_ cross Â· CrossMHA _ğ»ğ‘¡_ _,_ _ğ»_ [Ëœ] _ğ‘¡_ _,_ _ğ»_ [Ëœ] _ğ‘¡_ ï¿½ï¿½ _._ (23)

[ï¿½] [ï¿½]

The output is subsequently processed by the adapter A(Â·) defined in Section 4.2, yielding _ğ»ğ‘¡_ [cross] = A( _ğ»ğ‘¡_ [attn] ).


To handle the asynchronous nature of global closes rigorously, the Key/Value context _ğ»_ [Ëœ] _ğ‘¡_ is defined via the _Filtration-_
_Compliant Context_ :


1. **Directed Delay (Primary Protocol):** We strictly lag the entire cross-section to the previous close, _ğ»_ [Ëœ] _ğ‘¡_ = _ğ»ğ‘¡_ âˆ’1.
This enforces a causal information gap, compelling the model to learn predictive impulse-response functions
(Transfer Entropy).

2. **Cascading Filtration (Ablation):** We construct a composite context, using close times _ğ‘¡_ c where asset _ğ‘–_ sees
asset _ğ‘—_ â€™s state from day _ğ‘¡_ if and only if _ğ‘–_ closes earlier than _ğ‘—_ (e.g., US observing Japan).

_â„_ Ëœ [(] _ğ‘—,ğ‘¡_ _[ğ‘–]_ [)] [=][ I][(] _[ğ‘¡]_ [c] _[, ğ‘—]_ _[< ğ‘¡]_ [c] _[,ğ‘–]_ [)] _[ â„]_ _[ğ‘—,ğ‘¡]_ [+][ I][(] _[ğ‘¡]_ [c] _[, ğ‘—]_ [â‰¥] _[ğ‘¡]_ [c] _[,ğ‘–]_ [)] _[ â„]_ _[ğ‘—,ğ‘¡]_ [âˆ’][1] _[.]_ (24)



**Decision Time**
**(for Europe Close)** _**Future / Unknown**_


**Cascading:** Uses

JP _t_ (fresh close)



**Japan**


**Europe**


**US**



**Directed Delay:** Uses JP & EU _tâˆ’1_

(ignores fresh EU close)



t
12:00



tâˆ’1
06:00



tâˆ’1
12:00



tâˆ’1
18:00



t
00:00



**Common constraint:**

Must use US _tâˆ’1_
(US settles after Europe close)


t
06:00



t
16:30



t
20:00



t+1
00:00



Timeline (UTC)


Figure 4: **The Ragged Filtration Problem.** The timeline illustrates the asynchrony of global closes relative to the
portfolio decision time _ğ‘¡_ (Europe Close). A _Cascading Filtration_ (red dashed arrow) utilizes the most recent data
(in our case we only use close data, but an open could be used), maximizing freshness. DeePMâ€™s _Directed Delay_
(blue solid arrow) strictly lags cross-sectional attention to _ğ‘¡_ - 1, enforcing a robust causal gap to isolate predictive
impulse-responses.


**4.4** **Structural Regularization: Macro Graph Prior**


Data-driven attention can overfit in low-signal regimes. We regularize the cross-sectional representations using a _Graph_
_Attention Network (GAT)_ constrained to the fixed _Macroeconomic Prior Graph_ G = (V _,_ E).



**Structure and Edge Biases.** Similar to the cross-sectional layer, we use a _Post-Norm ReZero_ configuration. However,
unlike standard GATs which often ignore edge strength, we strictly enforce the economic prior by injecting the adjacency
weights directly into the attention mechanism. Inspired by the _Graphormer_ framework [Ying et al., 2021], instead of
using a hard mask based on edges, we add a _fixed structural bias_ derived from the adjacency matrix _ğ´_ to the attention
scores:







_,_ (25)



_ğ›¼ğ‘–ğ‘—,ğ‘¡_ = Softmax _ğ‘—_




( _ğ‘„â„ğ‘–,ğ‘¡_ ) [âŠ¤] ( _ğ¾_ _â„_ [Ëœ] _ğ‘—,ğ‘¡_ )



~~âˆš~~



+ ln( _ğ´ğ‘–ğ‘—_

_ğ‘‘_



12


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


where we handle _ğ´ğ‘–ğ‘—_ = 0 by masking non-edges, setting the corresponding logit to âˆ’âˆ. Since _ğ´ğ‘–ğ‘—_ encodes the strength
of the economic linkage (or 0 for no link), the term ln( _ğ´ğ‘–ğ‘—_ ) acts as a soft-masking mechanism: strong links ( _ğ´ğ‘–ğ‘—_ â‰ˆ 1)
pass the data-driven attention signal unhindered, while weaker or non-existent links ( _ğ´ğ‘–ğ‘—_ â†’ 0) introduce large negative
penalties, effectively pruning the connection.


The GNN aggregation serves as a residual update to the embeddings. The update is as follows:



âˆ‘ï¸
_â„_ [gnn] _ğ‘–,ğ‘¡_ [=][ LN][ ï¿½ï¿½] _â„_ [cross] _ğ‘–,ğ‘¡_ + _ğ›¼_ gnn Â·


_ğ‘—_ âˆˆN( _ğ‘–_

     


(26)



_â„_ [final] _ğ‘–,ğ‘¡_ = A( _â„_ [gnn] _ğ‘–,ğ‘¡_ [)] _[,]_



_ğ›¼ğ‘–ğ‘—,ğ‘¡ğ‘Š_ _â„_ [Ëœ] _ğ‘—,ğ‘¡_ [ï¿½] _,_

        _ğ‘—_ âˆˆN( _ğ‘–_ )

        


where N ( _ğ‘–_ ) are neighbors defined by the macro graph and _ğ›¼_ gnn is the ReZero gate. To rigorously validate the necessity
of this anisotropic attention mechanism, we also implement an _Isotropic GCN_ ablation [Kipf and Welling, 2017]. In the
GCN variant, the learnable attention weights _ğ›¼ğ‘–ğ‘—,ğ‘¡_ are replaced by fixed spectral weights 1/ ~~âˆš~~ deg( _ğ‘–_ )deg( _ğ‘—_ ), forcing the

model to aggregate all economic neighbors equally regardless of their current predictive relevance.


**4.5** **Theoretical Interpretations**


The architectural choices in DeePM can be unified under two theoretical frameworks: dynamical systems reconstruction
and approximate Bayesian inference.


**Temporal Encoder as State Space Reconstruction.** Financial markets can be viewed as high-dimensional, nonstationary dynamical processes observed through noisy, partial measurements. Motivated by _Takensâ€™ Embedding_
_Theorem_ [Takens, 1981, Sauer et al., 1991], delay-coordinate histories can provide a sufficient representation of latent
state under suitable regularity conditions. Our temporal backbone (V-VSN â†’ LSTM â†’ TempMHA) therefore learns a
data-driven delay map Î¦( _ğ‘¥ğ‘¡_ - _ğ¿_ : _ğ‘¡_ ) â†¦â†’ _â„ğ‘¡_, projecting the observable price/return history into a latent state space in which
predictive structure is easier to model. Using volatility-normalized returns helps stabilize scale over time and across
assets (reducing heteroskedasticity), which empirically improves the conditioning of this learned representation.


**Graph Layer as a Bayesian Structural Prior.** Standard attention mechanisms approximate a purely data-driven
posterior _ğ‘_ ( _ğ‘_ | D). In finance, where the signal-to-noise ratio is low, this often leads to overfitting on spurious
correlations. We rigorously interpret the Macro-Graph component as injecting a _Gaussian Markov Random Field_
_(GMRF) Prior_ over the asset embeddings, _ğ‘_ ( _ğ‘_ | S) âˆ exp(âˆ’ _[ğ›¼]_ 2 [Tr][(] _[ğ‘]_ [âŠ¤][L] _[ğ‘]_ [))][, where][ L][ =] _[ ğ¼]_ [âˆ’] _[ğ·]_ [âˆ’][1][/][2] _[ğ´ğ·]_ [âˆ’][1][/][2][ is the]

normalized graph Laplacian [Rue and Held, 2005]. The trace term represents the _Dirichlet energy_ of the signal on the
graph [Chung, 1997], which expands as:



2


_._ (27)

2



ï¿½ï¿½ï¿½ï¿½ï¿½



Tr( _ğ‘_ [âŠ¤] L _ğ‘_ ) = [1]

2



âˆ‘ï¸

_ğ´ğ‘–ğ‘—_

_ğ‘–, ğ‘—_



~~âˆš~~ _ğ‘§ğ‘–_ - _[ğ‘§]_ _[ğ‘—]_
ï¿½ï¿½ï¿½ï¿½ï¿½ _ğ‘‘ğ‘–_ ~~âˆš~~ _ğ‘‘_ _ğ‘—_



Minimizing this energy forces economically linked assets (e.g., crude oil and energy equities) to have similar latent
representations, scaled by their connectivity, unless the data strongly suggests otherwise. Consequently, the architecture
approximates the Maximum A Posteriori (MAP) estimate:




_[ğ›¼]_

2 [Tr][(] _[ğ‘]_ [âŠ¤][L] _[ğ‘]_ [)]
**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

- ï¿½ï¿½ Struct. Prior (Graph)



_ğ‘_ Ë† _ğ‘¡_ â‰ˆ arg max
_ğ‘_




log _ğ‘_ (D | _ğ‘_ )
**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

 - ï¿½ï¿½ Likelihood (Attn)




- _[ğ›¼]_




_._ (28)



Both the GCN and GAT layers can be viewed as implementing a gradient step on this objective. In high-noise regimes,
the prior dominates, smoothing representations across the economic graph to prevent overfitting. In high-signal regimes,
the attention mechanism (Likelihood) can override the prior to capture novel market dynamics that deviate from
historical economic structures. While a GCN uses the fixed isotropic prior L, the GAT layer allows for an anisotropic
refinement where the model learns to dynamically adjust the strength of specific economic linkages ( _ğ´ğ‘–ğ‘—_ ) based on the
current market regime.


**Directed Delay as Information-Theoretic Filtering.** The choice of filtration protocol fundamentally alters the
statistical dependency learned by the attention mechanism. A _Synchronous_ approach ( _ğ»ğ‘¡_ â†’ _ğ»ğ‘¡_ ) effectively learns
_Mutual Information ğ¼_ ( _ğ‘‹ğ‘¡_ ; _ğ‘Œğ‘¡_ ), which is symmetric and dominated by instantaneous co-movements. While high mutual
information implies strong correlation, it provides no insight into the direction of influence, making it brittle to regime
shifts where correlations break.


13


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


By enforcing the strict _Directed Delay_ ( _ğ»ğ‘¡_ âˆ’1 â†’ _ğ»ğ‘¡_ ), we constrain the Cross-Sectional Attention to approximate _Transfer_
_Entropy_ [Schreiber, 2000], a non-linear generalization of Granger Causality [2] [Granger, 1969]. Transfer Entropy T _ğ‘—_ â†’ _ğ‘–_
measures the reduction in uncertainty about asset _ğ‘–_ â€™s future state given asset _ğ‘—_ â€™s past, _conditional_ on asset _ğ‘–_ â€™s own history:


T _ğ‘—_ â†’ _ğ‘–_ = _ğ»_ ( _ğ‘¥ğ‘–,ğ‘¡_ | _ğ‘¥ğ‘–,ğ‘¡_ âˆ’1) âˆ’ _ğ»_ ( _ğ‘¥ğ‘–,ğ‘¡_ | _ğ‘¥ğ‘–,ğ‘¡_ âˆ’1 _, ğ‘¥_ _ğ‘—,ğ‘¡_ âˆ’1) _._ (29)


While our model optimizes a Sharpe utility rather than explicitly calculating Shannon entropy, the objectives are
aligned: maximizing the Sharpe ratio requires minimizing the variance of the residual return. Under standard Gaussian
assumptions [3], minimizing variance is equivalent to minimizing differential entropy. Consequently, a high attention
weight _ğ›¼ğ‘–ğ‘—_ implies that the latent state of asset _ğ‘—_ at _ğ‘¡_ - 1 resolves uncertainty about asset _ğ‘–_ at _ğ‘¡_ (predictive utility)
that was not resolved by _ğ‘–_ â€™s own temporal backbone. This asymmetry enables the model to learn structural market
hierarchies (e.g., US Rates driving Emerging Market FX) rather than just phenomenological clusters. In our ablation
studies (Section 7), this â€œCausal Sieveâ€ property proves more valuable than the data freshness offered by the Cascading
Filtration, suggesting that out-of-sample robustness relies on identifying _drivers_ rather than just _peers_ .


**5** **Learning Objective and Optimization**


Our training process optimizes the realized _Net Portfolio Return_ stream R = { _ğ‘…ğ‘,ğ‘¡_ [net] [(] _[ğœƒ]_ [) |] _[ ğ‘]_ [=][ 1] _[ . . . ğµ, ğ‘¡]_ [=][ 1] _[ . . .ğ‘‡]_ [}][,]
representing the returns across _ğµ_ independent sequences (batches) over a horizon of _ğ¿_ time-steps. Unlike standard
supervised learning, which minimizes MSE, we maximize a differentiable utility function targeting robust risk-adjusted
performance. Return premia are regime-dependent and can unwind abruptly (e.g., momentum â€œcrashesâ€ and fast
reversals), motivating objectives that do not concentrate risk in a small number of adverse windows. [Wood et al., 2022,
Daniel and Moskowitz, 2016].


We specifically focus on the _Sharpe Ratio_ [Sharpe, 1966] rather than raw returns for two reasons:


1. **Risk Adjustment:** Maximizing the Sharpe ratio aligns the training objective with the mandate of a portfolio
manager, who seeks to maximize returns per unit of risk rather than absolute performance.

2. **Statistical Significance:** Maximizing the Sharpe Ratio (SR) is mathematically equivalent to maximizing the
_ğ‘¡_ -statistic of the strategyâ€™s expected return [4] . For a batch of size _ğµğ¿_, the _ğ‘¡_ -statistic testing the null hypothesis of
zero mean return ( _ğ»_ 0 : _ğœ‡_ = 0) is given by:



âˆš

_ğµğ¿_ - _[ğœ‡]_ [Ë†]

_ğœ_ Ë† [=]



_ğœ‡_ Ë†
_ğ‘¡_ = ~~âˆš~~

_ğœ_ Ë† /



~~âˆš~~
=

_ğµğ¿_



_ğµğ¿_ - SR _._ (30)



Since the window length _ğµğ¿_ is constant during a training step, the gradient âˆ‡ _ğœƒ_ _ğ‘¡_ is proportional to âˆ‡ _ğœƒ_ SR. This
aligns the optimization landscape with the objective of finding statistically robust alpha, explicitly penalizing
high-variance strategies that might otherwise appear profitable due to â€œlucky" outliers.


**5.1** **Robust Objective: Pooled Sharpe with SoftMin Penalty**


Standard Sharpe maximization often yields policies that â€œcheat" by overfitting to specific high-return windows while
ignoring latent tail risks. To mitigate this, we partition the training sequence into _ğµ_ non-overlapping windows (e.g.,
quarterly blocks) and optimize a hybrid objective:



L( _ğœƒ_ ) = âˆ’ SRpool (R)
**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

    - ï¿½ï¿½    Long-Run




        -         
- _ğœ†_ - SoftMin _ğœ_ {SR _ğ‘_ } _ğ‘_ _[ğµ]_ =1

**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

  - ï¿½ï¿½  Robustness



_._ (31)



**1. Pooled Sharpe Ratio.** SRpool treats the entire mini-batch history (spanning _ğµ_ sequences of length _ğ¿_ ) as a single
contiguous equity curve. It explicitly targets the global risk-adjusted return:


EË† [R]
SRpool(R) := (32)
~~âˆš~~

Varï¿½ [R] + _ğœ€_


2In the linear-Gaussian VAR setting, transfer entropy is equivalent (up to log-base/scaling constants) to Granger causality [Barnett
et al., 2009].
3Whilst raw returns are clearly not Gaussian, we volatility-target returns to better approximate homoskedasticity; outside exogenous
shocks and abrupt regime shifts, an approximately Gaussian model is a reasonable working assumption.
4This equivalence holds under i.i.d. elliptical returns with finite variance.


14


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


where E [Ë†] [Â·] and Var[Â·] denote the empirical mean and variance operators over the full set of realized net returns across
all _ğµ_ blocks and _ğ¿_ [ï¿½] time-steps. Specifically, the pooled sample mean is E [Ë†] [R] := _ğµğ¿_ 1 - _ğ‘ğµ_ =1 - _ğ‘¡ğ¿_ =1 _[ğ‘…]_ _ğ‘,ğ‘¡_ [net] [, with] _[ ğœ€]_ [serving as a]
small numerical stability constant.


**Mathematical Motivation (Consistency and Bias):** Assume the strategy return process is stationary and ergodic. [5] The
pooled metric SRpool uses an effective sample size on the order of _ğ‘_ eff â‰ˆ _ğµ_ Ã— _ğ¿_ (up to dependence adjustments). Under
ergodicity, the sample mean and variance converge to their population counterparts, and by the continuous mapping
theorem SRpool converges (in probability) to the population Sharpe ratio as _ğµğ¿_ â†’âˆ. In contrast, the naÃ¯ve average of
window-wise Sharpes, SR = _ğµ_ [1] - _ğ‘ğµ_ =1 SR [ï¿½] _ğ‘_, aggregates _ratios_ computed on shorter samples and is therefore generally

biased and higher-variance in finite samples (since SR _ğ‘_ is itself a biased ratio estimator). Moreover, following Lo

[ï¿½]

[2002], the variance of SR scales as _ğ‘‚_ (1/ _ğ¿_ ) (under weak dependence); pooling increases the effective sample size,

[ï¿½]
reducing gradient noise and stabilizing optimization.


**2. SoftMin as an Adversarial Prior.** To ensure the policy survives adverse regimes, we penalize the smooth minimum
of the per-window Sharpe ratios {SR _ğ‘_ }. The SoftMin function with temperature _ğœ_ is defined as:



âˆ‘ï¸ e [âˆ’] [SR] _ğœ_ _[ğ‘]_


_ğ‘_ =1







SoftMin _ğœ_ ({SR _ğ‘_ }) := âˆ’ _ğœ_ log




1

_ğµ_



_ğµ_
âˆ‘ï¸



_ğœ_



_._ (33)



As discussed in Section 2.5, minimizing âˆ’SoftMin is mathematically equivalent to minimizing the dual form of _Entropic_
_Value-at-Risk (EVaR)_ [Ahmadi-Javid, 2012]. This effectively trains the model against an implicit adversary who
reweights the training windows to emphasize the worst-performing periods (minimax optimization).


The temperature parameter _ğœ>_ 0 governs the aggressiveness of this adversary. As _ğœ_ â†’ 0, the function approaches
the hard minimum (min _ğ‘_ SR _ğ‘_ ), forcing the model to focus exclusively on the single worst-performing window (pure
minimax). Conversely, as _ğœ_ â†’âˆ, the function converges to the arithmetic mean, recovering a standard risk-neutral
objective. In practice, _ğœ_ acts as a tunable â€œrobustness control": lower values produce more conservative policies that
prioritize survival in difficult regimes, while higher values allow the model to focus on average-case performance.


Empirically, we find that this soft adversarial mechanism significantly stabilizes training. By penalizing potential
collapse in â€œhard" windows, the gradient descent is prevented from greedily overfitting to â€œeasy" low-volatility regimes,
a common pathology in financial deep-learning. For an in-depth analysis, including the connection to EVaR, see App. D.


**5.2** **Ensembling and Implicit Cost Regularization**


Deep reinforcement learning is notoriously sensitive to initialization. We mitigate this via an ensemble of the top _ğ¾_
models, a practice well-supported in the literature for predictive uncertainty estimation [Lakshminarayanan et al., 2017].
Beyond variance reduction, we prove that ensembling acts as a structural regularizer for transaction costs.


**Proposition 1 (Convexity of Turnover Cost).** Let C( _ğ‘_ ) = _ğ›¾_ [ï¿½] _ğ‘¡_ [|][Î”] _[ğ‘¤]_ _ğ‘¡_ [(] _[ğ‘]_ [)|][ be the proportional transaction cost function]
associated with a policy _ğ‘_ . This function is convex in _ğ‘_ .


**Corollary (Jensenâ€™s Inequality for Trading).** For an ensemble of _ğ¾_ independent policies { _ğ‘_ [(] _[ğ‘˜]_ [)] } _[ğ¾]_ _ğ‘˜_ =1 [and their average]
_ğ‘_ Â¯ = _ğ¾_ [1] - _ğ‘_ ( _ğ‘˜_ ), Jensenâ€™s inequality implies:



1
C( Â¯ _ğ‘_ ) â‰¤
_ğ¾_



_ğ¾_
âˆ‘ï¸

C( _ğ‘_ [(] _[ğ‘˜]_ [)] ) _._ (34)

_ğ‘˜_ =1



**Implication:** The trading cost of the ensemble is strictly upper-bounded by the average cost of its constituents.
Idiosyncratic â€œnoise trades" made by individual models tend to cancel out in the average, structurally reducing turnover
and improving the net Sharpe ratio.


This theoretical guarantee informs our hyperparameter choice for the explicit turnover penalty _ğ›¾_ (Eq. 13). As shown
in Appendix B, the optimal training penalty _ğ›¾_ _[â˜…]_ is strictly lower for ensembles than for single models. We essentially
â€œoutsource" part of the regularization burden to the ensemble mechanism, allowing individual models to remain
responsive to valid signals.


5Financial returns are not strictly stationary over long horizons due to structural breaks and evolving market microstructure; we
adopt stationarity/ergodicity as a useful idealization that justifies interpreting empirical moments as long-run expectations.


15


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**5.3** **Optimization Protocol**


We train using AdamW with a rolling window approach. To ensure the Sharpe ratio statistics in Eq. (1) are stable,
we require a large effective batch size (e.g., spanning multiple years of data). Since GPU memory is limited, we
introduce _Exact Gradient Accumulation_ for global statistics. We accumulate the sufficient statistics ( [ï¿½] _ğ‘…ğ‘¡_ _,_ [ï¿½] _ğ‘…ğ‘¡_ [2][) across]
micro-batches to compute the exact _ğœ‡_ and _ğœ_ for the full logical batch before performing the backward pass, detailed in
Sec. C.2. This ensures the optimization landscape is invariant to GPU memory constraints.


Financial time series require a warm-up period for recursive states (e.g., LSTM hidden states) to stabilize. We implement
a _burn-in_ of _ğ¿_ 0 = 21 steps (approx. 1 month). During this phase, the model processes inputs to update its internal
state, but gradients are masked and returns do not contribute to the loss function. This ensures that only â€œmature"
representations, fully conditioned on the valid history, are passed to the optimizer, preventing initial state noise from
corrupting the learning process.


**6** **Experimental Design**


**6.1** **Benchmarks**


To isolate the contributions of DeePMâ€™s end-to-end architecture, we compare against three benchmark families: (A)
passive/rule-based allocations, (B) classical trend-following signals (producing risk weights directly), and (C) two-stage
â€œsignal â†’ allocationâ€ pipelines that combine a forecast signal with a covariance-aware risk allocator.


All benchmarks are evaluated under the same execution and return accounting as DeePM. Risk weights _ğ‘ğ‘–,ğ‘¡_ map to
vol-targeted notionals via Eq. (12), and net performance is computed using the same transaction-cost model. Throughout,
the covariance input Î£ [Ë†] _ğ‘¡_ to all two-stage allocators is estimated on vol-scaled daily returns using a 252-day rolling
window with Ledoitâ€“Wolf shrinkage [Ledoit and Wolf, 2004].


**6.1.1** **Passive and Rule-Based**


    - **Passive Equal Risk:** A constant long risk weight _ğ‘ğ‘–,ğ‘¡_ â‰¡ 1 for all tradable assets. Under the volatility targeting
framework (Eq. (12)), this ensures that _ğ‘¤ğ‘–,ğ‘¡_ âˆ 1/ _ğœ_ Ë† _ğ‘–,ğ‘¡_, representing a static risk-parity exposure to global macro
risk premia without active timing.


**6.1.2** **Classical Trend-Following**


These strategies generate risk weights _ğ‘ğ‘–,ğ‘¡_ directly from univariate price history.


    - **TSMOM (Time-Series Momentum):** Following Moskowitz et al. [2012], positions are determined by the
sign of the past 12-month return:

_ğ‘_ [TS] _ğ‘–,ğ‘¡_ [=][ sign][ ï¿½] _[ğ‘Ÿ][ğ‘–,ğ‘¡]_ [âˆ’][252:] _[ğ‘¡]_                 - = sign                 - _ğ‘ƒğ‘–,ğ‘¡_                 - 1ï¿½ _._ (35)
_ğ‘ƒğ‘–,ğ‘¡_ âˆ’252

    - **Multi-Scale MACD:** We implement a continuous trend signal using volatility-normalized Moving Average
Convergence Divergence (MACD) indicators across three time-scales ( _ğ‘†, ğ¿_ ) âˆˆ{(8 _,_ 24) _,_ (16 _,_ 48) _,_ (32 _,_ 96)}.
The raw MACD values are mapped to positions via a sigmoidal response function _ğœ™_ ( _ğ‘¥_ ) = _ğ‘¥_ e [âˆ’] _[ğ‘¥]_ [2][/][4] /0 _._ 89 [Baz
et al., 2015, Lim et al., 2019] to squash outliers while maintaining linearity near zero:




_._ (36)



_ğ‘_ [MACD] _ğ‘–,ğ‘¡_ = 3 [1]



3
âˆ‘ï¸



âˆ‘ï¸ - EWM _ğ‘†ğ‘˜_ ( _ğ‘ƒğ‘–_ ) âˆ’ EWM _ğ¿ğ‘˜_ ( _ğ‘ƒğ‘–_ )

_ğœ™_
_ğ‘˜_ =1 _ğœ_ Ë† _ğ‘–,ğ‘¡_



_ğœ_ Ë† _ğ‘–,ğ‘¡_



**6.1.3** **Two-Stage Signalâ€“Allocation Baselines**


These baselines separate forecasting from portfolio construction. First, a signal generator produces a raw cross-sectional
vector _ğ‘ ğ‘¡_ âˆˆ R _[ğ‘]_ (representing the TSMOM or MACD values across all _ğ‘_ assets at time _ğ‘¡_ ). Second, a covariance-aware
allocator maps this vector _ğ‘ ğ‘¡_ to final risk weights _ğ‘ğ‘¡_ .


    - **Risk Managed Trend:** A scalar approach where raw trend signals _ğ‘ ğ‘¡_ (from TSMOM or MACD) are normalized
to unit leverage Ëœ _ğ‘ğ‘¡_ = _ğ‘ ğ‘¡_ /âˆ¥ _ğ‘ ğ‘¡_ âˆ¥1 and then scaled to target portfolio volatility _ğœ_ tgt using the ex-ante covariance
matrix Î£ [Ë†] _ğ‘¡_ :
_ğœ_ tgt
_ğ‘ğ‘¡_ = _ğ‘_ Ëœ _ğ‘¡_ _._ (37)
~~âˆš~~

_ğ‘_ Ëœ [âŠ¤] _ğ‘¡_ [Î£][Ë†] _[ğ‘¡]_ _[ğ‘]_ [Ëœ] _[ğ‘¡]_


16


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


- **Rolling MVO (Meanâ€“Variance Optimization):** Following the classical meanâ€“variance form in (1), we
proxy expected returns by the trend signal ( Ë† _ğœ‡ğ‘¡_ âˆ _ğ‘ ğ‘¡_ ) and stabilize the inversion with ridge regularization. The
resulting positions are
_ğ‘_ [MVO] _ğ‘¡_ âˆ [ï¿½] Î£ [Ë†] _ğ‘¡_ + _ğœ†ğ¼_ [ï¿½] [âˆ’][1] _ğ‘ ğ‘¡_ _,_ (38)

where Î£ [Ë†] _ğ‘¡_ is a rolling shrinkage covariance estimate and _ğœ†>_ 0 controls conditioning. The covariance Î£ [Ë†] _ğ‘¡_ is
estimated using Ledoit-Wolf shrinkage [Ledoit and Wolf, 2004] over a rolling 252-day window. To manage
turnover, we optionally add a quadratic turnover penalty _ğœ…_ âˆ¥ **p** _ğ‘¡_  - **p** _ğ‘¡_ âˆ’1âˆ¥ [2] 2 [, yielding an analytic update that anchors]
the portfolio to the previous dayâ€™s weights:


_ğ‘_ [MVO-TP] _ğ‘¡_ âˆ(Î£ [Ë†] _ğ‘¡_ + _ğœ…ğ¼_ ) [âˆ’][1] ( _ğ‘ ğ‘¡_ + _ğœ…ğ‘ğ‘¡_ âˆ’1) _._ (39)


For the TSMOM two-stage pipeline, we set _ğœ…_ = 10. Given that our assets are scaled to unit daily variance
( _ğœ_ [2] â‰ˆ 1) and the input signals are binary (Â±1), a high penalty is required to dominate the covariance term. This
dampens the response to instantaneous signal flips, creating a slow-moving anchor that naturally suppresses
noise and limits leverage without requiring a hard cap.

- **Risk Parity (ERC):** An Equal Risk Contribution allocator [Maillard et al., 2010]. We solve for long-only
weights _ğ‘ğ‘¡_ âˆˆ Râ‰¥ _[ğ‘]_ 0 [such that every asset contributes equally to ex-ante portfolio risk. Formally, the risk]



(Î£ [Ë†] _ğ‘¡_ _ğ‘ğ‘¡_ ) _ğ‘–_
contribution of asset _ğ‘–_ is defined as RC _ğ‘–_ = _ğ‘ğ‘–_ ~~âˆš~~



_ğ‘¡_ _ğ‘¡_ _ğ‘–_

_ğ‘ğ‘¡_ [âŠ¤][Î£][Ë†] _[ğ‘¡]_ _[ğ‘][ğ‘¡]_ [. We optimize] _[ ğ‘][ğ‘¡]_ [such that][ RC] _[ğ‘–]_ [=][ RC] _[ğ‘—]_ [âˆ€] _[ğ‘–, ğ‘—]_ [. Finally, we]



impose the directionality of the original signal vector _ğ‘ ğ‘¡_ via element-wise multiplication:


_ğ‘ğ‘¡_ = sign( _ğ‘ ğ‘¡_ ) âŠ™ _ğ‘ğ‘¡_ _._ (40)


This ensures that while the position _magnitudes_ are determined by the optimizer to equalize risk, the _direction_
(long or short) is strictly preserved from the trend signal.


**6.1.4** **Learning-Based Baselines**


    - **Momentum Transformer:** A variant of the architecture proposed by Wood et al. [2023], which corresponds to
the temporal encoder of DeePM. This serves as our primary deep learning baseline to quantify the marginal value
of DeePMâ€™s Graph and Directed Delay components (see Wood et al. [2023] for comparisons of the Momentum
Transformer against other simpler models). It slightly deviates from the original _Momentum Transformer_ by
updating components such as the adapter blocks and VSN to align with more recent developments. Additionally,
for fairness, we use the same transaction cost regularizer as DeePM.


**6.2** **Ablation Study Design**


To rigorously evaluate the individual contributions of DeePMâ€™s architectural components and training objectives, we
conduct a systematic ablation study. Our experimental design is structured to test four core hypotheses: (1) that the
explicit macroeconomic graph structure acts as a necessary regularizer for cross-asset learning; (2) that the directed
delay mechanism is crucial for learning causal rather than spurious correlations; (3) that the robust SoftMin objective
provides superior stability; (4) that end-to-end internalization of transaction costs outperforms heuristic post-processing;
and (5) that end-to-end internalization of transaction costs is most effective when trained with a relaxed scalar _ğ›¾<_ 1,
as per Eqn. (13), (defaulting to _ğ›¾_ = 0 _._ 5 in our standard configuration) to balance turnover penalization against signal
capture. By systematically removing or replacing these components from the best-performing ensemble configuration,
we aim to isolate their marginal value to the final risk-adjusted performance.


**6.3** **Evaluation Metrics**


For fair comparison across strategies with different intrinsic risk profiles, all out-of-sample return series are rescaled
_ex-post_ to a uniform annualized volatility target of _ğœ_ tgt = 10%. Let _ğ‘…ğ‘¡_ denote the rescaled daily net return and _ğ‘ƒğ‘¡_ the
cumulative wealth index at time _ğ‘¡_ .


**Performance and Risk.**


    - **Net Sharpe Ratio (SR):** The primary metric for risk-adjusted return efficiency after transaction costs [Sharpe,
1966]. Defined as the annualized mean return divided by the annualized volatility:



âˆš
SR =



252 Â· ï¿½E[ _ğ‘…ğ‘¡_ ] _._ (41)
~~âˆš~~

Varï¿½ [ _ğ‘…ğ‘¡_ ]



17


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


We also report **Gross SR** to quantify the â€œexecution gap" (performance degradation due to trading frictions).
This metric tests whether the strategy generates excess returns per unit of total risk.

- **Compound Annual Growth Rate (CAGR):** The geometric rate of return, capturing the effect of volatility
drag on long-term wealth accumulation:




- 252/ _ğ‘‡_



CAGR =




- _ğ‘‡_


(1 + _ğ‘…ğ‘¡_ )

_ğ‘¡_ =1




- 1 _._ (42)



This metric evaluates the final wealth multiplier available to an investor, accounting for the compounding of
losses.

- **Maximum Drawdown (MDD):** The largest peak-to-trough decline in the cumulative equity curve over the test
period [Magdon-Ismail et al., 2004], measuring the worst historical loss:



MDD = min
_ğ‘¡_ âˆˆ[0 _,ğ‘‡_ ]




- _ğ‘ƒğ‘¡_ 
    - 1 _._ (43)
max _ğ‘ _ â‰¤ _ğ‘¡_ _ğ‘ƒğ‘ _



This tests the strategyâ€™s tail risk and potential for capital preservation during adverse regimes.

    - **Calmar Ratio:** A tail-risk-adjusted performance measure defined as the ratio of annualized return to absolute
maximum drawdown [Young, 1991]:

Calmar = [CAGR] (44)

|MDD| _[.]_

This metric assesses whether returns are sufficient to compensate for the psychological and financial cost of
deep drawdowns.

    - **Heteroskedasticity and Autocorrelation Consistent (HAC)** _ğ‘¡_ **-statistic (** _ğ‘¡_ **):** The _ğ‘¡_ -statistic for the null
hypothesis that the strategyâ€™s mean net return is zero ( _ğ»_ 0 : _ğœ‡_ = 0). To ensure valid inference under the serial
correlation and heteroskedasticity typical of financial time series, we utilize Newey-West standard errors [Newey
and West, 1987]. This provides a statistical significance test for the existence of a positive risk premium.


**Execution and Turnover.**


    - **Average Holding Period (Hold):** A direct proxy for trading frequency and transaction cost efficiency. We
define turnover _ğœ_ as the average daily absolute change in position weights. The implied holding period is
calculated relative to the average Gross Market Value (GMV) to account for leverage scaling:

2 Ã— 252
Hold =
_ğœ_ /Avg. GMV _[,]_



where _ğœ_ = [252]

_ğ‘‡_



_ğ‘‡_
âˆ‘ï¸


_ğ‘¡_ =1



1

_ğ‘ğ‘¡_



_ğ‘ğ‘¡_ (45)
âˆ‘ï¸

| _ğ‘¤ğ‘–,ğ‘¡_    - _ğ‘¤ğ‘–,ğ‘¡_ âˆ’1| _._

_ğ‘–_ =1



This diagnostic checks whether the strategyâ€™s alpha decay matches its trading horizon. Note that all strategies
are long/short, meaning positions _ğ‘¤ğ‘–,ğ‘¡_ can be positive (long) or negative (short).


**Benchmark Relative Metrics.** We compare all strategies against the _Passive Equal Risk_ benchmark ( _ğ‘…ğ‘¡_ [bench] ).


    - **Information Ratio (IR):** The risk-adjusted active return, defined as the annualized mean of the excess return
spread divided by the tracking error (standard deviation of the spread):



âˆš
IR =



ï¿½E[ _ğ‘…ğ‘¡_     - _ğ‘…ğ‘¡_ [bench] ]
252 Â· _._ (46)
~~âˆš~~

Varï¿½ [ _ğ‘…ğ‘¡_     - _ğ‘…ğ‘¡_ [bench] ]



This measures the consistency of the strategyâ€™s value-add over a passive allocation.

- **Alpha** _ğ‘¡_ **-statistic (** _ğ‘¡_ _ğ›¼_ **):** The HAC-adjusted _ğ‘¡_ -statistic testing the statistical significance of the excess return
( _ğ»_ 0 : E[ _ğ‘…ğ‘¡_  - _ğ‘…ğ‘¡_ [bench] ] â‰¤ 0). A value _>_ 2 _._ 0 usually indicates statistically significant outperformance. This
confirms whether the strategy provides genuine alpha beyond the risk premia available in the benchmark.

- **Correlation (** _ğœŒ_ **):** The Pearson correlation coefficient between the strategyâ€™s net returns and the benchmark
returns:

Covï¿½ ( _ğ‘…ğ‘¡_ _, ğ‘…ğ‘¡_ [bench] )
_ğœŒ_ = _._ (47)
_ğœ_ Ë† strat Â· Ë† _ğœ_ bench

Lower correlation implies the strategy captures unique structural alpha rather than static beta exposure.


18


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**6.4** **Training Protocol and Data Splits**


We evaluate the model using a strict _Walk-Forward Validation_ scheme to prevent look-ahead bias. We partition the
dataset (1990â€“2025) into five-year expanding blocks. For each block, the model is trained on all prior history, validated
on the subsequent 10% of the window, and tested on the following 5 years. The model is fully retrained every 5 years.
Scalers (volatility, costs) are fit on the training set and frozen for the test period. We report performance on the union of
out-of-sample blocks from 2010â€“2025 (inclusive).


To ensure robustness against initialization noise, we train 50 independent seeds for each architecture. We construct the
final policy by averaging the signals of the top half ( _ğ¾_ = 25) ranked by validation Sharpe ratio. This selection protocol
(50 seeds â†’ Top 25) was calibrated based on an out-of-sample study of the Momentum Transformer over the 2000â€“2010
period; the cross-sectional DeePM methods require the fuller history (training data up to 2010) to stabilize the learning
of the graph structure and thus rely on the protocol established by the temporal baseline. Our results Sec. 7 demonstrate
that there is little difference in performance between _ğ¾_ âˆˆ{10 _,_ 25 _,_ 50}, once we have benefited from ensembling.


We use rolling sequences of length 84 trading days. Motivated by [Wood et al., 2024], the first 21 steps of each sequence
are treated as a burn-in period and are used solely to initialize the recurrent states, with the 63 days of evaluation aligning
with the literature [Lim et al., 2019]. Gradients are masked during the burn-in period to ensure that optimization is
driven by stable internal representations. Additional experimental details are provided in Appendix E. Excluding the
burn-in steps, the training and validation data are constructed by segmenting the time series into non-overlapping
sequences. We use a 90/10 chronological split for training and validation. For out-of-sample backtesting, we again
use sequences of length 84, but increase the burn-in period to 63 days to provide a richer historical context for trading
decisions. Importantly, we do not explicitly retrain across regimes; instead, regime variation is handled implicitly
through the proposed robust loss.


The SoftMin loss hyperparameters _ğœ_ = 1/5 and _ğœ†_ = 0 _._ 1 were selected based on validation Sharpe, which does not
include the SoftMin loss term. All other hyperparameters are listed in Appendix E.


**7** **Empirical Findings**


**7.1** **Results**


We quantify the contribution of each architectural and objective component by ablating it from the best-performing
DeePM specification. The baseline is the full model trained on the proposed joint net-Sharpe objective ( _ğœ_ = 0 _._ 5 _, ğœ†_ = 1/5)
with macro-graph filtering (GAT), lagged cross-sectional attention, volatility scaling, top half of seeds based on validation
loss, and transaction costs with training scaler _ğ›¾_ = 0 _._ 5; it achieves a normalized out-of-sample Net Sharpe of 0.93.
Each ablation modifies exactly one component. For fairness, we also train the _Momentum Transformer_ with transaction
costs and training scaler _ğ›¾_ = 0 _._ 5. The empirical results for the 2010â€“2025 period are presented in Table 1.


It is important to note that our evaluation focuses on the post-2010 regime and incorporates a highly realistic transaction
cost model (incorporating tick-size constraints and liquidity scalars). This rigorous setting contrasts with studies that
benefit from the high-trend 1990s era or assume simplified execution costs (such as the original Momentum Transformer).
Our period aligns with the â€œCTA Winterâ€ of the 2010s, then the post-2020 volatility transition, which we further isolate
in Table 2, to assess resilience during the pandemic, inflation shocks, and the subsequent higher-for-longer regime.


**7.2** **Discussion of Empirical Findings**


**Economic Significance and Net Performance.** The DeePM Ensemble demonstrates statistically significant outperformance against both passive risk premia and deep learning baselines. Notably, the proposed model achieves a Net Sharpe
ratio of **0.93** with a highly significant HAC _ğ‘¡_ -statistic of **3.69**, confirming the existence of a robust risk premium net of
transaction costs. This performance exceeds the Passive Equal Risk (0.50) and TSMOM (0.45) benchmarks by a wide
margin. Crucially, it outperforms the state-of-the-art Momentum Transformer trained with the same transaction cost
regularization ( _ğ›¾_ = 0 _._ 5) which achieved a Net Sharpe of 0.66, highlighting the specific contribution of the structural
graph prior. The strategy achieves an Information Ratio of 0.44 relative to the passive benchmark with an average
holding period of 7.1 days, indicating that the model successfully identifies transient structural alpha opportunities
distinct from static factor exposure ( _ğ‘¡_ _ğ›¼_ = 1 _._ 85). This margin is particularly notable given that the 2010â€“2025 test set
corresponds to a historically exceptional regime of sustained asset appreciation (often termed an â€œEverything Bubbleâ€),
making passive risk-parity a uniquely challenging baseline to beat net of costs. Furthermore, the extended 100-seed
ensemble ( _ğ¾_ = 50) achieves a Net Information Ratio _ğ‘¡_ -statistic of 1.93 (approximate _ğ‘_ -value 0.05), confirming statistical
significance over this outlier passive history. The convergence between Gross (1.29) and Net (0.93) Sharpe ratios
suggests that the direct optimization of transaction-adjusted returns effectively internalizes execution constraints.


19


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Table 1: Out-of-Sample Performance 2010â€“2025 (end). All strategies are rescaled to 10% annualized volatility. **SR** :
annualized Sharpe ratio. **CAGR** : compound annual growth rate. **MDD** : maximum drawdown. **Calmar** : CAGR/|MDD|.
**Hold (days)** : implied average holding period in days (execution/turnover efficiency proxy; âˆ denotes buy-and-hold).
**Gross** metrics exclude transaction costs; **Net** metrics include transaction costs. **vs Bench. (Net)** : metrics relative to
Passive Equal Risk (IR: information ratio of excess return, _ğ‘¡_ _ğ›¼_ : HAC _ğ‘¡_ -statistic of excess return, _ğœŒ_ : return correlation). _ğ‘¡_ :
HAC _ğ‘¡_ -statistic of the strategyâ€™s own net return.


**Gross** **Net** **Exec** **vs Bench. (Net)**


**Strategy** **SR** **SR** _ğ‘¡_ **CAGR Calmar** **MDD (days)** **IR** _ğ‘¡_ _ğ›¼_ _ğœŒ_


**DeePM** 1.29 **0.93** **3.69** **9.2%** **0.58** -16.0% 7.1 0.44 1.85 0.52
DeePM (MACD features) 1.10 0.84 3.26 8.2% 0.49 -16.9% 10.4 0.38 1.57 0.61


**Passive Equal Risk (Bench.)** 0.50 0.50 1.90 4.6% 0.17 -27.1% âˆ       -       - 1.00
Trend (TSMOM) 0.51 0.45 1.75 4.1% 0.21 -19.8% 32.2 -0.03 -0.10 0.02
Risk Managed Trend 0.49 0.39 1.50 3.5% 0.13 -26.8% 14.6 -0.07 -0.26 0.02
MVO Trend 0.55 -0.07 -0.26 -1.2% -0.02 -51.4% 5.1 -0.41 -1.56 0.05
MVO-TP Trend 0.59 0.47 1.79 4.3% 0.15 -28.8% 15.1 -0.01 -0.05 0.06
Risk Parity Trend 0.35 0.18 0.75 1.4% 0.04 -32.2% 9.9 -0.22 -0.83 0.04
MACD Multi-Scale 0.28 0.25 1.00 2.0% 0.08 -23.8% **38.5** -0.17 -0.66 **-0.05**
Risk Managed MACD 0.26 0.20 0.81 1.5% 0.05 -27.9% 16.0 -0.20 -0.80 -0.04
MVO-TP MACD 0.24 0.15 0.61 1.0% 0.04 -23.6% 16.6 -0.24 -0.97 0.01
Mom. Transformer ( _ğ›¾_ = 0) 1.10 0.60 2.44 5.6% 0.21 -26.3% 4.0 0.10 0.42 0.45
Mom. Transformer ( _ğ›¾_ = 0 _._ 5) 1.02 0.66 2.54 6.2% 0.20 -31.9% 5.0 0.15 0.60 0.39


Cascading lag 1.19 0.84 3.29 8.2% 0.44 -18.7% 7.5 0.34 1.41 0.51
Independent (No Structure) 1.18 0.83 3.34 8.2% 0.48 -17.0% 7.7 0.33 1.35 0.50
No Cross-Attn (Graph Only) 1.15 0.84 3.24 8.2% 0.44 -18.4% 7.7 0.37 1.54 0.60
No Graph (Cross-Attn Only) 1.13 0.79 3.11 7.7% 0.39 -19.8% 7.5 0.29 1.19 0.51
Flip Graph/Cross-Attention 1.21 0.87 3.38 8.5% 0.43 -19.8% 7.5 0.39 1.64 0.58
No ReZero 0.85 0.71 2.70 6.8% 0.40 -17.0% 14.5 0.26 1.08 0.70
GCN (Isotropic) 1.09 0.81 3.12 7.9% 0.44 -17.9% 8.3 0.37 1.55 0.65


No SoftMin (Pooled Only) 0.79 0.68 2.70 6.5% 0.50 **-13.0%** 18.4 0.20 0.84 0.62
SoftMin _ğœ_ = 1 1.25 0.85 3.40 8.3% 0.54 -15.5% 6.9 0.34 1.42 0.49
SoftMin _ğœ_ = 0 _._ 05 1.17 0.83 3.33 8.1% 0.48 -16.7% 8.1 0.33 1.38 0.51
Zero Cost Training _ğ›¾_ = 0 1.17 0.56 2.29 5.2% 0.16 -32.1% 4.9 0.05 0.20 0.33
Full Cost Training _ğ›¾_ = 1 0.81 0.70 2.69 6.7% 0.41 -16.4% 19.0 0.25 1.07 0.70


Best Seed _ğ¾_ = 1 1.11 0.72 2.88 7.0% 0.41 -16.8% 6.6 0.20 0.79 0.38
Top 10 Seeds _ğ¾_ = 10 **1.30** 0.93 3.63 9.1% 0.56 -16.3% 6.7 0.40 1.67 0.45
All Seeds _ğ¾_ = 50 1.06 0.86 3.33 8.4% 0.57 -14.7% 10.7 0.44 1.85 0.68
100-seed _ğ¾_ = 50 (2x both) 1.26 0.93 3.64 9.2% 0.54 -16.9% 7.6 **0.46** **1.93** 0.57


Table 2: Out-of-Sample Performance 2020â€“2025 (end). All strategies are rescaled to 10% annualized volatility. See
Table 1 for the legend.


**Gross** **Net** **Exec** **vs Bench. (Net)**


**Strategy** **SR** **SR** _ğ‘¡_ **CAGR Calmar** **MDD (days)** **IR** _ğ‘¡_ _ğ›¼_ _ğœŒ_


DeePM 1.07 0.79 1.84 7.7% 0.56 **-13.8%** 8.0 0.45 1.16 0.57
DeePM (MACD features) **1.16** **0.97** **2.20** **9.6%** **0.65** -14.9% 11.4 **0.68** **1.67** 0.62


**Passive Equal Risk (Bench.)** 0.38 0.37 0.84 3.2% 0.17 -18.8% âˆ       -       - 1.00
Trend (TSMOM) 0.43 0.38 0.89 3.3% 0.18 -18.9% 24.5 0.00 0.01 -0.08
Risk Managed Trend 0.29 0.20 0.47 1.5% 0.07 -21.4% 13.0 -0.11 -0.26 -0.06
MVO Trend 0.42 -0.15 -0.36 -2.0% -0.06 -34.3% 5.2 -0.37 -0.85 0.01
MVO-TP Trend 0.31 0.20 0.48 1.6% 0.07 -22.1% 13.1 -0.11 -0.26 -0.01
Risk Parity Trend 0.50 0.35 0.84 3.1% 0.12 -25.7% 8.9 -0.01 -0.02 -0.08
MACD (Multi-Scale) 0.29 0.26 0.65 2.2% 0.09 -24.4% **35.9** -0.07 -0.17 **-0.09**
Risk Managed MACD 0.10 0.04 0.09 -0.1% -0.00 -27.2% 15.7 -0.23 -0.52 -0.08
MVO-TP MACD 0.11 0.03 0.08 -0.2% -0.01 -23.3% 16.3 -0.23 -0.56 -0.04
Mom. Transformer ( _ğ›¾_ = 0) 0.64 0.18 0.45 1.3% 0.07 -18.6% 3.8 -0.19 -0.49 0.52
Mom. Transformer ( _ğ›¾_ = 0 _._ 5) 0.65 0.38 0.87 3.3% 0.16 -20.7% 5.7 0.01 0.02 0.62


20


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Figure 5: Cumulative net-of-cost wealth growth for DeePM variants versus standard systematic baselines (2010â€“2025).
The y-axis utilizes a logarithmic scale to properly visualize long-term compounding differences.


**Role of the Two Spatial Inductive Biases.** The ablation study confirms a critical interaction between the topological
(Graph) and data-driven (Cross-Attention) components. Notably, neither mechanism succeeds in isolation. The _No_
_Graph_ variant (Cross-Attn Only) performs worse than the independent baseline (Net Sharpe 0.79 vs 0.83), suggesting
that unconstrained attention overfits spurious correlations. Conversely, the _Graph Only_ variant (Net Sharpe 0.84) offers
marginal improvement, implying that static economic priors are insufficient without dynamic weighting. The full
DeePM model (Net Sharpe 0.93) outperforms both. Furthermore, the ordering of these modules is significant; reversing
the sequence (Graph then Cross-Attention) degrades performance to a Net Sharpe of 0.87. This supports the hypothesis
that the economic graph functions best as a regularizing filter applied _after_ the model has learned raw cross-sectional
interactions, effectively â€œdenoisingâ€ the data-driven signals and reducing Maximum Drawdown from 19.8% (flipped) to
16.0%.


**Portfolio-Centric Supervision.** The results further highlight the superiority of portfolio-level supervision over
asset-level objectives. Even the â€œIndependent (No Structure)â€ ablation, which treats assets in isolation, achieves a Net
Sharpe of 0.83, comfortably outperforming the â€œMomentum Transformerâ€ baseline (0.66). Since both models utilize
similar temporal encoders, this confirms that, with suitable optimization stability, aligning the loss function with the
ultimate trading objective provides a superior optimization landscape, even in the absence of explicit cross-sectional
modelling.


**Information Latency and Causal Validity.** Comparing the â€œDirected Delayâ€ protocol against the â€œCascading Lagâ€
variant reveals a preference for causal robustness over information freshness. The model utilizing strictly lagged data
( _ğ‘¡_ - 1) outperforms the variant using same-day data from earlier-closing markets (Sharpe 0.93 vs. 0.84). This finding
implies that maximizing information freshness introduces non-stationary intraday noise that degrades generalization.
By enforcing a strict delay, the model is compelled to learn persistent impulse response functions and transfer entropy
rather than transient co-movements.


**Optimization Stability and Addressing the Inertia Trap.** Our ablation study identifies the choice of objective
function as the single most impactful component of the proposed framework, exceeding even the contribution of the
graph-based architectural priors. While removing the macroeconomic graph structure degrades performance from 0.93
to 0.79 (-15%), reverting from the Robust SoftMin to a standard Pooled Sharpe objective causes a catastrophic drop to


21


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


0.68 (-27%). This hierarchy suggests that while spatial structure is beneficial, the primary challenge in financial deep
learning is not merely representation learning, but optimization stability. We observed that the SoftMin loss significantly
stabilized training dynamics, reducing the divergence between training and validation loss curves.


This stability is critical for complex, ratio-based objectives like the Sharpe ratio, which are prone to collapsing into
degenerate local minima where the model minimizes the denominator (volatility) rather than maximizing the numerator
(return). A counter-intuitive artifact of this phenomenon is that the _No SoftMin_ ablation exhibits the lowest Maximum
Drawdown (-13.0%). This is not a sign of robustness, but of â€œinertiaâ€: without the worst-window penalty (approximating
Entropic Value-at-Risk), the model retreats to a low-frequency holding pattern (18.4 days) to minimize costs, effectively
becoming a slow-moving beta strategy. In contrast, the robust SoftMin objective compels the agent to trade actively
(7.1 days) to defend performance during adverse regimes, accepting marginally higher volatility for vastly superior
risk-adjusted returns (Sharpe 0.93).


**Adversarial Temperature.** We further analyse the role of the adversarial temperature _ğœ_, which controls the â€œparanoiaâ€
of the SoftMin operator (approximating EVaR). The baseline DeePM (optimized _ğœ_ ) outperforms both the looser _ğœ_ = 1
specification (Net Sharpe 0.85) and the strictly harder minimax limit _ğœ_ = 0 _._ 05 (Net Sharpe 0.83). This confirms
that a calibrated degree of robustness is required: _ğœ_ = 0 _._ 05 approximates a hard-minimax objective that can lead to
optimization instability by overfitting to the single worst historical window, while _ğœ_ = 1 is closer to a simple average that
fails to sufficiently penalize tail risks. The intermediate _ğœ_ = 0 _._ 2 enables the model to focus on the tail of the distribution
without discarding the signal from the body.


**Regime Robustness and Stability.** Figure 6 illustrates the rolling performance stability of the DeePM Ensemble
relative to classical baselines. A key deficiency of traditional trend-following (TSMOM) is its binary performance profile

- it excels in sustained divergence but suffers excessively during mean-reverting or sideways markets. In contrast, DeePM
demonstrates remarkable consistency across disparate regimes. By effectively navigating diverse market environments,
the model proves that its learned non-linear signal processing can identify profitable opportunities even when simple
persistent trends are absent, reducing the â€œfeast-or-famineâ€ cycle typical of systematic macro strategies.


This resilience is most distinct during the post-2020 block (Table 2), where the model delivered sustained alpha despite
the transition from pandemic-induced volatility to inflationary rate shocks. While classical Trend strategies faltered
(Sharpe 0.38) and passive allocation struggled (Sharpe 0.37) during this period, the DeePM framework maintained a
Net Sharpe of 0.79 (0.97 with MACD features), suggesting the learned representations successfully generalized from
the low-rate training era to the new high-rate regime without requiring retraining.


Figure 6: Rolling 12-month Sharpe Ratio of DeePM versus the TSMOM baseline. The proposed model exhibits superior
stability, maintaining positive performance during periods where classical trend-following suffers significant drawdowns
(e.g., 2016, post-2020).


22


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**Cost-Aware Learning.** The results validate the necessity of end-to-end cost optimization. The convergence between
Gross (1.29) and Net (0.93) Sharpe ratios suggests that the direct optimization of transaction-adjusted returns effectively
internalizes execution constraints. Training with _Zero Cost_ ( _ğ›¾_ = 0) leads to over-trading (Hold 4.9 days) and a collapse
in Net Sharpe to 0.56; without a penalty, the model aggressively fits high-frequency noise that appears profitable in
frictionless theory but is illusory in practice. This sensitivity is markedly more acute here than in univariate frameworks
like the Momentum Transformer. In a portfolio context, a zero-cost objective encourages the model to exploit spurious
cross-sectional arbitrages (e.g., long Asset A vs. short Asset B based on transient noise), creating a much larger surface
area for unmonetizable turnover than simple directional trend following. Conversely, _Full Cost_ training ( _ğ›¾_ = 1) results
in an overly conservative policy (Hold 19.0 days, Sharpe 0.70). The optimal performance (Sharpe 0.93) is achieved with
an intermediate penalty, confirming theoretical predictions that ensembling provides implicit regularization, allowing
the explicit penalty to be relaxed to capture higher-frequency alpha.


**Limitations of Two-Stage Mean-Variance Frameworks.** The comparison with MVO baselines highlights the
advantages of an end-to-end approach over separated estimation and optimization. The standard two-stage MVO fails
catastrophically (Sharpe -0.07) due to error maximization, where estimation noise in the covariance matrix is amplified
by the optimizer. While introducing a quadratic turnover penalty (MVO-TP) stabilizes the strategy and restores positive
performance (Sharpe 0.47), it remains significantly inferior to the deep learning approach. This suggests that the
performance gap is not merely a function of turnover management, but stems from DeePMâ€™s ability to learn non-linear
predictive signals that two-stage linear models cannot capture.


**Ensembling as Structural Regularization.** The results confirm that ensembling acts as an effective variance reduction
technique for portfolio construction. The ensemble model improves the Net Sharpe ratio from 0.72 (single best seed) to
0.93 (Top 10 seeds). Crucially, performance remains remarkably stable beyond this initial uplift: the Baseline ( _ğ¾_ = 25),
Top 10 ( _ğ¾_ = 10), and the extended 100-seed ensemble ( _ğ¾_ = 50) all converge to a Net Sharpe of approximately 0.93.
This insensitivity indicates that the specific ensemble size is not a cherry-picked hyperparameter, provided a sufficient
diversity of models is aggregated. By averaging over multiple initializations, the ensemble mitigates the idiosyncrasies
of individual training runs, resulting in a smoother position path that is more efficient to execute and stable during
drawdowns.


**8** **Conclusions and Future Work**


This paper introduces DeePM, an end-to-end portfolio management framework that integrates macroeconomic structural
priors with deep representation learning. Our empirical results demonstrate that imposing a static sector-graph structure
acts as a vital regularizer, enabling the model to learn robust cross-sectional alpha that generalizes out-of-sample
(Net Sharpe 0.93) where purely data-driven attention mechanisms fail (Net Sharpe 0.79). Furthermore, we show that
the choice of loss function is paramount; the robust SoftMin (EVaR) objective significantly outperforms standard
mean-variance and pooled objectives by preventing overfitting to low-volatility regimes and avoiding the â€œinertia trap.â€
Beyond architectural novelty, this work bridges the gap between academic deep learning and institutional practice. By
rigorously modelling transaction costs, enforcing filtration-compliant delays for asynchronous global markets, and
optimizing for regime-robust metrics rather than average-case performance, DeePM offers a deployable blueprint for
systematic macro managers seeking to modernize legacy trend-following systems.


Future research will focus on five directions: (1) _Dynamic Graph Learning_, allowing the adjacency matrix to evolve
over time rather than relying on fixed sector definitions; (2) _Interpretability_, leveraging GNN explainability techniques
to map learned attention weights back to specific economic transmission channels; (3) _Hierarchical Structure_, extending
the graph prior to explicitly model multi-level relationships across assets, sectors, and asset classes; (4) _Expanded_
_Information Sets_, integrating carry and macroeconomic data to capture drivers orthogonal to pure price momentum;
(5) _Freshness vs. Causality_, benchmarking against a fully synchronous data source to rigorously test whether the
â€œCausal Sieveâ€ benefit of the Directed Delay protocol outweighs the informational cost of sacrificing â€œinstantaneousâ€
data freshness; and (6) _High-Frequency Generalization_, extending the framework to intraday data to test whether the
proposed inductive biases persist at higher frequencies, and determining the time-scale at which the trade-off between
information freshness and causal validity inverts.


**9** **Acknowledgements**


KW would like to thank the Oxford-Man Institute of Quantitative Finance for its generous support. SR would like to
thank the U.K. Royal Academy of Engineering.


23


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**References**


Amir Ahmadi-Javid. Entropic value-at-risk: A new coherent risk measure. _Journal of Optimization Theory and_
_Applications_, 155(3):1105â€“1123, 2012.


Gerald Appel. _The Moving Average Convergenceâ€“Divergence Trading Method_ . Advanced Commodity Trading
Techniques, New York, 1979.


Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you
need: Fast convergence at large depth. In _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial_
_Intelligence_, volume 161 of _Proceedings of Machine Learning Research_, pages 1352â€“1361. PMLR, 2021. URL
`[https://proceedings.mlr.press/v161/bachlechner21a.html](https://proceedings.mlr.press/v161/bachlechner21a.html)` .


David H. Bailey, Jonathan M. Borwein, Marcos LÃ³pez de Prado, and Qiji Jim Zhu. The probability of backtest overfitting.
_Journal of Computational Finance_, 2016. doi: 10.21314/JCF.2016.318.


Lionel Barnett, Adam B. Barrett, and Anil K. Seth. Granger causality and transfer entropy are equivalent for
gaussian variables. _Physical Review Letters_, 103(23):238701, 2009. doi: 10.1103/PhysRevLett.103.238701. URL
`[https://link.aps.org/doi/10.1103/PhysRevLett.103.238701](https://link.aps.org/doi/10.1103/PhysRevLett.103.238701)` .


Jamil Baz, Nicolas Granger, Campbell R. Harvey, Nicolas Le Roux, and Sandy Rattray. Dissecting investment strategies
in the cross section and time series. Working paper, SSRN, 12 2015. Man Group / Man AHL working paper (SSRN
2695101).


John Bollinger. _Bollinger on Bollinger bands_ . McGraw-Hill New York, 2002.


StÃ©phane Boucheron, GÃ¡bor Lugosi, and Pascal Massart. _Concentration Inequalities: A Nonasymptotic Theory of_
_Independence_ . Oxford University Press, 2013.


Fan R. K. Chung. _Spectral Graph Theory_, volume 92 of _CBMS Regional Conference Series in Mathematics_ . American
Mathematical Society, 1997.


Kent Daniel and Tobias J. Moskowitz. Momentum crashes. _Journal of Financial Economics_, 122(2):221â€“247, 2016.
doi: 10.1016/j.jfineco.2015.12.002.


Werner FM De Bondt and Richard Thaler. Does the stock market overreact? _The Journal of Finance_, 40(3):793â€“805,
1985.


Adam N. Elmachtoub and Paul Grigas. Smart â€œpredict, then optimizeâ€. _Management Science_, 68(1):9â€“26, 2022. doi:
10.1287/mnsc.2020.3922.


Clive W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. _Econometrica_,
37(3):424â€“438, 1969.


Shihao Gu, Bryan Kelly, and Dacheng Xiu. Empirical asset pricing via machine learning. _The Review of Financial_
_Studies_, 33(5):2223â€“2273, May 2020. doi: 10.1093/rfs/hhaa009.


Larry Harris. _Trading and Exchanges: Market Microstructure for Practitioners_ . Oxford University Press, 2003. ISBN
9780195144703.


Campbell R. Harvey and Akhtar Siddique. Conditional skewness in asset pricing tests. _The Journal of Finance_, 55(3):
1263â€“1295, 2000.


Campbell R. Harvey, Edward Hoyle, Russell Korgaonkar, Sandy Rattray, Matthew Sargaison, and Otto Van Hemert.
The impact of volatility targeting. _The Journal of Portfolio Management_, 45(1):14â€“33, September 2018. doi:
10.3905/jpm.2018.45.1.014.


Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
_Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770â€“778, 2016. URL
`[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)` .


Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735â€“1780, 1997.


Narasimhan Jegadeesh and Sheridan Titman. Returns to buying winners and selling losers: Implications for stock
market efficiency. _The Journal of Finance_, 48(1):65â€“91, 1993.


Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International_
_Conference on Learning Representations (ICLR)_, 2017.


Albert S. Kyle. Continuous auctions and insider trading. _Econometrica_, 53(6):1315â€“1335, 1985.


Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation
using deep ensembles. In _Advances in Neural Information Processing Systems_, volume 30, pages 6402â€“6413, 2017.


24


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance matrices. _Journal of_
_Multivariate Analysis_, 88(2):365â€“411, 2004. doi: 10.1016/S0047-259X(03)00096-4.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A
framework for attention-based permutation-invariant neural networks. In _International Conference on Machine_
_Learning_, 2019.

Bryan Lim, Stefan Zohren, and Stephen Roberts. Enhancing time-series momentum strategies using deep neural
networks. _The Journal of Financial Data Science_, 1(4):19â€“38, 2019. doi: 10.3905/jfds.2019.1.015.

Bryan Lim, Sercan Ã–. ArÄ±k, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable
multi-horizon time series forecasting. _International Journal of Forecasting_, 37(4):1748â€“1764, 2021.


Andrew W. Lo. The statistics of sharpe ratios. _Financial Analysts Journal_, 58(4):36â€“52, 2002.


Andrew W. Lo and A. Craig MacKinlay. An econometric analysis of nonsynchronous trading. _Journal of Econometrics_,
45(1-2):181â€“211, 1990. doi: 10.1016/0304-4076(90)90098-E.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _International Conference on Learning_
_Representations (ICLR)_, 2019.

Malik Magdon-Ismail, Amir F. Atiya, Amrit Pratap, and Yaser S. Abu-Mostafa. On the maximum drawdown of a
brownian motion. _Journal of Risk_, 7(2):1â€“15, 2004.

SÃ©bastien Maillard, Thierry Roncalli, and JÃ©rÃ´me Teiletche. The properties of equally weighted risk contribution
portfolios. _The Journal of Portfolio Management_, 36(4):60â€“70, 2010. doi: 10.3905/jpm.2010.36.4.060.


Harry Markowitz. Portfolio selection. _The Journal of Finance_, 7(1):77â€“91, 1952.


Richard O. Michaud. The markowitz optimization enigma: Is â€œoptimizedâ€ optimal? _Financial Analysts Journal_, 45(1):
31â€“42, 1989. doi: 10.2469/faj.v45.n1.31.


Tobias J. Moskowitz, Yao Hua Ooi, and Lasse Heje Pedersen. Time series momentum. _Journal of Financial Economics_,
104(2):228â€“250, 2012. doi: 10.1016/j.jfineco.2011.11.003.

Whitney K. Newey and Kenneth D. West. A simple, positive semi-definite, heteroskedasticity and autocorrelation
consistent covariance matrix. _Econometrica_, 55(3):703â€“708, 1987.


Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term
forecasting with transformers. _arXiv preprint arXiv:2211.14730_, 2023. doi: 10.48550/arXiv.2211.14730. Accepted
at ICLR 2023.

Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2018. arXiv:1709.07871.

Pinnacle Data Corporation. Clc database (continuous futures contracts). Proprietary financial database, 2025. URL
`[https://pinnacledata2.com/clc.html](https://pinnacledata2.com/clc.html)` . Accessed via Pinnacle Data Corp.


R. Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. _Journal of Risk_, 2(3):21â€“41,
2000.


HÃ¥vard Rue and Leonhard Held. _Gaussian Markov Random Fields: Theory and Applications_ . Chapman and Hall/CRC,
2005.


Tim Sauer, James A. Yorke, and Martin Casdagli. Embedology. _Journal of Statistical Physics_, 65(3-4):579â€“616, 1991.

Thomas Schreiber. Measuring information transfer. _Physical Review Letters_, 85(2):461â€“464, 2000. doi: 10.1103/
PhysRevLett.85.461.


William F. Sharpe. Mutual fund performance. _The Journal of Business_, 39(1):119â€“138, 1966.


Noam Shazeer. Glu variants improve transformer. _arXiv preprint_, 2020. arXiv:2002.05202.


Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. _The journal of machine learning research_, 15(1):1929â€“1958, 2014.


Floris Takens. Detecting strange attractors in turbulence. In _Dynamical Systems and Turbulence, Warwick 1980_, volume
898 of _Lecture Notes in Mathematics_, pages 366â€“381. Springer, 1981.

Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph
attention networks. In _International Conference on Learning Representations (ICLR)_, 2018.


Kieran Wood, Stephen Roberts, and Stefan Zohren. Slow momentum with fast reversion: A trading strategy using deep
learning and changepoint detection. _The Journal of Financial Data Science_, 4(1):111â€“129, 2022. ISSN 2640-3943.
doi: 10.3905/jfds.2021.1.081. URL `[https://jfds.pm-research.com/content/4/1/111](https://jfds.pm-research.com/content/4/1/111)` .


25


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Kieran Wood, Sven Giegerich, Stephen Roberts, and Stefan Zohren. Trading with the momentum transformer: An intelligent and interpretable architecture, February 2023. URL `[https://www.risk.net/cutting-edge/7956074/](https://www.risk.net/cutting-edge/7956074/trading-with-the-momentum-transformer-an-interpretable-deep-learning-architecture)`
`[trading-with-the-momentum-transformer-an-interpretable-deep-learning-architecture](https://www.risk.net/cutting-edge/7956074/trading-with-the-momentum-transformer-an-interpretable-deep-learning-architecture)` .
Risk.net (Cutting Edge).


Kieran Wood, Samuel Kessler, Stephen J. Roberts, and Stefan Zohren. Few-shot learning patterns in financial time series
for trend-following strategies. _Journal of Financial Data Science_, 6(2):88â€“115, 2024. doi: 10.3905/jfds.2024.1.157.

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,
Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In _Proceedings of the_
_37th International Conference on Machine Learning (ICML)_, pages 10524â€“10533. PMLR, 2020. URL `[http:](http://proceedings.mlr.press/v119/xiong20b.html)`
`[//proceedings.mlr.press/v119/xiong20b.html](http://proceedings.mlr.press/v119/xiong20b.html)` .

Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu.
Do transformers really perform bad for graph representation? In _Advances in Neural Information Processing_
_Systems_, volume 34, pages 28877â€“28888, 2021. URL `[https://proceedings.neurips.cc/paper/2021/hash/](https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html)`
`[f1c1592588411002af340cbaedd6fc33-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html)` .


Terry W. Young. Calmar ratio: A smoother tool. _Futures_, 1991. Trade magazine article introducing the Calmar ratio.


Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, BarnabÃ¡s PÃ³czos, Ruslan Salakhutdinov, and Alexander J. Smola.
Deep sets. In _Advances in Neural Information Processing Systems_, 2017. URL `[https://papers.nips.cc/paper/](https://papers.nips.cc/paper/6931-deep-sets)`
`[6931-deep-sets](https://papers.nips.cc/paper/6931-deep-sets)` .


Zihao Zhang and Stefan Zohren. Deep learning in quantitative trading. _Elements in Quantitative Finance_, 2025.


Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep learning for portfolio optimization. _The Journal of Financial_
_Data Science_, 2(4):8â€“20, 2020. doi: 10.3905/jfds.2020.1.042.


**A** **Universe Specifications and Methodology**


This appendix details the investment universe, the macroeconomic graph topology used for structural regularization,
and the transaction cost models applied during backtesting.


**A.1** **Data Provenance and Processing**


We source historical daily open, high, low, and close (OHLC) prices for all 50 futures and FX contracts from the
Pinnacle Data Corp CLC Database [Pinnacle Data Corporation, 2025]; however, for all experiments we just use close to
construct features. To construct continuous return series from individual contract expirations, we utilize ratio-adjusted
(â€œPanamaâ€) continuous contracts.


Unlike additive adjustment methods, which shift historical prices by a fixed absolute amount and can lead to negative
prices or distorted percentage returns over long horizons, ratio adjustment scales historical prices by the ratio of the new
contract price to the old contract price at each roll date. This methodology rigorously preserves the relative percentage
returns and volatility structure of the asset series, ensuring that the inputs to the volatility-scaling mechanism (Section
3.2) and the training returns for the network faithfully represent the realized distribution of market returns.


**A.2** **Macro Graph Topology**


The macroeconomic prior graph G = (V _,_ E) is constructed deterministically based on economic first principles. The
adjacency matrix _ğ´_ is formed via the union of the following edge sets:


1. **Intra-Group Cliques (Sectoral Homophily):** All nodes within a specific Macro Group (e.g., `COMM_ENERGY` )
are fully connected, enforcing the view that assets within a sub-sector share a single latent factor.


2. **Risk-On Channel:** Connects Global Equities â†” Base Metals â†” Risk FX (AUD, CAD, MXN).


3. **Inflation Channel:** Connects Energy â†” US Treasuries â†” Precious Metals.


4. **Safe Haven Flows:** Connects US Treasuries â†” Safe FX (JPY, CHF) â†” Precious Metals.


5. **Commodity Exporters:** Connects Energy/Metals to their respective currency proxies (e.g., Oil â†” CAD/MXN,
Metals â†” AUD).


6. **Regional Integration:** Creates a triangular linkage between the primary Equity index, Sovereign Bond, and
Currency for major economic zones (US, EU, JP, UK, CA).


26


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**A.3** **Transaction Cost Methodology**


We implement a _Structural Minimum Cost Model_ that synthesizes the â€œTick Size Constraint" theory (Harris, 2003) with
market impact estimates. The cost _ğ‘ğ‘–_ (in basis points) is derived as:


_ğ‘ğ‘–_ â‰ˆ max( _ğ¶_ floor _, ğ¶_ struct _,ğ‘–_ Ã— _ğœ†ğ‘–_ ) (48)


where _ğ¶_ struct is the theoretical cost implied by the minimum price variation (tick size) relative to the contract value, and
_ğœ†ğ‘–_ â‰¥ 1 _._ 0 is a liquidity scalar.


**A.3.1** **Justification of Liquidity Scalars (** _ğœ†ğ‘–_ **)**


While liquid electronic markets (e.g., S&P 500, US Treasuries) typically trade near their structural tick constraints
( _ğœ†ğ‘–_ â‰ˆ 1 _._ 0 âˆ’ 1 _._ 5), we apply significant scalers to subsets of the universe to reflect institutional execution realities that
simple tick-based models miss.


**The â€œRoach Motel" Effect (Depth vs. Spread).** Certain markets, such as Palladium ( `PA` ) or specific Softs, may exhibit
tight top-of-book spreads that mask a lack of order book depth. For institutional-size execution, the â€œeffective spread" to
sweep the book is significantly wider than the quoted spread. We model this via high â€œStructure" scalars (e.g., Palladium
_ğœ†ğ‘–_ â‰ˆ 2 _._ 4, Orange Juice _ğœ†ğ‘–_ â‰ˆ 12 _._ 6) to penalize strategies that assume they can exit large positions at the touch price
during stress periods.


**Volatility-Constrained Market Making.** For assets like Natural Gas or Livestock (Feeder Cattle), the exchangemandated tick size is often non-binding. Market makers widen spreads to compensate for extreme inventory volatility.
In these regimes, the cost is driven by volatility rather than quantization noise. We apply scalars ( _ğœ†ğ‘–_ _>_ 2 _._ 0) to align the
modeled cost with the realized volatility-adjusted spreads observed in 2023-2024.


**Regional Session Impacts.** Assets such as the Nikkei 225 often show adequate liquidity during their local session but
suffer from â€œair pockets" during the US/EU overlap when global macro rebalancing occurs. We apply an â€œImpact"
scalar ( _ğœ†ğ‘–_ â‰ˆ 2 _._ 0 âˆ’ 2 _._ 3) to represent the higher slippage associated with trading these assets out of their primary liquidity
window.


**High-Velocity and Arbitrage Discounts (** _ğœ†ğ‘–_ _<_ 1 _._ 0 **).** Conversely, certain highly efficient markets (e.g., Hang Seng,
Dollar Index, FTSE 100) are assigned scalars below unity. In these cases, the exchange-mandated tick size ( _ğ¶_ struct) acts
as a legacy constraint that overestimates the effective cost for institutional participants. Deep spot-futures arbitrage links
(e.g., between the Dollar Index future and the underlying spot FX basket) allow market makers to quote aggressive
effective spreads, often facilitating execution better than the naked tick implies via block liquidity or EFP (Exchange for
Physical) mechanisms.


**A.3.2** **Cost Summary Statistics**


Table 3 summarizes the resulting cost distribution. The divergence between â€œMedian Calc" and the â€œFinal Band" in the
high-cost tiers reflects the necessity of the _ğœ†ğ‘–_ scalars.


Table 3: Transaction Cost Regimes: Theoretical vs. Modeled


**Final Band** **Median** _ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡_ **Mean** _ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡_ **Mean** _ğœ†ğ‘–_ **Typical Asset Class**


0.25 bps 0.19 0.19 1.6x Ultra-Liquid (S&P 500, US Treasuries)
0.50 bps 0.40 0.43 1.5x Very Liquid (Bund, JPY, FTSE)
0.75 bps 0.71 0.81 1.1x Liquid Physicals (Crude, 10Y Note)
1.00 bps 1.02 0.96 1.1x Standard (Silver, EuroStoxx)
1.50 bps 1.30 1.02 1.6x Mid-Liquidity (30Y Bond, Platinum)
2.50 bps 1.67 1.60 1.9x High Cost (Grains, Nat Gas, Livestock)
6.00 bps 1.75* 1.75* 4.2x Volatile / Thin (Palladium, Cocoa)
15.0 bps 1.19* 1.19* 12.6x Distressed (Orange Juice)


_*Note: For high-cost bands, the divergence between â€œMedian Calc" and the Final Band reflects the high Liquidity Scalar required for_
_volatile markets._


**A.4** **Master Data Universe**


Table 4 provides the granular derivation of costs for the full 50-asset universe.


    - **Calc:** The theoretical cost based on Tick Size ( _ğ¶ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡_ ).


27


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


    - **Scalar (** _ğœ†ğ‘–_ **):** The multiplier applied ( _ğ¹ğ‘–ğ‘›ğ‘ğ‘™_ = _ğ¶ğ‘ğ‘™ğ‘_ Ã— _ğœ†ğ‘–_ ).


    - **Type: Impact** (Size/Session adjustment) or **Structure** (Microstructure/Volatility override).


Table 4: Master Universe: Macro Groups and Transaction Cost Derivation


**Ticker** **Name** **Group** **Calc** _**ğ€ğ’Š**_ **Final** **Type / Note**


_**Sovereign Rates**_
TU US 2yr Note RATES_US 0.19 1.3x **0.25** Ultra-Liquid
FV US 5yr Note RATES_US 0.25 1.0x **0.25** Ultra-Liquid
TY US 10yr Note RATES_US 0.71 1.1x **0.75** Benchmark
US US 30yr Bond RATES_US 1.32 1.1x **1.50** Liquid
DU Euro Schatz RATES_EU 0.25 1.0x **0.25** Short-End EU
OE German Bobl RATES_EU 1.32 1.1x **1.50** Mid-Curve EU
RX Euro Bund RATES_EU 0.25 2.0x **0.50** **Impact** (Duration)
G Long Gilt RATES_OTHR 0.25 2.0x **0.50** **Impact** (Non-US)
CN Canada 10yr RATES_OTHR 0.25 2.0x **0.50** **Impact** (Depth)
_**Equities**_
ES S&P 500 EQUITY_US 0.18 1.4x **0.25** Global Benchmark
EN Nasdaq 100 EQUITY_US 0.05 5.0x **0.25** Floor Effect
YM Dow Jones EQUITY_US 0.10 2.5x **0.25** Floor Effect
RTY Russell 2000 EQUITY_US 0.18 2.8x **0.50** Liquid Small Cap
VG EuroStoxx 50 EQUITY_EU 1.04 1.0x **1.00** Standard EU Liq
Z FTSE 100 EQUITY_EU 0.65 0.8x **0.50** Developed Mkt
CF CAC 40 EQUITY_EU 0.65 0.8x **0.50** Developed Mkt
NK Nikkei 225 EQUITY_APAC 0.64 2.3x **1.50** **Impact** (Asian Session)
HI Hang Seng EQUITY_APAC 1.35 0.6x **0.75** High Velocity
_**Foreign Exchange**_
DX Dollar Index FX_G10 0.65 0.8x **0.50** Spot Arb
EU EUR/USD FX_G10 0.24 1.0x **0.25** Global Anchor
JY JPY/USD FX_G10 0.39 1.3x **0.50** **Impact** Scaling
BP GBP/USD FX_G10 0.39 1.3x **0.50** Standard G10
CD CAD/USD FX_G10 0.36 1.4x **0.50** Standard G10
AD AUD/USD FX_G10 0.40 1.3x **0.50** Standard G10
SF CHF/USD FX_G10 0.50 1.0x **0.50** Standard G10
PE Mexican Peso FX_EM 1.02 1.0x **1.00** High Vol/Low Price
_**Commodities: Energy**_
CL WTI Crude COMM_EN 0.71 1.1x **0.75** Global Benchmark
CO Brent Crude COMM_EN 0.71 1.1x **0.75** Global Benchmark
XB RBOB Gas COMM_EN 1.30 1.2x **1.50** Product Spread
QS Gasoil COMM_EN 1.30 1.2x **1.50** Product Spread
NG Natural Gas COMM_EN 1.67 1.5x **2.50** **Structure** (Vol)
_**Commodities: Metals**_
GC Gold COMM_PREC 0.19 1.3x **0.25** Tight Book
SI Silver COMM_PREC 0.81 1.2x **1.00** Standard
PL Platinum COMM_PREC 0.53 2.8x **1.50** **Structure** (Wide)
PA Palladium COMM_PREC 2.50 2.4x **6.00** **Structure** (Thin)
HG Copper COMM_BASE 0.58 1.3x **0.75** High Efficiency
_**Commodities: Agriculture & Livestock**_
C Corn COMM_AGRI 2.91 0.9x **2.50** Standard Grain
S Soybeans COMM_AGRI 1.26 1.2x **1.50** Deep Market
SM Soybean Meal COMM_AGRI 1.72 1.5x **2.50** **Impact** (Crush)
BO Soybean Oil COMM_AGRI 1.30 1.2x **1.50** Crush Spread
W Wheat COMM_AGRI 2.27 1.1x **2.50** Volatile
KW KC Wheat COMM_AGRI 2.50 1.0x **2.50** Volatile
SB Sugar COMM_SOFT 2.38 1.1x **2.50** Standard Soft
KC Coffee COMM_SOFT 0.81 3.1x **2.50** **Structure** (Vol)
CC Cocoa COMM_SOFT 1.00 6.0x **6.00** **Structure** (Extreme)
CT Cotton COMM_SOFT 0.70 3.6x **2.50** **Structure** (Thin)
JO Orange Juice COMM_SOFT 1.19 12.6x **15.0** **Structure** (Distressed)
LC Live Cattle COMM_LIVE 0.68 2.2x **1.50** **Structure** (Gappy)
FC Feeder Cattle COMM_LIVE 0.49 5.1x **2.50** **Structure** (Gappy)
LH Lean Hogs COMM_LIVE 1.47 1.7x **2.50** **Structure** (Gappy)


**A.5** **Specific Overrides**


We apply manual overrides where the theoretical tick cost underestimates institutional execution difficulty:


    - **Floor Effect:** Liquid indices like Nasdaq 100 have a structural cost _<_ 0 _._ 1 bps. We floor this at 0.25 bps to
represent clearing/execution minimums.


    - **â€œRoach Motel" Liquidity:** Palladium is theoretically cheap but lacks depth. We scale it to 6.0 bps.


    - **Distressed Liquidity:** Orange Juice is assigned 15.0 bps to reflect â€œwidowmaker" liquidity risks.


28


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**B** **Turnover Guarantees for Ensembles**


This appendix provides formal derivations regarding the convexity of the transaction cost objective and the regularization
properties of the ensemble.


**B.1** **Convexity of Turnover and Ensemble Guarantee**


**Proposition B.1** (Convexity of Turnover Cost) **.** _Let ğ’‘_ 0: _ğ¿_ _be a sequence of portfolio position vectors with ğ’‘ğ‘¡_ âˆˆ R _[ğ‘]_ _._
_Assume the scaling vectors satisfy ğ’—ğ‘¡_ âˆˆ R+ _[ğ‘]_ _[and are exogenous (i.e., do not depend on][ ğ’‘][). Define]_



C( _ğ’‘_ ) = _ğ›¾_



_ğ¿_
âˆ‘ï¸


_ğ‘¡_ =1



ï¿½ï¿½ _ğ’—ğ‘¡_ âŠ™( _ğ’‘ğ‘¡_ - _ğ’‘ğ‘¡_ âˆ’1)ï¿½ï¿½1 _[,]_ _ğ›¾_ â‰¥ 0 _._



_Then_ C( _ğ’‘_ ) _is convex in ğ’‘_ 0: _ğ¿._


_Proof._ For each _ğ‘¡_, the map _ğ’‘_ 0: _ğ¿_ â†¦â†’ _ğ’‘ğ‘¡_ - _ğ’‘ğ‘¡_ âˆ’1 is linear. Elementwise scaling by a fixed _ğ’—ğ‘¡_ is also linear, since
_ğ’—ğ‘¡_ âŠ™ _ğ’™_ = Diag( _ğ’—ğ‘¡_ ) _ğ’™_ . Thus the composition _ğ’‘_ â†¦â†’ _ğ’—ğ‘¡_ âŠ™( _ğ’‘ğ‘¡_ - _ğ’‘ğ‘¡_ âˆ’1) is linear. Because âˆ¥Â· âˆ¥1 is convex and convexity is
preserved under composition with an affine map, _ğ’‘_ â†¦â†’âˆ¥ _ğ’—ğ‘¡_ âŠ™( _ğ’‘ğ‘¡_ - _ğ’‘ğ‘¡_ âˆ’1)âˆ¥1 is convex. Summing over _ğ‘¡_ with nonnegative
weight _ğ›¾_ â‰¥ 0 preserves convexity, hence C is convex. 

**Corollary B.2** (Ensembling Reduces Cost (Executed Mean Policy)) **.** _Let_ { _ğ’‘_ [(] _[ğ‘˜]_ [)] } _[ğ¾]_ _ğ‘˜_ =1 _[be][ ğ¾]_ _[policies and define the]_
_executed mean policy_ Â¯ _ğ’‘_ := _ğ¾_ [1] - _ğ¾ğ‘˜_ =1 _[ğ’‘]_ [(] _[ğ‘˜]_ [)] _[ (averaged at the]_ [ position] _[ level). Then, by convexity of]_ [ C] _[,]_



C( Â¯ _ğ’‘_ ) â‰¤ [1]

_ğ¾_


_Proof._ Since C is convex, Jensenâ€™s inequality gives



_ğ¾_
âˆ‘ï¸

C( _ğ’‘_ [(] _[ğ‘˜]_ [)] ) _._ (49)

_ğ‘˜_ =1



_ğ¾_ âˆ‘ï¸

_ğ’‘_ [(] _[ğ‘˜]_ [)]

_ğ‘˜_ =1



â‰¤ [1]

_ğ¾_



_ğ¾_
âˆ‘ï¸ 
C _ğ’‘_ [(] _[ğ‘˜]_ [)] [ï¿½] _,_

_ğ‘˜_ =1



C




1

_ğ¾_



which is the desired result. 

_Note:_ This guarantee applies when the ensemble is implemented by executing the averaged positions Â¯ _ğ’‘_ . If instead each
policy _ğ’‘_ [(] _[ğ‘˜]_ [)] is traded separately and P&L is averaged ex post, transaction costs are incurred per-policy and the inequality
need not reflect realized costs.


**B.2** **Optimal Regularization under Ensembling**


**Proposition B.3** (Optimal Penalty Decreases with Ensemble Size) **.** _Let ğ›¾_ â‰¥ 0 _be the explicit turnover penalty strength_
_used during training, and let ğ¾_ _be the ensemble size. We model the realized Net Sharpe Ratio as_


SRnet ( _ğ›¾, ğ¾_ ) = SRgross ( _ğ›¾_ ) âˆ’ _ğœŒ_ ( _ğ¾_ ) Î”( _ğ›¾_ ) _,_


_where_ Î”( _ğ›¾_ ) _is the turnover cost and ğœŒ_ ( _ğ¾_ ) _is an ensemble-dependent cost reduction factor._


_Under the standard assumptions that:_


_1._ _**Gross performance is strictly concave in regularization**_ _(_ SRgross [â€²â€²] [(] _[ğ›¾]_ [)] _[ <]_ [ 0] _[): Intuitively, relaxing constraints]_
_yields diminishing returns; the initial units of freedom capture high-conviction opportunities, while further_
_relaxation allows the model to chase increasingly marginal and noisy signals._


_2._ _**Turnover cost is strictly decreasing and convex in regularization**_ _(_ Î” [â€²] ( _ğ›¾_ ) _<_ 0 _,_ Î” [â€²â€²] ( _ğ›¾_ ) _>_ 0 _): This reflects the_
_fact that a small penalty is sufficient to eliminate the most expensive high-frequency â€œnoise trading,â€ while_
_further increases yield progressively smaller reductions in turnover as the portfolio approaches a static hold._


_3._ _**Ensembling reduces realized turnover costs**_ _(ğœŒ_ [â€²] ( _ğ¾_ ) _<_ 0 _): As discussed in Sec. 5.2, independent models_
_often make uncorrelated errors, averaging their positions cancels out idiosyncratic â€œnoise trades,â€ naturally_
_dampening turnover without requiring explicit penalties._


29


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


_and assuming an interior maximizer exists, the optimal training penalty ğ›¾_ _[â˜…]_ ( _ğ¾_ ) _is strictly decreasing in the ensemble_
_size ğ¾._


**Lemma B.4** (Sufficient condition for an interior optimizer on (0 _,_ 1)) **.** _Assume ğ›¾_ âˆˆ[0 _,_ 1] _and_ Î¦( _ğ›¾, ğœŒ_ ) _is continuously_
_differentiable in ğ›¾. If_
Î¦ _ğ›¾_ (0 _, ğœŒ_ ) _>_ 0 _and_ Î¦ _ğ›¾_ (1 _, ğœŒ_ ) _<_ 0 _,_


_then there exists ğ›¾_ _[â˜…]_ âˆˆ(0 _,_ 1) _such that_ Î¦ _ğ›¾_ ( _ğ›¾_ _[â˜…]_ _, ğœŒ_ ) = 0 _._


_Proof._ By continuity of Î¦ _ğ›¾_ (Â· _, ğœŒ_ ) and the Intermediate Value Theorem, it must cross zero on (0 _,_ 1). 

_Specialization to our objective._ Here Î¦( _ğ›¾, ğœŒ_ ) = SRgross ( _ğ›¾_ ) âˆ’ _ğœŒ_ Î”( _ğ›¾_ ) and hence

Î¦ _ğ›¾_ ( _ğ›¾, ğœŒ_ ) = SRgross [â€²] [(] _[ğ›¾]_ [) âˆ’] _[ğœŒ]_ [Î”][â€²] [(] _[ğ›¾]_ [)] _[.]_


Therefore a sufficient condition for _ğ›¾_ _[â˜…]_ âˆˆ(0 _,_ 1) is


SRgross [â€²] [(][0][) âˆ’] _[ğœŒ]_ [Î”][â€²] [(][0][)] _[ >]_ [ 0] and SRgross [â€²] [(][1][) âˆ’] _[ğœŒ]_ [Î”][â€²] [(][1][)] _[ <]_ [ 0] _[.]_


_Proof of Proposition._ The objective is to maximize Î¦( _ğ›¾, ğœŒ_ ) = SRgross ( _ğ›¾_ ) âˆ’ _ğœŒ_ Î”( _ğ›¾_ ). Assume an interior maximizer _ğ›¾_ _[â˜…]_



exists and satisfies the second-order condition _[ğœ•]_ [2][Î¦]



exists and satisfies the second-order condition _[ğœ•]_ _ğœ•ğ›¾_ [2] [(] _[ğ›¾][â˜…][, ğœŒ]_ [)] _[ <]_ [ 0][. The first-order condition (FOC) for an interior maximum]

is:
_ğœ•_ Î¦



_ğœ•ğ›¾_ [=][ SR] gross [â€²] [(] _[ğ›¾]_ [) âˆ’] _[ğœŒ]_ [Î”][â€²] [(] _[ğ›¾]_ [)][ =][ 0] _[.]_ (50)



To determine how the optimal _ğ›¾_ _[â˜…]_ responds to changes in the ensemble factor _ğœŒ_, we apply the **Implicit Function**
**Theorem** . Let
_ğ¹_ ( _ğ›¾, ğœŒ_ ) = SRgross [â€²] [(] _[ğ›¾]_ [) âˆ’] _[ğœŒ]_ [Î”][â€²] [(] _[ğ›¾]_ [)] _[.]_



Then, locally around ( _ğ›¾_ _[â˜…]_ _, ğœŒ_ ),
_ğ‘‘ğ›¾_ _[â˜…]_



(51)
_ğœ•ğ¹_ / _ğœ•ğ›¾_ _[.]_




_[â˜…]_

_ğ‘‘ğœŒ_ [=][ âˆ’] _[ğœ•ğ¹]_ _ğœ•ğ¹_ [/] / _[ğœ•ğœŒ]_ _ğœ•ğ›¾_



Analyzing the components:


   - Numerator: _[ğœ•ğ¹]_ _ğœ•ğœŒ_ [=][ âˆ’][Î”][â€²] [(] _[ğ›¾]_ [)][. Since turnover decreases with penalty (][Î”][â€²] [(] _[ğ›¾]_ [)] _[ <]_ [ 0), this term is strictly positive.]


   - Denominator: _[ğœ•ğ¹]_ _ğœ•ğ›¾_ [=][ SR] gross [â€²â€²] [(] _[ğ›¾]_ [) âˆ’] _[ğœŒ]_ [Î”][â€²â€²] [(] _[ğ›¾]_ [)][. Given][ SR] gross [â€²â€²] [(] _[ğ›¾]_ [)] _[ <]_ [ 0][ and][ Î”][â€²â€²] [(] _[ğ›¾]_ [)] _[ >]_ [ 0][ with] _[ ğœŒ]_ [â‰¥] [0][, the entire term]

is strictly negative (satisfying the second-order condition for a maximum).


Substituting these signs into the sensitivity equation yields _[ğ‘‘ğ›¾]_ _ğ‘‘ğœŒ_ _[â˜…]_ _[>]_ [ 0.]


Since _ğœŒ_ ( _ğ¾_ ) is a decreasing function of _ğ¾_ (ensembling reduces realized turnover costs), by the chain rule we obtain:



_ğ‘‘ğ›¾_ _[â˜…]_



_ğ‘‘ğ›¾_ _[â˜…]_ _[ğ‘‘ğ›¾][â˜…]_

_ğ‘‘ğ¾_ [=] _ğ‘‘ğœŒ_



(52)
_ğ‘‘ğœŒ_ [Â·] _[ ğœŒ]_ [â€²] [(] _[ğ¾]_ [)] _[ <]_ [ 0] _[.]_



This proves that as the ensemble size increases, the optimal explicit regularization parameter _ğ›¾_ _[â˜…]_ decreases. 

**C** **Exact Gradient Accumulation for Non-Separable Objectives**


Training Sharpe-ratio-maximizing portfolio models often requires a large _effective_ batch size, but GPU memory limits
force microbatching. Unlike additive objectives (e.g., _ğ‘–_ _[â„“]_ _ğ‘–_ [), Sharpe-style losses are] _[ non-separable]_ [ because their]

[ï¿½]
gradients depend on _global_ batch statistics. Consequently, naive microbatch gradient accumulation (computing Sharpe
per microbatch and summing gradients) generally does _not_ equal the full-batch gradient.


We therefore use an _Exact Two-Pass Microbatching_ protocol: a first pass computes all global/sufficient statistics _without_
storing activations, and a second pass replays the forward pass and injects the _analyatical_ upstream gradient âˆ‡ _ğ‘Ÿ_ L into
autograd. This yields the same parameter gradient as a single full-batch backward pass under the conditions stated
below, while keeping activation memory constant.


30


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**C.1** **Notation and Objectives**


Let the model output net portfolio returns arranged as a matrix _ğ‘…_ âˆˆ R _[ğµ]_ [Ã—] _[ğ¿]_ with entries _ğ‘…ğ‘,ğ‘¡_ (sample _ğ‘_, time index _ğ‘¡_ ).
Let _ğ‘_ := _ğµğ¿_ be the number of return entries in the batch. Let _ğ´_ denote the annualization factor (e.g., _ğ´_ = 252). We use
_ğœ€_ _ğœ_ _>_ 0 for numerical stability in denominators and a variance floor _ğœ€_ var _>_ 0 to prevent _ğœ_ â†’ 0 gradient blow-ups.


Define pooled (batch-level) statistics



_ğ¿_
âˆ‘ï¸



âˆ‘ï¸

_ğ‘…ğ‘,ğ‘¡_ _,_ _ğ‘_ = [1]

_ğ‘_

_ğ‘¡_ =1



_ğ¿_
âˆ‘ï¸

_ğ‘…_ [2]
_ğ‘,ğ‘¡_ _[,]_

_ğ‘¡_ =1



(53)


(54)



_ğœ‡_ = [1]

_ğ‘_



_ğµ_
âˆ‘ï¸


_ğ‘_ =1



_ğ‘_



_ğµ_
âˆ‘ï¸


_ğ‘_ =1



~~âˆš~~
_ğœ_ = max ~~[ï¿½]~~ _ğ‘_           - _ğœ‡_ [2] _, ğœ€_ var ~~ï¿½~~ _._


Now, define per-sample statistics. For each sample _ğ‘_,



_ğœ‡ğ‘_ = [1]

_ğ¿_


âˆšï¸‚

_ğœğ‘_ =



_ğ¿_
âˆ‘ï¸




~~ï¿½~~ ~~ï¿½~~
max _ğ‘ğ‘_ - _ğœ‡_ [2] _ğ‘_ _[, ğœ€]_ [var] _._



âˆ‘ï¸

_ğ‘…ğ‘,ğ‘¡_ _,_ _ğ‘ğ‘_ = [1]

_ğ¿_

_ğ‘¡_ =1



_ğ¿_



_ğ¿_
âˆ‘ï¸

_ğ‘…_ [2]
_ğ‘,ğ‘¡_ _[,]_

_ğ‘¡_ =1



In implementation we use a minimum clamp on the variance; when the clamp is active, the gradient through the variance
term is zero, ensuring the 1/ _ğœ_ factors remain bounded.


**Loss.** We optimize
L = Lpool + _ğœ†_ Lsoft _,_ (55)
where the pooled Sharpe loss is
âˆš _ğœ‡_
Lpool = âˆ’ _ğ´_ _._ (56)

_ğœ_ + _ğœ€_ _ğœ_

The SoftMin term Lsoft is defined in the main text and is chosen to _maximize_ the soft-min (smooth worst-case) per-sample
Sharpe across groups. In the appendix we only require its upstream gradient (below).



Define per-sample Sharpe
âˆš
SR _ğ‘_ =



_ğœ‡ğ‘_
_ğ´_ _._ (57)
_ğœğ‘_ + _ğœ€_ _ğœ_



Let the _ğµ_ sequences be partitioned into _ğº_ groups of size _ğ¾_ (where _ğµ_ = _ğºğ¾_ ), indexed by group _ğ‘”_ âˆˆ{1 _, . . ., ğº_ } and
within-group index _ğ‘˜_ âˆˆ{1 _, . . ., ğ¾_ }. We map each sequence _ğ‘_ to a unique pair ( _ğ‘”, ğ‘˜_ ). Consistent with the adversarial
weights derived in App. D, we define the group-wise SoftMin weights as:

exp [ï¿½] âˆ’SR _ğ‘”,ğ‘˜_ / _ğœ_ [ï¿½]
_ğ‘_ _[â˜…]_ _ğ‘”,ğ‘˜_ [=] ~~ï¿½~~ _ğ¾_ (58)

_ğ‘—_ =1 [exp] ~~[ï¿½]~~ [âˆ’][SR] _[ğ‘”, ğ‘—]_ [/] _[ğœ]_ ~~[ï¿½]~~ _[.]_

For the (negative) soft-min objective used to maximize worst-case Sharpe, the upstream gradient is:
_ğœ•_ Lsoft
= âˆ’ [1] _ğ‘”,ğ‘˜_ _[.]_ (59)
_ğœ•_ SR _ğ‘”,ğ‘˜_ _ğº_ _[ğ‘][â˜…]_


**C.2** **Analyatical Gradients**


Differentiating (56) w.r.t. _ğ‘…ğ‘,ğ‘¡_ gives






_._




_._



_ğœ•_ Lpool âˆš

= âˆ’
_ğœ•ğ‘…ğ‘,ğ‘¡_



_ğ´_




1

_ğ‘_ ( _ğœ_ + _ğœ€_ _ğœ_ )


 - _ğœ‡_ ( _ğ‘…ğ‘,ğ‘¡_ - _ğœ‡_ )
_ğ‘_ ( _ğœ_ + _ğœ€_ _ğœ_ ) [2] _ğœ_



(60)


(61)



Differentiating (57) yields
_ğœ•_ SR _ğ‘_ ~~âˆš~~
= _ğ´_
_ğœ•ğ‘…ğ‘,ğ‘¡_




1

_ğ¿_ ( _ğœğ‘_ + _ğœ€_ _ğœ_ )


 - _[ğœ‡][ğ‘]_ [(] _[ğ‘…][ğ‘,ğ‘¡]_ [âˆ’] _[ğœ‡][ğ‘]_ [)]

_ğ¿_ ( _ğœğ‘_ + _ğœ€_ _ğœ_ ) [2] _ğœğ‘_


31


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


**Algorithm 1** Exact Two-Pass Microbatching for Non-Separable Sharpe Objectives

1: **Inputs:** dataset D, microbatch size _ğ‘€_, model parameters _ğœƒ_
2: **Initialize** pooled sums _ğ‘†_ 1 â† 0, _ğ‘†_ 2 â† 0
3: **Initialize** per-sample sums { _ğ‘†_ 1 _,ğ‘_ } _ğ‘_ _[ğµ]_ =1 [â†] [0,][ {] _[ğ‘†]_ [2] _[,ğ‘]_ [}] _ğ‘_ _[ğµ]_ =1 [â†] [0]
4: **// Pass 1: Collect Statistics (No Activations)**
5: **for** microbatch _ğ‘š_ in D **do**
6: **Set deterministic RNG seed** for microbatch _ğ‘š_
7: _ğ‘…ğ‘š_ â† Model _ğœƒ_ (input _ğ‘š_ ) _âŠ²_ forward only
8: Accumulate pooled: _ğ‘†_ 1 += _ğ‘…ğ‘š_, _ğ‘†_ 2 += _ğ‘…ğ‘š_ [2]

[ï¿½] [ï¿½]
9: Accumulate per-sample: _ğ‘†_ 1 _,ğ‘_ += [ï¿½] _ğ‘¡_ _[ğ‘…]_ _ğ‘,ğ‘¡_ [,] _[ ğ‘†]_ 2 _,ğ‘_ [+][=][ ï¿½] _ğ‘¡_ _[ğ‘…]_ _ğ‘,ğ‘¡_ [2]
10: **end for**
11: Compute _ğœ‡, ğ‘, ğœ_ from _ğ‘†_ 1 _, ğ‘†_ 2 via (53)
12: Compute _ğœ‡ğ‘, ğ‘ğ‘, ğœğ‘_ from _ğ‘†_ 1 _,ğ‘, ğ‘†_ 2 _,ğ‘_ via (54)
13: Compute all SR _ğ‘_ via (57) and SoftMin weights _ğœ‹ğ‘”,ğ‘˜_ via (58)
14: **// Pass 2: Inject analyatical Upstream Gradient**
15: **for** microbatch _ğ‘š_ in D **do**
16: **Reset deterministic RNG seed** for microbatch _ğ‘š_
17: _ğ‘…ğ‘š_ â† Model _ğœƒ_ (input _ğ‘š_ ) _âŠ²_ forward with autograd
18: Build _ğº_ _ğ‘š_ using (60), (61), (62), (63)
19: _ğ‘…ğ‘š._ backward( _ğº_ _ğ‘š_ ) _âŠ²_ inject upstream gradient
20: **end for**
21: Optimizer.step()


Using (59) and the fact that SR _ğ‘”,ğ‘˜_ depends only on its own sample,



_ğœ•_ Lsoft



_ğœ•_ SR _ğ‘_

[1] _._ (62)

_ğº_ _[ğœ‹][ğ‘”]_ [(] _[ğ‘]_ [)] _[,ğ‘˜]_ [(] _[ğ‘]_ [)] _ğœ•ğ‘…ğ‘,ğ‘¡_



_ğœ•_ Lsoft

= âˆ’ [1]
_ğœ•ğ‘…ğ‘,ğ‘¡_ _ğº_



The total upstream gradient, used for injection in Pass 2, is



_ğœ•_ L
_ğº_ _ğ‘,ğ‘¡_ = = _[ğœ•]_ [L][pool]
_ğœ•ğ‘…ğ‘,ğ‘¡_ _ğœ•ğ‘…ğ‘,ğ‘¡_



_._ (63)
_ğœ•ğ‘…ğ‘,ğ‘¡_




[L][pool]

+ _ğœ†_ _[ğœ•]_ [L][soft]
_ğœ•ğ‘…ğ‘,ğ‘¡_ _ğœ•ğ‘…ğ‘,ğ‘¡_



For implementation via autograd, we aggregate the element-wise analyatical gradients into a microbatch-level tensor.
Let B _ğ‘š_ denote the set of indices for the samples contained in microbatch _ğ‘š_ . We define the injected gradient tensor
_ğº_ _ğ‘š_ âˆˆ R [|B] _[ğ‘š]_ [|Ã—] _[ğ¿]_ such that its entries correspond to the scalar gradients _ğº_ _ğ‘,ğ‘¡_ derived in (63) for all _ğ‘_ âˆˆB _ğ‘š_ and
_ğ‘¡_ âˆˆ{1 _, . . ., ğ¿_ }. This tensor is then passed directly to the modelâ€™s output during the second pass.


**C.3** **Exact Two-Pass Microbatching Algorithm**


The procedure splits the training step into two distinct passes over the data to circumvent memory bottlenecks while
maintaining mathematical exactness.


**Phase 1: Statistical Accumulation.** The model processes the data in microbatches with autograd disabled. Since
gradients are not tracked, memory usage is minimal. We aggregate the sufficient statisticsâ€”sum of returns ( [ï¿½] _ğ‘…_ ) and
sum of squared returns ( [ï¿½] _ğ‘…_ [2] )â€”globally and per sample. These aggregates allow us to compute the exact batch-wide
mean _ğœ‡_ and volatility _ğœ_, which are required to define the true gradients.


**Phase 2: Gradient Injection.** The model processes the data a second time with autograd enabled. Crucially, we do not
compute the loss function forward. Instead, we use the pre-computed _ğœ‡_ and _ğœ_ from Phase 1 to evaluate the analyatical
gradient formulas derived in Section C.2. This yields a gradient tensor _ğº_ _ğ‘š_ for the microbatch outputs. We inject this
tensor directly into the backward pass using `r_m.backward(gradient=G_m)` . This propagates the exact gradient of
the full-batch Sharpe ratio through the computation graph of the microbatch.


We must consider some exactness conditions. The two-pass protocol reproduces the same parameter gradient as a single
full-batch backward pass _provided that_ the per-sample forward mapping is identical across passes and independent
of microbatch composition. Concretely: (i) **No batch-coupled layers:** the network must not contain layers whose
outputs depend on microbatch statistics (e.g., BatchNorm) or on other samples in the microbatch. (ii) **No stateful**
**forward updates:** modules that update running buffers during the forward pass must be disabled/frozen. (iii) **Identical**
**data order and grouping:** Pass 1 and Pass 2 must iterate over the same samples in the same order, and the SoftMin


32


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


grouping/indexing used to compute _ğœ‹ğ‘”,ğ‘˜_ must match the mapping used when injecting gradients in Pass 2 (i.e., no
reshuffling between passes). (iv) **Deterministic stochastic layers:** any stochasticity (e.g., Dropout) must be controlled
so that _ğ‘…ğ‘š_ is identical in both passes (handled by per-microbatch RNG reseeding above).


**D** **Analysis of the Robust Objective**


This appendix provides a formal interpretation of the windowed _SoftMin_ aggregation used in Sec. 5.1. We show that
the resulting objective admits (i) a KL-penalized distributionally robust optimization (DRO) interpretation via the
Gibbs/Donskerâ€“Varadhan variational principle, and (ii) a connection to the coherent risk measure _Entropic Value-at-Risk_
_(EVaR)_ .


**D.1** **SoftMin as KL-penalized DRO (Variational Form)**


We consider _ğµ_ training windows and their realized Sharpe ratios {SR _ğ‘_ } _ğ‘_ _[ğµ]_ =1 [. Define window] _[ losses][ ğ‘][ğ‘]_ [(] _[ğœƒ]_ [)][ :][=][ âˆ’][SR] _[ğ‘]_ [(] _[ğœƒ]_ [)][,]
and let _ğ‘ƒ_ denote the uniform empirical distribution over windows ( _ğ‘ƒ_ ( _ğ‘_ ) = 1/ _ğµ_ ). For a temperature _ğœ>_ 0, our
SoftMin-in-Sharpe objective is equivalently a softmax in the losses:



âˆ‘ï¸ - _ğ‘ğ‘_ ( _ğœƒ_ )

exp

_ğœ_

_ğ‘_ =1



_ğœ_




      -      - _ğ‘_

= _ğœ_ log E _ğ‘ƒ_ exp

_ğœ_




- [ï¿½]



ï¿½ï¿½
_._ (64)



Lsoft ( _ğœƒ_ ) = _ğœ_ log




1

_ğµ_



_ğµ_
âˆ‘ï¸



As _ğœ_ â†’ 0 [+], Lsoft ( _ğœƒ_ ) â†’ max _ğ‘_ _ğ‘ğ‘_ ( _ğœƒ_ ) = âˆ’ min _ğ‘_ SR _ğ‘_ ( _ğœƒ_ ), recovering a worst-window (min-Sharpe) criterion; as _ğœ_ â†’âˆ,
it approaches the average loss.

**Proposition D.1** (Gibbs / Donskerâ€“Varadhan variational principle (discrete)) **.** _The entropic aggregate in_ (64) _admits_
_the dual representation_
_ğœ_ log E _ğ‘ƒ_ ï¿½e _[ğ‘]_ [/] _[ğœ]_ [ï¿½] = sup ï¿½E _ğ‘„_ [ _ğ‘_ ] âˆ’ _ğœ_ KL( _ğ‘„_ âˆ¥ _ğ‘ƒ_ )ï¿½ _,_ (65)
_ğ‘„_ â‰ª _ğ‘ƒ_


_where ğ‘„_ _is an adversarial reweighting over windows and ğ‘„_ â‰ª _ğ‘ƒ_ _is automatic here since ğ‘ƒ_ _is uniform._


_Proof._ The duality follows directly from the variational representation of the log-sum-exp function, a standard result
in convex analysis [Boucheron et al., 2013]. For completeness, we derive it for the discrete case below. Let _ğ‘ğ‘_ be
the weights of _ğ‘„_ with [ï¿½] _ğ‘_ _[ğµ]_ =1 _[ğ‘][ğ‘]_ [=][ 1][. Since] _[ ğ‘ƒ]_ [(] _[ğ‘]_ [)][ =][ 1][/] _[ğµ]_ [,][ KL][(] _[ğ‘„]_ [âˆ¥] _[ğ‘ƒ]_ [)][ =][ ï¿½] _ğ‘_ _[ğµ]_ =1 _[ğ‘][ğ‘]_ [log][ï¿½] _[ğ‘][ğ‘]_ [/(][1][/] _[ğµ]_ [)][ï¿½] [=][ ï¿½] _ğ‘_ _[ğµ]_ =1 _[ğ‘][ğ‘]_ [log][(] _[ğµğ‘][ğ‘]_ [)] _[.]_
Consider the Lagrangian



_ğµ_
âˆ‘ï¸

_ğ‘ğ‘_

_ğ‘_ =1



_._ (66)



_ğµ_
âˆ‘ï¸

_ğ‘ğ‘_ log( _ğµğ‘ğ‘_ ) + _ğœ‚_

_ğ‘_ =1





1 âˆ’







_ğ½_ ( _ğ‘, ğœ‚_ ) =



_ğµ_
âˆ‘ï¸

_ğ‘ğ‘_ _ğ‘ğ‘_  - _ğœ_

_ğ‘_ =1



Stationarity gives 0 = _ğœ•ğ½_ / _ğœ•ğ‘ğ‘_ = _ğ‘ğ‘_ - _ğœ_ [ï¿½] 1 + log( _ğµğ‘ğ‘_ ) [ï¿½] - _ğœ‚,_ hence


exp( _ğ‘ğ‘_ / _ğœ_ )
_ğ‘_ _[â˜…]_ _ğ‘_ [=] ~~ï¿½~~ _ğµ_ _._ (67)
_ğ‘—_ =1 [exp][(] _[ğ‘]_ _[ğ‘—]_ [/] _[ğœ]_ [)]

Substituting _ğ‘_ _[â˜…]_ into [ï¿½] _ğ‘_ _[ğ‘]_ _ğ‘_ _[ğ‘]_ _ğ‘_ [âˆ’] _[ğœ]_ [ï¿½] _ğ‘_ _[ğ‘]_ _ğ‘_ [log][(] _[ğµğ‘]_ _ğ‘_ [)][ yields] _[ ğœ]_ [log][ï¿½] _ğµ_ [1] - _ğ‘ğµ_ =1 [e] _[ğ‘][ğ‘]_ [/] _[ğœ]_ [ï¿½][, i.e. (64).] 

The weights _ğ‘_ _[â˜…]_ _ğ‘_ [in][ (67)][ provide a diagnostic: concentrated mass on a small set of windows indicates that those regimes]
dominate the gradient signal and are the current â€œstress scenariosâ€ for the model.


**D.2** **Connection to KL-ball DRO and EVaR**


A â€œhardâ€ DRO variant constrains the adversary to a KL-divergence ball of radius _ğ¶_ :


R _ğ¶_ ( _ğ‘_ ) = sup E _ğ‘„_ [ _ğ‘_ ] _._ (68)
_ğ‘„_ : KL( _ğ‘„_ âˆ¥ _ğ‘ƒ_ )â‰¤ _ğ¶_


By Lagrangian duality together with Proposition D.1, this is equivalent to


R _ğ¶_ ( _ğ‘_ ) = inf            - _ğœ†_ log E _ğ‘ƒ_ ï¿½e _[ğ‘]_ [/] _[ğœ†]_ [ï¿½] + _ğœ†ğ¶_            - _._ (69)
_ğœ†>_ 0


Choosing the divergence budget _ğ¶_ = âˆ’ log(1 âˆ’ _ğ›¼_ ) yields _Entropic Value-at-Risk (EVaR)_ at confidence level _ğ›¼_ âˆˆ(0 _,_ 1):


EVaR _ğ›¼_ ( _ğ‘_ ) = inf            - _ğœ†_ log Eï¿½e _[ğ‘]_ [/] _[ğœ†]_ [ï¿½]            - _ğœ†_ log(1 âˆ’ _ğ›¼_ )ï¿½ _._ (70)
_ğœ†>_ 0


33


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


In DeePM, the training temperature _ğœ_ can be viewed as selecting a particular member of this entropic/EVaR family (i.e.,
evaluating (70) at _ğœ†_ = _ğœ_, up to the additive constant âˆ’ _ğœ_ log(1 âˆ’ _ğ›¼_ ) when _ğ›¼_ is fixed). Thus SoftMin training implements
a smooth, tail-sensitive surrogate that emphasizes poor historical windows, while remaining differentiable and stable.


**E** **Experimental Implementation Details**


This appendix provides the granular specifications required to reproduce the experimental results, including hyperparameter search spaces, and the specific optimization protocols employed.


**E.1** **Optimization and Training Dynamics**


We optimize the parameters using **AdamW** [Loshchilov and Hutter, 2019] to decouple weight decay from gradient
updates, which is crucial for stabilizing Transformers. We use a fixed learning rate of _ğœ‚_ = 10 [âˆ’][4] without annealing, as
the non-stationary nature of financial data often makes cyclic or decaying schedules suboptimal for continuous learning.


Given the low signal-to-noise ratio in financial time series, validation metrics can be highly volatile across epochs. To
prevent premature stopping due to lucky/unlucky noise, we implement an **Exponential Moving Average (EMA)** on the
validation metric (Sharpe Ratio). Let _ğ‘†_ [val] _ğ‘˜_ [be the raw validation Sharpe at iteration] _[ ğ‘˜]_ [. The smoothed metric][ Ëœ] _[ğ‘†][ğ‘˜]_ [updates]
as:
_ğ‘†_ Ëœ _ğ‘˜_ = _ğ›¼_ smooth _ğ‘†_ [val] _ğ‘˜_ [+ (][1][ âˆ’] _[ğ›¼]_ [smooth][)][ Ëœ] _[ğ‘†][ğ‘˜]_ [âˆ’][1] _[,]_ (71)

with _ğ›¼_ smooth = 0 _._ 45. Training terminates if _ğ‘†_ [Ëœ] _ğ‘˜_ fails to improve by _ğ›¿_ min = 0 _._ 001 for a patience defined per architecture
(see Table 5). We employ a â€œburn-in" of 20 iterations before enabling the early stopping trigger to allow the optimizer to
stabilize.


Models were trained on a single NVIDIA GeForce RTX 3090 (24GB) GPU.


**E.2** **Existence Masking and Missingness Handling**


Since the architecture is permutation equivariant, it inherently handles variable-sized sets of assets ( _ğ‘ğ‘¡_ â‰¤ _ğ‘_ ). However,
for practical training on GPUs requiring fixed tensor shapes, we construct the data tensor on a complete ( _ğ‘–, ğ‘¡_ ) grid with
forward-filling. To prevent learning from synthetically filled values when a contract is not present, DeePM uses: (i) an
existence indicator appended to dynamic inputs, (ii) a key-padding mask kpm _â„“_ in cross-sectional attention, and (iii)
explicit zeroing of invalid node rows after spatial blocks. Finally, positions are masked via _ğ‘šğ‘–,ğ‘¡_ in Eq. (13).


**E.3** **Hyperparameter Search Space**


We perform a random grid search over the key architectural and regularization parameters. For the final ensemble, we
select the top seeds from 100 random trials based on the best smoothed validation Sharpe ratio. Table 5 details the
search grid for the proposed DeePM model versus the Momentum Transformer baseline.


34


DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management


Table 5: Hyperparameter Search Space (DeePM vs. Baseline)


**Parameter** **DeePM (Proposed)** **Momentum Transformer**


_Architecture_
Hidden Dimension ( _ğ‘‘_ model) {64 _,_ 128} {64 _,_ 128}
Attention Heads {2 _,_ 4} 4
Dropout {0 _._ 3 _,_ 0 _._ 4 _,_ 0 _._ 5} {0 _._ 3 _,_ 0 _._ 4 _,_ 0 _._ }


_Optimization_
Batch Size 64 {64 _,_ 128}
Learning Rate 10 [âˆ’][4] {10 [âˆ’][4] _,_ 3 Â· 10 [âˆ’][4] }
Weight Decay {10 [âˆ’][5] _,_ 10 [âˆ’][4] _,_ 10 [âˆ’][3] } {10 [âˆ’][5] _,_ 10 [âˆ’][4] _,_ 10 [âˆ’][3] }
Max Gradient Norm {0 _._ 5 _,_ 1 _._ 0} {0 _._ 5 _,_ 1 _._ 0}
Training Burn-in Steps 50 5
SoftMin Temp ( _ğœ_ ) 0.2    SoftMin Scalar ( _ğœ†_ ) 0.1    

_Training Confg_
Sequence Length 84 days 84 days
Burn-in Steps 21 days 21 days
Iterations 1000 200
Early Stopping Patience 50 25
Top _ğ‘_ Ensemble 25 25


35


