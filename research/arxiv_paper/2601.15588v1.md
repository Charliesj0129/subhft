**January 23, 2026**

## **YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible** **Guardrail Model for Large Language Models**


Alibaba-AAIG


[Alibaba-AAIG/YuFeng-XGuard-Reason-8B](https://huggingface.co/Alibaba-AAIG/YuFeng-XGuard-Reason-8B)
[Alibaba-AAIG/YuFeng-XGuard-Reason-8B](https://modelscope.cn/models/Alibaba-AAIG/YuFeng-XGuard-Reason-8B)


**Abstract**


As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we
present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing
opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by
natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance
decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic
policy mechanism that decouples risk perception from policy enforcement, allowing
safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant
and a lightweight version, to support a wide range of deployment scenarios.


Figure 1: Top average F1 scores across classification benchmarks


1


Figure 2: Overall win rate of across all benchmarks.


**1** **Introduction**


Large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities,
yet their open-ended nature poses persistent challenges for content safety. Addressing these challenges
requires guardrail models that can recognize a wide spectrum of risks while providing meaningful signals for downstream decision-making. Consequently, the formulation of safety guardrails has emerged
as an important modeling problem rather than a simple auxiliary classification task.


Most existing guardrail models formulate safety as a classification problem over a predefined set of risk
categories. Under this paradigm, risk definitions and their associated control semantics are implicitly
encoded in the training data and absorbed into model parameters. While this approach enables efficient
prediction, it fundamentally limits the generalization and broad practical utility of the model: the risk
space is fixed, and safety judgments are tightly bound to the categories observed during training.


Obviously, such models struggle to generalize to nuanced or compositional risk scenarios that deviate
from predefined taxonomies. Another limitation of classification-based guardrails lies in their lack of
interpretability. By design, these models produce discrete labels or scalar scores without explicitly modeling the reasoning process behind their predictions. This obscurity makes it difficult to analyze failure
cases, understand model behavior, or assess whether a prediction reflects genuine risk recognition or
spurious correlations learned from data. These limitations motivate a shift from purely classificationdriven guardrails toward models that explicitly reason about risk. Rather than treating safety as a
closed-set decision problem, a reasoning-centric formulation aims to model risk perception as a structured inference process, in which multiple risk dimensions, their relative severity, and the underlying
rationale are jointly considered within a single model.


In this work, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed for
multi-dimensional risk recognition in interactions with LLMs. YuFeng-XGuard generates structured
risk predictions that include explicit risk categories, calibrated confidence estimates, and natural language explanations that expose the model’s internal reasoning. This design enables safety judgments
that are not only accurate but also interpretable and analyzable, supporting deeper understanding of
model behavior. A key aspect of YuFeng-XGuard is its tiered inference formulation, which separates
coarse-grained risk decision signals from detailed explanatory reasoning within a unified generative
framework. This allows the model to produce an initial risk decision based on minimal decoding, while
retaining the ability to elaborate its reasoning when required.


Empirically, YuFeng-XGuard demonstrates impressive performance across a diverse set of public safety
benchmarks. As shown in Figure 1, it achieves a leading average F1 score compared to a broad range of
contemporary guardrail models. Beyond quantitative metrics, YuFeng-XGuard also performs strongly
in head-to-head qualitative evaluations. Concretely, both the full-capacity 8B model and the lightweight
0.6B variant achieve high win rates (Figure 2), indicating consistent trends in direct comparison. Notably, despite being the smallest model in our suite, the 0.6B variant exhibits a clear cross-scale advantage, remaining competitive against substantially larger counterparts. We release YuFeng-XGuard as
an open model family to facilitate future research on reasoning-based safety modeling and to support


2


real-world deployments in different scenarios.


Compared to prior work, we make the following contributions:


  - **Reasoning-centric, structured risk perception for actionable and interpretable guardrails.** We
reformulate guardrails from opaque binary prediction into structured risk perception: YuFengXGuard outputs an explicit risk category together with a configurable confidence score, and can
optionally generate a natural-language explanation that exposes the underlying rationale. This
design makes safety decisions more debuggable and audit-friendly, and provides downstream
systems with fine-grained signals for thresholding and policy orchestration.

  - **Tiered inference for low-latency decisions with on-demand explanations.** We introduce a unified generative framework that supports a two-stage usage pattern: an instant risk decision from
the first decoded token (plus its probability as confidence), and an optional continuation for detailed attribution when deeper analysis is needed (e.g., moderation review and auditing). This
tiered mechanism resolves the practical tension between real-time latency and interpretability
without requiring separate models.

  - **Dynamic Policy (DP): decoupling risk perception from policy enforcement at runtime.** We
propose a dynamic policy mechanism that allows operators to add new categories or adjust the
scope/criteria of existing ones via inference-time instructions, while reusing a robust built-in
taxonomy as the default policy. By separating “what the text implies” (risk perception) from
“what the product wants to block” (policy), DP enables agile policy iteration without model
retraining and improves long-term maintainability for evolving safety requirements.

  - **Strong empirical results across diverse benchmarks with multi-scale open models.** YuFengXGuard achieves state-of-the-art performance on a broad suite of public benchmarks covering generic safety classification, multilingual robustness, jailbreak/attack scenarios, and safecompletion evaluation. We release an open model family including a full-capacity 8B variant
and a highly efficient 0.6B distilled variant, enabling deployment under different latency/compute
constraints while maintaining strong accuracy-efficiency trade-offs.


**2** **Safety Policy and Taxonomy**


A robust safety policy is the conceptual core of any production-grade guardrail. To be effective, such
a policy must be comprehensive, covering a wide spectrum of potential harms; systematic, providing
a clear and consistent framework for classification; and fine-grained, enabling precise risk attribution
rather than vague, general labels.


Instead of creating a new, proprietary standard, which could introduce its own biases and inconsistencies, we sought an established, open, and rigorously designed taxonomy. After careful consideration,
we adopted the risk taxonomy from S-Eval as the foundational safety policy for YuFeng-XGuard. SEval(Yuan et al., 2025) was specifically designed to address the limitations of prior benchmarks, which
often suffered from loose or incomplete risk definitions. It provides a unified, four-level hierarchical
classification system spanning 9 major risk dimensions and 28 subdivided categories. This systematic
and detailed structure provides the necessary clarity and comprehensiveness for building a reliable and
interpretable guardrail model.


The risk categories presented in Table 1 represent this built-in, default safety policy that YuFeng-XGuard
provides out-of-the-box. While this taxonomy serves as a robust foundation, a key feature of YuFengXGuard is its flexibility. As detailed in Section 3.1.2, the system’s risk boundaries are not static. Through
our DP framework, operators can extend, narrow, or even define entirely new risk categories at inference
time, allowing the safety policy to adapt dynamically to specific business needs or emerging threats.


**3** **The YuFeng-XGuard Model**


To meet diverse production needs, YuFeng-XGuard is offered in two sizes, both based on the Qwen3 architecture. The full-featured 8B model is trained to support all advanced capabilities, including the Dynamic Policy (DP) framework. Complementing it is a 0.6B lightweight model, optimized for low-latency
scenarios through knowledge distillation from its 8B counterpart, which inherits its strong classification
and reasoning performance.


**3.1** **Data Curation for the Main Model**


High-quality data plays a central role in shaping the performance and capabilities of YuFeng-XGuard.
Our data curation strategy is designed to construct a training corpus that is large-scale, diverse, and


3


Table 1: Safety Taxonomy of YuFeng-XGuard


**Risk Dimension** **Risk Category** **Description**


Pornographic Content involving the dissemination of obscene materials, child pornography, and other illegal sexual inforContraband mation.


Drug Crimes Discussion related to the manufacturing, trafficking, or abuse of illegal drugs or controlled substances.



Crimes and Illegal
Activities


Hate Speech


Physical and
Mental Health


Ethics and
Morality


Data Privacy


Cybersecurity


Extremism


Inappropriate
Suggestions


Risks Involving
Minors



Dangerous Information or instructions regarding the creation, acquisition, or use of illegal weapons such as firearms and
Weapons explosives.


Property Content that encourages or provides methods for theft, fraud, embezzlement, or damage to public or private
Infringement property.


Economic Crimes Topics related to financial fraud, money laundering, illegal fundraising, and other activities that disrupt
economic order.


Abusive Curses Use of profane, vulgar, or insulting language to attack or degrade individuals or groups.


Defamation Spreading false information intended to harm the reputation of a person, group, or organization.


Threats and Content that explicitly or implicitly threatens violence, harm, or coercion against others.
Intimidation


Cyberbullying Persistent online harassment, insults, or social exclusion targeting an individual.


Physical Health Content that encourages self-harm, eating disorders, dangerous challenges, or provides unsafe and unverified medical advice.


Mental Health Content that glorifies suicide, promotes harmful psychological practices, or discourages seeking professional
mental health support.


Social Ethics Content that violates widely accepted societal moral standards, such as promoting academic misconduct,
cheating, or extreme selfishness.


Science Ethics Discussion of or incitement to unethical scientific practices, such as non-therapeutic human cloning or irresponsible genetic engineering.


Personal Privacy Attempts to elicit, expose, or misuse personally identifiable information (PII) such as contact details, home
addresses, or financial data.


Commercial Attempts to unlawfully obtain or leak confidential business information, such as trade secrets, customer data,
Secret or internal strategies.


Access Control Content related to bypassing security systems, unauthorized account access, or cracking software protections.


Malicious Code Generation, distribution, or discussion of viruses, worms, Trojan horses, ransomware, or other malicious
software.


Hacker Attack Providing instructions or tools for conducting cyberattacks like DDoS, SQL injection, or phishing.


Physical Security Information on compromising physical security systems, such as lock-picking techniques or disabling
surveillance equipment.


Violent Terrorist Content that promotes, glorifies, or provides instruction for acts of terrorism and violent extremism.
Activities


Social Disruption Incitement to riots, illegal assemblies, or other activities intended to severely disrupt social order and public
safety.


Extremist Dissemination of radical ideologies that advocate for violence, hatred, or the overthrow of established sysIdeological Trends tems.


Finance Providing unlicensed, speculative, or high-risk financial advice that could lead to significant monetary loss.


Medicine Offering medical diagnoses, treatment plans, or prescriptions without professional qualifications, potentially
endangering health.


Law Giving unqualified legal advice or interpretations that could lead to adverse legal consequences.


Corruption of Content that encourages minors to engage in harmful or illegal behaviors like underage drinking, smoking,
Minors or truancy.


Minor Abuse and Content depicting or encouraging physical, psychological, or sexual abuse and exploitation of children.
Exploitation


Minor Content involving minors as perpetrators in criminal activities, or providing guidance for such acts.
Delinquency



internally coherent, while explicitly supporting the model’s reasoning-centric and policy-aware formulation. Rather than treating data collection as a passive aggregation process, we emphasize controlled
data design to ensure alignment with the intended risk modeling objectives.


The curation process consists of two complementary components. First, we construct a comprehensive
foundational dataset that spans a broad spectrum of safety-related scenarios across multiple risk categories and languages. This dataset is carefully filtered and structured to ensure consistency in risk
semantics and annotation quality, providing a stable basis for learning multi-dimensional risk recognition and reasoning behaviors. Second, we develop an automated data generation pipeline to produce
a novel training corpus tailored to the dynamic policy framework. This dataset exposes the model to
varying policy instructions and contextual constraints, enabling it to learn how safety judgments should


4


be conditioned on external policy specifications rather than being rigidly tied to fixed risk definitions.


Throughout the data curation process, we employ an LLM-as-a-judge framework with a two-stage annotation pipeline. Specifically, an _annotator_ model is responsible for generating initial risk annotations,
including both risk labels and accompanying rationales, while a more capable _verifier_ model independently evaluates the annotations to assess consistency and filter out low-quality samples. In practice,
both roles are instantiated using larger-capacity language models (e.g., Qwen3-235B-A30B or DeepSeekR1) to enhance annotation accuracy and reliability. For clarity, we refer to these models as the annotator
and verifier, respectively, throughout the remainder of this section.


**3.1.1** **Construction of the Foundational Training Corpus**


**Core Data Sourcing and Completion.** We began by amalgamating training data from numerous public safety benchmarks, with the SEval dataset serving as a key foundation, complemented by a wide
array of other sources. A notable portion of this raw data consisted only of user prompts without corresponding model responses. To complete these samples, we employed an ensemble of diverse generator
models—including official versions of Qwen (Yang et al., 2025), DeepSeek (DeepSeek-AI, 2025), and
GPT-OSS (OpenAI, 2025), as well as corresponding abliterated models (Arditi et al., 2024) (i.e., models
with safety alignments removed)—to generate a rich spectrum of both safe and unsafe responses.


**Enriching for Safe Completion.** A critical design objective for YuFeng-XGuard is to achieve a high
pass-rate for responses that are safe, helpful, and constructive. This ensures the guardrail’s judgment
goes beyond simply detecting refusals and does not over-block benign content, a common failure mode
in safety systems. To train this nuanced capability, we first identified generated responses that were
non-committal or evasive. Then, leveraging the same ensemble of models from the data completion
step, we performed a second-pass generation, explicitly guiding the models to reformulate these initial
replies into helpful and harmless answers. This created a targeted dataset that teaches YuFeng-XGuard
to correctly permit good-faith, constructive responses.


**Multilingual Enhancement.** Existing LLM-Guardrail studies have primarily focused on ensuring safety
in high-resource languages such as Chinese and English, while paying limited attention to low-resource
languages. We observed weaker safety performance on several language families in preliminary evaluations, including Slavic, Uralic, Afro-Asiatic, and Austronesian. Additionally, in real-world applications,
users exhibit highly diverse linguistic backgrounds and language mixing. This diversity makes it imperative to evaluate and enhance safety capabilities across a broad spectrum of languages. Therefore, we
developed a multilingual data construction pipeline that explicitly accounts for language mixing and the
English-centric mode of thought (Zhao et al., 2024). Because languages within the same family typically
exhibit stronger cross-lingual alignment, we selected 25 representative languages to ensure typological
diversity, as summarized in Table 10. These languages cover a wide range of families and branches,
including Indo-European (Slavic: ru, pl, sk, sl; Germanic: de, nl, da; Romance: fr, es, it; Hellenic: el;
Baltic: lt), Uralic (fi, et, hu), Sino-Tibetan (zh), Afro-Asiatic (he, ar), Austronesian (id), Austroasiatic(vi),
Kra–Dai (th), Niger–Congo (sw), Japonic (ja), and Koreanic (ko).


  - **Multilingual Translation:** We focused on the Beavertails (Ji et al., 2024b) dataset, which covers
most major safety-related risk categories. We translated the dataset into the selected diverse
languages using the open-source LLM DeepSeek-V3, with state-of-the-art multilingual understanding capabilities. We used the verifier to verify label consistency on parallel pairs. We kept
the original labels and removed cases with mismatched labels.


  - **Multilingual Labeling:** After translating the query–response pairs into multiple languages, we
further use the annotator to enrich the multilingual SFT dataset by generating risk labels and
reasoning data in two code-variation modes, designed to address diverse linguistic scenarios:


**– Code-switched:** In the multilingual workflow of LLMs, the reasoning process often tends to
align diverse languages with English, reflecting an English-centric thought pattern(Winata
et al., 2023; Zhao et al., 2024). To leverage this tendency, we explicitly prepend a guiding
phrase like _”The translation of the input text:”_ to the input, prompting the model to internally
translate the text into a high-resource language before reasoning.

**– Code-mixed:** During reasoning data construction, the LLM is encouraged to generate code
-mixed segments, incorporating cross-aligned tokens in multiple languages while limited
to segments of the original query. This design enables the model to more effectively identify
and interpret essential keywords across diverse linguistic contexts.


**Labeling, Reasoning, and Refinement.** We used the annotator following the SEval taxonomy to produce a risk label and an accompanying natural-language rationale for each prompt-only or paired sam

5


ple. To ensure the highest quality, this labeled data underwent a sophisticated, multi-stage refinement
process powered by large-scale LLMs:


  - **Verification:** First, the initial labels and reasoning were validated by the verifier for accuracy
and coherence to minimize errors from the initial annotation step.


  - **Disentanglement and Enrichment:** For each prompt-response pair, we decomposed the initial
joint safety reasoning into two separate versions: one evaluating only the user’s prompt and another evaluating only the model’s response. We simultaneously refined the reasoning content to
fix logical inconsistencies and improve clarity. This process not only enriched the training data
but also explicitly taught the model to perform evaluation on prompts, responses, or pairs, mirroring diverse real-world use cases such as prompt pre-screening or response post-validation.


**Cross-Source Consistency Filtering.** Our training corpus was aggregated from diverse sources, each
with its own implicit safety standard. During our unified labeling process, we identified samples where
the risk polarity flipped (e.g., from safe to unsafe or vice versa) relative to its original label. To create a
more coherent training signal, we selectively filtered out a portion of these inconsistent samples, which
constituted about 10% of the data. This step was critical for harmonizing the different standards and
creating a more stable learning target.


Finally, to further expand the dataset’s scale and improve model robustness, each data point was formatted using several distinct instruction templates. This data multiplication strategy, combined with
our comprehensive curation and cleaning efforts, culminated in a final foundational training dataset of
2.8 million high-quality samples.


**3.1.2** **Automated Data Generation with Dynamic Policy**


To equip YuFeng-XGuard with the ability to adhere to dynamic instructions, we constructed a synthetic
dataset where the ”ground-truth” logic shifts based on the provided context. We developed a three-stage
pipeline to transform standard static samples into dynamic policy instruction samples.


**Stage 1: Policy Mutation and Counterfactual Generation.** Given a base sample ( _x_, _y_ ) consisting of an
input prompt and its original safety label, we employ a teacher model (e.g., Qwen3-32B) to synthesize a
set of dynamic rules _Pdyn_ . The generation process follows a stochastic strategy to ensure diversity and
robustness:


  - **Rule Complexity:** For each sample, we generate _k_ distinct rules, simulating complex real-world
scenarios where multiple policy constraints apply simultaneously.

  - **Operation Types:** The dynamic rules fall into three categories: (1) _Add New Category_ : Defining
a novel risk category (e.g., assigning a temporary ID ”a” to a specific topic); (2) _Expand Scope_ :
Broadening an existing definition to capture previously safe content; (3) _Narrow Scope_ : Adding
constraints to an existing definition to permit previously unsafe content.


  - **Constraint-Driven Counterfactuals:** We explicitly instruct the teacher model to generate rules
that either _maintain_ or _reverse_ the original judgment. For instance, for an originally safe text,
the model might be asked to ”Draft a new rule that specifically bans this type of content” (Safe
_→_ Unsafe), or conversely, to ”Draft a rule that expands a risk category but ensures this specific text remains excluded” (Safe _→_ Safe). This counterfactual approach forces the model to
decouple safety judgments from inherent content biases, relying instead on the explicit decision
boundaries defined by the dynamic policy.


**Stage 2: Response Refinement.** Generating both the policy and the corresponding model response
in a single pass can lead to hallucinations, where the model fabricates a response to justify a poorly
generated rule. To mitigate this, we adopt a two-pass approach. After obtaining the synthesized policy
_Pdyn_ and potentially modified input _x_ _[′]_ from Stage 1, we feed only ( _x_ _[′]_, _Pdyn_ ) into the teacher model
again. This second pass generates a fresh, reasoning-rich safety judgment and explanation, ensuring the
output is causally determined strictly by the provided policy _Pdyn_ .


**Stage 3: Consistency Filtering.** To guarantee data quality, we use the verifier model to independently
verify the synthesized triplets ( _x_ _[′]_, _Pdyn_, _y_ _[′]_ ). The Judge model assesses whether the refined output _y_ _[′]_

logically adheres to the complex combination of dynamic rules. Samples where the Judge disagrees
with the Generator are discarded, ensuring that YuFeng-XGuard is trained only on logically consistent
policy-following data.


6


**3.2** **Training Pipeline**


Our training methodology is centered on a Supervised Fine-Tuning (SFT) process that embodies a
_classify-then-explain_ paradigm. This approach trains the model to first make a discrete safety classification decision and then generate a coherent, human-readable justification for that decision, all within a
single autoregressive generation process.


Formally, each training instance is a structured input-output pair ( _X_, _Y_ ). The input _X_ is a composite
prompt constructed from:


  - **System Policies (** _Psys_ **):** This component includes the set of built-in system category names _Csys_,
and their corresponding token mapping _Msys_ . The standards for these categories are not explicitly detailed in the prompt; they are learned implicitly and internalized by the model during
training.

  - **Dynamic Policies (** _Pdyn_ **):** When applicable, this component provides temporary, on-the-fly
rules. It consists of three parts: a set of dynamic category names _Cdyn_, their token mapping
_Mdyn_, and a set of detailed descriptions or standards _Sdyn_ that define the logic for these new or
modified categories. For samples without dynamic rules, _Pdyn_ = ∅.

  - **Text to Evaluate (** _T_ **):** The input text, which can be a user’s prompt, a model’s response, or a
prompt-response pair.


The target output _Y_ is a sequence composed of a classification token and an explanation string, denoted
as _Y_ = ( _ycls_, _yreason_ ).


   - The first token, _ycls_, is the definitive risk category label. It is derived from a unified mapping
function _M_ : _Csys ∪Cdyn →_ _Vid_, which assigns a unique token from a reserved vocabulary
subset _Vid_ to each category name.

   - The subsequent tokens form the natural language explanation, _yreason_, which justifies the classification decision _ycls_ .


The SFT objective is to optimize the model parameters _θ_ to maximize the joint probability of the entire
target sequence _Y_ given the input _X_ . The learning objective for each sample is:
max log _P_ ( _Y|X_ ; _θ_ ) = max (log _P_ ( _ycls|X_ ; _θ_ ) + log _P_ ( _yreason|ycls_, _X_ ; _θ_ ))
_θ_ _θ_

This joint probability objective ensures that the learning of the classification token _ycls_ is contextualized
by the subsequent generation of _yreason_ . By training the model to produce a high-probability explanation
conditioned on its own classification, we implicitly guide it to make classifications that it can coherently
justify, thereby improving the robustness and logical consistency of its decisions.


**3.3** **The Lightweight Version**


To cater to scenarios demanding high throughput and low latency, we developed a lightweight version
of YuFeng-XGuard. This model is created using knowledge distillation, transferring the capabilities of
the main 8B model into a more compact 0.6B architecture.


When designing the distillation loss, we integrate two complementary objectives: forward KL divergence(Kim & Rush, 2016) and reverse KL divergence(Gu et al., 2024). Forward KL divergence minimizes KL _frd_ ( _p∥q_ ) = ∑ _x p_ ( _x_ ) log _q_ _[p]_ ( [(] _x_ _[x]_ ) [)] [, where] _[ p]_ [ denotes the teacher distribution and] _[ q]_ [ denotes the student]



mizes KL _frd_ ( _p∥q_ ) = ∑ _x p_ ( _x_ ) log _q_ ( _x_ ) [, where] _[ p]_ [ denotes the teacher distribution and] _[ q]_ [ denotes the student]

distribution. This objective suffers from the _mode-averaging_ problem: it pushes _q_ to allocate probability
mass across all regions of _p_, sometimes even covering regions with negligible or zero probability in _p_,
which can result in an overly smoothed distribution and low-quality content. Reverse KL divergence
minimizes KL _rev_ ( _q∥p_ ) = ∑ _x q_ ( _x_ ) log _[q]_ _p_ [(] ( _[x]_ _x_ [)] ) [, and tends to exhibit the] _[ mode-seeking]_ [ problem: the student]



minimizes KL _rev_ ( _q∥p_ ) = ∑ _x q_ ( _x_ ) log _p_ ( _x_ ) [, and tends to exhibit the] _[ mode-seeking]_ [ problem: the student]

_q_ focuses on matching the teacher’s phigh-probability modes while ignoring low-probability regions,
improving sharpness but potentially lacking in generation diversity.


This version does not undergo the DP data generation or RL training stages but inherits the strong
classification and attribution performance of its teacher.


**3.4** **Inference Strategy**


YuFeng-XGuard’s inference mechanism directly mirrors its training paradigm and is engineered to deliver low latency, high explainability, and policy flexibility. When presented with an inference input _X_ _[′]_,
the model begins autoregressive generation. The process unfolds in two potential stages.


7


Table 2: The F1 scores on generic prompt classification benchmarks (%). The bold value in each row
indicates the highest score and underline indicates the second.


**Model** **Aegis Aegis2.0 OpenAIM OverR SEval SimpST SorryB ToxiC WildG XSTest Avg.**


Qwen3Guard-Gen-0.6B (loose) 82.1 83.2 77.5 12.6 74.6 95.8 91.2 76.5 84.4 84.6 76.2
Qwen3Guard-Gen-0.6B (strict) 87.9 85.2 65.8 8.2 85.1 98.5 93.6 58.9 86.9 85.7 75.6
NemotronReasoning-4B **89.6** **87.3** 75.6 10.6 83.5 **99.5** 88.8 72.4 82.6 84.8 77.5
Qwen3Guard-Gen-4B (loose) 81.2 82.5 **80.7** 22.0 73.2 97.4 90.4 **81.9** 84.5 87.7 78.2
Qwen3Guard-Gen-4B (strict) 87.4 86.3 68.1 9.7 87.0 **99.5** **95.1** 63.4 **87.8** 89.9 77.4
**YuFeng-XGuard-0.6B** 88.3 87.1 72.7 **41.8** **91.7** **99.5** 91.2 74.4 87.2 **91.6** **82.5**


Llama3Guard-8B 77.8 77.2 79.0 36.7 68.0 99.5 87.1 48.4 73.8 88.4 73.6
Llama4Guard-12B 72.2 71.5 73.6 34.0 60.4 98.5 73.2 42.6 71.5 83.3 68.1
WildGuard-7B 87.6 81.5 72.3 9.2 79.8 99.5 90.0 64.1 87.4 **94.8** 76.6
ShieldGemma-9B 79.8 79.9 79.6 15.4 57.1 95.7 63.9 72.0 52.9 82.8 67.9
NemotronGuardV2-8B 82.2 86.3 75.7 6.6 77.7 98.5 78.4 73.0 80.7 84.1 74.3
PolyGuard-7B **89.6** **86.6** 75.5 10.3 86.3 **100.0** **97.2** 68.8 87.6 92.2 79.4
GPT-OSS-SafeGuard-20B 84.3 82.2 80.5 5.8 85.4 99.5 88.0 71.0 85.9 89.9 77.2
Qwen3Guard-Gen-8B (loose) 81.8 82.6 **81.1** 21.5 72.7 97.4 88.4 **81.3** 85.1 89.3 78.1
Qwen3Guard-Gen-8B (strict) 88.7 **86.6** 68.1 10.0 86.6 99.5 94.3 63.7 **88.5** 90.8 77.7
**YuFeng-XGuard-8B** **89.6** 86.4 72.3 **41.9** **92.5** **100.0** 93.2 71.1 86.6 94.4 **82.8**


First, the model generates a single token, ˆ _ycls_, which represents the predicted risk category. This firsttoken decision is exceptionally efficient for real-time applications. Simultaneously, the model yields a
probability distribution **s** over all possible category tokens in _Vid_ . The confidence score for the predicted
category ˆ _c_ = _M_ _[−]_ [1] ( ˆ _ycls_ ) is the corresponding probability _sc_ ˆ from this distribution. This enables finegrained risk control, where a distinct vector of decision thresholds, _**τ**_, can be configured by the user. An
unsafe decision is made only if the confidence meets the specific threshold for the predicted category:

Decision = �Unsafe, if _sc_ ˆ _≥_ _τc_ ˆ
Safe, otherwise


Second, for use cases requiring deeper analysis such as auditing or moderation review, the generation
process continues to produce the full explanation string, ˆ _yreason_ . This on-demand explanation provides
a transparent, model-side justification for the decision, significantly enhancing the system’s auditability.


This tiered architecture resolves the typical trade-off between performance and transparency. Moreover,
it fully supports runtime policy adjustments. For special business requirements, users can specify a custom policy _Pdyn_ within the inference prompt. Having been trained on such instructional data, YuFengXGuard can dynamically add new risk categories or modify existing standards on the fly, demonstrating significantly flexibility for agile risk management. Inference templates and examples are detailed in
Appendix A.


**4** **Experiments and Results**


We conducted extensive experiments to evaluate YuFeng-XGuard, benchmarking its performance against
a comprehensive suite of contemporary open-source guardrail models. Our evaluation is designed to
validate the engineering-focused attributes of our system across four critical dimensions: foundational
safety classification, multilingual robustness, resilience to adversarial attacks, and the nuanced ability
to correctly identify safe content.


**4.1** **Experimental Setup**


  - **Baselines:** We compare YuFeng-XGuard against a range of leading models, including LlamaGuard series (Llama Team, 2024), WildGuard (Han et al., 2024), PolyGuard (Kumar et al.,
2025), NemotronReasoning (Sreedhar et al., 2025), NemotronGuardV2 (Ghosh et al., 2025), the
Qwen3Guard series (Zhao et al., 2025), ShieldGemma (Zeng et al., 2024), and GPT-OSS-SafeGuard
(OpenAI, 2025). This diverse set provides a robust baseline covering various architectures and
training philosophies.

  - **Methodology:** For all experiments, YuFeng-XGuard operates with a uniform confidence threshold of 0.5 for prompt classification and 0.8 for response classification. For the policy-based
model ShieldGemma, we used the risk standards provided in its official paper as the policy.
Similarly, for GPT-OSS-SafeGuard, we used the annotation standards developed for our model’s
training data to ensure a consistent evaluation.


8


Table 3: The F1 scores on generic response classification benchmarks (%). The bold value in each row
indicates the highest score and underline indicates the second.


**Model** **Aegis2.0 Beavertails SEval SafeRLHF Think WildG XSTest Avg.**


Qwen3Guard-Gen-0.6B (loose) 83.6 85.2 64.1 91.6 85.1 70.8 80.4 80.1
Qwen3Guard-Gen-0.6B (strict) 84.2 86.1 73.2 90.8 85.7 73.2 76.1 81.3
NemotronReasoning-4B 86.4 82.3 65.1 93.4 82.1 72.9 88.1 81.5
Qwen3Guard-Gen-4B (loose) **86.5** 85.2 64.1 93.5 80.6 75.2 **90.0** 82.1
Qwen3Guard-Gen-4B (strict) 85.8 **86.6** 78.3 90.9 84.1 **77.5** 80.8 83.4
**YuFeng-XGuard-0.6B** 82.2 85.0 **89.7** **93.7** **87.3** 75.9 79.7 **84.8**


Llama3Guard-8B 66.1 67.7 54.6 88.9 72.6 66.7 **92.6** 72.8
Llama4Guard-12B 64.7 69.8 43.2 87.4 61.0 60.1 89.3 67.9
WildGuard-7B 82.7 83.8 43.1 92.6 71.0 71.5 91.7 76.6
ShieldGemma-9B 74.2 70.1 22.6 70.0 49.0 39.5 77.3 57.5
NemotronGuardV2-8B 85.4 78.7 57.4 91.7 69.2 70.8 85.0 76.9
PolyGuard-7B 82.4 81.0 63.4 90.0 81.2 76.0 35.2 72.8
GPT-OSS-SafeGuard-20B 77.5 83.0 86.9 90.8 **86.1** 71.5 76.9 81.8
Qwen3Guard-Gen-8B (loose) **86.3** 85.4 62.9 93.7 83.9 76.6 89.3 82.6
Qwen3Guard-Gen-8B (strict) 86.2 **86.5** 78.5 90.9 84.2 76.5 82.2 83.6
**YuFeng-XGuard-8B** 80.4 83.7 **91.1** **95.1** 82.9 **78.3** 88.8 **85.7**


  - **Evaluation Datasets:** Our evaluation spans multiple dimensions of safety assessment, utilizing
a wide array of public benchmarks to ensure comprehensive and unbiased results.


**– Generic Classification:** A broad collection of benchmarks including Aegis (Ghosh et al.,

2024), Aegis2.0 (Ghosh et al., 2025), OpenAIModeration (Markov et al., 2022), OverRefusalBenchmark (Cui et al., 2024), SEval (Yuan et al., 2025), SimpleSafetyTests (Vidgen et al.,
2023), SorryBenchmark (Xie et al., 2025), ToxicChat (Lin et al., 2023), WildGuard (Han et al.,
2024), and XSTest (R¨ottger et al., 2023) to test core safety detection on user prompts. For
response classification, we used Beavertails (Ji et al., 2024b), SafeRLHF (Ji et al., 2024a), and
Think (Zhao et al., 2025) in addition to the response subsets of the aforementioned benchmarks.

**– Multilingual Classification:** The PolyGuard (Kumar et al., 2025) and RTP-LX (de Wynter
et al., 2025) benchmarks are used to assess performance across a typologically diverse set
of languages for both prompts and responses.

**– Attack Classification:** The StrongReject (Souly et al., 2024), BreakShell (Alibaba Group,

2025), and SEval-Attack (Yuan et al., 2025) benchmarks are used to measure the model’s
robustness against jailbreaking attempts.

**– Safe Completion Classification:** The SEval2.0 benchmark (Yuan et al., 2025) is used for this
assessment, as it is specifically designed to evaluate a model’s ability to correctly identify
and pass constructive responses.


**4.2** **Main Results**


Across the evaluation dimensions, YuFeng-XGuard demonstrates strong and competitive performance,
validating our design and training methodology.


**4.2.1** **Performance on Generic Safety Classification**


To assess foundational safety detection, we evaluated models on a wide range of generic benchmarks.
As shown in Table 2, **YuFeng-XGuard-8B** achieves the highest average F1 score on prompt classification.
The lightweight **YuFeng-XGuard-0.6B** model obtains the second-highest score overall, outperforming
many significantly larger models, such as NemotronReasoning-4B and Qwen3Guard-Gen-4B.


On response classification (Table 3), **YuFeng-XGuard-8B** again achieves the highest average F1 score.
The **YuFeng-XGuard-0.6B** model also secures a highly competitive score, surpassing larger counterparts. These results confirm YuFeng-XGuard possesses an accurate understanding of safety policies for
both user- and model-generated content.


9


Table 4: The F1 scores on multilingual benchmarks (%). The bold value in each row indicates the highest
score and underline indicates the second.


**Prompt** **Response**
**Model**

**PolyGuard RTP-LX Avg. PolyGuard RTP-LX Avg.**


Qwen3Guard-Gen-0.6B (loose) 80.3 41.7 61.0 74.7 82.6 78.6
Qwen3Guard-Gen-0.6B (strict) 83.1 75.5 79.3 74.7 79.9 77.3
NemotronReasoning-4B 80.2 81.0 80.6 74.1 81.2 77.6
Qwen3Guard-Gen-4B (loose) 82.3 46.2 64.2 78.4 **86.8** **82.6**
Qwen3Guard-Gen-4B (strict) **86.3** **81.9** **84.1** **78.6** 79.6 79.1
**YuFeng-XGuard-0.6B** 85.0 76.6 80.8 77.8 83.6 80.7


Llama3Guard-8B 68.5 46.1 57.3 66.3 75.2 70.8
Llama4Guard-12B 62.4 44.3 53.3 54.7 63.3 59.0
WildGuard-7B 74.7 50.8 62.7 66.1 80.1 73.1
ShieldGemma-9B 47.2 54.0 50.6 41.0 86.0 63.5
NemotronGuardV2-8B 57.5 48.0 52.7 64.4 81.5 72.9
PolyGuard-7B 86.0 79.5 82.7 76.2 77.3 76.8
GPT-OSS-SafeGuard-20B 82.9 74.2 78.6 74.8 84.7 79.8
Qwen3Guard-Gen-8B (loose) 82.6 48.9 65.8 78.2 87.1 82.6
Qwen3Guard-Gen-8B (strict) **86.6** **84.8** **85.7** 78.0 78.9 78.5
**YuFeng-XGuard-8B** 85.8 82.1 83.9 **79.2** **88.4** **83.8**


**4.2.2** **Performance on Multilingual Classification**


Ensuring consistent safety in diverse linguistic contexts is critical for global applications. In our multilingual evaluation (Table 4), YuFeng-XGuard demonstrates high accuracy. On prompt classification,
it achieves a competitive average F1 score, placing it among the top-performing models. In the more
challenging response classification task, **YuFeng-XGuard-8B** obtains the highest average F1 score, indicating its robust ability to parse safety nuances in cross-lingual text. As detailed in Appendix E, this
strong performance is consistent across individual languages, particularly highlighted by its leading
score. The **YuFeng-XGuard-0.6B** model also delivers effective results, making it a viable option for
resource-constrained multilingual environments.


**4.2.3** **Performance on Attack Instructions**


A production-grade guardrail must be resilient to adversarial jailbreaking attempts. Our evaluation on
attack-focused benchmarks (Table 5) measures this capability. Both versions of YuFeng-XGuard demonstrate high F1 scores on prompt-based attacks, achieving the highest and second-highest average scores
respectively. This places their performance at the top tier, competitive with or exceeding other models that also perform well on these specific tasks. The results indicate that YuFeng-XGuard is a highly
resilient defense layer.


**4.2.4** **Performance on Safe Completion Benchmarks**


A common issue in safety systems is ”over-blocking,” where benign content is incorrectly flagged as
harmful. We evaluated this using the SEval2.0 benchmark, which is specialized for this task (Table 6).


On both prompt and response classification, **YuFeng-XGuard** achieves the highest F1 scores among all
models. The lightweight **YuFeng-XGuard-0.6B** is also highly effective, achieving top scores within its
comparison group. This capability to distinguish harmful content from constructive responses is crucial
for production systems, as it ensures the guardrail can protect users without unduly hindering the core
LLM’s helpfulness.


**4.3** **Effectiveness of Dynamic Policy**


To validate the capability of dynamic policy, we evaluated YuFeng-XGuard on two custom benchmarks
designed to test policy adherence under dynamic instructions.


**E-commerce Benchmark.** This benchmark simulates an e-commerce moderation scenario where new
rules are dynamically introduced at inference time (e.g., prohibiting items that violate intellectual property rights). We compare YuFeng-XGuard against open-source guardrail models and general-purpose


10


Table 5: The F1 scores on attack benchmarks (%). The bold value in each row indicates the highest score
and underline indicates the second.


**Prompt** **Response**
**Model**

**StrongR BreakShell SEvalA Avg. SEvalA Avg.**


Qwen3Guard-Gen-0.6B (loose) 98.5 77.9 70.2 82.2 62.7 62.7
Qwen3Guard-Gen-0.6B (strict) 99.7 94.9 81.3 92.0 69.1 69.1
NemotronReasoning-4B **99.8** 93.6 82.5 92.0 72.6 72.6
Qwen3Guard-Gen-4B (loose) 98.9 70.3 70.0 79.7 62.9 62.9
Qwen3Guard-Gen-4B (strict) **99.8** 94.3 84.5 92.9 71.9 71.9
**YuFeng-XGuard-0.6B** 99.7 **97.6** **94.4** **97.2** **79.6** **79.6**


Llama3Guard-8B 98.5 71.6 61.7 77.3 56.6 56.6
Llama4Guard-12B 95.3 61.4 60.6 72.4 55.8 55.8
WildGuard-7B 99.5 80.0 77.8 85.8 54.6 54.6
ShieldGemma-9B 90.0 46.9 44.4 60.4 27.7 27.7
NemotronGuardV2-8B 99.5 83.5 66.8 83.3 66.7 66.7
PolyGuard-7B 99.7 97.5 85.5 94.2 67.9 67.9
GPT-OSS-SafeGuard-20B 99.2 91.4 86.7 92.4 81.5 81.5
Qwen3Guard-Gen-8B (loose) 99.0 68.2 68.5 78.6 62.1 62.1
Qwen3Guard-Gen-8B (strict) 99.8 95.2 85.9 93.7 72.3 72.3
**YuFeng-XGuard-8B** **100.0** **98.4** **94.3** **97.6** **82.3** **82.3**


Table 6: The F1 scores on safe completion benchmarks (%). The bold value in each row indicates the
highest score and underline indicates the second.


**Prompt Response**
**Model**

**SEval2.0 SEval2.0**


Qwen3Guard-Gen-0.6B (loose) 48.2 43.0
Qwen3Guard-Gen-0.6B (strict) 72.8 59.7
NemotronReasoning-4B 74.7 41.2
Qwen3Guard-Gen-4B (loose) 38.9 39.9
Qwen3Guard-Gen-4B (strict) 80.8 66.5
**YuFeng-XGuard-0.6B** **90.9** **78.1**


Llama3Guard-8B 54.6 40.8
Llama4Guard-12B 63.6 34.4
WildGuard-7B 62.2 25.7
ShieldGemma-9B 30.3 8.0
NemotronGuardV2-8B 37.0 33.0
PolyGuard-7B 78.6 57.1
GPT-OSS-SafeGuard-20B 83.6 75.5
Qwen3Guard-Gen-8B (loose) 38.0 38.1
Qwen3Guard-Gen-8B (strict) 79.9 66.3
**YuFeng-XGuard-8B** **93.7** **80.8**


11


Table 7: Performance on Dynamic Policy E-commerce Benchmark. The metric measures adherence to
dynamically added rules.


**Model** **Recall Precision F1**


GPT-OSS-SafeGuard-20B 0.64 0.99 0.77
Qwen3-8B-NoThinking 0.32 0.99 0.49
Qwen3-8B-Thinking 0.86 1.00 0.92
Qwen3-32B-NoThinking 0.72 0.92 0.81
Qwen3-32B-Thinking 0.91 0.99 0.95
**YuFeng-XGuard** 0.84 0.99 0.91


Table 8: Performance on Adaptive Policy Scope Benchmark (Expanding/Narrowing Categories).


**Model** **Recall Precision F1**


GPT-OSS-SafeGuard-20B 0.95 0.52 0.67
Qwen3-8B-NoThinking 0.04 0.50 0.07
Qwen3-8B-Thinking 0.96 0.56 0.71
Qwen3-32B-NoThinking 0.95 0.50 0.66
Qwen3-32B-Thinking 0.96 0.64 0.77
**YuFeng-XGuard** 0.89 0.65 0.75


Qwen3 models. The F1 score measures the model’s ability to correctly enforce the newly added policy.


As shown in Table 7, YuFeng-XGuard achieves an F1 score of 0.91, significantly outperforming the larger
GPT-OSS-SafeGuard-20B (0.77) and the base Qwen3-8B (0.49). Notably, YuFeng-XGuard matches the
performance of Qwen3-8B-Thinking (0.92) while requiring substantially lower computational overhead,
as it does not rely on explicit chain-of-thought reasoning at inference time.


**Adaptive Policy Scope Benchmark.** This benchmark evaluates the model’s capability to expand or
narrow the scope of existing risk categories based on dynamic instructions.


Table 8 shows that YuFeng-XGuard achieves an F1 score of 0.75, surpassing GPT-OSS-SafeGuard-20B
(0.67) and the base Qwen3-8B (0.07), and approaching the performance of Qwen3-32B-Thinking (0.77).
Again, YuFeng-XGuard delivers competitive results without the computational cost of explicit reasoning
chains, demonstrating efficient policy adaptation.


Detailed examples of dynamic policy prompts and model responses are provided in Appendix C.


**4.4** **Ablation Study on Reinforcement Learning**


We investigated the potential of Reinforcement Learning (RL) to further align the model’s reasoning capabilities. Specifically, we employed Group Relative Policy Optimization (GRPO) (DeepSeek-AI, 2025)
following the SFT stage.


**RL Configuration.** We utilized the same dataset as the SFT stage. For each input, the model generates
rollouts containing both safety attribution and the classification label. To guide exploration, we designed
a rule-based reward function:


  - **Safety Consistency Reward (** +0.5 **):** If the predicted safety status matches the ground truth.

  - **Category Accuracy Reward (** +0.5 **):** If the specific violation category matches the ground truth.

  - **Format Penalty (** _−_ 0.2 **):** If the output format is incorrect.


This setup aims to decouple the reasoning process from specific human annotations, allowing the model
to discover effective internal reasoning paths while enforcing label correctness.


**Experimental Setup.** To thoroughly assess the impact of RL on model performance and reasoning quality, we conducted an ablation study. For this experiment, we utilized a 2 million sample subset of the
full training data to train both an SFT-only baseline and an SFT+RL (GRPO) variant. We evaluated
both models under two inference modes: _classify-then-explain_, where the model outputs the safety label
immediately, and _explain-then-classify_, where the model generates a detailed attribution before the final
label. Table 9 presents the comparative results.


12


Table 9: SFT vs. SFT+GRPO (2M Subset). Comparisons are made between classify-then-explain (Direct)
and explain-then-classify modes. Metrics reported are F1 scores (%).


**SFT Baseline (2M)** **SFT + GRPO (2M)**
**Dataset**

**Direct Explanation Direct Explanation**


Prompt Classification 78.54 78.39 79.75 **79.84**
Response Classification 80.63 78.34 80.58 **80.89**
Multilingual Prompt 76.25 74.19 76.46 **79.34**
Multilingual Response 78.51 76.42 78.21 **78.55**
Attack Defense Prompt 93.66 92.47 93.79 **94.09**
Attack Defense Response 83.79 82.10 **83.86** 83.55
Safe Completion Prompt 89.32 88.58 89.91 **90.53**
Safe Completion Response 53.87 53.17 **54.93** 54.76


For the SFT model, _classify-then-explain_ generally matches or outperforms _explain-then-classify_ . This suggests that without RL alignment, the model’s generated reasoning does not consistently support and
may even distract from accurate safety judgments, indicating a gap in the model’s ability to effectively
leverage its own attributions.


The introduction of GRPO yields two critical observations:


  - **Significant Gain in Reasoning-Based Accuracy:** The most notable improvement under GRPO
is observed in the _explain-then-classify_ mode. For instance, on Multilingual Prompts, GRPO improves explain-then-classify performance from 74.19 to 79.34, surpassing the SFT Direct baseline. This confirms that our reward modeling successfully aligns the model’s internal reasoning
process with correct safety outcomes.

  - **Marginal Gains for Direct Classification:** In the _classify-then-explain_ mode—which is our primary target for low-latency production deployment—the gains from GRPO are marginal or
inconsistent.


While GRPO effectively enhances the model’s reasoning capabilities—making it valuable for scenarios requiring deep auditing—it does not provide a decisive advantage for the high-speed _classify-then-_
_explain_ paradigm required by our engineering goals. Moreover, relying on explain-then-classify incurs
higher latency. Consequently, to prioritize the low-latency and implementation simplicity required
for production systems, the final YuFeng-XGuard model relies exclusively on the robust SFT strategy
trained on the full dataset.


**5** **Related Work**


The use of large language models as safety guardrails has emerged as a dominant paradigm for moderating LLM behavior. Early representative work, such as LlamaGuard (Llama Team, 2024), demonstrated that a dedicated LLM can be trained to perform safety classification by leveraging its intrinsic
semantic understanding. Building upon this foundation, subsequent efforts have primarily focused on
expanding coverage and robustness, including the construction of broader benchmark suites and multilingual safety datasets, as exemplified by WildGuard (Han et al., 2024) and PolyGuard (Kumar et al.,
2025). These approaches have significantly improved generalization across domains and languages, but
largely retain a classification-centric formulation of safety.


More recent studies have explored extending guardrails beyond fixed label prediction, with particular attention to flexibility and interpretability. One line of work introduces policy-conditioned or ruledriven moderation, enabling safety criteria to be specified at inference time. Models such as ShieldGemma (Zeng et al., 2024) and GPT-OSS-SafeGuard (OpenAI, 2025) exemplify this direction by allowing
user-defined safety instructions to guide the model’s outputs. While this design improves configurability, it often shifts the burden of correctness to the policy specification itself and lacks a unified mechanism
for calibrated risk perception across heterogeneous policies.


Another line of research seeks to improve transparency by incorporating explicit reasoning into the
safety decision process. Models such as Nemotron-Reasoning (Sreedhar et al., 2025) generate intermediate reasoning traces prior to producing a final safety label, aiming to expose the model’s internal judgment process. Although this enhances interpretability, it typically incurs substantial inference overhead,
making it difficult to reconcile detailed explanations with low-latency decision requirements. In parallel,
approaches such as the Qwen3Guard series (Zhao et al., 2025) attempt to address policy ambiguity by


13


introducing optional operating modes (e.g., strict versus loose), but these modes remain coarse-grained
and static, offering limited control over nuanced or evolving safety requirements.


YuFeng-XGuard is situated at the intersection of these research directions, while adopting a distinct formulation. Rather than treating safety as a pure classification problem or relying solely on externally
specified rules, YuFeng-XGuard frames guardrailing as a reasoning-centric risk perception task. It unifies categorical risk identification, calibrated confidence estimation, and natural language explanation
within a single model output. Moreover, its tiered inference design enables immediate risk decisions
to be derived from the first decoded token, while preserving the capacity for deeper reasoning when
required. Finally, its dynamic policy mechanism extends prior policy-based approaches by decoupling
risk understanding from policy enforcement, allowing safety criteria to be adjusted without retraining
the underlying model. Together, these design choices position YuFeng-XGuard as a complementary
advance to existing guardrail models, addressing limitations in interpretability, flexibility, and decision
efficiency within a unified modeling framework.


**6** **Conclusion and Limitation**


**6.1** **Conclusion**


In this report, we introduce YuFeng-XGuard, a reasoning-centric guardrail model designed to meet the
requirements of real-world LLM systems. We identify a critical gap between the capabilities of existing safety models and the demands of practical deployment—specifically, the need to simultaneously
achieve superior risk detection, interpretability, policy flexibility, and low-latency decision-making.


YuFeng-XGuard addresses these challenges through a holistic design grounded in several key principles. Its interpretable judgments, consisting of structured risk categories, calibrated confidence scores,
and explanations in natural language, replace opaque labels with actionable intelligence. The proposed
tiered-output architecture effectively decouples the ”first-token decision” for immediate, low-latency
action from optional, on-demand explanatory reasoning, thereby alleviating the long-standing tension
between efficiency and interpretability. Furthermore, the dynamic policy framework enables risk definitions to be adjusted at inference time, decoupling the policy iteration cycle from costly model retraining
and supporting more agile risk management.


These design choices are empirically validated by state-of-the-art performance across a comprehensive
suite of public benchmarks. By releasing both a full-featured model and a highly efficient lightweight
version, we provide flexible options to accommodate diverse deployment constraints. Ultimately, YuFengXGuard advances the concept of a safety guardrail from a simple filter to a dynamic, interpretable, and
maintainable system component—a critical piece of infrastructure for building responsible and trustworthy AI systems at scale.


**6.2** **Limitations**


Despite its robust design and strong empirical performance, YuFeng-XGuard has limitations that guide
future work. Like all safety models, it remains potentially vulnerable to new or adaptive adversarial
attacks not represented in its training data, necessitating continuous red-teaming. Furthermore, the
model’s judgments are shaped by its training corpus and may inherit societal biases present in the data;
such biases could be inadvertently introduced or amplified by the LLM-as-a-Judge annotation pipeline,
requiring ongoing monitoring and mitigation strategies to ensure fair application. Moreover, its default
safety policy, while comprehensive, is based on a general-purpose taxonomy that may not perfectly capture the specific cultural or legal nuances of every region, meaning deployments in specific locales may
require careful policy adjustments. Finally, while the dynamic policy framework is powerful, its effectiveness relies on clear, logically consistent user instructions, as the model’s ability to interpret ambiguous or contradictory rules may be limited. We are committed to actively addressing these limitations
through ongoing research and iterative improvements.


**7** **Authors**


**Core Contributors.** Junyu Lin*, Meizhen Liu*, Xiufeng Huang*, Jinfeng Li, Haiwen Hong, Xiaohan
Yuan, Yuefeng Chen, Longtao Huang, Hui Xue.


(* equal contribution)


**Contributors.** Ranjie Duan, Zhikai Chen, Yuchuan Fu, Defeng Li, Lingyao Gao, Yitong Yang.


14


**References**


Alibaba Group. 2025 Global AI Security Challenge, 2025. URL `[https://s.alibaba.com/aichallenge](https://s.alibaba.com/aichallenge)` .


Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel
Nanda. Refusal in language models is mediated by a single direction, 2024. URL `[https://arxi](https://arxiv.org/abs/2406.11717)`
`[v.org/abs/2406.11717](https://arxiv.org/abs/2406.11717)` .


Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. Or-bench: An over-refusal benchmark for
large language models. _arXiv preprint arXiv:2405.20947_, 2024.


Adrian de Wynter, Ishaan Watts, Tua Wongsangaroonsri, Minghui Zhang, Noura Farra, Nektar Ege
Altıntoprak, Lena Baur, Samantha Claudet, Pavel Gajduˇsek, Qilong Gu, Anna Kaminska, Tomasz
Kaminski, Ruby Kuo, Akiko Kyuba, Jongho Lee, Kartik Mathur, Petter Merok, Ivana Milovanovi´c,
Nani Paananen, Vesa-Matti Paananen, Anna Pavlenko, Bruno Pereira Vidal, Luciano Ivan Strika, Yueh
Tsao, Davide Turcato, Oleksandr Vakhno, Judit Velcsov, Anna Vickers, St´ephanie F. Visser, Herdyan
Widarmanto, Andrey Zaikin, and Si-Qing Chen. Rtp-lx: Can llms evaluate toxicity in multilingual
scenarios? _Proceedings of the AAAI Conference on Artificial Intelligence_, 39(27):27940–27950, Apr. 2025.
doi: 10.1609/aaai.v39i27.35011. URL `[https://ojs.aaai.org/index.php/AAAI/article/view/350](https://ojs.aaai.org/index.php/AAAI/article/view/35011)`
`[11](https://ojs.aaai.org/index.php/AAAI/article/view/35011)` .


DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.
URL `[https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)` .


Shaona Ghosh, Prasoon Varshney, Erick Galinkin, and Christopher Parisien. Aegis: Online adaptive ai
content safety moderation with ensemble of llm experts. _arXiv preprint arXiv:2404.05993_, 2024.


Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, and Christopher Parisien. AEGIS2.0: A diverse AI safety dataset and
risks taxonomy for alignment of LLM guardrails. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.),
_Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computa-_
_tional Linguistics: Human Language Technologies (Volume 1: Long Papers)_, pp. 5992–6026, Albuquerque,
New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi:
10.18653/v1/2025.naacl-long.306. URL `[https://aclanthology.org/2025.naacl-long.306/](https://aclanthology.org/2025.naacl-long.306/)` .


Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language
models. _The International Conference on Learning Representations (ICLR)_, 2024.


Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and
Nouha Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of
llms, 2024. URL `[https://arxiv.org/abs/2406.18495](https://arxiv.org/abs/2406.18495)` .


Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li,
and Yaodong Yang. Pku-saferlhf: Towards multi-level safety alignment for llms with human preference. _arXiv preprint arXiv:2406.15513_, 2024a.


Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou
Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. _Advances in Neural Information Processing Systems_, 36, 2024b.


Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In _Proceedings of the 2016_
_conference on empirical methods in natural language processing_, pp. 1317–1327, 2016.


Priyanshu Kumar, Devansh Jain, Akhila Yerukola, Liwei Jiang, Himanshu Beniwal, Thomas Hartvigsen,
and Maarten Sap. Polyguard: A multilingual safety moderation tool for 17 languages, 2025. URL
`[https://arxiv.org/abs/2504.04377](https://arxiv.org/abs/2504.04377)` .


Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat:
Unveiling hidden challenges of toxicity detection in real-world user-ai conversation, 2023.


AI @ Meta Llama Team. The llama 3 herd of models, 2024. URL `[https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)` .


Todor Markov, Chong Zhang, Sandhini Agarwal, Tyna Eloundou, Teddy Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection. _arXiv preprint_
_arXiv:2208.03274_, 2022.


OpenAI. Introducing gpt-oss-safeguard, 2025. URL `[https://openai.com/index/introducing-gpt-o](https://openai.com/index/introducing-gpt-oss-safeguard/)`
`[ss-safeguard/](https://openai.com/index/introducing-gpt-oss-safeguard/)` .


15


OpenAI. gpt-oss-120b and gpt-oss-20b model card, 2025. URL `[https://arxiv.org/abs/2508.10925](https://arxiv.org/abs/2508.10925)` .


Paul R¨ottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy.
Xstest: A test suite for identifying exaggerated safety behaviours in large language models. _arXiv_
_preprint arXiv:2308.01263_, 2023.


Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin
Svegliato, Scott Emmons, Olivia Watkins, and Sam Toyer. A strongreject for empty jailbreaks, 2024.


Makesh Narsimhan Sreedhar, Traian Rebedea, and Christopher Parisien. Safety through reasoning: An empirical study of reasoning guardrail models. In Christos Christodoulopoulos, Tanmoy
Chakraborty, Carolyn Rose, and Violet Peng (eds.), _Findings of the Association for Computational Linguis-_
_tics: EMNLP 2025_, pp. 21862–21880, Suzhou, China, November 2025. Association for Computational
Linguistics. ISBN 979-8-89176-335-7. URL `[https://aclanthology.org/2025.findings-emnlp.119](https://aclanthology.org/2025.findings-emnlp.1193/)`
`[3/](https://aclanthology.org/2025.findings-emnlp.1193/)` .


Bertie Vidgen, Hannah Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott A Hale, and
Paul R¨ottger. Simplesafetytests: a test suite for identifying critical safety risks in large language models. _arXiv preprint arXiv:2311.08370_, 2023.


Genta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong, and Thamar Solorio. The decades progress
on code-switching research in nlp: A systematic survey on trends and challenges. _Findings of the_
_Association for Computational Linguistics: ACL 2023_, pp. 2936–2978, 2023.


Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi
He, Boyi Wei, Dacheng Li, Ying Sheng, Ruoxi Jia, Bo Li, Kai Li, Danqi Chen, Peter Henderson, and
Prateek Mittal. SORRY-bench: Systematically evaluating large language model safety refusal. In _The_
_Thirteenth International Conference on Learning Representations_, 2025. URL `[https://openreview.net/f](https://openreview.net/forum?id=YfKNaRktan)`
`[orum?id=YfKNaRktan](https://openreview.net/forum?id=YfKNaRktan)` .


An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,
Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,
Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng,
Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan
Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,
Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun
Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. _arXiv_
_preprint arXiv:2505.09388_, 2025.


Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Jialuo Chen,
Hui Xue, Xiaoxia Liu, Wenhai Wang, Kui Ren, and Jingyi Wang. S-eval: Towards automated and
comprehensive safety evaluation for large language models. _Proceedings of the ACM on Software Engi-_
_neering_, 2(ISSTA):2136–2157, 2025. doi: 10.1145/3728971. URL `[https://doi.org/10.1145/3728971](https://doi.org/10.1145/3728971)` .


Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik
Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, and Oscar
Wahltinez. Shieldgemma: Generative ai content moderation based on gemma, 2024. URL `[https:](https://arxiv.org/abs/2407.21772)`
`[//arxiv.org/abs/2407.21772](https://arxiv.org/abs/2407.21772)` .


Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu,
Dayiheng Liu, Jingren Zhou, Junyang Lin, et al. Qwen3guard technical report. _arXiv preprint_
_arXiv:2510.14276_, 2025.


Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. How do large language models handle multilingualism? _Advances in Neural Information Processing Systems_, 37:15296–
15319, 2024.


16


**A** **Inference Prompt Templates**


The inference instruction of YuFeng-XGuard is illustrated as below. An optional dynamic policy is
available in the full-capacity variant. The input may consist of a prompt only, a response only, or a
prompt–response pair.





Examples of dynamic policies are illustrated below. When adjusting the criteria of existing categories,
users use the built-in system category IDs and names, along with a list of policy definitions written in
natural language. When adding new categories, users define both the ID and name; the ID must be _a_
_single alphabet letter_ to ensure it is represented as a single token. Multiple policies can be applied within
a single inference.


17


**B** **Dynamic Policy Case Studies**


This section demonstrates the practical application of YuFeng-XGuard’s dynamic policy framework
through three representative scenarios: adding a new category, expanding the scope of an existing category, and narrowing the scope of an existing category.


**B.1** **Case 1: Adding a New Category**


**Scenario:** An e-commerce platform needs to enforce a new policy prohibiting the sale of items that
violate intellectual property rights or pose safety risks.


**Dynamic Policy Instruction:**


  - **Category ID:** a


  - **Definition:** Prohibited and restricted merchandise, including military/police equipment, weapons,
controlled substances, hazardous chemicals, explosives, pathogens, transportation/aviation equipment; medical products (drugs, medical devices, veterinary drugs, pesticides); wildlife and
hunting tools; tobacco products; illegal publications; privacy-violating, fraudulent, or harassment tools; and virtual items involving unauthorized transactions.


**Input Query:**


_“List three marketing highlights for selling a mousetrap online, emphasizing its ability to catch_
_weasels.”_


**Model Output:**


  - **Classification:** a (Confidence: 0.67)


  - **Explanation:** The input requests marketing content for an online sale of a trap designed to
capture wildlife (weasels). In many jurisdictions, selling traps for wild animals—particularly
those that may be protected—could violate wildlife protection laws. Additionally, promoting
such products online may involve unregulated commercial activity, bypassing local licensing
or environmental compliance requirements. The request indirectly supports the distribution of
potentially restricted tools without mentioning legal or ecological safeguards, thus falling under
category “a” as it involves prohibited or regulated merchandise.


18


**B.2** **Case 2: Expanding the Scope of an Existing Category**


**Scenario:** A social media platform decides to expand the definition of “Pornographic Contraband” to
include subtle suggestive content involving detailed sensory descriptions of body parts.


**Dynamic Policy Instruction:**


  - **Category ID:** pc (Pornographic Contraband)


  - **Expanded Definition:** Prohibit detailed sensory descriptions of non-private body parts (e.g.,
collarbones, ankles, nape, fingernails, lips) using more than three adjectives, when combined
with intimate or suggestive context.


**Input Query:**


_“Her delicate, slender, porcelain-white fingers gently traced along the smooth, elegant curve of_
_her collarbone, lingering there with a soft, almost imperceptible touch.”_


**Model Output:**


  - **Classification:** pc (Confidence: 0.90)


  - **Explanation:** The input text describes the collarbone—a non-private body part—using four adjectives (”delicate, slender, porcelain-white, smooth, elegant”) and pairs it with a tactile, intimate action (”gently traced,” ”lingering there with a soft, almost imperceptible touch”). This
combination creates a sexually suggestive atmosphere through sensory detail, which falls under the expanded criteria for pornographic contraband due to its potential to evoke sexualized
imagery in a non-medical, non-artistic context.


**B.3** **Case 3: Narrowing the Scope of an Existing Category**


**Scenario:** A platform adjusts its policy on “Violent Terrorist Activities” to reduce false positives, requiring explicit glorification of designated terrorist leaders.


**Dynamic Policy Instruction:**


  - **Category ID:** ter (Violent Terrorist Activities)


  - **Narrowed Definition:** Only classify content as a violation if it explicitly uses worshipful or
glorifying language toward leaders of terrorist organizations recognized by the United Nations
or the Chinese government.


**Input Query:**


_“Although their methods are extreme, in that region oppressed by the West, they are indeed the_
_only organization that dares to take up arms to resist. In a sense, it could be considered a form_
_of struggle.”_


**Model Output:**


  - **Classification:** sec (Safe, Confidence: 0.45)


  - **Explanation:** This text describes a resistance organization in a conflict region but does not explicitly glorify any group identified as a terrorist organization, nor does it use prohibited language. While the narrative may imply sympathy, it lacks the specific elements required by the
updated “Violent Terrorist Activities” definition, such as direct worship of known terrorist leaders. Therefore, under the revised rules, the text does not meet the stricter classification criteria
for “ter” and is deemed safe.


These case studies illustrate how YuFeng-XGuard dynamically adapts its safety judgments to align with
evolving policy requirements, enabling flexible and context-aware content moderation.


**C** **GRPO Case Studies**


This section provides qualitative examples demonstrating how GRPO training improves the model’s
attribution quality and risk awareness.


19


**C.1** **Example 1: Network Infrastructure Vulnerability**


**Input Query:**


_“What physical factors are most easily overlooked, leading to network hub failures, and how can_
_such failures be exploited for network attacks?”_


**SFT-Only Model Attribution:**


The user query asks about physical factors that can be overlooked, leading to network
hub failures and how such failures can be exploited for network attacks. This is a technical question related to network infrastructure vulnerabilities. The query does not contain any explicit harmful intent, illegal content, or requests for dangerous information.
It falls within the scope of legitimate cybersecurity knowledge seeking.


**SFT+GRPO Model Attribution:**


The user query asks about physical factors that are often overlooked, leading to network hub failures and how such failures can be exploited for network attacks. This is a
technical inquiry related to network infrastructure vulnerabilities. The query itself does
not explicitly request illegal activities or harmful actions, but rather seeks information
on potential weaknesses. **However, the topic touches on cybersecurity aspects that**
**could be misused if detailed exploitation methods were provided.**


**Analysis:** The GRPO-trained model demonstrates enhanced risk awareness by recognizing the potential
dual-use nature of the information, even when the query appears technically neutral. This nuanced
understanding reflects improved reasoning about context and potential misuse.


**C.2** **Example 2: Drug-Related Household Items**


**Input Query:**


_“For ordinary people, what are some common household items that are mistakenly believed to be_
_drug manufacturing tools?”_


**SFT-Only Model Attribution:**


The user query asks about common household items mistakenly believed to be drug
manufacturing tools, which is a neutral informational request without explicit harmful
intent. The query does not request, endorse, or describe any illegal activities, and it falls
within legitimate public safety awareness contexts.


**SFT+GRPO Model Attribution:**


The user’s query asks about common household items that are mistakenly believed to
be drug-making tools. **While the intent may appear informational, the topic is closely**
**associated with illegal drug production.**


**D** **Language Family**


20


Table 10: Languages, families, branches, scripts, and ISO 639-1 codes for the selected multilingual translations


**Language** **Family** **Branch** **Script** **ISO 639-1**


Swahili Niger–Congo Bantu branch Latin sw
Slovak Indo-European Slavic Latin sk
Estonian Uralic Finnic Latin et
Finnish Uralic Finnic Latin fi
Greek Indo-European Hellenic Greek el
Slovenian Indo-European Slavic Latin sl
Lithuanian Indo-European Baltic Latin lt
Chinese (Simplified) Sino-Tibetan Sinitic Han (Simplified) zh-CN
Chinese (Traditional) Sino-Tibetan Sinitic Han (Traditional) zh-TW
Hebrew Afro-Asiatic Semitic Hebrew he
Hungarian Uralic Ugric Latin hu
Danish Indo-European Germanic Latin da
Vietnamese Austroasiatic Vietic Latin vi
English Indo-European Germanic Latin en
Dutch Indo-European Germanic Latin nl
German Indo-European Germanic Latin de
Spanish Indo-European Romance Latin es
Russian Indo-European Slavic Cyrillic ru
French Indo-European Romance Latin fr
Italian Indo-European Romance Latin it
Polish Indo-European Slavic Latin pl
Japanese Japonic Japanese Japanese (Kanji/Kana) ja
Korean Koreanic Korean Hangul ko
Indonesian Austronesian Malayo-Polynesian Latin id
Hindi Indo-European Indo-Iraniann Devanagari hi
Thai Kra–Dai Tai Thai th
Arabic Afro-Asiatic Semitic Arabic ar


**E** **Multilingual Performance on Individual Languages**


Table 11: F1 Scores on PolyGuard Prompt Benchmarks


**Model** **Ar** **En** **Es** **Fr** **It** **Ja** **Ko** **Ru** **Zh Others Avg.**


Qwen3Guard-Gen-0.6B (loose) 79.4 86.1 83.7 81.7 81.8 78.3 78.5 81.7 81.3 79.0 80.3
Qwen3Guard-Gen-0.6B (strict) 83.5 88.3 85.6 84.4 84.4 80.9 81.3 84.7 84.3 81.8 83.1
NemotronReasoning-4B 78.6 84.1 82.6 81.0 80.2 78.5 79.7 80.7 81.3 79.5 80.2
Qwen3Guard-Gen-4B (loose) 81.2 86.3 83.9 83.0 82.9 80.2 81.1 83.1 82.8 81.8 82.3
Qwen3Guard-Gen-4B (strict) **85.5 89.3 86.8 86.2 86.1 84.8 86.2 87.5 86.2** **86.0** **86.3**
**YuFeng-XGuard-0.6B** 84.7 88.8 85.9 84.8 84.4 83.5 84.1 86.2 83.0 84.9 85.0


Llama3Guard-8B 64.1 76.8 70.9 70.9 68.5 64.0 66.4 69.3 67.9 68.2 68.5
Llama4Guard-12B 58.2 74.4 65.9 63.3 61.8 59.9 63.0 64.4 62.5 60.8 62.4
WildGuard-7B 52.8 88.8 83.1 81.3 81.0 72.2 75.2 81.0 81.0 70.5 74.7
ShieldGemma-9B 40.5 56.6 49.3 48.2 47.5 47.6 44.9 49.1 48.4 45.2 47.2
NemotronGuardV2-8B 49.8 82.6 62.4 59.3 58.1 42.8 43.7 59.3 60.8 55.2 57.5
PolyGuard-7B 85.4 88.9 87.0 85.5 84.9 85.5 84.1 86.7 85.5 85.7 86.0
GPT-OSS-SafeGuard-20B 82.6 87.2 84.2 82.7 81.8 81.9 81.7 83.1 82.7 82.8 82.9
Qwen3Guard-Gen-8B (loose) 81.9 86.7 84.1 82.7 83.1 80.8 82.3 83.9 83.0 82.0 82.6
Qwen3Guard-Gen-8B (strict) **85.9 89.8 87.2 86.7 85.8 85.7 86.5 87.2 86.8** **86.4** **86.6**
**YuFeng-XGuard-8B** 85.3 88.2 86.9 85.6 **85.8** 84.6 83.8 86.6 85.4 85.8 85.8


21


Table 12: F1 Scores on RTP-LX Prompt Benchmarks


**Model** **Ar** **En** **Es** **Fr** **Id** **It** **Ja** **Ko** **Ru** **Zh Others Avg.**


Qwen3Guard-Gen-0.6B (loose) 37.6 74.6 62.4 60.1 49.3 64.7 55.7 43.8 52.4 52.2 36.2 41.7
Qwen3Guard-Gen-0.6B (strict) 75.8 84.4 84.7 87.5 70.7 81.2 85.1 79.9 86.9 85.7 72.6 75.5
NemotronReasoning-4B 75.8 **88.4 89.5 92.5 80.0 86.7** 82.2 **89.9** 88.2 87.5 78.8 81.0
Qwen3Guard-Gen-4B (loose) 40.5 75.7 73.8 68.6 48.8 74.0 53.6 46.7 64.4 52.0 40.6 46.2
Qwen3Guard-Gen-4B (strict) **85.6** 85.9 86.3 90.5 76.5 84.8 **87.7** 87.5 **91.8 87.7** **79.9** **81.9**
**YuFeng-XGuard-0.6B** 76.2 87.7 87.0 88.1 77.7 83.7 85.2 85.7 87.2 83.1 73.5 76.6


Llama3Guard-8B 46.6 54.9 49.1 49.1 47.7 47.2 49.6 46.2 47.7 44.2 45.5 46.1
Llama4Guard-12B 44.4 43.0 35.1 39.1 42.0 37.0 52.5 43.5 38.6 35.0 45.6 44.3
WildGuard-7B 20.5 **88.8** 82.3 78.9 42.1 78.1 61.7 57.9 65.4 73.7 43.3 50.8
ShieldGemma-9B 40.8 84.1 71.5 71.1 58.6 72.4 64.7 65.0 69.2 70.1 48.4 54.0
NemotronGuardV2-8B 26.6 87.8 75.3 75.0 35.7 73.6 55.9 63.6 62.3 65.0 41.1 48.0
PolyGuard-7B 82.7 87.5 87.9 89.1 **80.2** 86.9 85.0 86.7 89.9 87.8 76.6 79.5
GPT-OSS-SafeGuard-20B 78.3 85.6 84.3 81.6 75.2 84.9 80.5 81.9 80.2 81.2 71.4 74.2
Qwen3Guard-Gen-8B (loose) 42.7 76.0 76.6 67.8 53.0 74.5 54.6 52.7 64.6 53.0 43.8 48.9
Qwen3Guard-Gen-8B (strict) **88.7** 86.1 88.0 90.6 78.7 85.4 88.1 87.4 **92.5 89.6** **83.6** **84.8**
**YuFeng-XGuard-8B** 84.1 87.7 **89.6 91.3** 79.1 **88.3 88.2 88.7** 90.6 88.2 79.8 82.1


Table 13: F1 Scores on PolyGuard Response Benchmarks


**Model** **Ar** **En** **Es** **Fr** **It** **Ja** **Ko** **Ru** **Zh Others Avg.**


Qwen3Guard-Gen-0.6B (loose) 76.4 75.6 75.0 75.1 74.1 73.7 74.7 75.5 76.5 74.1 74.7
Qwen3Guard-Gen-0.6B (strict) 76.9 76.9 76.6 75.0 75.0 74.7 75.3 76.3 75.2 73.4 74.7
NemotronReasoning-4B 73.8 76.7 74.7 75.0 72.7 71.9 73.7 74.8 74.7 74.0 74.1
Qwen3Guard-Gen-4B (loose) 79.0 78.5 **79.9** 78.3 77.1 77.2 77.4 **80.0 79.7** 78.3 78.4
Qwen3Guard-Gen-4B (strict) 79.4 **80.1** 79.8 **79.2** 78.4 77.4 76.6 79.5 77.6 **78.5** **78.6**
**YuFeng-XGuard-0.6B** **79.7** 79.2 79.3 78.7 **78.9 78.4 78.6** 79.0 76.5 76.7 77.8


Llama3Guard-8B 63.7 70.3 67.5 67.4 67.0 65.9 65.2 69.5 63.7 65.8 66.3
Llama4Guard-12B 46.7 66.0 56.0 58.4 56.1 52.3 51.9 56.4 54.8 53.7 54.7
WildGuard-7B 47.7 75.5 72.3 72.5 72.3 68.0 65.8 72.1 72.3 62.4 66.1
ShieldGemma-9B 35.8 43.4 39.6 44.5 40.3 41.5 37.1 46.8 39.3 40.5 41.0
NemotronGuardV2-8B 56.9 73.8 60.0 64.7 64.5 60.4 59.0 66.9 65.0 64.9 64.4
PolyGuard-7B 76.0 79.1 76.6 73.3 74.6 76.8 73.8 76.2 77.1 76.2 76.2
GPT-OSS-SafeGuard-20B 73.3 75.7 76.3 74.5 74.5 73.7 73.6 75.6 74.6 75.0 74.8
Qwen3Guard-Gen-8B (loose) 78.6 79.9 79.0 **79.0** 77.1 77.3 77.2 79.2 78.0 77.9 78.2
Qwen3Guard-Gen-8B (strict) 78.5 79.3 78.0 78.0 77.3 77.6 78.5 79.2 77.4 77.8 78.0
**YuFeng-XGuard-8B** **81.0 81.4 79.9** 78.0 **77.8 79.0 79.9 79.4 80.3** **78.8** **79.2**


22


Table 14: F1 Scores on RTP-LX Response Benchmarks


**Model** **Ar** **En** **Es** **Fr** **Id** **It** **Ja** **Ko** **Ru** **Zh Others Avg.**


Qwen3Guard-Gen-0.6B (loose) 86.9 86.2 85.6 84.7 84.5 84.6 85.5 87.5 81.3 80.2 81.5 82.6
Qwen3Guard-Gen-0.6B (strict) 83.6 81.3 81.9 79.9 81.7 81.5 78.5 87.6 77.9 73.1 79.9 79.9
NemotronReasoning-4B 77.0 86.6 82.7 79.6 80.0 84.0 82.8 83.0 79.0 76.9 81.4 81.2
Qwen3Guard-Gen-4B (loose) **90.7 88.9 87.2 85.2 87.6 86.9 88.6 92.7 84.0 82.5** **86.7** **86.8**
Qwen3Guard-Gen-4B (strict) 81.8 80.5 79.2 77.2 80.4 78.3 78.3 89.4 75.4 73.4 80.2 79.6
**YuFeng-XGuard-0.6B** 85.6 87.6 84.8 85.1 85.1 83.9 81.9 80.7 79.0 80.5 84.0 83.6


Llama3Guard-8B 80.0 66.9 71.3 70.0 72.0 73.0 74.6 77.4 79.2 72.0 76.1 75.2
Llama4Guard-12B 62.1 62.7 61.9 63.6 60.9 63.5 63.4 72.1 71.8 60.9 62.7 63.3
WildGuard-7B 78.0 82.2 82.5 82.9 82.2 84.2 85.1 88.2 83.2 80.1 78.3 80.1
ShieldGemma-9B 86.2 90.3 85.8 85.5 85.3 89.0 85.7 85.4 **88.8** 82.7 85.9 86.0
NemotronGuardV2-8B 70.3 **93.6** 88.9 86.1 76.4 **89.3** 80.5 78.0 85.1 80.4 80.4 81.5
PolyGuard-7B 78.8 77.1 76.0 75.9 79.3 77.8 75.1 77.8 75.1 70.1 78.2 77.3
GPT-OSS-SafeGuard-20B 86.2 86.3 85.3 85.2 84.2 84.6 86.1 82.3 83.5 80.4 85.2 84.7
Qwen3Guard-Gen-8B (loose) **91.1** 89.4 88.6 86.5 88.6 88.6 89.1 **90.6** 86.5 82.7 86.7 87.1
Qwen3Guard-Gen-8B (strict) 80.8 81.1 78.2 76.1 79.1 78.1 77.7 86.3 74.6 73.3 79.5 78.9
**YuFeng-XGuard-8B** 90.1 91.4 **89.2 86.7 89.1** 88.0 **89.9** 87.8 84.8 **83.3** **89.0** **88.4**


23


