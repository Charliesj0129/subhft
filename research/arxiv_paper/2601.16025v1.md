1


## EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery

Yajuan Xu, Xixian Han, Xiaolong Wan



_**Abstract**_ **—Functional dependencies (FDs) are fundamental in-**
**tegrity constraints in relational databases, but discovering them**
**under incremental updates remains challenging. While static**
**algorithms are inefficient due to full re-execution, incremen-**
**tal algorithms suffer from severe performance and memory**
**bottlenecks. To address these challenges, this paper proposes**
**EAIFD, a novel algorithm for incremental FD discovery. EAIFD**
**maintains the partial hypergraph of difference sets and reframes**
**the incremental FD discovery problem into minimal hitting**
**set enumeration on hypergraph, avoiding full re-runs. EAIFD**
**introduces two key innovations. First, a multi-attribute hash**
**table (** _MHT_ **) is devised for high-frequency key-value mappings**
**of valid FDs, whose memory consumption is proven to be**
**independent of the dataset size. Second, two-step validation**
**strategy is developed to efficiently validate the enumerated**
**candidates, which leverages** _MHT_ **to effectively reduce the**
**validation space and then selectively loads data blocks for batch**
**validation of remaining candidates, effectively avoiding repeated**
**I/O operations. Experimental results on real-world datasets**
**demonstrate the significant advantages of EAIFD. Compared to**
**existing algorithms, EAIFD achieves up to an order-of-magnitude**
**speedup in runtime while reducing memory usage by over two**
**orders-of-magnitude, establishing it as a highly efficient and**
**scalable solution for incremental FD discovery.**


_**Index Terms**_ **—Database, Algorithm, Functional dependency,**
**Incremental data**


I. INTRODUCTION

UNCTIONAL dependencies (FDs) are fundamental integrity constraints in relational databases [1], which are
# **F**
widely applied in many areas, e.g., schema design and normalization [2], [3], query optimization [4], data cleaning [5]
and data integration [6]. Formally, for a relation instance _r_ of
schema _R_, a functional dependency (FD) _X_ → _A_ is valid iff
for any two tuples in _r_, if their values on _X_ are the same, then
they must have the same value on _A_, where _X_ ⊆ _R_ and _A_ ∈ _R_
are referred to as left-hand side (LHS) and right-hand side
(RHS), respectively. The FD _X_ → _A_ is non-trivial if _A_ ∉ _X_,
and minimal if there is no proper subset _X_ [′] ⊂ _X_ such that
_X_ [′] → _A_ holds in _r_ . FD discovery focuses on _the complete set_
F _of minimal and non-trivial FDs_ .


**Example I.1.** _A student table is shown in Table 1 of_
_Fig. 1. SName_ → _SAge is invalid, as the tuple pairs (s_ 2
_and s_ 6 _) share the same name but have different ages._
{ _SNO,_ _SName_ } → _SMajor is a valid but not minimal FD. And_
F = { _SNO_ → _SName, SNO_ → _SAge, SNO_ → _SGrade, SNO_ →
_SMajor, SNO_ → _SDorm, SMajor_ → _SDorm_ } _._


Yajuan Xu, Xixian Han and Xiaolong Wan are with the School of Computer
Science and Technology, Harbin Institute of Technology, China. (e-mail:
xuyajuan@stu.hit.edu.cn, hanxx@hit.edu.cn, wxl@hit.edu.cn)
Manuscript received XX XX, XXXX; revised XX XX, XXXX.



Fig. 1: The figure of two student information tables. Table 1 contains
6 student information tuples, Table 2 contains 4 student information
tuples, and the tuples in Table 2 are incrementally added to Table 1.


The real-life datasets typically grow continuously through
the addition of tuples. For example, the modern newspaper
database [1] adds at least 100 new newspaper titles annually. The
Ncvoter dataset [2] is incrementally updated as North Carolina
counties finalize voter histories after each election. The incremental updates of a database often lead to changes in F. As
illustrated in Fig. 1, when new tuples from Table 2 are added
to Table 1, previously valid FD SMajor → SDorm is invalid,
due to the tuple pair ( _s_ 6 _,s_ 9). {SGrade _,_ SMajor} → SDorm
becomes a new minimal valid FD.
Traditional FD discovery algorithms [7], [8], [9], [10], [11],

[12], [13], [14], [15], [16] are designed for static datasets
and must be re-executed upon data inserts, rendering them
impractical for continuous data growth. Therefore, researchers
have explored FD discovery on incremental databases. While
incremental algorithms, DynFD [17] and DHSFD [18], perform better than static FD discovery algorithms, they still face
the following challenges. Specifically, (1) **Long preprocessing**
**time.** Both DynFD and DHSFD require costly preprocessing,
with time complexity of _O_ ( _m_ ⋅ 2 _[m]_ ) and _O_ ( _n_ [2] ), which is
impractical in real-life scenarios; (2) **Poor scalability on wide**
**schema.** As ∣ _R_ ∣ increases, the candidate search space grows
exponentially, degrading validation and maintenance cost; (3)
**Excessive memory consumption.** They need to maintain
many auxiliary data structures, which grow rapidly with data
and cause high memory usage.


[1https://www.nlcpress.com/DigitalPublishingView.aspx?DPId=10.](https://www.nlcpress.com/DigitalPublishingView.aspx?DPId=10)
[2https://www.ncsbe.gov/results-data/voter-history-data.](https://www.ncsbe.gov/results-data/voter-history-data)


To address these challenges, this paper presents EAIFD,
an efficient algorithm for incremental FDs discovery, that
provides fast initialization, scalability on high-dimensional
datasets, and low memory consumption. EAIFD systematically tackles the aforementioned challenges through _three key_
_contributions_ . **Firstly**, EAIFD addresses the challenge of long
preprocessing time by performing a sampling-based initialization. It computes difference sets only on the sampled subset
and builds partial hypergraphs, which significantly reduces
_O_ ( _n_ [2] ) time complexity and greatly reduces preprocessing
time. EAIFD generates candidate FDs by enumerating hitting
sets on partial hypergraphs. **Secondly**, EAIFD optimizes the
validation phase to reduce the high costs caused by the
exponential growth of candidates. Specifically, it employs
iterative grouping hash validation (IGHV) combined with
a multi-attribute hash table ( _MHT_ ) that stores mappings
between LHS and RHS values. After data updates, IGHV
compares the updated data with items in pre-built _MHT_ and
restricts validation to relevant blocks and validates candidates
in batches. Valid FDs are added to F, while invalid FDs are
used to update the hypergraph and iteratively generate new
candidates. **Thirdly**, to control memory usage, EAIFD applies
a _high-frequency mapping items preservation strategy_, keeping
only mapping items that appear above a specified frequency
threshold, ensuring _MHT_ remains lightweight and balancing
memory consumption with performance efficiency. Comprehensive experiments are conducted on real-life datasets. The
experimental results verify that EAIFD is more efficient than
the incremental FD discovery algorithms. Notably, compared
with the existing algorithms, EAIFD achieves up to an orderof-magnitude speedup in runtime, while reducing memory
usage by over two orders of magnitude.
The rest of this paper is organized as follows. Section II
surveys the related work, followed by problem statement and
preliminaries in Section III. Section IV introduces EAIFD
algorithm. Section V provides a comprehensive experimental
evaluation. Section VI concludes the paper.


II. RELATED WORK


This section surveys the related work of FD discovery,
and introduces two related research lines to our work: (i)
FD discovery algorithms designed for static datasets, (ii) FD
discovery algorithms that support incremental datasets update.


_A. FD discovery for static data_


Researchers propose many FD discovery algorithms for
static relational databases [7], [8], [19], [20], [12], [9], [10],

[11], which consist of two key phases: (i) candidate FDs
generation and (ii) candidate FDs validation [13]. The existing
algorithms improve FD discovery efficiency from different
aspects and can be roughly divided into the following three
categories.
**Attribute-based algorithms** [7], [21], [9], [12], [15], [8]
enumerate candidate FDs in powerset lattices of attribute
combinations and traverse lattices with level-wise or depthfirst strategies. They usually use position list indexes (PLIs)
to validate candidate FDs with many pruning rules to reduce



2


the search space. They perform well on datasets with many
tuples, but due to their candidate-driven search strategy, they
scale poorly on datasets with many attributes.
**Tuple-based algorithms** [11], [14], [19], [20] are based
on the value comparisons of all tuple pairs. These algorithms
discover FDs by constructing agree sets of attributes with
identical tuple values and difference sets of attributes with
distinct values. These algorithms scale well with the number
of attributes. However, comparing all tuple pairs often leads
to a prohibitive _O_ ( _n_ [2] ) complexity, which severely degrades
performance on long datasets.
**Hybrid algorithms** [10], [16] integrate the advantages of
both attribute-based and tuple-based algorithms. By employing
a row-based strategy on sampled data and a column-based
strategy on the full dataset, they effectively avoid numerous
ineffective candidate FDs validations and tuple comparisons.
Experimental results demonstrate that this hybrid design outperforms both attribute-based and tuple-based algorithms.


_B. FD discovery for incremental data_


Traditional static FD discovery algorithms must re-execute
the entire discovery process after each data update. In recent
years, different researchers have turned their efforts to developing FD discovery algorithms suitable for incremental data
scenarios. These methods aim to discover the complete set of
minimal and non-trivial FDs at a lower computational cost
after data updates without needing to re-execute the entire
algorithm. DynFD [17] and DHSFD [18] are proposed for
incremental FD discovery, which perform incremental FD discovery from the attribute and tuple perspectives, respectively.
**Attribute-based algorithm:** DynFD is the first algorithm
capable of maintaining the complete set of minimal non-trivial
FDs for incrementally updated data. Instead of frequently
recomputing all FDs, DynFD detects changes in the data and
infers their impact on FDs by incrementally updating the FDs
set based on prior results and a batch of update operations.
It maintains a positive cover of minimal FDs and a negative
cover of maximal non-FDs. After incremental data updates,
DynFD validates whether the most general FDs still hold.
If an FD is invalid, it is removed from the positive cover,
added to the negative cover, and specialized by adding new
attributes to its LHS. The specialized candidates are then
validated using only PLI clusters that include newly inserted
tuples, improving efficiency. If the specialized FD holds, it is
added to the positive cover. When the proportion of invalidated
FDs in a validation round exceeds 10%, DynFD switches
to a violation-driven strategy that compares new tuples with
relevant existing records to locate invalid FDs. For each invalid
FD, the positive and negative covers are updated accordingly
to maintain consistency.
**Tuple-based algorithm:** DHSFD transforms the problem of
incremental FD discovery into a dynamic hitting set enumeration problem over hypergraphs constructed from difference
sets of entire dataset. DHSFD maintains auxiliary structures,
including PLIs and difference sets with weights. After incremental data updates, PLIs are used to update the difference sets
and their weights. These updates are treated as edge additions


in the hypergraph, where each edge represents a difference
set. DHSFD then computes minimal hitting sets over the
updated hypergraph. It validates minimality using critical
edges and extends candidate FDs using a depth-first strategy
called WalkDown. The resulting minimal hitting sets form the
LHSs of newly discovered minimal FDs. By comparing only
tuples in clusters containing incremental tuples and avoiding
reconstruction of the search tree, DHSFD achieves higher
efficiency than static algorithms.
**Discussion.** This paper focuses on the problem of FD discovery for incremental data, which are continuously updated
by adding tuples. Based on the current data scenarios, current
incremental FD discovery faces the following challenges.
_Preprocessing time is too long._ In the preprocessing phase,
DynFD derives the negative cover from the positive cover,
while DHSFD needs to compare all the tuples pair to compute
difference sets of the dataset, this step has a time complexity
_O_ ( _n_ [2] ) for a dataset with _n_ tuples. When the dataset or ∣F∣ is
large, both methods severely impact the startup time.
_Inefficient algorithm performance._ DynFD is inefficient in
high-dimensional datasets due to the exponential growth in
the search space and increased memory usage for maintaining positive and negative covers. When the invalidation
rate exceeds a threshold, DynFD switches to violation-driven
validation and the cost of traversing PLI clusters containing
new tuples becomes extremely expensive. DHSFD still faces
a significant increase in the cost of updating its difference sets
and hypergraphs.
_Auxiliary data structure memory space consumption is high._
Two algorithms often rely on auxiliary data structures that
consume a large amount of memory. DynFD maintains the
positive cover, negative cover, position list indexes (PLIs),
and dictionary encodings. Likewise, DHSFD needs to maintain
PLIs, difference sets and hypergraphs. As new data is continuously appended, these structures grow in size, resulting in a
significant increase in memory usage.
Motivated by these limitations, our goal is to design
an incremental FD discovery algorithm that supports highdimensional datasets, avoids expensive preprocessing, and
operates with minimal memory overhead.


III. PROBLEM DEFINITION


Let _r_ be a relation instance over schema _R_ . We denote
incremental data as ∆ _r_ . After incremental update, the updated
dataset is denoted as _r_ ∪ ∆ _r_ . ∀ _t_ ∈ _r_, we use _t_ [ _X_ ] or _t_ [ _A_ ] to
denote the projection of _t_ on attribute set _X_ ⊆ _R_ or single
attribute _A_ ∈ _R_, respectively.

**Definition III.1.** _**(FD).**_ _The FD X_ → _A with X_ ⊆ _R and_
_A_ ∈ _R is valid for instance r of R, iff_ ∀ _t_ 1 _,t_ 2 ∈ _r_ ∶ _t_ 1[ _X_ ] =
_t_ 2[ _X_ ] ⇒ _t_ 1[ _A_ ] = _t_ 2[ _A_ ] _. If there is no Y_ ⊂ _X,Y_ → _A and_
_A_ ∉ _X, then X_ → _A is minimal and non-trivial._

Given FDs _X_ → _A_ and _Y_ → _A_ where _X_ ⊂ _Y_, _X_ → _A_ is
a **generalization** of _Y_ → _A_ and _Y_ → _A_ is a **specialization**
of _X_ → _A_ . FD discovery focuses on the complete set F of
minimal and non-trivial FDs, as all other valid FDs can be
derived from them using Armstrong’s axioms [22].



3


**Definition III.2.** _**(Violated tuple pair).**_ _If a candidate FD X_ →
_A is invalid, then there must exist a pair of tuples t_ 1 _,t_ 2 _r_
∈
_where t_ 1[ _X_ ] = _t_ 2[ _X_ ] _but t_ 1[ _A_ ] ≠ _t_ 2[ _A_ ] _. The_ ( _t_ 1 _,t_ 2) _is called_
_a violated tuple pair for the candidate FD X_ → _A._


This paper transforms FD discovery into hitting set enumeration in hypergraphs, following tuple-based algorithms [14],

[23], [18]. The necessary definitions are introduced below.


**Definition III.3.** _**(Difference set).**_ _For t_ 1 _,t_ 2 _r, their differ-_
∈
_ence set is D_ ( _t_ 1 _, t_ 2) = { _A_ ∈ _R_ ∣ _t_ 1[ _A_ ] ≠ _t_ 2[ _A_ ]} _, representing_
_the set of attributes on which two tuples have different values._

The difference sets of _r_ is denoted as _Dr_ = { _D_ ( _t_ 1 _, t_ 2) ∣
_t_ 1 _, t_ 2 ∈ _r, D_ ( _t_ 1 _, t_ 2) ≠∅}. For attribute _A_ ∈ _R_, the difference
sets of _r_ modulo _A_ is denoted as _Dr_ _[A]_ = { _D_ ∖{ _A_ } ∣
_D_ ∈ _Dr_ ∧ _A_ ∈ _D_ }. A difference set _D_ ∈ _Dr_ is minimal
if there does not exist _D_ [′] ∈ _Dr_ and _D_ [′] ⊂ _D_ . _Dr_ =
{ _D_ ∈ _Dr_ ∣ _D_ [′] ∈ _Dr_ ∧ _D_ [′] ⊆ _D_ ⇒ _D_ [′] = _D_ }, where _Dr_ consists
of all minimal difference sets of _Dr_ . The **hypergraph** of _r_
is denoted as H _r_ = ( _R,Dr_ ), where _R_ is the vertex set and
_Dr_ is the hyperedge set. The **sub-hypergraph** of attribute
_A_ is denoted by H _A_ = ( _R,Dr_ _[A]_ [)][. The minimal hypergraph]
H _r_ = ( _R,_ _Dr_ ) contains only the minimal hyperedges of H _r_ .

**Definition III.4.** _**(Hitting set).**_ _Given a sub-hypergraph_ H _A_ =
( _R,Dr_ _[A]_ [)] _[,][ X][ is a hitting set of]_ [ H] _[A]_ _[iff it intersects with every]_
_hyperedge in Dr_ _[A][. If no subset of][ X][ is a hitting set,][ X][ is a]_
_minimal hitting set. HS_ (⋅) _denotes the set of minimal hitting_
_sets of a hypergraph._

**Property III.1.** _If an FD X_ → _A (A_ ∉ _X) holds on r, X must_
_be a hitting set of_ H _A. Moreover, X_ → _A is a minimal and_
_non-trivial FD iff X is a minimal hitting set of_ H _A._

**Property III.2.** _Any hitting set of_ H _A_ _is also a hitting set of_
H _A. The complete minimal hitting sets of_ H _A are equal to the_
_complete minimal hitting sets of_ H _A._

It suffices to consider H _r_ when enumerating hitting sets.
Actually, H _r_ is usually much smaller than H _r_ . In the following
sections, both hypergraphs and the sub-hypergraphs refer to
their minimal forms.


**Problem statement.** _Given a relation instance r of schema_
_R, the incremental FD discovery aims to find the complete set_
F _of minimal and non-trivial FDs that hold on the dataset_
_after each data update, without re-executing the algorithm._


IV. EAIFD ALGORITHM


In this section, we introduce EAIFD, a novel FD discovery
algorithm designed for incremental databases. EAIFD can
efficiently discover the complete set F of minimal and nontrivial FDs on a dataset and rapidly update F after each
data increment. Compared with existing incremental methods,
EAIFD enables fast initialization, scales efficiently on highdimensional datasets, and reduces memory consumption. The
overall framework of EAIFD is illustrated in Fig. 2.
The core idea of EAIFD is to generate candidate FDs
through hitting set enumeration on hypergraphs constructed
from difference sets and to validate candidate FDs. The EAIFD


4



is composed of two core components: a _one-time discovery_
_process_ for the initial dataset, and an efficient _incremental_
_update mechanism_ for all subsequent data increments. The
_one-time discovery process_ leverages sampling for rapid initialization and builds the crucial multi-attribute hash table
( _MHT_ ) to accelerate future updates. The _incremental update_
_mechanism_ intelligently utilizes _MHT_ for rapid validation
pruning and employs the IGHV on data blocks for validating
remaining candidates efficiently. The details of two components are described in Sections IV-A and IV-B, respectively.


_A. One-time discovery process on initial dataset r_


_Preprocessing_ . The input dataset _r_ is first sorted in ascending order by each attribute and stored on disk, with the
index range of each distinct value recorded for rapid loading
data blocks in following steps. Then EAIFD applies uniform
random sampling without replacement to _r_ at a sampling
ratio _ε_ = 0 _._ 3, selecting (( _[n]_ 2 [))] _[ε]_ [ record pairs from total pairs]
in a dataset with _n_ records. This approach balances data
coverage with efficiency and has proved high effectiveness in
FD discovery [11], [24]. We denote the sampled data as _s_ .
_One-time discovery process_, as listed in Algorithm 1, generates the complete set F of minimal non-trivial FDs holding on
_r_ and constructs _MHT_ through two core steps: candidate FD
generation (Section IV-A1) and validation (Section IV-A2).
_1) Generating candidate FDs with sampled data s:_
EAIFD generates candidate FDs by processing each attribute _A_ ∈ _R_ as RHS (Lines 2-28). For each _A_, it
constructs a partial sub-hypergraph H _A_ = ( _R,Ds_ _[A]_ [)][ from]
sampled data _s_ (Line 2), where hyperedges in _Ds_ _[A]_ =
_D_ ( _ti,tj_ ) ∖ _A_ ∣ _ti,tj_ ∈ _s_ ∧ _A_ ∈ _D_ ( _ti,tj_ ) represent difference
sets of tuple pairs that differ on _A_, with _A_ itself removed. _Ds_ _[A]_
captures differences involving _A_ and defines the search space
for candidate LHSs with _A_ as the RHS. EAIFD then applies
MMCS algorithm for each H _A_ to enumerate minimal LHS
candidates for _A_ as RHS (Line 6), forming candidate FDs set
C _F Ds_ . Here, MMCS [25] is a hyperedge-based branching algorithm for enumerating minimal hitting sets from a hypergraph,
which explores the search space efficiently by constructing
a search tree and integrating minimality checking to avoid
unnecessary branching. Since these candidate FDs are derived
from partial sub-hypergraphs based on the sampled dataset _s_,
they require validation to ensure validity on _r_ .
_2) Validating candidate FDs with IGHV:_ We propose an
efficient validation method with a low memory usage, called
iterative grouping hash validation (IGHV), to check the validity of candidate FDs derived from partial sub-hypergraphs
on _r_ . Fig. 3 illustrates its main process. IGHV applies to both
static data and incremental updates with few differences.
To understand the novelty and advantages of IGHV, it is
helpful to first review the principles and limitations of a
standard hash-based validation approach. Let B denote the
complete set of hash buckets, and B _X_ represent the hash
bucket set for candidate FD _X_ → _A_ . ∀ _t_ ∈ _r_, it applies a
hash function _f_ to _t_ [ _X_ ] to compute the hash value _h_, and
assigns _t_ to the bucket B _X_ _[h]_ [. Any two tuples] _[ t][i][,t][j]_ [ in the same]
bucket B _X_ _[h]_ [satisfy] _[ t][i]_ [[] _[X]_ [] =] _[ t][j]_ [[] _[X]_ []][. If] _[ t][i]_ [[] _[A]_ [] ≠] _[t][j]_ [[] _[A]_ []][, a violated]



**25** Clear B _Xi_


**26** F ←F ∪ _g_


**27** **until** ∆ _diff is empty_


**28 return** F _, MHT,_ H


tuple pair ( _ti,tj_ ) is detected, and _X_ → _A_ is declared invalid.
To validate _X_ → _A_, this method traverses the entire dataset
to build B _X_ . It works well on small datasets. However, as
the number of candidate FDs increases or the dataset scale
expands, the memory consumption of hash buckets becomes
excessive and may lead to memory overflow.
IGHV significantly improves validation efficiency while effectively controlling memory usage. Algorithm 1 describes the
IGHV process for validating candidate FDs C _F Ds_ obtained by
the MMCS tree search (Lines 7-26). IGHV first groups C _F Ds_
such that the FDs in each group share at least one common
LHS attribute (Line 7). If two candidate FDs _X_ [′] → _A_ and
_X_ [′′] → _A_ satisfy _X_ [′] ∩ _X_ [′′] ≠∅, they can be placed in the same
group _g_ . For each group _g_ = { _X_ 1 → _A,X_ 2 → _A,...,Xk_ →
_A_ }, where _X_ 1 ∩ _X_ 2 ∩⋅⋅⋅∩ _Xk_ ≠∅, a common attribute with
uniform value distribution _B_ ∈ _X_ 1 ∩ _X_ 2 ∩⋅⋅⋅∩ _Xk_ is selected as
a common sort attribute (Line 9). This ensures balanced data
block sizes, thereby avoiding excessive memory consumption
from overly large blocks and frequent loading overhead from
too many small blocks. During preprocessing, data is sorted
by _B_ and can be divided into blocks _r_ = _r_ 1 ∪ _r_ 2 ∪⋅⋅⋅∪ _rm_,



**Algorithm 1:** EAIFD initial FD discovery


**Input:** Sample data _s_ and frequency threshold _θ_
**Output:** The complete set of valid FDs F, multi-attribute
hash table _MHT_, and sub-hypergraphs H

**1** F, ∆ _diff_, B initialize to empty sets

**2** Build H containing sub-hypergraphs for each attribute

**3 for** _A_ ∈ _R_ **do**

**4** **repeat**

**5** Update H by ∆ _diff_ and clear ∆ _diff_
**6** C _F Ds_ ← MMCS(H _A_ )

**7** G ← Group(C _F Ds_ )

**8** **foreach** _g_ ∈G **do**

**9** Select a common attribute _B_ of all LHSs of _g_

**10** **foreach** _block ri in r sorted by B_ **do**

**11** **foreach** _tuple t_ ∈ _ri_ **do**

**12** **foreach** _Xi_ → _A_ ∈ _g_ **do**

**13** _h_ ← _f_ ({ _t_ [ _Xi_ ]})

**14** B _X_ _[h]_ _i_ [←B] _X_ _[h]_ _i_ [∪{] _[t]_ [}]



**15** **foreach** B _Xi_ ∈B **do**



**16** **foreach** B _X_ _[h]_ _i_ [∈B] _[X]_ _i_ **[do]**



**17** **if** ∃ _t_ 1 _, t_ 2 ∈B _X_ _[h]_ _i_ _[with][ t]_ [1][[] _[A]_ [] ≠] _[t]_ [2][[] _[A]_ []]
**then**



**18** Compute difference sets of
( _t_ 1 _, t_ 2) and add to ∆ _diff_
**19** _g_ ← _g_ ∖{ _Xi_ → _A_ }

**20** **break**


**21** **if** _Xi_ → _A is valid_ **then**



**22** **foreach** B _X_ _[h]_ _i_ [∈B] _[X]_ _i_ **[do]**



**23** **if** _the count of tuples in_
B _X_ _[h]_ _i_ [≥] _[θ]_ [ × ∣] _[r]_ [∣] **[then]**



**24** Add ( _h, t_ [ _A_ ]) to
_MHT_ ( _X_ → _A_ )


5



Fig. 2: Overview of EAIFD and its components.



where all tuples in a block _ri_ share the same value for _B_ .
During validation, the dataset is loaded in the order sorted
by _B_ . Each block _ri_ is processed as follows (Lines 1025): (i) For each tuple _t_ ∈ _ri_, IGHV computes the hash
values _f_ ( _t_ [ _X_ 1]) _,f_ ( _t_ [ _X_ 2]) _,...,f_ ( _t_ [ _Xk_ ]) of the LHSs of all
candidate FDs in the current group _g_, and inserts them into the
corresponding hash bucket sets B _X_ 1 _,_ B _X_ 2 _,...,_ B _Xk_ (Lines 1114). (ii) For each candidate FD _X_ → _A_ ∈ _g_, it checks each
bucket in the hash bucket set B _X_ to validate if all tuples in
the bucket have the same value on attribute _A_ (Lines 15-20).
If a hash bucket contains tuples with different values on _A_, a
violated tuple pair ( _tp,tq_ ) is identified, where _tp_ [ _X_ ] = _tq_ [ _X_ ]
but _tp_ [ _A_ ] ≠ _tq_ [ _A_ ], indicating that _X_ → _A_ does not hold. The
difference set of these violated tuple pairs is added to the new
hyperedge set ∆ _diff_ for subsequent hypergraph updates. To
avoid redundant validation, the invalid candidate FD is then
removed (Lines 17-19).

After validating data block _ri_, IGHV clears the corresponding hash buckets (Line 25) and proceeds to validate the next
block. Once all data blocks have been processed, the validation
for this group ends, the valid FDs from the group are added
to F (Line 26). Before validating the next group, the newly
collected difference set ∆ _diff_ is used to update the partial subhypergraphs in H corresponding to the difference sets (Line 5).
MMCS then continues to generate new candidate FDs, which
undergo the same validation process until no new difference
sets are produced. Finally, the same procedure is applied to
the remaining attribute sub-hypergraphs.


_3) Constructing multi-attribute hash table:_ During _one-_
_time discovery process_, we introduce multi-attribute hash table
( _MHT_ ), which keeps LHS to RHS value mappings of valid
FDs, and integrates with IGHV to prune the search space and
accelerate candidate FD validation in incremental scenarios.

The _MHT_ is designed for FD validation and conflict
detection, based on the principle for a valid FD _X_ → _A_, all
tuples agreeing on _X_ must also agree on _A_ . For each tuple _t_
in _r_, a mapping item ( _f_ ( _t_ [ _X_ ]) _,t_ [ _A_ ]) is formed by applying
a hash function _f_ to the LHS projection _t_ [ _X_ ], generating
a hash key which is then paired with the RHS value _t_ [ _A_ ].
This approach reduces memory usage and speeds up lookups.
To ensure correctness, the hash function _f_ ensures that the
hash key uniquely identifies an equivalence class of tuples
with respect to _X_, for any tuples _ti,tj_ ∈ _r_ if _ti_ [ _X_ ] = _tj_ [ _X_ ],



then _f_ ( _ti_ [ _X_ ]) = _f_ ( _tj_ [ _X_ ]); and if _ti_ [ _X_ ] ≠ _tj_ [ _X_ ], then
_f_ ( _ti_ [ _X_ ]) ≠ _f_ ( _tj_ [ _X_ ]). The complete _MHT_ is built by
computing mapping items for all tuples, clearly reflecting the
value mapping from LHS to RHS of valid FDs. If _X_ → _A_
holds, each hash key _f_ ( _ti_ [ _X_ ]) must correspond to a unique
value _t_ [ _A_ ]. _MHT_ allows fast conflict detection, if a hash
key maps to distinct values, indicating a violated tuple pair
_ti,tj_ such that _ti_ [ _X_ ] = _tj_ [ _X_ ] but _ti_ [ _A_ ] ≠ _tj_ [ _A_ ]. In validation
phase, the _MHT_ supports _O_ ( _k_ ) time complexity, making it
particularly efficient for FD validation.


For large datasets with many candidate FDs, the number
of mapping items in _MHT_ may grow rapidly, leading to
high memory usage. For this reason, EAIFD employs a _high-_
_frequency mapping items preservation strategy_, caching only
the mapping items whose frequency in the dataset exceeds a
frequency threshold _θ_ . For any valid FD _X_ → _A_, IGHV counts
the mapping frequency between _X_ and _A_ (Lines 21-24). A
hash mapping item ( _h,a_ ) is cached in _MHT_ ( _X_ → _A_ ) if it
satisfies the frequency condition: [∣] _[t]_ [∈] _[r][j]_ [∣] _[r][j]_ [∈] _[r,f]_ [(] _[t]_ [[] ∣ _r_ _[X]_ ∣ [])=] _[h,t]_ [[] _[A]_ []=] _[a]_ [∣] ≥

_θ_ . The frequent mappings represent commonly observed value
associations between LHS and RHS.


The choice of the threshold _θ_ must balance a high threshold, which creates a sparse _MHT_ with a weak pruning
effectiveness, against a low threshold, which results in an
oversized _MHT_ and excessive memory use. In this paper,
we set _θ_ = 80% based on the three considerations. First,
adopting a high-frequency threshold to retain only the most
significant elements is well-established practice [26]. Second, value mappings with occurrence frequencies above 80%
can effectively capture the most representative attribute-value
combinations in real-life datasets. Third, the effectiveness of
this choice is further confirmed by the experimental results
presented later. This strategy allows for efficient pruning of
the validation space using high-frequency mapping items while
avoiding _MHT_ becoming excessively large, thereby achieving
a balance between performance efficiency and memory usage.


Once all sub-hypergraphs are processed, _one-time discovery_
_process_ ends. It derives F holding on _r_, multi-attribute hash
table _MHT_ for each valid FD _X_ → _A_ which contains only
high frequency value mapping items, and the hypergraph set
H containing sub-hypergraphs for each attribute.


6



Fig. 3: The process of IGHV. After grouping, select some C _F Ds_ in _g_ 2 and show its core validation steps on data blocks _r_ 1 and _r_ 4



_B. Incremental update mechanism on r_ ∪ ∆ _r_
When the initial dataset _r_ is updated with incremental data
∆ _r_, _incremental update mechanism_ is triggered to efficiently
update F and _MHT_ over dataset _r_ ∪ ∆ _r_ . Similar to _one-_
_time discovery process_, this mechanism also uses an iterative
process where candidate FDs are generated by MMCS and
validated by a two-step validation strategy. The mechanism iteratively updates hypergraphs until no new candidates are produced, completing the update of F. The complete procedure
of _incremental update mechanism_ is outlined in Algorithm 2.
_1) Generating candidate FDs with incremental data_ ∆ _r:_
After the incremental update, tuples in the incremental data ∆ _r_
are compared pairwise to generate a new difference set ∆ _diff_
(Line 1). Since the scale of ∣∆ _r_ ∣ is typically small, the time
cost of this operation remains acceptable. Based on ∆ _diff_,
the corresponding sub-hypergraphs are updated (Line 4), and
MMCS tree search is performed on the updated hypergraphs
to generate candidate FDs, which is also denoted as C _F Ds_
(Line 5). MMCS searches from the tree nodes corresponding
to the minimal hitting sets of the hypergraphs constructed from
the initial dataset _r_ obtained in the _one-time discovery process_
to avoid a full restart. MMCS first updates the hypergraphs by
∆ _diff_ as new hyperedges, and identifies the affected initial
minimal hitting sets that fail to cover these new hyperedges.
Subsequently, for each affected initial minimal hitting set,
MMCS selects candidate attributes from uncovered attributes
and extends it to generate new candidate FDs.
However, the candidate FDs C _F Ds_ obtained through the
MMCS are only guaranteed to hold locally on _r_ and ∆ _r_ but
not on _r_ ∪∆ _r_ . This is because the hypergraphs are constructed
solely from the difference sets generated from _r_ during the
_one-time discovery process_ and from pairwise comparisons
within incremental data ∆ _r_, and exclude difference sets derived from tuple pairs between _r_ and ∆ _r_ . A candidate FD
_X_ → _A_ from MMCS, valid on _r_ and ∆ _r_ separately, but may
be invalid on _r_ ∪ ∆ _r_ if tuples _ti_ ∈ _r_ and _tj_ ∈ ∆ _r_ satisfy
_ti_ [ _X_ ] = _tj_ [ _X_ ] but _ti_ [ _A_ ] ≠ _tj_ [ _A_ ]. Therefore, each candidate
FD in C _F Ds_ must undergo further validation to confirm its
validity on _r_ ∪ ∆ _r_ .
The candidate FD C _F Ds_ must hold on _r_ due to two reasons.
First, the _one-time discovery process_ iteratively expands partial
hypergraphs until their minimal hitting sets coincide with
those of the global hypergraph constructed from all minimal
difference sets of _r_, ensuring that F is complete for all



**Algorithm 2:** EAIFD incremental FD discovery


**Input:** ∆ _r_, F, _MHT_, H
**Output:** F, _MHT_
1: Compute difference sets of ∆ _r_ and add to ∆ _diff_
2: **for** _A_ ∈ _R_ **do**
3: **while** ∆ _diff_ ≠∅ **do**
4: Update H by ∆ _diff_ and then clear ∆ _diff_
5: C _F Ds_ ← MMCS(H _A_ )
6: C1 ←C _F Ds_ ∩F
7: C2 ←C _F Ds_ ∖C1
// validate candidate FDs in C1
8: res1 ← VCF(C1 _,_ “C1_ _MODE_ ”)
// validate candidate FDs in C2
9: res2 ← VCF(C2 _,_ “C2_ _MODE_ ”)
10: Update F, ∆ _diff_, _MHT_ by res1 and res2


11: **return** F _, MHT_


minimal non-trivial FDs on _r_ . It is demonstrated that, during
this iterative expansion, each minimal hitting set of the partial
hypergraph is either a minimal hitting set of the global hypergraph, or it can be extended into one [17]. Once the expansion
yields no new hyperedges, the sets of minimal hitting sets
in the partial and global hypergraphs are same, indicating
that all minimal non-trivial FDs on _r_ have been captured
in F. Second, during incremental updates, the hypergraphs
are dynamically updated with difference sets ∆ _diff_ from ∆ _r_ .
Unless ∆ _diff_ introduces new minimal hyperedges, the original
minimal hitting sets remain unchanged. It is proven [18] that
each new minimal hitting set in the updated hypergraph must
contain a previous one, implying it is a specialization of the
previous FD. This ensures that candidate FDs generated from
updated hypergraphs only exclude those in F invalidated by
∆ _r_, while all remaining and new candidates still hold on _r_ .
To improve validation efficiency, _incremental update mech-_
_anism_ divides C _F Ds_ into two categories: (i) minimal FDs that
hold on _r_, denoted as C1 = C _F Ds_ ∩F; and (ii) non-minimal FDs
that also hold on _r_, denoted as C2 = C _F Ds_ ∖C1, which can be
regarded as specializations of C1. Two categories are validated
separately based on the availability of _MHT_ (Lines 8-9). The
distinct validation mechanisms for C1 and C2 are detailed in
Section IV-B2 and Section IV-B3, outlined in Algorithm 3.
_2) Validating candidate FDs in_ C1 _with IGHV and MHT_ _:_
Since ∆ _r_ is typically much smaller than _r_, most updates only
affect the validity of a small portion of FDs. Most tuples still


Fig. 4: The process of validating C _F Ds_ in two types: C1 and C2.
For clarity, ∆ _r_ contains only 3 tuples, and two candidate FDs are
validated in each type.


support the original FDs, and most FDs still remain valid after
updates. As a result, C1 accounts for a high proportion of
the total candidate FDs. Therefore, improving the validation
efficiency of C1 has become key to optimizing the overall
efficiency.
To efficiently validate the candidates in C1 over _r_ ∪ ∆ _r_,
EAIFD adopts a two-step validation strategy. The first step,
_MHT_ _-based validation_, compares the local hash table built
on ∆ _r_ (denoted as _MHT_ ∆) with the pre-computed _MHT_
on _r_ to rapidly identify conflicts and efficiently prune the
validation space. Any candidates that cannot be resolved in
_MHT-based validation_ step proceed to the second step, _table-_
_scan validation_, which loads the relevant data blocks from
_r_ and applies IGHV for validation. Figure 4 illustrates how
EAIFD processes candidate FDs in C1.
_Step_ 1 _: MHT_ _-based validation._ For a candidate FD _X_ →
_A_, EAIFD first builds a local multi-attribute mapping hash
table on ∆ _r_, denoted as _MHT_ ∆( _X_ → _A_ ) (Line 3). Since
∣∆ _r_ ∣ is small, the memory usage for _MHT_ ∆( _X_ → _A_ ) is
manageable. After constructing _MHT_ ∆, _MHT_ _-based vali-_
_dation_ compares the mapping items in _MHT_ ∆( _X_ → _A_ ) with
_MHT_ ( _X_ → _A_ ) built on _r_ (Line 5). During the comparison,
the validation result for a candidate FD _X_ → _A_ must be one
of the following three cases.

  - _Valid._ If _MHT_ ∆( _X_ → _A_ ) ⊆ _MHT_ ( _X_ → _A_ ), all
mappings in _MHT_ ∆ already exist in _MHT_, meaning
∆ _r_ does not introduce new LHS to RHS value mappings.
_X_ → _A_ naturally holds on _r_ ∪ ∆ _r_ and is updated to F,
and is removed from C1 without further validation.

  - _Invalid._ If an item ( _f_ ( _tj_ [ _X_ ]) _,tj_ [ _A_ ]) in _MHT_ ∆( _X_ →
_A_ ) and an item ( _f_ ( _ti_ [ _X_ ]) _,ti_ [ _A_ ]) in _MHT_ ( _X_ → _A_ )
satisfy _f_ ( _ti_ [ _X_ ]) = _f_ ( _tj_ [ _X_ ]) but _ti_ [ _A_ ] ≠ _tj_ [ _A_ ], then
tuples _tj_ ∆ _r_ and _ti_ _r_ form a violated tuple pair.
∈ ∈
This pair shares the same hash key but has different RHS



7


values, thus _X_ → _A_ is invalid on _r_ ∪∆ _r_ and removed from
C1. The comparison of ( _ti,tj_ ) yields a new difference set,
which is added to ∆ _diff_ and used to update the relevant
sub-hypergraphs in H before the next iteration.

  - _Uncertain._ During the comparison between _MHT_ ∆( _X_ →
_A_ ) and _MHT_ ( _X_ → _A_ ), any mapping item ( _h,a_ ) in
_MHT_ ∆( _X_ → _A_ ) that also exists in _MHT_ is removed
to reduce redundancy. If _MHT_ ∆( _X_ → _A_ ) ≠∅ after
comparison, it indicates that _MHT_ ∆( _X_ → _A_ ) contains
mapping items that do not appear in _MHT_ ( _X_ → _A_ ).
Such items occur for two reasons: (i) they exist in the
initial dataset _r_ but their occurrence frequency does not
meet the frequency threshold _θ_, and thus are not recorded
in _MHT_ ; or (ii) they represent new LHS to RHS value
mappings introduced by ∆ _r_ . In this case, the validity of
_X_ → _A_ remains uncertain and triggers the _table-scan_
_validation_ .
_Step_ 2 _: table-scan validation._ After the _MHT-based valida-_
_tion_, only candidate FDs classified as _Uncertain_ require further
table-scan validation, which employs IGHV on the selectively
loaded data blocks from the initial dataset.
Specifically, the _table-scan validation_ selectively loads corresponding data blocks from _r_ based on LHS attribute values
in ∆ _r_ . We only need to detect potential conflicts between a
tuple from ∆ _r_ and a tuple from _r_ to check candidate FDs.
Conflicts may occur when tuples in _r_ contain the same LHS
attribute values as tuples in ∆ _r_, generating same hash keys
while having different RHS values. Since tuples within blocks
of _r_ that do not share the sort attribute value with any tuple
in ∆ _r_ cannot produce same hash keys with ∆ _r_ tuples, no
conflicts can be detected from them. Scanning such blocks
would only incur redundant I/O and computational costs. Since
∣∆ _r_ ∣ is typically small, the number of blocks to be loaded is
also limited, making this strategy highly effective in improving
validation efficiency while ensuring correctness.
_Table-scan validation_ first builds local _MHT_ ∆ for each
candidate FD on ∆ _r_ (Line 3). Then, the candidate FDs
are grouped using the same grouping strategy as employed
in the IGHV of the _one-time discovery process_ (Line 7).
After grouping, a sort attribute _B_ is selected for each group
(Line 9).When validating the candidate FDs in this group,
EAIFD only loads data blocks from _r_ sorted by _B_ and
whose values of _B_ also exist in ∆ _r_ . Formally, the set of
data blocks to be loaded is defined as: _r_ [′] = { _ri_ ∣ _ti_ ∈
_ri_ ∧ _ti_ [ _B_ ] ∈{ _tj_ [ _B_ ] ∣ _tj_ ∈ ∆ _r,f_ ( _tj_ [ _X_ ]) ∈ _K_ ∆}} where
_K_ ∆ = KeySet( _MHT_ ∆( _X_ → _A_ )) (Line 10). The method then
validates this group candidates on blocks in _r_ [′] instead of the
full _r_, sequentially performing batch validation on each block.
After obtaining relevant data block set _r_ [′], EAIFD sequentially loads each data block and performs batch validation on
the current block. For each candidate FD _X_ → _A_ in the group,
it first builds a local temporary _MHT_ ( _X_ → _A_ ) to record
the mappings from LHS hash key value to RHS value based
on the current data block tuples; then, it compares the local
temporary _MHT_ ( _X_ → _A_ ) and _MHT_ ∆( _X_ → _A_ ) (Line 13).
_X_ → _A_ is valid only if no conflict is found across all blocks
in _r_ [′] . Conversely, if there exist _tj_ ∆ _r_ and _ti_ _r_ satisfying
∈ ∈
_f_ ( _ti_ [ _X_ ]) = _f_ ( _tj_ [ _X_ ]) but _ti_ [ _A_ ] ≠ _tj_ [ _A_ ], the _X_ → _A_ is


**Algorithm 3:** Validate Candidate FDs (VCF)


**Input:** Candidate set C and validation mode _mode_
**Output:** _V alidF Ds_, ∆ _diff_
// When _mode_ equals "C1_ _MODE_ ": C = C1
// When _mode_ equals "C2_ _MODE_ ": C = C2
**1** _V alidF Ds_, ∆ _diff_ initialize to empty sets

**2 for** _each X_ → _A_ ∈C **do**

**3** Build _MHT_ ∆( _X_ → _A_ ) on ∆ _r_

**4** **if** _mode equals “_ C1 __MODE”_ **then**

**5** Check _MHT_ ∆( _X_ → _A_ ) and _MHT_ ( _X_ → _A_ ) and
collect result


**6** Update _V alidF Ds_, C, ∆ _diff_ by result

**7** G ← Group(C)

**8 for** _each g_ ∈G **do**

**9** Select a common attribute _B_ of all LHSs of _g_

**10** **for** _each block ri_ ∈ _r_ [′] **do**

**11** **for** _each X_ → _A_ ∈ _g_ **do**

**12** Build _MHT_ ( _X_ → _A_ ) on _ri_
**13** Check _MHT_ ∆( _X_ → _A_ ) and _MHT_ ( _X_ → _A_ )
and collect result


**14** Update _V alidF Ds_, ∆ _diff_ by compare result


**15 return** _V alidF Ds,_ ∆ _diff_


judged invalid, and the difference set of ( _ti,tj_ ) is added to
∆ _diff_ for hypergraph updating in the next iteration. After each
block validation, mapping items in the local temporary _MHT_
with frequency exceeding _θ_ are recorded and updated to the
global complete _MHT_, and then local temporary _MHT_ is
cleared to reduce memory usage. After processing all blocks
in _r_ [′], the valid FDs and the new difference set are recorded
(Line 14). The procedure repeats for all groups (Lines 814). After processing all groups, the validation concludes by
returning the final _V alidF Ds_ and ∆ _diff_ (Line 15).
Thus, through _MHT_ _-based comparison_ between _MHT_ ∆
and _MHT_, followed by selective _table-scan validation_ using
IGHV on loaded data blocks, EAIFD efficiently validates C1
and identifies minimal FDs that remain valid.
_3) Validating candidate FDs in_ C2 _with IGHV:_ For candidate FDs in C2, which hold on _r_ but are non-minimal,
no corresponding mapping items exist in _MHT_ since it
only stores value mappings for minimal and non-trivial FDs.
EAIFD validates them using the same procedure as the _table-_
_scan validation_ described in Section IV-B2 for the uncertain
candidate FDs in C1, loading data blocks from _r_ containing
attribute values present in ∆ _r_ . This approach ensures correctness while avoiding redundant computation. The process is
illustrated in Fig. 4.
After validating C1 and C2, EAIFD updates FDs that hold
on _r_ ∪ ∆ _r_ into F and returns a difference set ∆ _diff_ from
the violated tuple pairs. This ∆ _diff_ updates the relevant
hypergraphs in H for the next iteration.


_C. Algorithm analysis_


To establish the theoretical soundness and efficiency of
EAIFD algorithm, this part first provides a formal proof
of correctness and then a detailed complexity analysis. The
former guarantees the accuracy of the algorithm, while the
latter quantifies its performance advantages.



8


_1) Correctness Proof:_ EAIFD involves candidate generation and validation in both _one-time discovery process_ and
_incremental update mechanism_, with overall correctness relying on two steps. For candidate generation, its correctness is
guaranteed in three aspects: (i) completeness, (ii) minimality,
and (iii) non-triviality.
**Completeness.** For each RHS attribute _A_, the _one-time_
_discovery process_ builds a sub-hypergraph H _A_ and iteratively
updates it with new hyperedges derived from violated tuple
pairs until no new hyperedges can be generated. As proved
by Lemma IV.1, Corollaries IV.1 and IV.2, when no new
hyperedges are generated, H _A_ has same minimal hitting sets as
the global hypergraph constructed from all minimal difference
sets in _r_ . Hence, H _A_ guarantees complete candidate FD
generation. MMCS enumerates all minimal hitting sets of
H _A_, each corresponding to a valid minimal FD _X_ → _A_ per
Definition III.4.
Completeness in the _incremental update mechanism_ relies
on the same principle of iterative hypergraph refinement used
in the _one-time discovery process_ . This mechanism starts with
H _A_, upon which completeness in _one-time discovery process_
is proven, and updates it iteratively by adding two types of new
difference sets: those from pairwise comparisons within ∆ _r_,
and those generated by violated tuple pairs between ∆ _r_ and _r_ .
Once no new hyperedges appear, Lemma IV.1 and Corollaries IV.1–IV.2 ensure that MMCS enumerates the same minimal
hitting sets as those derived from all minimal difference sets
over _r_ ∪ ∆ _r_, thereby guaranteeing completeness in candidate
FD generation in the _incremental update mechanism_ .

**Lemma IV.1.** _For any two hypergraphs_ H _and_ H [′] _,_ H ≺H [′] _if_
_and only if HS_ (H) ⊇ _HS_ (H [′] ) _. [24]_


**Corollary IV.1.** _A minimal hitting set X for the partial_
_hypergraph_ P _A is a global minimal FD (i.e., X_ ∈ _HS_ (D _A_ ) _)_
_if and only if the FD X_ → _A holds on the dataset. [24]_


**Corollary IV.2.** _When the algorithm iteration produces no_
_new difference sets (i.e., new hyperedges), the minimal hitting_
_sets HS_ (P _A_ ) _of the current hypergraph_ P _A constitute all_
_minimal FDs, i.e., HS_ (P _A_ ) = _HS_ (D _A_ ) _. [24]_


**Minimality.** As stated in Property III.1, MMCS guarantees
the minimality of candidate FDs in both the _one-time discov-_
_ery process_ and the _incremental update mechanism_, since it
enumerates only minimal hitting sets. Each hitting set _X_ on
H _A_ directly corresponds to a minimal FD _X_ → _A_ .
**Non-triviality.** To ensure non-triviality, _one-time discovery_
_process_ constructs sub-hypergraph H _A_ using difference sets
generated from tuple pairs different on _A_, while excluding
_A_ . _One-time discovery process_ and the _incremental update_
_mechanism_ update H _A_ with new difference sets generated
from tuple pairs different on _A_, also excluding _A_ . MMCS then
enumerates hitting sets on H _A_ . According to Definitions III.3
and III.4, since _A_ ∉ _X_ and _X_ determines _A_, the FD _X_ → _A_
is non-trivial.
The second essential step ensuring correctness of EAIFD is
candidate validation, whose validation strategies in both phases
accurately determine the validity of candidates and identify all
violated tuple pairs necessary for hypergraph refinement.


9



_One-time discovery process_ employs IGHV to validate
candidate FDs. IGHV first groups the candidates and selects
a common sort attribute for each group. It then sequentially
loads data blocks sorted by this attribute and performs validation within each block. Under the hash function _f_, tuples
with identical LHS values share the same hash value, while
those with different LHS values have distinct hashes, ensuring
that tuples with equal hashes are located in the same block
after sorting. Consequently, validation only needs to examine
tuple pairs within the current block for possible violations,
which localizes comparisons and greatly reduces cost while
preserving correctness. If any block contains violated tuple
pairs, the FD is deemed invalid; IGHV generates difference
sets to update the hypergraphs. If no conflicts are found across
all blocks, the FD is considered valid.
_Incremental update mechanism_ ensures validation correctness by a two-step validation strategy. Candidate FDs are
divided into C1 and C2 and validated through distinct methods.
Candidates in C1 are rapidly validated by the _MHT-based_
_validation_ through comparing items in _MHT_ ∆ with the items
in _MHT_ . Since _MHT_ stores the high-frequency hash LHS
value to RHS value mapping items of candidate FDs from
_r_ and _MHT_ ∆ stores all hash LHS value to RHS value
mapping items from ∆ _r_, the comparison between them can
accurately detect violated tuple pairs between ∆ _r_ and _r_ . The
remaining candidate FDs in C1 that cannot be validated by
_MHT-based validation_, along with C2, are validated using
_table-scan validation_, which performs IGHV through selective
loading of data blocks. After grouping the candidate FDs
and selecting a common sort attribute, this method does not
load all data blocks of _r_ . Instead, it selectively loads only
those blocks containing tuples that share the same value as
the tuples in ∆ _r_ on the common sort attribute. Only tuples
within these blocks may share the same LHS hash values as
the incremental data, thus requiring validation of their RHS
attributes to detect violated tuple pairs. Since tuples in other
blocks have different sort values and thus distinct LHS hashes,
they are safely skipped. This method guarantees correctness
while substantially reducing I/O and computation cost.
Therefore, based on the demonstrated correctness of both
candidate generation and validation steps, the EAIFD algorithm is proven to be correct.
_2) Complexity Analysis:_ This part comprehensively analyzes the time complexity of EAIFD, covering both _one-time_
_discovery process_ and _incremental update mechanism_, and
concludes with the analysis of space complexity. We consider
a dataset _r_ with _n_ tuples and _m_ attributes, and the incremental
data ∆ _r_ with _n_ ∆ tuples.
**Time complexity.** We begin with the _one-time discovery_
_process_ . Its preprocessing involves sorting _r_ by each attribute
for subsequent validation, taking _O_ ( _m_ × _n_ log _n_ ) time. Then,
tuples in sample data _s_ are compared pairwise to compute
the difference sets with complexity _O_ (∣ _s_ ∣ [2] ). This avoids the
_O_ ( _m_ × _n_ [2] ) complexity of DHSFD requiring all pairwise tuple
comparisons and enables rapid initialization of EAIFD.
Following preprocessing, _one-time discovery process_ iteratively performs candidate generation and validation for each
RHS attribute _A_ . Within each iteration, candidate generation



loaded blocks contain roughly _n_ ∣ _V_ [∆] ∣ [tuples. Candidates are]

pruned immediately upon invalidation. Let _b_ [′] be the average
number of blocks validated per candidate. The total cost of per
group containing ∣G∣ _k_ [candidates is] _[ O]_ [ (] ∣G∣ _[k]_ [×] _[ b]_ [′][ ×] ∣ _Vn_ ∣ [)][, giving]

a total complexity of _O_ ( _k_ × ( _n_ ∆ + _b_ [′] × ∣ _Vn_ ∣ [))][. Compared to]
_O_ ( _k_ × _b_ × ∣ _Vn_ ∣ [)][ complexity of validating] _[ k]_ [ candidates using]
IGHV in the _one-time discovery process_, _table-scan validation_



relies on MMCS algorithm. As proved in [25], the time complexity for a single search step of MMCS on sub-hypergraph
H _A_ = ( _R,Dr_ _[A]_ [)][ is] _[ O]_ [(∣H] _[A]_ [∣)][, where][ ∣H] _[A]_ [∣= ∑] _D_ ∈ _Dr_ _[A]_ [∣] _[D]_ [∣] [denotes]
the total size of hyperedges. Discovering one minimal hitting
set typically requires multiple MMCS search steps, with most
search steps used only for state updates or pruning. Generating
_k_ minimal hitting sets in an iteration takes time complexity of
_O_ ( _k_ × ∣H _A_ ∣× _α_ ), where _α_ is the average number of search
steps to discover one minimal hitting set.
In the same iteration, candidate validation employs IGHV
to validate the _k_ candidate FDs generated by MMCS. In the
worst case, each candidate FD needs to be validated across all
data blocks with complexity _O_ ( _k_ × _n_ ), while candidates are
pruned immediately upon conflict. Let _b_ be the average number
of blocks validated per candidate FD and ∣ _V_ ∣ the distinct
values of the common sort attribute in _r_ for the group. With
approximate block size ∣ _Vn_ ∣ [, the validation cost per candidate]
FD is _O_ ( _b_ × ∣ _V_ _[n]_ ∣ [)][, yielding average complexity] _[ O]_ [(] _[k]_ [ ×] _[ b]_ [ ×] ∣ _V_ _[n]_ ∣ [)]



FD is _O_ ( _b_ × ∣ _V_ _[n]_ ∣ [)][, yielding average complexity] _[ O]_ [(] _[k]_ [ ×] _[ b]_ [ ×] ∣ _V_ _[n]_ ∣ [)]

for _k_ candidates.
Combining the candidate generation and validation, the time
complexity for one iteration is _O_ ( _k_ ×(∣H _A_ ∣× _α_ + _b_ × ∣ _V_ _[n]_ ∣ [))][. The]




_[n]_ _[n]_

∣ _V_ ∣ [)][, yielding average complexity] _[ O]_ [(] _[k]_ [ ×] _[ b]_ [ ×] ∣ _V_



complexity for one iteration is _O_ ( _k_ ×(∣H _A_ ∣× _α_ + _b_ × ∣ _V_ _[n]_ ∣ [))][. The]

total cost of _one-time discovery process_ can be obtained by
multiplying this one iteration cost by the number of iterations,
which is dependent on the underlying data.
Next, we analyze the time complexity of _incremental update_
_mechanism_ . This mechanism first computes new difference
sets from pairwise comparisons of tuples in ∆ _r_, with time
complexity _O_ ( _n_ [2] ∆ [)][. It then iteratively generates candidate FDs]
using MMCS and validates them. Since the complexity of
MMCS is discussed previously, our focus here is on validation.
Candidate FDs are divided into C1 and C2, and a two-step
validation strategy is applied. We then formally characterize
the time complexity of both validation steps for _k_ candidates.
For candidate FDs in C1, _MHT-based validation_ constructs
a local _MHT_ ∆ for each _X_ → _A_ in ∆ _r_ at a cost of _O_ ( _k_ × _n_ ∆).
Comparing _MHT_ ∆ with _MHT_ requires traversing _MHT_ ∆
items and querying _MHT_, which takes _O_ (1) per candidate
FD due to hash table lookups, yielding _O_ ( _k_ ) total for _k_
candidates. The overall complexity for validating _k_ candidates
in C1 is _O_ ( _k_ × ( _n_ ∆ + 1)), which is highly efficient since _n_ ∆
is typically small.
_Table-scan validation_ handles uncertain candidates in C1
and all candidate FDs in C2. Assuming a total of _k_ candidates, a local _MHT_ ∆ is first built for each FD on ∆ _r_,
costing _O_ ( _k_ × _n_ ∆). Candidates are grouped by common LHS
attributes, and each group is validated sequentially. Unlike the
_one-time discovery process_ that loads all blocks, _table-scan_
_validation_ selectively loads only blocks containing attribute
values in ∆ _r_ . Let _V_ ∆ and _V_ denote the distinct values of
the common sort attribute in ∆ _r_ and _r_, respectively. The
loaded blocks contain roughly _n_ × [∣] _[V]_ ∣ _V_ [∆] ∣ [∣] [tuples. Candidates are]




_[k]_ _n_

∣G∣ [×] _[ b]_ [′][ ×] ∣ _V_ ∣ [)][, giving]


is more efficient since _n_ ∆ ≪ _n_ and _b_ [′] ≪ _b_ .
**Space complexity.** EAIFD uses _MHT_ as its core auxiliary structure to accelerate candidate FD validation while
controlling memory. Without filtering, each FD could store
up to _n_ hash entries, yielding a worst-case space complexity
of _O_ (∣F∣× _n_ ). To reduce memory, EAIFD applies a _high-_
_frequency mapping preservation_ with threshold _θ_ ∈(0 _,_ 1),
caching only hash keys _h_ satisfying _[count]_ [(] _[h]_ [)] ≥ _θ_ . Let _p_ denote



caching only hash keys _h_ satisfying _[count]_ _n_ [(] _[h]_ [)] ≥ _θ_ . Let _p_ denote

the cached items per FD; since _p_ ×( _θ_ × _n_ ) ≤ _n_, we have _p_ ≤ _θ_ [1] [,]



the cached items per FD; since _p_ _θ_ _n_ _n_, we have _p_ _θ_ [,]

giving an upper bound of _O_ ( _θ_ [1] [)][ per FD. Consequently, the]



giving an upper bound of _O_ ( _θ_ [1] [)][ per FD. Consequently, the]

overall space complexity is _O_ ( [∣F∣] _θ_

[)][, independent of dataset size]



overall space complexity is _O_ ( [∣F∣] _θ_

[)][, independent of dataset size]
_n_ and scalable for large datasets, enabling EAIFD to maintain
controllable memory usage even for large-scale datasets.
In summary, EAIFD reduces preprocessing to _O_ ( _m_ ×
_n_ log _n_ ), avoiding the _O_ ( _m_ × _n_ [2] ) cost of pairwise comparisons and enabling rapid initialization. Its optimized validation achieves near-linear time in both the _one-time discovery_
_process_ and _incremental update mechanism_ . Along with the
_high-frequency mapping items preservation strategy_, space
complexity is bounded by _O_ ( [∣F∣] _θ_

[)][, allowing EAIFD to main-]



complexity is bounded by _O_ ( [∣F∣] _θ_

[)][, allowing EAIFD to main-]
tain correctness and scale efficiently on incrementally updated
datasets.



V. PERFORMANCE EVALUATION

This section evaluates the performance of EAIFD on a
DELL OptiPlex Tower Plus 7010 desktop PC equipped with
an Intel(R) Core(TM) i7-13700 CPU (16 cores, 3.00 GHz),
32 GB of RAM, running 64-bit Windows 11. All experiments
are implemented in Java (JDK 8).
**Datasets.** We evaluate EAIFD on 20 different real datasets
to assess its practical performance. Table I summarizes their
properties, which differ in tuple number, attribute number,
and data distribution, enabling a comprehensive performance
analysis. All real datasets are publicly available [3] .
**NULL Semantics.** In relational databases, NULL represents
missing or inapplicable values, which are common in realworld datasets. Different NULL-handling strategies impact the
validity of related FDs. We adopt the NULL-equals-NULL
semantics, treating all missing values as equal, which is the
common default in FD discovery [27], [16].
**Competitors.** The static FD discovery algorithm FDHITS [11] and two incremental FD discovery algorithms,
DynFD [17] and DHSFD [18], are selected for comparison [4] .
FDHITS outperforms other static algorithms [11]. DynFD
(an attribute-based incremental algorithm) maintains positive
and negative covers to generate and validate candidate FDs
via PLIs, while DHSFD (a tuple-based incremental algorithm) formulates FD discovery as a hitting set enumeration
problem over hypergraphs of difference sets with a dynamic
enumeration strategy. Comparing these three algorithms with
EAIFD provides a comprehensive evaluation of its incremental
effectiveness.
**Parameters.** EAIFD performance is influenced by two
parameters: the sampling ratio _ε_ and the mapping frequency


[3Datasets are available at https://hpi.de/naumann/projects/repeatability/](https://hpi.de/naumann/projects/repeatability/data-profiling/fds.html)
[data-profiling/fds.html and https://www.kaggle.com/datasets.](https://hpi.de/naumann/projects/repeatability/data-profiling/fds.html)
[4All codes are obtained from https://hpi.de/naumann/projects/repeatability/](https://hpi.de/naumann/projects/repeatability/data-profiling/fds.html)
[data-profiling/fds.html and https://github.com/RangerShaw/DHSFD.](https://hpi.de/naumann/projects/repeatability/data-profiling/fds.html)



10


threshold _θ_ . Given a dataset with _n_ records, the total number of record pairs is _p_ = ( _[n]_ 2 [)][. EAIFD uniformly samples]
approximately _p_ _[ε]_ record pairs (0 < _ε_ < 1) without replacement
to build initial partial hypergraphs. Prior studies indicate that
setting _ε_ = 0 _._ 3 achieves a good balance between coverage
and efficiency [24], [11]. The mapping frequency threshold _θ_
determines how many items are stored in the _MHT_, thus
controlling pruning strength and memory use. Experiments
show _θ_ = 80% provides an effective balance. Thus, we adopt
_ε_ = 0 _._ 3 and _θ_ = 80% as default settings.
**Other settings.** From each dataset, we randomly select 80%
of tuples as _r_ and generate ∆ _r_ from the remaining 20% as in
DHSFD [18]. All times are in seconds unless specially stated;
TL indicates a run exceeding the limit of 5 hours. Results
are averaged over three executions, with the machine restarted
each time.
**Experimental Structure.** We conduct comprehensive experiments to evaluate the efficiency, scalability, and unique
features of EAIFD. Experiment 1 on real-life datasets demonstrates the performance advantages of EAIFD. Experiment
2, 3 and 4 analyze scalability across tuple number, attribute
number, and incremental data size. Experiment 5 compares
EAIFD against the current state-of-the-art static algorithm
in incremental updates. Experiments 6, 7 and 8 respectively
demonstrate the key advantages of EAIFD: rapid initialization,
a memory-light _MHT_ with _high-frequency mapping items_
_preservation strategy_, and the effect of _θ_ across a wide range.
**Exp** 1 **: Overall performance comparison.** Experiment 1
evaluates EAIFD against current incremental FD discovery
algorithms on 20 real-world datasets, with [∣][∆] ∣ _r_ _[r]_ ∣ [∣] ranging from

10% to 30%, _ε_ = 0 _._ 3, and _θ_ = 80%. As reported in Table I.
EAIFD is consistently faster than DynFD and DHSFD across
all insertion proportions.

Against DynFD, EAIFD achieves speedups of up to two
orders of magnitude on several datasets (e.g., Plista). The
advantages are particularly significant on larger datasets like
Flights, Census, Fd-Reduced-30, and CAB, where DynFD
consistently exceeds the time limit (TL). This confirms the
scalability limitations of DynFD due to its layer-by-layer
candidate generation and PLI-based validation. In contrast,
EAIFD transforms candidate generation as a hitting set enumeration problem on hypergraphs and employs a two-step
validation strategy, achieving a higher scalability.

Although both DHSFD and EAIFD can handle large-scale
datasets, EAIFD runs nearly an order of magnitude faster than
DHSFD on several datasets (e.g., Nursery). The significant
speedup validates the effectiveness of two-step validation
strategy in EAIFD. _MHT-based validation_ rapidly processes
the majority of candidates (C1) using efficient _O_ (1) hash
lookups. Subsequently, _table-scan validation_ selectively loads
data blocks and validates the remaining FDs in batch. This
strategy substantially improves overall efficiency.
**Exp** 2 **: Scalability with tuple number** ∣ _r_ ∣ **.** Experiment 2
evaluates the row scalability of EAIFD, DynFD, and DHSFD
on the CAB and Pitches datasets by varying ∣ _r_ ∣, as shown
in Fig. 5(a) and Fig. 5(d). We keep [∣][∆] ∣ _r_ _[r]_ ∣ [∣] = 20% and fix ∣ _R_ ∣

(54 for CAB, 40 for Pitches). Specifically, ∣ _r_ ∣ ranges from


11


TABLE I: The experimental results on real-life datasets. Runtime in seconds for EAIFD compared with two incremental algorithms DynFD
and DHSFD. The fastest runtimes are highlighted in bold. TL indicates that the algorithm exceeded the time limit.


Dataset Properties Incremental (∣∆ _r_ ∣= 10% ∣ _r_ ∣) Incremental (∣∆ _r_ ∣= 20% ∣ _r_ ∣) Incremental (∣∆ _r_ ∣= 30% ∣ _r_ ∣)
Dataset ∣ _r_ ∣ ∣ _R_ ∣ ∣F∣ DynFD DHSFD EAIFD DynFD DHSFD EAIFD DynFD DHSFD EAIFD
Bridges 108 13 142 0.2 0.0013 **0.001** 0.23 0.004 **0.003** 0.28 0.008 **0.005**
Balance 625 5 1 0.13 0.03 **0.006** 0.15 0.06 **0.007** 0.21 0.071 **0.009**
Abalone 4177 9 137 0.1 0.03 **0.008** 0.21 0.07 **0.011** 0.31 0.094 **0.024**
Iris 147 5 4 0.166 0.03 **0.009** 0.21 0.037 **0.009** 0.2 0.049 **0.012**
Nursery 13000 9 1 0.8 0.17 **0.015** 1.1 0.21 **0.018** 1.6 0.235 **0.027**
NCV 1000 19 758 0.3 0.04 **0.02** 0.5 0.09 **0.05** 0.54 0.11 **0.07**
Hepatitis 155 20 8250 1.2 0.25 **0.07** 2.5 0.3 **0.09** 2.8 0.384 **0.121**
Claim 20000 11 12 0.42 1.8 **0.27** 0.47 2.5 **0.45** 0.7 3.5 **0.507**
Letter 20000 17 61 14 1.5 **0.53** 22.1 1.9 **0.77** 28 2.6 **1.21**
Plista 1001 63 173409 143.4 4.6 **0.5** 147.5 7.5 **1.3** 145.1 12.8 **3.2**
Horse 300 29 128726 30.1 2.1 **0.9** 33 4.1 **1.8** 34.5 5.5 **3.906**
Bioentry 184292 9 19 69 5.6 **2.6** 83 14.8 **8.4** 103 34.1 **17.9**
Tax 1000000 15 263 346 52 **29** 387 93 **57** 494 176 **87**
Ditag_feature 3960124 13 58 TL 358 **44** TL 1439 **287** TL 5204 **743**
Fd-Reduced-30 250000 30 89571 TL 144 **76** TL 367 **174** TL 583 **297**
Flight 250000 31 117367 TL 190 **83** TL 410 **198** TL 630 **410**
Pitches 250000 40 608928 TL 310 **132** TL 600 **420** TL 920 **780**
CAB 67300 54 3353531 TL 330 **197** TL 700 **580** TL 1350 **990**
Census 196000 42 41861 TL 341 **206** TL 622 **461** TL 1077 **845**
Lineitem 6000000 16 3984 TL 1308 **968** TL 3314 **2767** TL 7123 **6594**



10 _,_ 000 to 14 _,_ 000 (∣∆ _r_ ∣ from 2 _,_ 000 to 2 _,_ 800) for CAB, and
from 10 _,_ 000 to 50 _,_ 000 (∣∆ _r_ ∣ from 2 _,_ 000 to 10 _,_ 000) for Pitches.
Runs exceeding 5 hours are omitted.
DynFD fails to complete within the time limit on both
datasets because its PLI-based validation cost grows rapidly
with ∣ _r_ ∣, resulting in poor row scalability. In contrast, DHSFD
and EAIFD scale more gently with increasing ∣ _r_ ∣, showing
stable performance. Notably, EAIFD consistently outperforms
DHSFD. The row scalability of DHSFD is limited by the
increasing costs associated with updating PLI, computing
new difference sets, and refining hypergraphs as ∣ _r_ ∣ increases.
By updating the hypergraph through difference sets derived
from tuple pairs in ∆ _r_, EAIFD avoids redundant full-pair
comparisons involving _r_ . Then, two-step validation does not
scan the initial dataset but leverages _MHT_ and loads relevant
data blocks for validation, ensuring high row scalability and
stable performance as ∣ _r_ ∣ increases.
**Exp** 3 **: Scalability with attribute number** ∣ _R_ ∣ **.** Experiment 3 evaluates the column scalability of EAIFD, DynFD,
and DHSFD on the CAB and Pitches datasets by varying ∣ _R_ ∣,
with ∣ _r_ ∣ and ∣∆ _r_ ∣ fixed (10 _,_ 000 and 2 _,_ 000 for CAB; 50 _,_ 000
and 10 _,_ 000 for Pitches). Fig. 5(b) and Fig. 5(e) show results
when varying ∣ _R_ ∣ from 34–54 for CAB and 20–40 for Pitches,
respectively.
DynFD exhibits poor column scalability on both datasets.
On CAB, its runtime rises sharply with ∣ _R_ ∣ (713s-8 _,_ 831s) and
fails to finish within the limit at ∣ _R_ ∣= 54. On Pitches, DynFD
failed to complete within the time limit across all tested
attribute numbers (∣ _R_ ∣ from 20 to 40). This severe performance
degradation stems from its reliance on PLI-based validation,
where increasing attribute numbers lead to a rapid growth in
both candidate number and set intersection operations.
As observed in Fig. 5(b) and (e), both tuple-based algorithms, DHSFD and EAIFD, indicate significantly better column scalability than column-based DynFD, exhibiting moderate runtime growth as ∣ _R_ ∣ increases. EAIFD consistently out


performs DHSFD, showing lower runtime and growth trend.
The performance of DHSFD is still affected by the increasing
complexity of updating PLI, difference sets and hypergraphs
as more attributes are involved. In contrast, EAIFD achieves
a superior column scalability due to two key choices. For
one thing, EAIFD prevents excessive candidates growth by
modeling FD generation as hitting set enumeration on partial
hypergraph and generating new candidates selectively. For
another, its two-step validation strategy replaces costly PLIbased set intersections by _MHT_ structure for rapid validation,
and then validating the remaining candidate FDs in batches
over relevant data blocks, ensuring high column scalability.

**Exp** 4 **: Scalability with incremental size** ∣∆ _r_ ∣ **.** Experiment
4 evaluates the incremental scalability of EAIFD, DynFD, and
DHSFD on CAB (∣ _r_ ∣= 10 _,_ 000, ∣ _R_ ∣= 54, ∣∆ _r_ ∣= 2 _,_ 000–10 _,_ 000,
∣∆ _r_ ∣

_r_ = 20%  - 100%) and Pitches (∣ _r_ ∣= 50 _,_ 000, ∣ _R_ ∣= 40, ∣∆ _r_ ∣=
10 _,_ 000–22 _,_ 500, [∣][∆] _r_ _[r]_ [∣] = 20% − 45%) as shown in Fig. 5(c) and

Fig. 5(f).


On both datasets, DynFD fails to complete within the
time limit, confirming its poor scalability, as its PLI-based
validation across _r_ ∪ ∆ _r_ becomes prohibitive. The runtimes of
DHSFD and EAIFD both grow noticeably as the incremental
ratio [∣][∆] _r_ _[r]_ [∣] increases, incurred by higher costs in difference set

computation, hypergraph refinement and validation operation.
However, EAIFD consistently outperforms DHSFD and exhibits a lower growth rate in runtime. The key advantage for
EAIFD stems directly from two aspects. First, EAIFD avoids
enumerating all cross-set pairs and updates difference sets
within ∆ _r_, while DHSFD computes difference sets between
∆ _r_ and _r_ . Second, a two-step validation strategy enables
efficient candidate validation. Even when [∣][∆] _r_ _[r]_ [∣] increases, MHT
based validation in EAIFD handles most C1 candidates efficiently via rapid conflict detection. The subsequent table-scan
validation only loads relevant data blocks for the remaining
candidates in batch, reducing the unnecessary I/O cost.


12











Fig. 5: Time performance of EAIFD compared with DynFD, and DHSFD on the CAB and Pitches. Subfigures (a, b, c) correspond to CAB
with varying tuple number ∣ _r_ ∣, attribute number ∣ _R_ ∣, and incremental data size ∣∆ _r_ ∣, respectively, while (d, e, f) show the same for Pitches.







































Fig. 6: Time performance of EAIFD compared with FDHITS across
∣∆ _r_ ∣
different incremental ratios _r_ [. Subfigures (a) and (b) correspond]

to Lineitem and Ditag_feature, respectively.



**Exp** 5 **: EAIFD against static FD discovery algorithm.**
Experiment 5 evaluates the performance of EAIFD and FDHITS (a state-of-the-art static algorithm) on Lineitem (∣ _r_ ∣=
6 _,_ 000 _,_ 000, ∣ _R_ ∣= 16) and Ditag_feature (∣ _r_ ∣= 3 _,_ 960 _,_ 124, ∣ _R_ ∣=
13). For FDHITS, _sep_ variant is selected over _joint_ variant for
two reasons. First, the original paper [11] demonstrates that
_sep_ and _joint_ variants have highly competitive performance,
while the former outperforms the latter on many real-world
datasets. Second, the performance of _joint_ variant is highly
sensitive to sampled data characteristics, while _seq_ variant
provides more stable and predictable performance. We fix ∣ _r_ ∣
and ∣ _R_ ∣, and vary incremental ratios [∣][∆] _r_ _[r]_ [∣] across a wide range



and _R_, and vary incremental ratios _r_ across a wide range

(0 _._ 01% _,_ 0 _._ 05% _,_ 0 _._ 1% _,_ 0 _._ 5% _,_ 1% _,_ 5% _,_ 10% _,_ 15%) to identify a
performance break-even point between EAIFD and FDHITS.

The results in Figure 6(a) and (b) show two distinct trends.
The runtime of FDHITS is high but grows modestly across all
incremental ratios, since its runtime is determined by the total
dataset size (∣ _r_ ∣+ ∣∆ _r_ ∣). In contrast, EAIFD shows significant
∣∆ _r_ ∣
efficiency advantages in small increments (Lineitem: ≤



∣∆ _r_ ∣
efficiency advantages in small increments (Lineitem: _r_ ≤

5%; Ditag_feature: [∣][∆] _[r]_ [∣] ≤ 10%), achieving much lower run



_[r]_ [∣]

_r_ ≤ 10%), achieving much lower run


time. As the incremental ratio increases, the runtime of EAIFD
grows rapidly and eventually exceeds that of FDHITS. This
is because the runtime of EAIFD is primarily determined by
the incremental data size (∣∆ _r_ ∣). Nevertheless, this experiment
confirms the practical value of EAIFD: in typical applications,
the incremental update ∆ _r_ typically constitutes a small portion
of the large and growing data set _r_ . Thus, EAIFD consistently
stays on the favorable side of the performance break-even
point, making it more suitable for real-world scenarios.


TABLE II: Preprocessing of three algorithms on different datasets.


**Dataset Properties** **Preprocessing Time**
**DataSet** **∣** _**r**_ **∣** **∣** _**R**_ **∣** **∣F∣** **DynFD** **DHSFD** **EAIFD**
Iris 147 5 4 0.06s 0.03s **0.017s**
Bridges 108 13 142 0.15h 1.1s **0.77s**
Hepatitis 155 20 8250 0.45h 1.6s **0.88s**
Horse 300 29 128726 5.1h 2.5s **0.63s**
Balance 625 5 1 **0.02s** 0.62s 0.03s
NCV 1000 19 758 67.4s 4.6s **0.7s**
Plista 1001 63 173409 5.5h 6.7s **2.8s**
Claim 20000 11 12 128s 123s **62s**
Letter 20000 17 61 164s 145s **92s**
CAB 67300 54 3353531 6.5h 1.34h **0.77h**
Bioentry 184292 9 19 0.25h 0.42h **0.12h**
Flight 250000 31 117367 6.6h 3.45h **0.34h**
Pitches 250000 40 608928 7.9h 5.6h **0.56h**
Ditag_Feature 3960124 13 58 7.2h 7.4h **0.67h**


**Exp** 6 **: The preprocessing time of algorithms.** Experiment 6 evaluates the one-time preprocessing cost of EAIFD,
DynFD, and DHSFD on real-world datasets. The startup mechanisms of the three algorithms differ significantly. DynFD
computes positive and negative covers from the initial dataset.
DHSFD compares all tuple pairs to build the hypergraph.
In contrast, EAIFD first sorts the dataset by attributes and
performs pairwise comparisons only on sampled data.
Experimental results in Table II show that EAIFD achieves
shortest preprocessing times, often over an order of magnitude


faster on large datasets. DynFD is slow on large datasets with
many FDs, while DHSFD can take several hours on long
datasets with many tuples, making startup virtually impractical
on very large datasets. EAIFD avoids building auxiliary structures such as PLIs or positive and negative covers. Instead, it
sorts each attribute ( _O_ (∣ _R_ ∣×∣ _r_ ∣ log ∣ _r_ ∣)) and performs pairwise
comparisons only on a small sampled dataset ( _O_ (∣ _s_ ∣ [2] )). The
limited sample size keeps preprocessing efficient, enabling
rapid initialization in practice.
**Exp** 7 **: The memory usage of auxiliary structures.**
Experiment 7 evaluates the memory usage of PLIs and _MHT_
(with _θ_ = 80%) of _r_ . As PLIs are the common validation
structures, they provide a key benchmark. To analyze the
factors influencing memory usage, we record ∣ _r_ ∣, ∣ _R_ ∣, ∣F∣, and
memory size of datasets. Results are summarized in Table III.
Experimental results show that PLI memory usage grows
rapidly with increasing ∣ _r_ ∣ and ∣ _R_ ∣, due to maintaining complete attribute partition indices. In contrast, _MHT_ employs a
_high-frequency mapping items preservation strategy_, caching
only key value mapping items of valid FDs exceeding a
frequency threshold _θ_ . Thus each FD stores at most _O_ ( _θ_ [1] [)]



frequency threshold _θ_ . Thus each FD stores at most _O_ ( _θ_ [1] [)]

mapping items, making the overall space complexity _O_ ( [∣F∣] _θ_

[)][,]



mapping items, making the overall space complexity _O_ ( [∣F∣] _θ_

[)][,]
which is independent of dataset size. Consequently, on datasets
with few FDs, such as Claim and Bioentry, _MHT_ consumes
substantially less memory than PLIs, and its memory usage
remains controllable even for large-scale datasets.
On complex datasets or with many FDs, the memory usage
of _MHT_ may exceed that of PLIs, which also aligns perfectly
with its _O_ ( [∣F∣] _θ_

[)][ space complexity. Nevertheless, this is our]



with its _O_ ( [∣F∣] _θ_

[)][ space complexity. Nevertheless, this is our]
time-memory tradeoff design, where EAIFD accepts moderate
memory for a substantial performance speedup.



TABLE III: Auxiliary structures on different datasets.


**Dataset Properties** **Auxiliary Structures**
**DataSet** **∣** _**r**_ **∣** **∣** _**R**_ **∣** **∣F∣** **Data** **PLIs** **MHT (** _**θ**_ **= 80%)**
Iris 147 5 4 5KB 1.7KB **0.04KB**
Bridges 108 13 142 6KB 3KB **1.4KB**
Hepatitis 155 20 8250 6.1KB **3.6KB** 8.4KB
Horse 300 29 128726 18KB **6.3KB** 1.2MB
Balance 625 5 1 7KB 2.3KB **0**
NCV 1000 19 758 151KB 76KB **10.3KB**
Plista 1001 63 173409 496KB **312KB** 1.7MB
Claim 20000 11 12 2.6MB 1.5MB **5KB**
Letter 20000 17 61 696KB 221KB **15KB**
CAB 67300 54 3353531 11.4MB **8.9MB** 98MB
Bioentry 184292 9 19 24MB 16.7MB **6KB**
Flight 250000 31 117367 19.7KB **11KB** 1.1MB
Pitches 250000 40 608928 50MB 27.3MB **19MB**
Ditag_Feature 3960124 13 58 348MB 202MB **14KB**


**Exp** 8 **: The effect of frequency threshold** _θ_ **.** Experiment 8
evaluates the scalability of EAIFD under different mapping
frequency thresholds _θ_ for retaining high-frequency items in
the _MHT_ . The experiment uses four datasets: Plista, Pitches,
CAB, and Flight, with an incremental ratio of [∣][∆] ∣ _r_ _[r]_ ∣ [∣] [=][ 10%][.]

Table IV illustrates the performance and memory consumption of the _MHT_ for _r_ when the frequency threshold _θ_
increases from 70% to 90%. When _θ_ = 70%, EAIFD achieves
the highest efficiency with the largest _MHT_ memory usage,
since more high-frequency mapping items are retained in the
initial _MHT_ . This allows a large portion of validations to be



13


TABLE IV: Runtime and memory usage of _MHT_ under different
mapping frequency _θ_ on four datasets.


**Runtime (sec)** **Memory of** _MHT_ **(MB)**
**Dataset**
70% 75% 80% 85% 90% 70% 75% 80% 85% 90%

Plista 0.09 0.32 0.50 2.91 4.51 1.90 1.75 1.70 1.43 1.27

Flight 42 68 83 174 406 3.27 1.93 1.10 0.82 0.56

Pitches 72 95 132 357 630 54.7 31.6 19.0 13.6 10.8

CAB 78 144 197 522 927 272.6 134.2 98.7 77.3 45.5


efficiently performed via hash lookups with _MHT_ ∆, without
revalidating on the initial data blocks. This reflects a tradeoff between space consumption and time performance. As _θ_
increases, fewer mapping items satisfy the threshold, leading to
reduced memory usage of the _MHT_ . Since fewer candidates
can be verified by comparing items in the _MHT_, more data
blocks need to be loaded and validated, thereby increasing the
runtime.
The results clearly demonstrate that _θ_ is a highly sensitive
parameter for the time–memory trade-off. Considering both
_MHT_ memory usage and runtime, the results confirm that _θ_ =
80% is a well-balanced parameter choice. Overall, the _high-_
_frequency mapping item preservation strategy_ enables EAIFD
to balance storage efficiency and computational performance.
**Summary of Experimental Analysis.** The experimental
results demonstrate that EAIFD achieves superior performance in incremental FD discovery. By reformulating FD
discovery as a partial hypergraph hitting set enumeration and
employing _MHT_ to preserve historical computations combined with efficient validation methods, EAIFD outperforms
existing static and incremental algorithms. It maintains stable
performance across varying tuple, attribute, and incremental
data scales, while reducing preprocessing time. Moreover, the
_high-frequency mapping items preservation strategy_ of _MHT_
balances memory and computational efficiency, and the core
frequency threshold _θ_ shows acceptable performance across
a wide range. Overall, EAIFD provides an efficient, scalable,
and practical solution for incremental FD discovery.


VI. CONCLUSION


This paper proposes EAIFD, an efficient algorithm for FD
discovery in relational databases under continuous incremental
updates. EAIFD overcomes redundant cost caused by reexecution in static algorithms and the performance bottlenecks
of existing incremental methods. It introduces two core components: (1) modeling incremental FD discovery as hitting set
enumeration over partial hypergraphs, (2) employing efficient
two-step incremental validation strategy. This design achieves
high performance with low memory consumption.
Extensive experiments demonstrate that EAIFD consistently
outperforms existing incremental FD discovery (achieving
up to an order-of-magnitude speedup on datasets such as
Plista) and shows significant performance compared with
static algorithms when the incremental data ratio is small.
Its efficiency advantage stems from its core operations, which
depend primarily on the incremental data size ∣∆ _r_ ∣ rather than
the full dataset size ∣ _r_ ∣. This characteristic further enhances the
efficiency and scalability in long-term applications where [∣][∆] _r_ _[r]_ [∣]


decreases over time. In terms of memory, EAIFD achieves
a low memory consumption independent of ∣ _r_ ∣ by utilizing
_MHT_ with the _high-frequency mapping items preservation_
_strategy_ . Furthermore, EAIFD enables fast initialization with
preprocessing complexity _O_ (∣ _R_ ∣× ∣ _r_ ∣ log ∣ _r_ ∣).
While EAIFD achieves remarkable progress in incremental
FD discovery, several directions remain for future work, such
as deletions and modifications for fully dynamic FD discovery,
and explore parallel or distributed implementations to improve
scalability on massive datasets.


REFERENCES


[1] A. Silberschatz, H. F. Korth, and S. Sudarshan, _Database System_
_Concepts, Seventh Edition_ . McGraw-Hill Book Company, 2020.

[2] T. Papenbrock and F. Naumann, “Data-driven schema normalization,” in
_Proc. 20th Int. Conf. Extending Database Technol._, 2017, pp. 342–353.

[3] Z. Wei and S. Link, “Embedded functional dependencies and datacompleteness tailored database design,” _ACM Trans. Database Syst._,
vol. 46, no. 2, pp. 7:1–7:46, 2021.

[4] J. Kossmann, T. Papenbrock, and F. Naumann, “Data dependencies for
query optimization: a survey,” _VLDB J._, vol. 31, no. 1, pp. 1–22, 2022.

[5] P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis,
“Conditional functional dependencies for data cleaning,” in _Proc. Int._
_Conf. Data Eng._, 2007, pp. 746–755.

[6] A. Doan, A. Y. Halevy, and Z. G. Ives, _Principles of Data Integration_ .
Morgan Kaufmann, 2012.

[7] Y. Huhtala, J. Kärkkäinen, P. Porkka, and H. Toivonen, “TANE: an
efficient algorithm for discovering functional and approximate dependencies,” _Comput. J._, vol. 42, no. 2, pp. 100–111, 1999.

[8] N. Novelli and R. Cicchetti, “Functional and embedded dependency
inference: a data mining point of view,” _Inf. Syst._, vol. 26, no. 7, pp.
477–506, 2001.

[9] Z. Abedjan, P. Schulze, and F. Naumann, “DFD: efficient functional
dependency discovery,” in _Proc. ACM Int. Conf. Inf. Knowl. Manag._,
2014, pp. 949–958.

[10] T. Papenbrock and F. Naumann, “A hybrid approach to functional
dependency discovery,” in _Proc. 2016 ACM Int. Conf. Manag. Data_,
2016, pp. 821–833.

[11] T. Bleifuß, T. Papenbrock, T. Bläsius, M. Schirneck, and F. Naumann,
“Discovering functional dependencies through hitting set enumeration,”
_Proc. ACM Manag.Data_, vol. 2, no. 1, pp. 43:1–43:24, 2024.

[12] J. Liu, F. Ye, J. Li, and J. Wang, “On discovery of functional dependencies from data,” _Data Knowl. Eng._, vol. 86, pp. 146–159, 2013.

[13] X. Wan, X. Han, J. Wang, and J. Li, “Efficient discovery of functional
dependencies on massive data,” _IEEE Trans. Knowl Data Eng._, vol. 36,
no. 1, pp. 107–121, 2024.

[14] C. M. Wyss, C. Giannella, and E. L. Robertson, “Fastfds: A heuristicdriven, depth-first algorithm for mining functional dependencies from
relation instances - extended abstract,” in _Proc. Int. Conf. Data Ware-_
_hous. Knowl. Discov._, 2001, pp. 101–110.

[15] H. Yao and H. J. Hamilton, “Mining functional dependencies from data,”
_Data Min. Knowl. Discov._, vol. 16, no. 2, pp. 197–219, 2008.

[16] Z. Wei and S. Link, “Discovery and ranking of functional dependencies,”
in _Proc. Int. Conf. Data Eng._, 2019, pp. 1526–1537.

[17] P. Schirmer, T. Papenbrock, S. Kruse, F. Naumann, D. Hempfing,
T. Mayer, and D. Neuschäfer-Rube, “Dynfd: Functional dependency
discovery in dynamic datasets,” in _Proc. Int. Conf. Extending Database_
_Technol._, 2019, pp. 253–264.

[18] R. Xiao, Y. Yuan, Z. Tan, S. Ma, and W. Wang, “Dynamic functional
dependency discovery with dynamic hitting set enumeration,” in _Proc._
_Int. Conf. Data Eng._, 2022, pp. 286–298.

[19] P. A. Flach and I. Savnik, “Database dependency discovery: A machine
learning approach,” _AI Commun._, vol. 12, no. 3, pp. 139–160, 1999.

[20] S. Lopes, J. Petit, and L. Lakhal, “Efficient discovery of functional
dependencies and armstrong relations,” in _Proc. Int. Conf. Extending_
_Database Technol._, ser. Lect. Notes Comput. Sci., vol. 1777, 2000, pp.
350–364.

[21] T. Papenbrock, J. Ehrlich, J. Marten, T. Neubert, J. Rudolph, M. Schönberg, J. Zwiener, and F. Naumann, “Functional dependency discovery:
An experimental evaluation of seven algorithms,” _Proc. VLDB Endow._,
vol. 8, no. 10, pp. 1082–1093, 2015.

[22] H. Garcia-Molina, J. D. Ullman, and J. Widom, _Database systems - the_
_complete book (2. ed.)_ . Pearson Education, 2009.



14


[23] H. Mannila and K. Räihä, “Algorithms for inferring functional dependencies from relations,” _Data Knowl. Eng._, vol. 12, no. 1, pp. 83–99,
1994.

[24] J. Birnick, T. Bläsius, T. Friedrich, F. Naumann, T. Papenbrock, and
M. Schirneck, “Hitting set enumeration with partial information for
unique column combination discovery,” _Proc. VLDB Endow._, vol. 13,
no. 11, pp. 2270–2283, 2020.

[25] K. Murakami and T. Uno, “Efficient algorithms for dualizing large-scale
hypergraphs,” _Discret. Appl. Math._, vol. 170, pp. 83–94, 2014.

[26] R. Agrawal, T. Imielinski, and A. N. Swami, “Mining association rules
between sets of items in large databases,” in _Proc. 1993 ACM Int. Conf._
_Manag. Data_, 1993, pp. 207–216.

[27] L. Berti-Équille, H. Harmouch, F. Naumann, N. Novelli, and S. Thirumuruganathan, “Discovery of genuine functional dependencies from
relational data with missing values,” _Proc. VLDB Endow._, vol. 11, no. 8,
pp. 880–892, 2018.


