**Quantum Dimension Reduction of Hidden Markov Models**


Rishi Sundar [1, 2] and Thomas J. Elliott [1, 3, 2]

1 _Department of Physics & Astronomy, University of Manchester, Manchester M13 9PL, United Kingdom_
2 _Centre for Quantum Science and Engineering, University of Manchester, Manchester M13 9PL, United Kingdom_
3 _Department of Mathematics, University of Manchester, Manchester M13 9PL, United Kingdom_
(Dated: January 22, 2026)


Hidden Markov models (HMMs) are ubiquitous in time-series modelling, with applications ranging from chemical reaction modelling to speech recognition. These HMMs are often large, with
high-dimensional memories. A recently-proposed application of quantum technologies is to execute
quantum analogues of HMMs. Such quantum HMMs (QHMMs) are strictly more expressive than
their classical counterparts, enabling the construction of more parsimonious models of stochastic
processes. However, state-of-the-art techniques for QHMM compression, based on tensor networks,
are only applicable for a restricted subset of HMMs, where the transitions are deterministic. In this
work we introduce a pipeline by which _any_ finite, ergodic HMM can be compressed in this manner,
providing a route for effective quantum dimension reduction of general HMMs. We demonstrate the
method on both a simple toy model, and on a speech-derived HMM trained from data, obtaining
favourable memory–accuracy trade-offs compared to classical compression approaches.



**I.** **INTRODUCTION**


Hidden Markov models (HMMs) provide a compact description of discrete-time stochastic processes in terms of
a finite internal memory that evolves stochastically while
emitting observable symbols [1, 2]. They are used widely
across scientific modelling and data analysis, including
speech recognition and other sequential data [3–13]. As
learned HMMs grow in size and connectivity, a central
challenge is to reduce their effective memory cost while
retaining the observable statistics that make them useful.
Classical reduction techniques, such as state merging,
aim to simplify a model while preserving its predictions

[14–18]. In practice, however, these methods can involve large intermediate representations or trade state
reduction against loss of longer-range predictive structure when applied to realistic learned models with many
states and rich transition patterns. This raises a basic
question: given a complex HMM that fits data well, how
much memory is _really_ required to reproduce its observable statistics, and can that memory cost be reduced in
a controlled way?
Quantum models of stochastic processes provide another route to memory reduction. By encoding predictive information into non-orthogonal quantum memory
states, quantum simulators can reproduce the statistics
of a classical process while reducing the required memory [19–29]. The _q_ -sample construction makes this connection concrete by mapping the stationary statistics of
a process to a translationally invariant quantum state
on an infinite chain, with entanglement across a cut
quantifying the memory resources needed for sampling

[30–32]. For processes admitting a finite-dimensional _q_ sample iMPS, standard uniform-MPS algorithms allow
controlled bond-dimension truncation, trading a small
loss in fidelity rate for a large reduction in bond dimension [32, 33], corresponding to quantum models of
stochastic processes with significantly reduced memory
dimension compared to classical HMMs.



A key limitation is that existing iMPS compression
results apply most cleanly when the underlying process
admits a _deterministic_ (or _unifilar_ ) presentation, where
the next internal state is fixed by the current state and
emitted symbol. In this case the _q_ -sample admits a _nor-_
_mal_ iMPS representation with a primitive transfer operator and a well-conditioned canonical form [31, 32, 34].
HMMs learned directly from data, however, are typically non-deterministic, and naive tensor-network constructions can yield non-normal iMPS (or mixed-state
tensor-network descriptions) for which stable infinitesystem truncation is not available [32]. This creates a
practical gap between realistic learned HMMs and the
class of truncated quantum models accessible to current
iMPS-based compression pipelines.


Here we bridge that gap. We introduce a dilation that
augments the output alphabet of any finite-state, stationary, ergodic HMM so that the resulting model is deterministic while preserving all finite-length word statistics
on the original alphabet – without increasing the model’s
memory dimension. The _q_ -sample of the dilated process is guaranteed to be representable as a normal iMPS,
enabling stable variational truncation to a target bond
dimension _d_ [˜] . From the compressed tensors we can reconstruct an effective compressed quantum model of the
original process.


We demonstrate the method on (i) a tunable nondeterministic source family of toy models and (ii) a
speech-derived HMM trained from data. In both cases
we obtain a controlled trade-off between bond dimension
and distortion. The compressed iMPS provides a direct
specification of a quantum sampler with memory dimension _d_ [˜] .


**II.** **BACKGROUND**


**A.** **Classical models of stochastic processes**


We consider a discrete-time stochastic process _P_ that
generates a bi-infinite sequence of random variables
_{Xt}t∈_ Z, each taking values in a finite alphabet _X_ [35].
The process is fully specified by the joint distribution
Pr( _. . ., X−_ 1 _, X_ 0 _, X_ 1 _, . . ._ ) over all times. In practice, modelling and simulating such a process requires a finite description of the statistical dependencies between past and
future [36].
Hidden Markov models (HMMs) provide one such description [1, 37]. An HMM is specified by a tuple
( _S, X_ _, {T_ _[x]_ _}_ ), where _S_ is a finite set of hidden states, _X_ is
the set of observable symbols, and the transition tensor
_T_ _[x]_ has elements


_Ts_ _[x][′]_ _s_ [= Pr(] _[S][t]_ [+1] [=] _[ s][′][, X][t]_ [=] _[ x][ |][ S][t]_ [=] _[ s]_ [)] (1)


for all _s, s_ _[′]_ _∈S_ and _x ∈X_ . For each fixed state _s_ the matrices satisfy [�] _s_ _[′]_ _,x_ _[T][ x]_ _s_ _[′]_ _s_ [= 1, so that they define a proper]

conditional distribution over next states and outputs.
A process has a _deterministic_ HMM representation if
the next internal state is uniquely determined by the current state and the emitted symbol [2, 38]. In terms of the
transition structure this means that for every state _s_ and
symbol _x_ there is at most one successor state _s_ _[′]_ with
nonzero transition probability:
�� _{ s′_ : _Ts x_ _[′]_ _s_ _[>]_ [ 0] _[ }]_ �� _≤_ 1 _∀_ _s ∈S, x ∈X_ _._ (2)


Equivalently, given the initial state and the observed
symbol sequence, the present internal state is uniquely
determined. Deterministic models play a central role
in the branch of complexity science known as computational mechanics: the _ε_ –machine corresponds to the minimal deterministic HMM that exactly reproduces a process, and its memory cost is the Shannon entropy of the
stationary distribution over causal states, the statistical
complexity _Cµ_ [2, 38], often interpreted as a measure of
structure [36, 39, 40]. Non-deterministic HMMs, in contrast, can represent the same process with fewer states,
but at the cost of ambiguous internal state trajectories
conditioned on observations [41, 42].
Throughout this work we restrict attention to finitestate, finite-alphabet, stationary and ergodic processes,
so that a unique stationary distribution over internal
states exists and time-translation-invariant descriptions
are well defined.


**B.** **Quantum models of stochastic processes**


Quantum models extend this framework by encoding predictive information into quantum memory states
_{|σj⟩}_ associated with internal configurations of the
model [19, 23, 24]. The simulation proceeds via a joint
evolution of the memory system and an output register,



2


implemented by an isometry (equivalently, a unitary on
a larger space). Starting from an initial memory state,
one applies this evolution and measures the output register to obtain the next symbol _x_, leaving the memory
updated to the post-measurement state conditioned on
that outcome. Repeated application of this procedure
generates sequences with the same statistics as the target process. Such a procedure has been experimentallyrealised in multiple platforms [43–46].
More generally, a quantum hidden Markov model
(QHMM) [25, 26, 28, 29, 47–49] on an alphabet _X_ can
be specified by the tuple ( _X_ _, H, ρ_ 0 _, {Ex}x∈X_ ), where _ρ_ 0
is a density operator on the memory Hilbert space _H_
and _{Ex}_ is a quantum instrument: each _Ex_ is completely
positive and trace non-increasing, and [�] _x_ _[E][x]_ [ is trace pre-]
serving. The probability of a word _w_ = _x_ 1 _· · · xL_ is


Pr( _w_ ) = Tr( _ExL ◦· · · ◦Ex_ 1( _ρ_ 0)) _._ (3)


The quantum statistical memory _Cq_ is defined as the
von Neumann entropy of the stationary memory state,

      _Cq_ = _S_ ( _ρ_ ) = _−_ Tr( _ρ_ log2 _ρ_ ) _,_ _ρ_ = _πj |σj⟩⟨σj|,_ (4)

_j_


where _πj_ is the stationary probability of internal configuration _j_ . Many processes admit models for which
_Cq ≤_ _Cµ_, demonstrating a quantum memory advantage,
and for some families the separation between _Cq_ and
_Cµ_ can be unbounded [50–56]. Finite-dimensional quantum models can be constructed directly from the classical
HMM [25, 29].


**C.** **Matrix product states for deterministic models**



_xt_ : _t_ + _L∈X_ _[L]_


and takes _L →∞_ in a manner compatible with stationarity.
When the _q_ -sample admits an infinite matrix product
state (iMPS) representation with site tensors _{A_ _[x]_ _}x∈X_
and transfer matrix

    _E_ = _A_ _[x]_ _⊗_ ( _A_ _[x]_ ) _[∗]_ _,_ (6)

_x∈X_


all finite-block measurement statistics are obtained from
a _double-layer_ contraction. Concretely, if the leading
eigenvalue of _E_ is nondegenerate, then boundaries are



A stochastic process _P_ over a finite alphabet _X_
can be encoded into a translationally invariant quantum state via its _q-sample_ [31]. Informally, the _q_ sample is the infinite-chain limit of the pure state whose
computational-basis amplitudes are square roots of classical word probabilities. For a length- _L_ block _xt_ : _t_ + _L_ :=
_xtxt_ +1 _· · · xt_ + _L_ 1, one considers

_−_




   _|P_ ( _Xt_ : _t_ + _L_ ) _⟩_ :=







_P_ ( _xt_ : _t_ + _L_ ) _|xt_ : _t_ + _L⟩_ _,_ (5)


irrelevant in the thermodynamic limit and one may express the block distribution in terms of the leading left
and right eigenmatrices _Vl_ and _Vr_ of _E_ as [31]

         -          _P_ MPS( _xt_ : _t_ + _L_ ) = Tr _A_ _[x][t]_ [+] _[L][−]_ [1] _[†]_ _· · · A_ _[x][t][†]_ _Vl A_ _[x][t]_ _· · · A_ _[x][t]_ [+] _[L][−]_ [1] _Vr_ _._
(7)
Equation (7) is the appropriate probability-extraction
formula for _q_ -sample iMPS, and it replaces any singlelayer expression of the form _⟨l|A_ _[x][t]_ _· · · A_ _[x][t]_ [+] _[L][−]_ [1] _|r⟩_ .
For unifilar HMMs (in particular, _ε_ -machines), Yang
_et al._ show that the choice



3


non-deterministic HMM, a dilated deterministic process
whose _q_ –sample iMPS is normal by construction. This
provides the starting point for stable variational compression and for the compressed representations that we
analyse in the following sections.


**III.** **THEORETICAL RESULTS**


Our aim is to start from a general finite-state, discretealphabet, stationary HMM and obtain a normal infinite
matrix product state (iMPS) that can be compressed by
standard tensor-network methods. The overall pipeline
has four steps: (i) dilate the original HMM to a deterministic process on an augmented output alphabet; (ii)
construct its _q_ –sample iMPS; (iii) variationally truncate
this iMPS to a reduced bond dimension; and (iv) reconstruct an effective model for the original outputs and
quantify the approximation quality using the co-emission
divergence rate (CDR).
Throughout we assume that the original HMM is finitestate, finite-alphabet, stationary and ergodic.


**A.** **Dilation to a deterministic process**


Let ( _S, X_ _, {T_ _[x]_ _}_ ) be an HMM for a stationary, ergodic
process _P_ . In general this model need not be deterministic: for a given state _s_ and symbol _x_, there may be
several possible successor states _s_ _[′]_ with _Ts_ _[x][′]_ _s_ _[>]_ [ 0.]
We quantify this local branching by

_k_ ( _s, x_ ) = �� _{ s′ ∈S_ : _Ts x_ _[′]_ _s_ _[>]_ [ 0] _[ }]_ �� (9)


and define


_dy_ = max (10)
_s_ _, x_ _[k]_ [(] _[s, x]_ [)] _[,]_
_∈S_ _∈X_

so that 1 _≤_ _dy ≤|S|_ . We introduce an auxiliary output alphabet _Y_ = _{y_ 1 _, . . ., ydy_ _}_ and choose a labelling
function


_f_ : _S × S × X →Y,_ (11)


which assigns a label _y_ = _f_ ( _s, s_ _[′]_ _, x_ ) to each allowed transition ( _s →_ _s_ _[′]_ ) emitting symbol _x_ . The only constraint
on _f_ is that it be injective for each fixed ( _s, x_ ):


_s_ _[′]_ = _s_ _[′′]_ = _⇒_ _f_ ( _s, s_ _[′]_ _, x_ ) _̸_ = _f_ ( _s, s_ _[′′]_ _, x_ ) _∀_ _s ∈S, x ∈X_ _._
(12)
Such a labelling always exists, because _dy_ is at least as
large as the maximal branching _k_ ( _s, x_ ).
The dilated model has the same hidden-state space _S_
and a composite output alphabet _X × Y_ . Its transition
tensor is




  _A_ _[x]_ _s_ _[′]_ _s_ [=]



_Ts_ ~~_[x]_~~ _[′]_ _s_ (8)



fully represents the process, in the sense that
_P_ MPS( _xt_ : _t_ + _L_ ) = _P_ ( _xt_ : _t_ + _L_ ) for all _L_, and moreover that
the process is ergodic if and only if the transfer matrix _E_
has a nondegenerate leading eigenvalue [31]. In addition,
the canonical form of an iMPS can be constructed systematically from _Vl_ and _Vr_ via factorizations _Vr_ = _WrWr_ _[†]_
and _Vl_ = _Wl_ _[†][W][l]_ [ followed by an SVD of] _[ W][l][W][r]_ [ to obtain]
Schmidt coefficients and canonical tensors [31].
In this work, we will use Eq. (7) as the operative link
between iMPS tensors and classical block statistics, and
we explicitly gauge-fix our tensors into canonical form
before interpreting them as Kraus operators. A compatible canonical-form framework and completeness relations are reviewed in Ref. [32] (Appendix A), where
left- and right-canonical tensors satisfy [�] [(] _[A][x]_ [)] _[†][A][x]_ [=][ I]



left- and right-canonical tensors satisfy _x_ [(] _[A]_ _l_ _[x]_ [)] _[†][A]_ _l_ _[x]_ [=][ I]

and [�] _[A][x]_ [(] _[A][x]_ [)] _[†]_ [ =][ I][, enabling a channel (instrument)]



and _x_ _[A]_ _r_ _[x]_ [(] _[A][x]_ _r_ [)] _[†]_ [ =][ I][, enabling a channel (instrument)]

interpretation of the site tensors [32].



**D.** **Difficulties with non-deterministic HMMs**


Direct application of MPS methods to nondeterministic HMMs encounters significant challenges.
A naive construction might attempt to use the same
element-wise square-root mapping _T_ _[x]_ _�→_ _A_ _[x]_ as above.
For non-deterministic models this typically yields a
transfer operator _E_ that is not primitive: it may have
multiple dominant eigenvalues or leading eigenvectors
that are not strictly positive. The resulting iMPS is
non-normal, so its canonical form is either not unique
or poorly conditioned. Without normality, standard
truncation methods and tangent-space based infinite
MPS algorithms become inapplicable or numerically
unstable, and straightforward singular-value truncations
along the virtual bonds often produce suboptimal
approximations [32].
From the perspective of quantum implementations,
one can represent general non-deterministic processes
using matrix product density operators (MPDOs) or
mixed-state tensor networks [25, 57, 58]. These constructions are more flexible but less suited to controlled
dimension reduction, and they do not directly provide
the normal iMPS structure required by existing compression schemes. In the remainder of this work we
address these difficulties by constructing, for any finite



_Ts_ [(] _[′][x,y]_ _s_ [)] =




_Ts_ _[x][′]_ _s_ _[,]_ if _Ts_ _[x][′]_ _s_ _[>]_ [ 0 and] _[ y]_ [ =] _[ f]_ [(] _[s, s][′][, x]_ [)] _[,]_

(13)
0 _,_ otherwise.



This dilation has three key structural properties:


4




  - _Determinism:_ for each state _s_ and composite symbol ( _x, y_ ) there is at most one successor state _s_ _[′]_

with _Ts_ [(] _[′][x,y]_ _s_ [)] _>_ 0.


  - _Preservation of observable statistics:_ marginalising
over the auxiliary outputs _y_ reproduces the same
finite-length block probabilities on _X_ as the original
process _P_ .


  - _Ergodicity:_ if the original HMM is ergodic, then the
dilated HMM is also ergodic, since the underlying
Markov chain on _S_, obtained by summing over outputs, is unchanged.


Proofs are collected in Appendix B. The dilation therefore yields a finite-state, stationary, ergodic and deterministic representation of _P_ on the augmented alphabet
_X × Y_ .


**B.** **Constructing the** _q_ **–sample iMPS**


Given the deterministic dilated HMM ( _S, X ×_
_Y, {T_ [(] _[x,y]_ [)] _}_ ), we construct its _q_ –sample iMPS [31] by taking the element-wise principal square root of the transition tensor,



_A_ ˜ [(] _ij_ _[x,y]_ [)]


_xy_





_Tij_ _[x]_


_x_


_A_ ˜ [(] _[x,y]_ [)]



_Tij_ [(] _[x,y]_ [)]


_x y_








  _A_ [(] _s_ _[x,y][′]_ _s_ [)] =



_Ts_ [(] _[′][x,y]_ _s_ [)] _,_ (14)



FIG. 1. Tensor network representations of stochastic models and their compression. (a) A transition matrix _Tij_ _[x]_ [of a]
HMM can be represented as a rank-3 tensor; analogously,
(b) the transition matrix of the dilated process _Tij_ [(] _[x,y]_ [)] can be
represented by a rank-4 tensor, as can (c) its element-wise
square root _A_ [(] _ij_ _[x,y]_ [)] . An array of these form an injective iMPS
when the two output legs are grouped, enabling the application of iMPS compression methods to deduce a dimensionreduced iMPS with site tensors _A_ [˜][(] _ij_ _[x,y]_ [)] . The (d) transfer matrix of the reduced iMPS defines a quantum channel _E_ [˜] when
a Kronecker- _δ_ is applied to the two visible output legs, implementing a compressed model of the original process. This
can then (e) be paired with the original process to efficiently
calculate their CDR.


**C.** **Variational truncation at reduced bond**
**dimension**


The iMPS defined by _{A_ [(] _[x,y]_ [)] _}_ is an exact representation of the dilated process, with bond dimension equal
to the number of internal states _ds_ = _|S|_ of the original
HMM. To obtain a more compact model we seek an approximate iMPS with a reduced bond dimension _d < d_ [˜] _s_
that is as close as possible, in the many-body sense, to
the original.
We adopt a uniform tangent-space variational approach for iMPS with fixed bond dimension [32, 33].
Given the normal iMPS _{A_ [(] _[x,y]_ [)] _}_ with bond dimension
_ds_, we project it onto the manifold of translationally invariant iMPS with bond dimension _d_ [˜] by solving the associated optimisation problem for a new set of tensors
_{A_ [˜][(] _[x,y]_ [)] _}_ . In practice this is implemented as an effective
eigenvalue problem in the tangent space of the target
manifold, solved iteratively using standard MPS routines.
Each iteration involves contractions of the transfer operator _E_ with trial tensors of bond dimension _d_ [˜] . We
need never form _E_ explicitly as a dense _d_ [2] _s_ _[×][ d]_ _s_ [2] [ma-]
trix; instead, we apply it as a linear map with cost that
scales polynomially in _ds_, _d_ [˜] and the physical dimension



for all _s, s_ _[′]_ _∈S_ and ( _x, y_ ) _∈X × Y_ . Treating the composite symbol ( _x, y_ ) as a single physical index, these tensors define a translationally invariant iMPS on an infinite
chain with physical dimension _d_ phys = _|X| |Y|_ .
The probability of a finite block ( _x_ 1 _, y_ 1) _, . . .,_ ( _xL, yL_ ) is
obtained by contracting the iMPS with stationary boundary data, for example in the standard transfer-matrix
form. By construction and by the preservation property
above, the marginal distribution over ( _x_ 1 _, . . ., xL_ ) coincides with that of the original process _P_ .
The corresponding transfer operator for the iMPS is




 _E_ =

_x∈X_




- 
_A_ [(] _[x,y]_ [)] _⊗_ _A_ [(] _[x,y]_ [)][�] _[∗]_ _,_ (15)
_y∈Y_



where _[∗]_ denotes complex conjugation. Since the dilated
HMM is finite-state, stationary, ergodic and deterministic, the associated _q_ –sample iMPS has a _primitive_ transfer operator with a unique leading eigenvalue _η_ 0 = 1
and strictly positive left and right eigenvectors [31]. The
iMPS is therefore _normal_, and blocking a finite number
of sites yields an injective MPS with a unique canonical
form and minimal bond dimension [34]. In practical computations we bring _{A_ [(] _[x,y]_ [)] _}_ into mixed canonical form
using standard gauge-fixing procedures.
The overall construction, from the original transition tensor through dilation to the site tensors of the
iMPS and the associated transfer operators, is illustrated
schematically in Fig. 1.


_d_ phys = _|X| |Y|_ . The outcome is a compressed normal
iMPS, specified by tensors _{A_ [˜][(] _[x,y]_ [)] _}_, that approximates
the original dilated process while retaining translational
invariance. For algorithmic details of how the variational
truncation approach for iMPS is performed, we refer the
reader to Ref. [33], with its application to quantum compression of deterministic HMMs discussed in Ref. [32]


**D.** **Reconstructing an effective model**



The truncated tensors _{A_ [˜][(] _[x,y]_ [)] _}_ define a translationally invariant iMPS approximation to the dilated ( _x, y_ )process. From this we can construct a compressed quantum model of the dilated process, from which we can in
turn construct a dimension-reduced QHMM of the original process by coarse-graining over the dilation.
A crucial technical point is that Kraus (instrument)
normalisation is a gauge property of the iMPS representation. Following standard iMPS canonicalisation, we
first gauge-fix the truncated tensors into a canonical form
obtained from the leading left and right eigenmatrices of
the truncated transfer operator [31, 32]. In left-canonical
form the site tensors satisfy the completeness relation

  - ��   







- _A_ ˜( _x,y_ ) _†_ ˜ _A_ ( _x,y_ ) = I _,_ (16)



5


**E.** **Quantifying distance from exact process**


For notational convenience we pass to the Liouville
representation. Assign column-stacking vectorisation vec
defined on matrix units by


vec( _|i⟩⟨j|_ ) := _|i⟩⊗|j⟩_ _._ (21)


With this convention one has the standard identity


vec( _AρB_ ) = ( _A ⊗_ _B_ [T] ) vec( _ρ_ ) _,_ (22)


where [T] denotes transpose in the computational basis.
Write _|ρ⟩⟩_ := vec( _ρ_ ) _∈_ C _[d]_ [˜][2] . Then the CP map _E_ [˜] _x_ in
Eq. (17) is represented by a linear operator _G_ [(] _[x]_ [)] via

     -     -     _|ρ_ _[′]_ _⟩⟩_ = _G_ [(] _[x]_ [)] _|ρ⟩⟩,_ _G_ [(] _[x]_ [)] = _A_ ˜ [(] _[x,y]_ [)] _⊗_ _A_ ˜( _x,y_ ) _∗,_ (23)

_y∈Y_

where _[∗]_ denotes complex conjugation. The trace functional is _⟨⟨ω|_ := _|_ I _⟩⟩_ _[†]_, so that _⟨⟨ω||ρ⟩⟩_ = Tr( _ρ_ ). Hence the
triple ( _X_ _, |ρ⋆⟩⟩, {G_ [(] _[x]_ [)] _}_ ) defines a finite-state linear generator – a generalised HMM (GHMM) [29, 37] – where for
any word _w_ = _x_ 1 _· · · xL_,


Pr (24)
˜ [(] _[w]_ [) =] _[ ⟨⟨][ω][|][ G]_ [(] _[x][L]_ [)] _[ · · ·][ G]_ [(] _[x]_ [1][)] _[|][ρ][⋆][⟩⟩][.]_
_P_

The operators _G_ [(] _[x]_ [)] need not be elementwise nonnegative or stochastic; the requirement is simply that Eq. (24)
yields valid word probabilities.
To quantify how well _P_ ˜ approximates the original process _P_, we use the co-emission divergence rate
(CDR) [59]. Let _P_ and _Q_ be stationary, ergodic processes described by finite-state linear generators (either
HMMs or GHMMs) _{L_ _[x]_ _}_ and _{L_ [ˆ] _[x]_ _}_ . We define the associated transfer operators on the product state spaces
by



_x∈X_



_y∈Y_



and hence define a completely positive trace-preserving
(CPTP) map on the bond space [32]. This is the sense
in which the truncated iMPS specifies a physically valid
sequential quantum generator.
We now group the Kraus operators by the observed
symbol _x_ by summing over the auxiliary label _y_ . For
each _x ∈X_ define the completely positive (CP) map

    _E_ ˜ _x_ ( _ρ_ ) := _A_ ˜ [(] _[x,y]_ [)] _ρ_ ˜ _A_ [(] _[x,y]_ [)] _[†]_ _._ (17)

_y∈Y_

Equation (16) implies that the unconditional channel
_E_ ˜ := [�] _x_ _[E]_ [˜] _[x]_ [ is trace preserving. Let ˜] _[ρ][⋆]_ [denote its sta-]
tionary state, i.e., a fixed point satisfying

    _ρ_ ˜ _⋆_ = _E_ ˜ _x_ (˜ _ρ⋆_ ) _._ (18)

_x∈X_

Then _{E_ [˜] _x}_ forms a quantum instrument that generates
a stationary process _P_ [˜] on _X_ via the usual update rule:
conditioned on observing _xt_, the bond state updates as

_ρ_ ˜ _t_ +1 = Tr _E_ ˜ _xE_ [˜] _tx_ (˜ _tρ_ (˜ _tρ_ ) _t_ ) _,_ Pr _P_ ˜ ( _xt_ ) = Tr _E_ [˜] _xt_ (˜ _ρt_ ) _._ (19)

Consequently, the probability of a word _w_ = _x_ 1 _· · · xL_ is

         -         Pr( _w_ ) = Tr _E_ ˜ _xL ◦· · · ◦_ _E_ ˜ _x_ 1(˜ _ρ⋆_ ) _._ (20)
_P_ ˜

The dimension-reduced QHMM is then specified by the
tuple ( _X_ _,_ _H_ [˜] _,_ ˜ _ρ_ 0 _, {E_ [˜] _x}x_ ), where _H_ [˜] is the Hilbert space
_∈X_
of the memory of the compressed QHMM, and ˜ _ρ_ 0 is the
initial state (generally, taken to be the steady-state ˜ _ρ⋆_ ).




 _EP_ :=

_x_    - _∈X_

_EQ_ :=



_L_ _[x]_ _⊗_ _L_ _[x]_ _,_
_x_ - _∈X_

_L_ ˆ _[x]_ _⊗_ _L_ ˆ _[x]_ _,_
_x_ - _∈X_




  _EP Q_ :=



_L_ _[x]_ _⊗_ _L_ [ˆ] _[x]_ _._ (25)
_x∈X_



and let _µP_, _µQ_, and _µP Q_ denote their leading eigenvalues. The CDR is then given by


_µP Q_

_RC_ ( _P, Q_ ) = _−_ [1] ~~_√_~~ _._ (26)

2 [log][2] ~~_µ_~~ _P_ ~~_µ_~~ _Q_


While calculated from GHMM representations, this expression depends only on the induced word distributions,
valid for any stationary and ergodic processes [59].
In our setting we take _P_ to be the original HMM process _P_ on _X_, with generators _L_ _[x]_ = _T_ _[x]_, and _Q_ to be the
compressed process _P_ [˜] generated by _L_ [ˆ] _[x]_ = _G_ [(] _[x]_ [)] as defined
above. Their CDR, _RC_ ( _P,_ _P_ [˜] ), is the main observable
figure of merit we report in our numerics.


**F.** **Analytic bounds via fidelity divergence rates**


The pipeline above produces, for each target bond dimension _d_ [˜], a compressed process _P_ [˜] together with its
CDR _RC_ ( _P,_ _P_ [˜] ), which we evaluate numerically from
finite-state generators. Analytically, the dimensionreduction results of Ref. [32] control _fidelity-type_ errors of
the underlying _q_ –sample iMPS. We therefore derive rigorous upper bounds on a _X_ -level _classical fidelity diver-_
_gence rate_ (CFDR), obtained from the dilated _q_ –samples
by data processing. This CFDR is a different, but closely
analogous, distinguishability measure from the CDR _RC_
used in our plots.
Let _{λk}_ _[d]_ _k_ _[s]_ =1 [denote the] _[ bond spectrum]_ [ of the dilated]
_q_ –sample iMPS across a single cut, i.e. the eigenvalues of
the stationary bond state _ρ⋆_ in canonical form (equivalently, squared Schmidt singular values), ordered so that
_λ_ 1 _≥_ _λ_ 2 _≥· · · ≥_ _λds_ and [�] _k_ _[λ][k]_ [ = 1. For a truncated]

bond dimension _d_ [˜] define the discarded tail weight

    _ε_ ˜ _d_ := _λk._ (27)

_k>d_ [˜]

          We write _RF_ ( _|Pxy⟩_ _,_ ��� _P_ ˜ _xy_ ) for the quantum fidelity


divergence rate (QFDR) between the exact and truncated ( _x, y_ )-dilated _q_ –sample states, and _RF_ ( _P,_ _P_ [˜] ) for
the CFDR on _X_, defined by



1          _RF_ ( _P,_ _P_ [˜] ) := _−_ lim
_L_ 2 _L_ [log][2]
_→∞_ _⃗x_




~~�~~

_P_ [(] _[L]_ [)] ( _⃗x_ ) _P_ [˜][(] _[L]_ [)] ( _⃗x_ ) _._



_⃗x∈X_ _[L]_



(28)
By applying dephasing in the ( _x, y_ ) basis followed by
tracing out the auxiliary register _Y_, fidelity monotonicity
implies the data-processing bound

               _RF_ ( _P,_ _P_ [˜] ) _≤_ _RF_ ( _|Pxy⟩_ _,_ ��� _P_ ˜ _xy_ ) _,_ (29)


proved in Appendix B. Ref. [32] then implies that, for
sufficiently small _ε_ ˜ _d_, there exists a constant _c >_ 0 (depending on the local physical dimension and the spectral
gap of the transfer operator) such that


_RF_ ( _P,_ _P_ [˜] ) _≤_ _c εd_ ˜ _._ (30)


The tail weight can in turn be bounded in terms of the
bond entropy

    _H_ ( _λ_ ) := _−_ _λk_ log2 _λk,_ (31)

_k_


giving (Appendix B)



_ε_ ˜ _d ≤_ _[H]_ [(] _[λ]_ [)] ( _d_ [˜] _≥_ 2) _,_ (32)

log2 _d_ [˜]


and hence an entropy-based guarantee


_RF_ ( _P,_ _P_ [˜] ) _≤_ _c_ _[H]_ [(] _[λ]_ [)] _._ (33)

log2 _d_ [˜]



6


Finally, we connect _H_ ( _λ_ ) to a simple algebraic quantity depending on the dilation. Let _K_ be the “slice”
matrix formed by horizontally concatenating all nonzero
site tensors _A_ [(] _[x,y]_ [)], viewed as linear maps on the bond
space,

          -           _K_ = _A_ [(] _[x]_ [1] _[,y]_ [1][)] _A_ [(] _[x]_ [2] _[,y]_ [2][)] _· · ·_ _._ (34)


The column space of _K_ contains the support of the stationary bond state, so rank( _K_ ) upper-bounds the number
of nonzero eigenvalues of _ρ⋆_, and therefore


_H_ ( _λ_ ) _≤_ log2 rank _K._ (35)


Substituting into Eq. (33) yields the structural, labelaware bound


_RF_ ( _P,_ _P_ [˜] ) _≤_ _c_ [log][2][ rank] _[ K]_ _._ (36)

log2 _d_ [˜]


In the numerical results below we report the CDR
_RC_, because it is efficiently computable from finite-state
generators. The bounds above instead provide rigorous
certificates for the classical fidelity divergence rate _RF_
that is directly controlled by iMPS truncation theorems.
Without additional assumptions one should not interpret
these as direct upper bounds on _RC_ .


**IV.** **EXAMPLES AND ANALYSIS**


We now demonstrate and analyse the dilation–
compression methodology on two examples. First, we
consider a _Tunable Nondeterministic Source_ (TNS), a
generalisation of a non-deterministic generator [60] that
has previously been used to exhibit quantum reductions
in memory [25]. Here we use it as a controlled test bed
for how compression performance depends on internal parameters and on the entanglement structure induced by
the dilation. Second, we apply the method to a hidden Markov model trained on a real speech dataset and
compare the compressed representations to a standard
classical reduction baseline.


**A.** **Illustrative model:** _N_ **-state simple**
**non-deterministic source**


As a controlled test bed we use a generalised _N_ -state
TNS with internal states _S_ = _{_ 0 _,_ 1 _, . . ., N −_ 1 _}_ and a single parameter _p ∈_ (0 _,_ 1) tuning the transition structure.
For generic _p_ and _N ≥_ 2 the model is non-deterministic:
conditioning on the emitted symbol does not uniquely
fix the successor state. The transition structure is shown
schematically in Fig. 2.
Starting from this TNS HMM we construct the dilated
deterministic model according to Sec. III A, build its _q_ sample iMPS as in Sec. III B, and perform variational
truncation at a sequence of reduced bond dimensions _d_ [˜]


7







FIG. 2. HMM representation of the generalised _N_ -state tunable non-deterministic source (TNS). Transition labels show
‘probability _|_ emitted symbol’, with the auxiliary label symbol in red.












|TNS: N=15, probability descending|TNS: N|N=15, prob|bability de|escendin|ng|Col7|Col8|p = 0.2|
|---|---|---|---|---|---|---|---|---|
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>||||||||p = 0.2<br>p = 0.4<br>|
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>||||||||~~p = 0.6~~|
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>|||||||||
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>|||||||||
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>|||||||||
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>|||||||||
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>|||||||||
|2<br>4<br>6<br>8<br>1<br> <br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>  10<br>2<br>||||||0<br>1|2<br>1|4|






|TNS (probability descending) N=8, p=0.3 N=10, p=0.3<br>log scale N=8, p=0.7 N=10, p=0.7<br>10 2<br>10 3<br>10 4<br>2 3 4 5 6 7 8 9<br>Truncation bond dimension d|T|NS (pro|babi|lity d|escend|ing)|Col8|N=8, p|=0.3|N=10, p=|0.3|
|---|---|---|---|---|---|---|---|---|---|---|---|
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7|~~lo~~|~~g scale~~|~~g scale~~|||||<br>N=8, p|<br> =0.7|<br>N=10, p=|<br> 0.7|
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||
|2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>Truncation bond dimensiond<br>10<br>4<br>10<br>3<br>10<br>2<br> <br>~~TNS (probability descending)~~<br>~~log scale~~<br>N=8, p=0.3<br>N=8, p=0.7<br>N=10, p=0.3<br>N=10, p=0.7||||||||||||



FIG. 3. TNS model compression results: co-emission divergence rate _RC_ between the original process and the reconstructed effective model as a function of truncated bond dimension _d_ [˜] . Each curve corresponds to a different choice of
the number of states _N_ and internal parameter _p_ .


following Sec. III C. For each choice of ( _N, p_ ) and target _d_ [˜] we reconstruct the effective model for the original
alphabet and compute the co-emission divergence rate
_RC_ ( _P,_ _P_ [˜] ) between the original and compressed processes,
as described in Sec. III E.
Figure 3 shows the resulting CDR as a function of the
truncated bond dimension _d_ [˜] for several representative parameter choices. Each curve corresponds to a fixed pair
( _N, p_ ). In all cases the CDR decreases systematically
as _d_ [˜] increases, indicating that the variational procedure
produces families of compressed models that converge toward the original process as more virtual resources are
allowed. The rate of decrease depends strongly on both
_N_ and _p_ : some parameter regimes reach a given CDR at
much smaller _d_ [˜] than others, reflecting differences in how
strongly the process resists low-dimensional compression.
This behaviour is also in line with the analytic picture of
Sec. III F, where the loss induced by truncation is controlled by the tail of the bond spectrum of the (dilated)
iMPS.
To understand this dependence more structurally, we
examine the Schmidt spectra of the dilated iMPS before truncation. For each ( _N, p_ ) we bring the iMPS into
canonical form and compute the singular values across a
single bond, which quantify the bipartite entanglement
across that cut. Figure 4 shows the Schmidt coefficients



FIG. 4. Schmidt spectra of the dilated iMPS of the TNS for
fixed _N_ = 15 and varied _p_, ordered by decreasing magnitude.
The shape of the specra can vary, but parameter regimes with
faster-decaying spectra are expected to be more amenable to
truncation to small bond dimension.


as a function of their index for a fixed _N_ and varied
_p_ . The decay patterns vary markedly: some spectra are
rapidly decaying (concentrating most weight into a few
leading modes), while others are much flatter. This also
motivates the use of the variational method, as standard
SVD truncation methods which work by discarding singular values work poorly on flat distributions.
Comparing Figs. 3 and 4, one finds that parameter
choices with more rapidly decaying Schmidt spectra tend
to achieve low CDR at smaller _d_ [˜], while flatter spectra
require larger _d_ [˜] to reach comparable accuracy. This is
consistent with the intuition that the entanglement encoded in the dilated iMPS is a practical proxy for the
structural complexity that survives the dilation and resists compression.


**B.** **Application to a speech-derived HMM**


To assess the practical utility of the method we apply it to a hidden Markov model trained on a subset of
the Google Speech Commands dataset [13]. Each audio
clip is mapped to a sequence of acoustic feature vectors
using Mel-frequency cepstral coefficients (MFCCs), including time-derivative features [61]. We then discretise
these features by vector quantisation using MiniBatch _k_ means, yielding a finite output alphabet [62]. Finally,
we fit a categorical (edge-emitting) HMM by maximum
likelihood via the expectation-maximisation (or _Baum–_
_Welch_ ) procedure [3, 63]. The resulting learned model is
strongly non-deterministic, making it a realistic test case
for the dilation step.
We take this trained HMM as the target process
_P_ and apply our dilation-compression pipeline to obtain a compressed quantum model of the process. To
benchmark against a classical reduction pipeline, we also
perform greedy state merging directly on the original


|log scale;<br>10 2<br>symbol)<br>10 4<br>per<br>(bits<br>10 6<br>CDR<br>10 8<br>5|log scale;|values < 1e-0|1e-0|9 capped|Col6|quan|tum classic|al|
|---|---|---|---|---|---|---|---|---|
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;|||||||||
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;|||||||||
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;|||||||||
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;|||||||||
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;|||||||||
|10<br>8<br>10<br>6<br>10<br>4<br>10<br>2<br>CDR (bits per symbol)<br>log scale;||1|1|0<br>1|5<br>20|5<br>20|25<br>30|25<br>30|



FIG. 5. Speech-derived HMM compression results: comparison of quantum dimension reduction and a classical greedy
state-merging baseline. Horizontal axis reports memory dimension as classical state count or quantum bond dimension.
Vertical axis reports CDR _RC_ (bits per symbol, log scale).
Values below 10 _[−]_ [9] are displayed at 10 _[−]_ [9] for visibility.


learned HMM. In each merge step we choose the pair
of states that minimises a stationary-weighted KL objective defined on the one-step predictive distributions
_ps_ ( _x, s_ _[′]_ ) = _Ts_ _[x][′]_ _s_ [, and we update the merged state by a]
simple stationary-mixture lumping rule. We then evaluate the CDR between the original HMM and each merged
HMM at the corresponding state counts.


We emphasise that this merging routine is intended as
a transparent and computationally inexpensive _baseline_
rather than a best-in-class classical reduction method. It
is greedy, uses a local one-step objective, and does not
optimise sequence-level divergences such as the CDR directly; more sophisticated classical reductions (e.g. global
objectives, multi-step lookahead, or spectral methods)
may achieve stronger performance in some regimes. The
purpose of the baseline is to provide a concrete reference
curve against which the effect of the dilation–compression
pipeline can be assessed on the same learned model.


Figure 5 compares the quantum reduction curve (memory dimension _d_ [˜] ) with the classical merging curve (retained HMM state count). In the strongly compressed
regime (memory dimension ≲ 20), the quantum models achieve substantially smaller CDR, often by orders
of magnitude, than this greedy classical baseline at the
same nominal memory dimension. Notably, the quantum curve exhibits a sharp improvement around _d_ [˜] _≈_ 12,
consistent with the variational truncation needing only
few states to capture the dominant modes of the dilated
iMPS. By contrast, the classical baseline only approaches
the numerical floor once the retained state count is sufficiently large (here around ≳ 22 states), suggesting that
a moderate number of learned states can be merged with
negligible sequence-level distortion, whereas more aggressive classical reductions rapidly degrade performance.



8


**C.** **Impact of the labelling function**


The dilation procedure of Sec. III A contains a genuine
degree of freedom: the choice of the labelling function
_f_ : _S × S × X →Y_ used to assign auxiliary outputs to
transitions. Different admissible labellings (all satisfying
the injectivity constraint) can induce different entanglement structures in the dilated iMPS. Since truncation
performance is sensitive to the bond spectrum, it is natural to ask how strongly the choice of _f_ affects compressibility in practice.
To probe this, we fix a TNS instance and compare
several simple labelling strategies that differ only in how
they assign the auxiliary symbols _y_ to the allowed transitions, while keeping the auxiliary alphabet size _|Y|_ fixed.
For each choice of _f_ we construct the corresponding dilated iMPS, perform variational truncation across a range
of bond dimensions _d_ [˜], reconstruct the effective models,
and compute the CDR.
Figure 6(a) shows _RC_ ( _P,_ _P_ [˜] ) as a function of _d_ [˜] for the
different labelling strategies. The curves differ markedly:
some labellings yield low CDR already at small _d_ [˜], while
others require substantially larger bond dimensions to
reach comparable accuracy. In this sense the labelling is
not merely cosmetic; it acts as an optimisation parameter
that can materially change the achievable CDR– _d_ [˜] tradeoff. This observation also matches the structural picture
in Sec. III F, where label-dependent quantities (such as
the slice structure) can upper-bound entropic tails and
hence influence error guarantees.
The corresponding Schmidt spectra of the dilated
iMPS for the same labellings are shown in Fig. 6(b).
Different assignments of _y_ produce distinct decay patterns. Labellings with more rapidly decaying spectra
tend to yield the lowest CDR at small _d_ [˜], while flatter
spectra yield poorer compression performance, reinforcing the role of the induced entanglement structure as a
practical predictor of compressibility.
A full optimisation of _f_ over all admissible assignments is a nontrivial combinatorial problem and is beyond the scope of the present work. The examples here
show, however, that even simple heuristic labellings can
significantly change the practical trade-off between bond
dimension and CDR, suggesting that learning or optimising good labelling strategies is an important direction for
future research.


**V.** **DISCUSSION AND CONCLUSION**


We introduced a general framework for mapping nondeterministic HMMs to matrix product state representations, enabling techniques for their implementation with
compressed quantum models to be applied. We achieve
with a dilation procedure that takes any finite, stationary, ergodic HMM to a deterministic model on an
enlarged output alphabet that preserves all observable
statistics on the original symbols. The _q_ –sample of this


**a)** TNS, _N_ = 15, _p_ = 0.2





10 [−2]


10 [−3]


10 [−4]


10 [−5]




|Col1|Col2|Col3|Col4|Sequential<br>Prob. desc|ending|Prob. ascendi<br>Random perm|ng<br>.|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||



2 4 6 8 10 12 14

Truncation dimension _d_


**b)** TNS, _N_ = 15, _p_ = 0.2







10 [−1]


6 × 10 [−2]


4 × 10 [−2]


3 × 10 [−2]

|Col1|Col2|Col3|Col4|Sequential<br>Prob. desc|ending|Prob. as<br>Random|cending<br>perm.|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||



2 4 6 8 10 12 14
Index _i_


FIG. 6. (a) Impact of labelling function on CDR for TNS
model with fixed parameters ( _N, p_ ). Each curve corresponds
to a different strategy for assigning auxiliary labels _y_ to transitions. (b) Schmidt spectra of the dilated iMPS for each of the
above labelling strategies; different assignments of auxiliary
labels _y_ produce distinct entanglement spectra that correlate
with compression performance. In this result the probability
ascending strategies values exactly coincide with the sequential strategy.


dilated process is representable by a normal iMPS with
bond dimension _ds_ = _|S|_, which can be compressed using
standard uniform-MPS tangent-space methods. From
the compressed tensors we reconstruct quantum model
of the original process, with reduced memory dimension.
We applied this pipeline to a TNS toy model and to
a speech-derived HMM learned from data. In both cases
we obtain families of compressed models that interpolate smoothly between the original process and low-bond
approximations. Empirically the CDR decreases monotonically with _d_ [˜], and there is a broad regime in which
_d_ ˜ _≪_ _ds_ while _RC_ ( _P,_ ˜ _P_ ) remains small. This supports


[1] L. Rabiner and B. Juang, An introduction to hidden
markov models, IEEE Acoustics, Speech and Signal Processing magazine **3**, 4 (1986).

[2] C. R. Shalizi and J. P. Crutchfield, Computational mechanics: Pattern and prediction, structure and simplicity,



9


the practical value of the dilation–compression pipeline
as a dimension-reduction tool for practical quantum compression of HMMs
The examples also clarify how compressibility depends
on the entanglement structure of the dilated _q_ –sample.
For the TNS family, different choices of the internal parameter _p_ produce markedly different Schmidt spectra:
rapidly decaying spectra admit accurate truncations at
small _d_ [˜], while flatter spectra require larger bond dimensions to achieve comparable CDR. This supports the view
that entanglement in the dilated iMPS is a useful proxy
for structural complexity that resists compression.
The most immediate extensions of this work are practical. First, the labelling function _f_ is a genuine design
degree of freedom: our results show that it can reshape
the entanglement structure of the dilated _q_ –sample and
substantially change the achievable CDR at fixed _d_ [˜] . This
suggests an algorithmic direction in which the dilation is
not treated as a fixed preprocessing step, but as something to be chosen (or learned) to produce dilations that
are intrinsically easier to compress.
Second, the dilation–compression pipeline provides a
systematic route from large learned HMMs to small quantum samplers with an explicit and interpretable resource
parameter _d_ [˜] . This opens the door to using tensornetwork diagnostics (bond spectra, entropies, canonical
form data) as practical predictors of which learned models are likely to admit aggressive compression without
significantly distorting sequence statistics.
Finally, while we reported CDR because it is operationally meaningful and computationally accessible from
finite-state generators, the analytic results in Sec. III F
give rigorous control of a fidelity-type divergence rate
that is directly inherited from iMPS truncation theorems.
Bridging these two viewpoints more tightly is a natural theoretical direction: one would like conditions under which guarantees in fidelity rate translate into guarantees for process distinguishability measures like CDR,
and conversely, diagnostics that predict CDR behaviour
directly from tensor-network data. Developing such links
would strengthen the pipeline as both a practical reduction tool and a route to principled performance guarantees for quantum stochastic simulation.


**ACKNOWLEDGMENTS**


This work was funded by the University of Manchester
Dame Kathleen Ollerenshaw Fellowship.


Journal of Statistical Physics **104**, 817 (2001).

[3] L. R. Rabiner, A tutorial on hidden markov models and
selected applications in speech recognition, Proceedings
of the IEEE (1989).

[4] P. Baldi, Y. Chauvin, T. Hunkapiller, and M. A.


McClure, Hidden markov models of biological primary sequence information., Proceedings of the National
Academy of Sciences **91**, 1059 (1994).

[5] Z. Ghahramani and M. I. Jordan, Factorial hidden
markov models, in _Advances in Neural Information Pro-_
_cessing Systems_ (1996) pp. 472–478.

[6] S. Fine, Y. Singer, and N. Tishby, The hierarchical hidden markov model: Analysis and applications, Machine
learning **32**, 41 (1998).

[7] K. Seymore, A. McCallum, and R. Rosenfeld, Learning
hidden markov model structure for information extraction, in _AAAI-99 workshop on machine learning for in-_
_formation extraction_ (1999) pp. 37–42.

[8] A. Krogh, B. Larsson, G. Von Heijne, and E. L.
Sonnhammer, Predicting transmembrane protein topology with a hidden markov model: application to complete genomes, Journal of Molecular Biology **305**, 567
(2001).

[9] M. Stanke and S. Waack, Gene prediction with a hidden
markov model and a new intron submodel, Bioinformatics **19**, ii215 (2003).

[10] C. Karlof and D. Wagner, Hidden markov model cryptanalysis, in _International Workshop on Cryptographic_
_Hardware and Embedded Systems_ (Springer, 2003) pp.
17–34.

[11] R. Bhar and S. Hamori, _Hidden Markov models: appli-_
_cations to financial economics_, Vol. 40 (Springer Science
& Business Media, 2004).

[12] S. Gammelmark, K. Mølmer, W. Alt, T. Kampschulte,
and D. Meschede, Hidden markov model of atomic quantum jump dynamics in an optically probed cavity, Physical Review A **89**, 043839 (2014).

[13] P. Warden, Speech commands: A dataset for limitedvocabulary speech recognition (2018), arXiv:1804.03209

[cs.CL].

[14] A. Stolcke and S. M. Omohundro, Best-first model merging for hidden markov model induction (1994), technical
Report TR-94-003, International Computer Science Institute; also arXiv:cmp-lg/9405017.

[15] A. Stolcke and S. M. Omohundro, Model merging for
hidden markov model induction (1996), technical report/manuscript version available online.

[16] P. Dupont, F. Denis, and Y. Esposito, Links between probabilistic automata and hidden markov models:
Probability distributions, learning models and induction
algorithms, Pattern Recognition (2005).

[17] P. Dupont, Probabilistic dfa inference using kullbackleibler divergence and alergia, in _Proceedings of the In-_
_ternational Conference on Machine Learning (ICML)_
(2000).

[18] S. Singh, M. R. James, and M. R. Rudary, Predictive
state representations: A new theory for modeling dynamical systems (2004).

[19] M. Gu, K. Wiesner, E. Rieper, and V. Vedral, Quantum
mechanics can reduce the complexity of classical models,
Nature Communications **3**, 762 (2012).

[20] J. R. Mahoney, C. Aghamohammadi, and J. P. Crutchfield, Occam’s quantum strop: Synchronizing and compressing classical cryptic processes via a quantum channel, Scientific Reports **6**, 20495 (2016).

[21] P. M. Riechers, J. R. Mahoney, C. Aghamohammadi,
and J. P. Crutchfield, Minimized state complexity of
quantum-encoded cryptic processes, Physical Review A
**93**, 052317 (2016).



10


[22] C. Aghamohammadi, S. P. Loomis, J. R. Mahoney, and
J. P. Crutchfield, Extreme quantum memory advantage
for rare-event sampling, Physical Review X **8**, 011025
(2018).

[23] F. C. Binder, J. Thompson, and M. Gu, Practical unitary
simulator for non-Markovian complex processes, Physical
Review Letters **120**, 240502 (2018).

[24] Q. Liu, T. J. Elliott, F. C. Binder, C. Di Franco, and
M. Gu, Optimal stochastic modeling with unitary quantum dynamics, Physical Review A **99** (2019).

[25] T. J. Elliott, Memory compression and thermal efficiency
of quantum implementations of nondeterministic hidden
markov models, Physical Review A **103** (2021).

[26] S. Adhikary, S. Srinivasan, J. Miller, G. Rabusseau,
and B. Boots, Quantum tensor networks, stochastic processes, and weighted automata, in _International Con-_
_ference on Artificial Intelligence and Statistics_ (PMLR,
2021) pp. 2080–2088.

[27] T. J. Elliott, M. Gu, A. J. Garner, and J. Thompson,
Quantum adaptive agents with efficient long-term memories, Physical Review X **12**, 011007 (2022).

[28] M. Zonnios, A. Boyd, and F. Binder, Quantum generation of stochastic processes: spectral invariants and memory bounds, New Journal of Physics (2025).

[29] P. M. Riechers and T. J. Elliott, Identifiability and
minimality bounds of quantum and post-quantum models of classical stochastic processes, arXiv preprint
arXiv:2509.03004 (2025).

[30] M. Schuld, F. Petruccione, M. Schuld, and F. Petruccione, Quantum advantages, Supervised Learning with
Quantum Computers, 127 (2018).

[31] C. Yang, F. C. Binder, V. Narasimhachar, and M. Gu,
Matrix product states for quantum stochastic modeling,
Physical Review Letters **121** (2018).

[32] C. Yang, M. Florido-Llin`as, M. Gu, and T. J. Elliott,
Dimension reduction in quantum sampling of stochastic
processes, npj Quantum Information **11**, 34 (2025).

[33] L. Vanderstraeten, J. Haegeman, and F. Verstraete,
Tangent-space methods for variational optimization of
uniform matrix product states, SciPost Physics Lecture
Notes, 007 (2019).

[34] D. Perez-Garcia, F. Verstraete, M. M. Wolf, and J. I.
Cirac, Matrix product state representations, Quantum
Info. Comput. **7**, 401 (2007).

[35] A. Khintchine, Korrelationstheorie der station¨aren
stochastischen Prozesse, Mathematische Annalen **109**,
604 (1934).

[36] J. P. Crutchfield, Between order and chaos, Nature
Physics **8**, 17 (2012).

[37] D. R. Upper, _Theory and algorithms for hidden Markov_
_models and generalized hidden Markov models_, Ph.D. thesis, University of California, Berkeley (1997).

[38] J. P. Crutchfield and K. Young, Inferring statistical complexity, Physical Review Letters **63**, 105 (1989).

[39] J. P. Crutchfield, The calculi of emergence: computation,
dynamics and induction, Physica D: Nonlinear Phenomena **75**, 11 (1994).

[40] J. P. Crutchfield and D. P. Feldman, Statistical complexity of simple one-dimensional spin systems, Physical Review E **55**, R1239 (1997).

[41] W. L¨ohr and N. Ay, Non-sufficient memories that are
sufficient for prediction, in _International Conference on_
_Complex Sciences_ (Springer, 2009) pp. 265–276.

[42] J. B. Ruebeck, R. G. James, J. R. Mahoney, and J. P.


Crutchfield, Prediction and generation of binary Markov
processes: Can a finite-state fox catch a Markov mouse?,
Chaos: An Interdisciplinary Journal of Nonlinear Science
**28**, 013109 (2018).

[43] M. S. Palsson, M. Gu, J. Ho, H. M. Wiseman, and
G. J. Pryde, Experimentally modeling stochastic processes with less memory by the use of a quantum processor, Science Advances **3**, e1601302 (2017).

[44] F. Ghafari, N. Tischler, J. Thompson, M. Gu, L. K.
Shalm, V. B. Verma, S. W. Nam, R. B. Patel, H. M.
Wiseman, and G. J. Pryde, Dimensional quantum memory advantage in the simulation of stochastic processes,
Physical Review X **9**, 041013 (2019).

[45] F. Ghafari, N. Tischler, C. Di Franco, J. Thompson,
M. Gu, and G. J. Pryde, Interfering trajectories in experimental quantum-enhanced stochastic simulation, Nature
Communications **10**, 1630 (2019).

[46] K.-D. Wu, C. Yang, R.-D. He, M. Gu, G.-Y. Xiang, C.F. Li, G.-C. Guo, and T. J. Elliott, Implementing quantum dimensionality reduction for non-markovian stochastic simulation, Nature Communications **14**, 2624 (2023).

[47] A. Monras, A. Beige, and K. Wiesner, Hidden quantum
Markov models and non-adaptive read-out of many-body
states, arXiv preprint arXiv:1002.2337 (2010).

[48] A. Monr`as and A. Winter, Quantum learning of classical
stochastic processes: The completely positive realization
problem, Journal of Mathematical Physics **57**, 015219
(2016).

[49] M. Fanizza, J. Lumbreras, and A. Winter, Quantum theory in finite dimension cannot explain every general process with finite memory, Communications in Mathematical Physics **405**, 50 (2024).

[50] A. J. P. Garner, Q. Liu, J. Thompson, V. Vedral, _et al._,
Provably unbounded memory advantage in stochastic
simulation using quantum mechanics, New Journal of
Physics **19**, 103009 (2017).

[51] C. Aghamohammadi, J. R. Mahoney, and J. P. Crutchfield, Extreme quantum advantage when simulating classical systems with long-range interaction, Scientific Reports **7** (2017).

[52] T. J. Elliott and M. Gu, Superior memory efficiency of
quantum devices for the simulation of continuous-time
stochastic processes, npj Quantum Information **4**, 18
(2018).

[53] T. J. Elliott, A. J. P. Garner, and M. Gu, Memoryefficient tracking of complex temporal and symbolic dynamics with quantum simulators, New Journal of Physics
**21**, 013021 (2019).

[54] T. J. Elliott, C. Yang, F. C. Binder, A. J. Garner,
J. Thompson, and M. Gu, Extreme dimensionality reduction with quantum modeling, Physical Review Letters
**125** (2020).

[55] T. J. Elliott, Quantum coarse graining for extreme dimension reduction in modeling stochastic temporal dynamics, PRX Quantum **2**, 020342 (2021).

[56] T. J. Elliott and M. Gu, Embedding memory-efficient
stochastic simulators as quantum trajectories, Physical
Review A **109**, 022434 (2024).

[57] L. Banchi, Accuracy vs memory advantage in the quantum simulation of stochastic processes, Machine Learning: Science and Technology **5**, 025036 (2024).

[58] S. Srinivasan, S. Adhikary, J. Miller, G. Rabusseau,
and B. Boots, Quantum tensor networks, stochastic processes, and weighted automata (2020), arXiv:2010.10653



11


[cs.LG].

[59] C. Yang, F. C. Binder, M. Gu, and T. J. Elliott, Measures of distinguishability between stochastic processes,
Physical Review E **101** (2020).

[60] S. E. Marzen and J. P. Crutchfield, Informational and
causal architecture of discrete-time renewal processes,
Entropy **17**, 4891 (2015).

[61] S. B. Davis and P. Mermelstein, Comparison of parametric representations for monosyllabic word recognition
in continuously spoken sentences, IEEE Transactions on
Acoustics, Speech, and Signal Processing **28**, 357 (1980).

[62] D. Sculley, Web-scale K-means clustering, in _Proceedings_
_of the 19th International Conference on World Wide Web_
_(WWW)_ (2010) pp. 1177–1178.

[63] A. P. Dempster, N. M. Laird, and D. B. Rubin, Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical Society: Series B
(Methodological) **39**, 1 (1977).


**Appendix A: Properties of the dilation**


In this Appendix we collect the technical arguments
underlying the structural properties of the dilation We
work throughout with the dilated _q_ –sample iMPS for the
process over ( _x, y_ ) and its compressed version at bond
dimension _d_ [˜], as constructed in Secs. III B and III C.
We start by recording the basic structural properties of
the dilation defined in Sec. III A. Recall that the original
HMM ( _S, X_ _, {T_ _[x]_ _}_ ) is mapped to a dilated HMM ( _S, X ×_
_Y, {T_ [(] _[x,y]_ [)] _}_ ) with transition tensor




- 
_Ts_ [(] _[′][x,y]_ _s_ [)] =
_y∈Y_ _y_ : _y_ = _f_ (



_Ts_ _[x][′]_ _s_ [=] _[ T]_ _s_ _[ x][′]_ _s_ _[,]_ (A3)
_y_ : _y_ = _f_ ( _s,s_ _[′]_ _,x_ )



_Ts_ [(] _[′][x,y]_ _s_ [)] =




_Ts_ _[x][′]_ _s_ _[,]_ if _Ts_ _[x][′]_ _s_ _[>]_ [ 0 and] _[ y]_ [ =] _[ f]_ [(] _[s, s][′][, x]_ [)] _[,]_

(A1)
0 _,_ otherwise,



where the labelling function _f_ : _S×S×X →Y_ is assumed
to be injective in its second argument for each fixed ( _s, x_ ).


**Lemma 1** (Deterministicity of the dilation) **.** _For each_
_state s ∈S and composite symbol_ ( _x, y_ ) _∈X × Y there is_
_at most one successor s_ _[′]_ _∈S with Ts_ [(] _[′][x,y]_ _s_ [)] _>_ 0 _._

_Proof._ Fix _s_ and _x_ . For each _s_ _[′]_ with _Ts_ _[x][′]_ _s_ _[>]_ [ 0 the con-]
struction assigns a single label _y_ = _f_ ( _s, s_ _[′]_ _, x_ ). The injectivity of _f_ for fixed ( _s, x_ ) implies that _s_ _[′]_ = _s_ _[′′]_ gives
_f_ ( _s, s_ _[′]_ _, x_ ) _̸_ = _f_ ( _s, s_ _[′′]_ _, x_ ). Hence for any pair ( _x, y_ ) there is
at most one _s_ _[′]_ such that _Ts_ [(] _[′][x,y]_ _s_ [)] _>_ 0.


**Lemma 2** (Preservation of observable statistics) **.** _Let_
_P be the process on X generated by the original HMM_
_and let P_ dil _be the process on X obtained from the dilated_
_HMM by marginalising over Y. Then for all block lengths_
_L and strings_ ( _x_ 1 _, . . ., xL_ ) _,_


Pr [(] _[x]_ [1] _[, . . ., x][L]_ [) = Pr] dil [(] _[x]_ [1] _[, . . ., x][L]_ [)] _[.]_ (A2)
_P_ _P_



_Proof._ For any _s, s_ _[′]_ and _x_ we have

  -  

since at most one _y_ contributes. Thus the conditional
distribution of ( _St_ +1 _, Xt_ ) given _St_ is unchanged when
we marginalise over _Yt_ . Iterating this equality along the
chain yields equality of all finite-length block probabilities on _X_ .


**Lemma 3** (Ergodicity of the dilation) **.** _If the original_
_HMM is stationary and ergodic, then the dilated HMM is_
_also stationary and ergodic._


_Proof._ The underlying Markov chain on _S_, obtained by
summing over outputs, is the same in both models:



12


_sL_ = _δ_ ( _· · · δ_ ( _s_ 0 _, a_ 1) _· · ·, aL_ ) when the path exists. Moreover, along that path,





_|_ ( _Mw_ ) _sLs_ 0 _|_ [2] =
_sL_



_Ts_ _[a]_ _t_ _[t]_ _st−_ 1 = _⇒_ _|_ ( _Mw_ ) _sLs_ 0 _|_ [2] =








- _L_

_Ts_ _[a]_ _t_ _[t]_ _st_ 1 _[.]_

_−_
_t_ =1



( _Mw_ ) _sLs_ 0 =




- _L_


_t_ =1



(A8)
Summing over _sL_ in (A7) therefore removes the finalstate index without introducing cross-terms:




- _L_

_Ts_ _[a]_ _t_ _[t]_ _st_ 1 _[,]_ (A9)

_−_
_t_ =1







_Ts_ _[x][′]_ _s_ _[.]_ (A4)
_x_



with the understanding that the product is zero if the
path does not exist. Substituting back into (A7) gives




- 
_Ts_ [(] _[′][x,y]_ _s_ [)] =
_x,y_ _x_




- _L_

_Ts_ _[a]_ _t_ _[t]_ _st_ 1 _[.]_ (A10)

_−_
_t_ =1



Irreducibility and aperiodicity of this chain are therefore
inherited from the original HMM, and the stationary distribution over _S_ carries over unchanged. Hence the dilated HMM is stationary and ergodic.


**Lemma 4** (Square-root tensors reproduce word statistics for unifilar generators) **.** _Let_ ( _S, A, {T_ _[a]_ _}a∈A_ ) _be a_
_finite-state, edge-emitting_ unifilar _HMM for a station-_
_ary process on alphabet A (in our application A_ =
_X × Y)._ _Write the transition probabilities as Ts_ _[a][′]_ _s_ [=]
Pr( _St_ +1 = _s_ _[′]_ _, At_ = _a | St_ = _s_ ) _._ _Assume the underlying_
_hidden-state Markov chain P_ := [�] _a_ _[T][ a][ is ergodic with]_

_∈A_
_stationary distribution π._
_Define site matrices A_ _[a]_ _∈_ R _[|S|×|S|]_ _by_




   Tr( _Mwρ⋆Mw_ _[†]_ [) =] _πs_ 0

_s_ 0 _∈S_




  _A_ _[a]_ _s_ _[′]_ _s_ [:=]



_Ts_ ~~_[a]_~~ _[′]_ _s_ _[.]_ (A5)



The right-hand side is exactly the standard unifilar-HMM
expression for the stationary word probability PrHMM( _w_ )
(sum over initial state weighted by _π_, with the internal
path fixed by unifilarity). This proves (A6).


_Relation to the q-sample construction_ Lemma 4 shows
that the square-root tensors always reproduce the _clas-_
_sical_ word statistics for any unifilar presentation when
probabilities are extracted via the induced instrument.
For predictive presentations (e.g. _ε_ -machines) this iMPS
further coincides with the _q_ -sample state in the sense of
Ref. [31]; for general nonpredictive unifilar presentations
one should not assume this stronger identification, even
though the measured statistics agree.


**Appendix B: Error Bounds**


**1.** **From quantum fidelity rate to a classical**
**Bhattacharyya rate**


        Let _|Pxy⟩_ and ��� _P_ ˜ _xy_ denote the infinite-chain _q_ –sample
states of the exact and truncated dilated processes over
_X × Y_ . For concreteness we define the quantum fidelity
divergence rate (QFDR) as

       - 1       -       _RF_ ( _|Pxy⟩_ _,_ ��� _P_ ˜ _xy_ ) = _−_ lim _ρL,_ ˜ _ρL_ _,_ (B1)
_L_ 2 _L_ [log][2] _[ F]_
_→∞_


where _ρL_ and ˜ _ρL_ are the reduced density operators on _L_
sites and _F_ ( _ρ, σ_ ) is the quantum fidelity.
The following lemma relates QFDR to a classical fidelity divergence rate on _X_, defned by the blockwise



_Let ρ⋆_ := diag( _π_ ) _be the diagonal matrix with entries πs._
_Then for every word w_ = _a_ 1 _a_ 2 _· · · aL ∈A_ _[L]_ _,_

         Pr _A_ _[a][L]_ _· · · A_ _[a]_ [1] _ρ⋆_ _A_ _[a]_ [1] _[†]_ _· · · A_ _[a][L][†]_ [�] _._ (A6)
HMM [(] _[w]_ [) = Tr]


_Equivalently, the completely positive maps Ea_ ( _ρ_ ) :=
_A_ _[a]_ _ρA_ _[a][†]_ _define a quantum instrument whose word prob-_
_abilities from the stationary bond state ρ⋆_ _agree exactly_
_with the original unifilar HMM._


_Proof._ Fix a word _w_ = _a_ 1 _· · · aL_ and write _Mw_ :=
_A_ _[a][L]_ _· · · A_ _[a]_ [1] . Since _ρ⋆_ is diagonal,




   Tr( _Mwρ⋆Mw_ _[†]_ [) =]




- 
( _ρ⋆_ ) _s_ 0 _s_ 0
_s_ 0 _∈S_ _sL_




 =




- 
_πs_ 0
_s_ 0 _∈S_ _sL_



_|_ ( _Mw_ ) _sLs_ 0 _|_ [2] _._ (A7)
_sL∈S_



_|_ ( _Mw_ ) _sLs_ 0 _|_ [2]
_sL∈S_



Now use unifilarity: for each current state _s_ and symbol _a_ there is at most one successor state _δ_ ( _s, a_ ) with
_Tδ_ _[a]_ ( _s,a_ ) _,s_ _[>]_ [ 0.] Hence, for fixed _s_ 0, there is at most
one compatible internal-state path _s_ 1 _, s_ 2 _, . . ., sL_ satisfying _st_ = _δ_ ( _st−_ 1 _, at_ ) for all _t_ . Therefore each column _s_ 0
of _Mw_ has at most one nonzero entry, located at row



Bhattacharyya coefficient [�]




~~�~~



_⃗x_



_P_ [(] _[L]_ [)] ( _⃗x_ ) _P_ [˜][(] _[L]_ [)] ( _⃗x_ ).



**Lemma 5** (Classical fidelity rate bounded by QFDR) **.**
_Let P and_ _P_ [˜] _be the processes on X obtained by marginal-_
_ising the exact and truncated dilated q–samples over Y._


_Define the classical fidelity divergence rate_



1          _RF_ ( _P,_ _P_ [˜] ) := _−_ lim
_L_ 2 _L_ [log][2]
_→∞_ _⃗x_




~~�~~



_P_ [(] _[L]_ [)] ( _⃗x_ ) _P_ [˜][(] _[L]_ [)] ( _⃗x_ ) _._



_⃗x∈X_ _[L]_



(B2)
_Then_




           _RF_ ( _P,_ _P_ [˜] ) _≤_ _RF_ ( _|Pxy⟩_ _,_ ��� _P_ ˜ _xy_ ) _._ (B3)



13


**3.** **Tail weight and Schmidt entropy**


We now bound the tail weight _εd_ ˜ in terms of the
Schmidt entropy

    _H_ ( _λ_ ) := _−_ _λk_ log2 _λk._ (B8)

_k_


**Lemma 7** (Entropy controls tail) **.** _Let {λk} be a prob-_

- _ability distribution in non-increasing order and let εd_ ˜ =

_k>d_ [˜] _[λ][k][ for some integer]_ [ ˜] _[d][ ≥]_ [2] _[. Then]_



_Proof._ Consider the reduced density operators _ρL_ and
_ρ_ ˜ _L_ on _L_ sites. Apply two completely positive tracepreserving (CPTP) maps to both states: (i) dephasing
in the computational basis _{|x_ 1 _, y_ 1; _. . ._ ; _xL, yL⟩}_ ; and (ii)
partial trace over the auxiliary outputs _y_ 1 _, . . ., yL_ . Let Φ
denote the composition of these maps. Fidelity is monotone under CPTP maps, so

              -              _F_ ( _ρL,_ ˜ _ρL_ ) _≤_ _F_ Φ( _ρL_ ) _,_ Φ(˜ _ρL_ ) _._ (B4)



_ε_ ˜ _d ≤_ _[H]_ [(] _[λ]_ [)]



_._ (B9)
log2 _d_ [˜]



_Proof._ For _k >_ _d_ [˜] we have _λk ≤_ _λd_ ˜. Since [�]



The outputs Φ( _ρL_ ) and Φ(˜ _ρL_ ) are diagonal density matrices on _X_ _[L]_ encoding the classical block distributions _P_ [(] _[L]_ [)]

and _P_ [˜][(] _[L]_ [)] . Their quantum fidelity reduces to the clas


_Proof._ For _k >_ _d_ we have _λk ≤_ _λd_ ˜. Since _k_ _d_ _[λ][k][ ≤]_ [1, it]

_≤_ [˜]
follows that _d λ_ [˜] _d_ ˜ _≤_ 1 and hence _λd_ ˜ _≤_ 1 _/d_ [˜] . Thus _λk ≤_ 1 _/d_ [˜]
for all _k >_ _d_ [˜] .
Restricting the entropy sum to the tail gives




  _H_ ( _λ_ ) = _−_



_λk_ log2 _λk_
_k>d_ [˜]



sical fidelity coefficient [�]




~~�~~



sical fidelity coefficient _⃗x_ _P_ [(] _[L]_ [)] ( _⃗x_ ) _P_ [˜][(] _[L]_ [)] ( _⃗x_ ). Taking

_−_ [1] [log] [(] _[·]_ [) and passing to] _[ L][ →∞]_ [yields the claim.]




 =



1
_λk_ log2 _λk_ _._ (B10)
_k>d_ [˜]




   _λk_ log2 _λk ≥−_
_k_



_⃗x_



2 _L_ [log][2][(] _[·]_ [) and passing to] _[ L][ →∞]_ [yields the claim.]



**2.** **Truncation and quantum fidelity rate**



Let _{λk}_ _[d]_ _k_ _[s]_ =1 [be the Schmidt coefficients of the dilated]
iMPS across a fixed bond, ordered so that _λ_ 1 _≥_ _λ_ 2 _≥_

_· · · ≥_ _λds_ and [�] _k_ _[λ][k]_ [ = 1. For a target bond dimension]



For _k >_ _d_ [˜] we have _λk ≤_ 1 _/d_ [˜], so log2(1 _/λk_ ) _≥_ log2 _d_ [˜], and
therefore




  _H_ ( _λ_ ) _≥_




_· · · ≥_ _λds_ and _k_ _[λ][k]_ [ = 1. For a target bond dimension]

_d_ ˜ we define the discarded tail weight



_λk_ log2 _d_ [˜] = _ε_ ˜ _d_ log2 _d._ [˜] (B11)
_k>d_ [˜]




 _ε_ ˜ _d_ :=



_λk._ (B5)
_k>d_ [˜]



Rearranging yields the claimed inequality.


Substituting Lemma 7 into Eq. (B7) gives the entropybased bound quoted in the main text:


_RF_ ( _P,_ _P_ [˜] ) _≤_ _c_ _[H]_ [(] _[λ]_ [)] _._ (B12)

log2 _d_ [˜]


**4.** **Slice matrix and support of the stationary bond**
**state**


To relate _H_ ( _λ_ ) to an algebraic quantity depending on
the dilation, we consider the stationary bond density operator _ρ⋆_ of the dilated iMPS. In canonical form, _ρ⋆_ is
the fixed point of the bond channel



The effect of discarding Schmidt weight at a single
bond of a normal iMPS and replacing it with a variationally optimal truncated iMPS is analysed in Ref. [32].
We state the relevant scaling result:


**Lemma 6** (QFDR versus Schmidt tail) **.** _For the normal_
_dilated iMPS and its variational truncation at bond di-_
_mension_ _d_ [˜] _, there exists a constant c >_ 0 _(depending on_
_the local physical dimension and the spectral gap of the_
_transfer operator) such that, for sufficiently small εd_ ˜ _,_

           _RF_ ( _|Pxy⟩_ _,_ ��� _P_ ˜ _xy_ ) _≤_ _c εd_ ˜ _._ (B6)


_Sketch._ Discarding Schmidt weight _εd_ ˜ and constructing
the optimal variational approximation at bond dimension _d_ [˜] perturbs the iMPS by an amount linear in _εd_ ˜ in
appropriate norms. For a normal iMPS with a primitive transfer operator, a finite-block overlap bound can
be derived in terms of _εd_ ˜ and the spectral gap. Passing
to the infinite-chain limit yields the stated bound on the
fidelity divergence rate. The full argument is given in
Ref. [32].


Combining Lemmas 5 and 6 yields

_RF_ ( _P,_ _P_ [˜] ) _≤_ _c εd_ ˜ _._ (B7)




 _E_ ( _ρ_ ) =

_x∈X_





_A_ [(] _[x,y]_ [)] _ρA_ [(] _[x,y]_ [)] _[†]_ _._ (B13)
_y∈Y_



The eigenvalues of _ρ⋆_ are the Schmidt coefficients _{λk}_
across the corresponding bond.
Define the slice matrix _K_ by horizontally concatenating all nonzero site tensors _A_ [(] _[x,y]_ [)], viewed as linear maps
on the bond space:

          -           _K_ = _A_ [(] _[x]_ [1] _[,y]_ [1][)] _A_ [(] _[x]_ [2] _[,y]_ [2][)] _· · ·_ _._ (B14)


The column space of _K_ is the span of the ranges of the
individual _A_ [(] _[x,y]_ [)] .


**Lemma 8** (Support contained in the slice span) **.** _Let ρ⋆_
_be the stationary bond state of the dilated iMPS. Then_
_the support of ρ⋆_ _is contained in the column space of K,_
_and hence_


rank( _ρ⋆_ ) _≤_ rank( _K_ ) _._ (B15)


_Proof._ Let _ρ_ 0 be any initial positive operator with full
support. Iterating the bond channel gives _ρn_ +1 = _E_ ( _ρn_ ).
Each term _A_ [(] _[x,y]_ [)] _ρnA_ [(] _[x,y]_ [)] _[†]_ has columns in the range of
_A_ [(] _[x,y]_ [)], and therefore in the column space of _K_ . Hence
_ρn_ has support contained in that space for all _n ≥_ 1. For
a primitive channel _E_, the sequence _ρn_ converges to the
unique fixed point _ρ⋆_ as _n →∞_ . The column space of
_K_ is closed, so _ρ⋆_ also has support contained in it. The
rank bound follows.


The Schmidt entropy is the von Neumann entropy of _ρ⋆_
in bits. For any density matrix of rank _r_, the entropy is
maximised by the uniform distribution on its support and



14


satisfies _H_ ( _λ_ ) _≤_ log2 _r_ . Applying this with _r_ = rank( _ρ⋆_ )
and using Lemma 8 yields


_H_ ( _λ_ ) _≤_ log2 rank( _ρ⋆_ ) _≤_ log2 rank( _K_ ) _._ (B16)


**5.** **Combined slice-rank bound**


Combining Eq. (B12) with Eq. (B16) gives the slicerank bound


_RF_ ( _P,_ _P_ [˜] ) _≤_ _c_ [log][2][ rank] _[ K]_ _,_ (B17)

log2 _d_ [˜]


which appears as Eq. (36) in the main text. The rank
of _K_ depends explicitly on the chosen labelling function
_f_ through the set of nonzero slices _A_ [(] _[x,y]_ [)], making this
a genuinely label-aware certificate that links the achievable compression error to the structure induced by the
dilation.


