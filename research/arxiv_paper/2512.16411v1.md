## Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection

Matthieu Garcin [a,][∗], Louis Perot [b]


December 19, 2025


**Abstract**


Relative entropy, as a divergence metric between two distributions, can be used for offline
change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of
empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic
regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities
for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical
contributions cover both one- and two-sample empirical relative entropies. We then detail a
change-point detection procedure built on relative entropy and compare it, through extensive
simulations, with classical methods based on moments or on information criteria. Finally, we
illustrate its practical relevance on two real datasets involving temperature series and volatility
of stock indices.


_**Keywords –**_ Berry-Esseen bounds, concentration inequalities, information theory, Kullback-Leibler
divergence, structural break detection, two-sample divergence testing

### **1 Introduction**


Detecting abrupt changes in time series is crucial in many fields, from climatology [61, 27] to
finance [46, 5, 11]. It enables one to assess the validity of a model over a given time interval and
to specify models that appropriately describe or predict time series.


One of the most widespread online methods for change-point detection is the cumulative sum
(CUSUM) procedure [56]. It tracks down significant deviations from the mean and is thus mainly
based on moments. In this paper, we focus on offline methods. Beyond mean shifts, structural
breaks may occur within parametric models when some parameters suddenly move to new values.
Offline approaches in this setting may be based on moments or on information criteria [67, 29].


∗Corresponding author: matthieu.garcin@m4x.org.
a De Vinci Higher Education, De Vinci Research Center, Paris, France.
b D´epartement de math´ematiques et applications, ´Ecole normale sup´erieure, 45 rue d’Ulm, 75005 Paris, France.
Acknowledgements: MG acknowledges the support of the Chair “Deep Finance Statistics” between QRT, Ecole
Polytechnique and its foundation. The authors would like to thank Olivier Benhamou for useful discussions and
support.


1


The latter case consists in finding the time partition that minimizes the information criterion for a
model built as a succession of parameter regimes, each associated with a segment of the partition.


In an offline non-parametric setting, one aims at detecting variations in a non-parametric probability distribution [43, 33]. A distribution may indeed change without affecting the mean or the
variance, which limits the relevance of moment-based approaches such as CUSUM [26, 69]. In this
distribution-based framework, change-point detection amounts to assessing the statistical significance of the deviation between two empirical distributions. This is precisely the practical objective
of the present paper.


Among the existing divergence metrics that could be used in this context, one can cite Wasserstein
distance, Hellinger distance, Kolmogorov-Smirnov statistic, or relative entropy [36, 33, 49]. We
will focus on the latter metric, which is the expectation of the log-likelihood ratio. This ratio
is the statistic leading to the uniformly highest power among the statistical tests of probability
divergence, under the assumptions of Neyman-Pearson lemma [24].


The use of relative entropy in the context of change-point detection was already mentioned sporadically in literature [52, 42]. But the challenge of knowing the exact distribution of empirical relative
entropy often leads to the construction of statistical tests with a threshold based on simulated
quantiles [59]. The theoretical objective of the present paper is to introduce approximations of this
distribution. We focus on three types of them. The most natural is the asymptotic distribution,
which may however not always be relevant in the context of change-point detection, where we may
be interested in small samples, for example to rapidly draw an alert after a break. We obtain as
well pre-asymptotic and finite-sample bounds of the distribution, based either on a Berry-Esseen
approach or on concentration inequalities. One can then use these bounds, instead of the asymptotic approximation, to build conservative statistical tests. Our Berry-Esseen bounds are obtained
for a nonlinear function of a sum statistic, whose limit distribution is non-Gaussian. Our inequality
controls two effects: the classical speed of convergence for an approximation of our statistic and
the error related to this approximation. This method can easily be replicated to other kinds of
nonlinear statistics. We propose as well extensions to two samples, that is approximations of the
distribution of the relative entropy between two empirical probabilities, which is particularly useful
in the context of change-point detection. This question is challenging because relative entropy does
not satisfy a triangle inequality and because its empirical version may become unbounded when
the reference probability is estimated from few observations.


In a simulation study, we show the benefit of using change-point detection methods based on relative
entropy, compared to methods based on moments or on information criteria. Two applications to
a climate dataset and to financial time series highlight the practical relevance of the method. We
study a daily time series of temperatures at Embrun, France, during more than 25 years, as well
as six daily volatility series of stock indices, during about 20 years.


The paper is organized as follows. Section 2 introduces theoretical results about the distribution
of empirical relative entropy. In Section 3, we present the change-point detection method based
on relative entropy along with baseline approaches. Section 4 contains a simulation study and
Section 5 the application to temperature and volatility series. Section 6 concludes.

### **2 Distribution of empirical relative entropy**


We want to compare to each other two discrete probability distributions, with a finite number of
possible states, in order to build a statistical test of equality of the distributions. This can be
done either by testing one after the other the equality of probabilities for each possible state of
the random variable, or by aggregating all these probabilities in a single statistic, thus leading to


2


a single test. It is the purpose of relative entropy.


After a brief introduction on the concept, we detail the asymptotic distribution of the empirical version of relative entropy, with either one or two samples. We also propose pre-asymptotic
and finite-sample bounds of its distribution, deriving both a Berry-Esseen inequality and various
concentration inequalities.


**2.1** **Relative entropy**


We consider a discrete probability, with a finite number _k ≥_ 2 of possible categories: _p_ =
( _p_ 1 _, ..., pk_ ) _[t]_ _∈_ (0 _,_ 1) _[k]_, _Z_ _[t]_ standing for the transposed vector of _Z_ . The Shannon entropy related to
this categorical distribution is



_H_ ( _p_ ) = _−_



_k_

- _pi_ log( _pi_ ) _,_


_i_ =1



where we use the convention 0 log(0) = 0 [21]. The entropy quantifies the uncertainty of the
distribution [21, 31]. The minimum uncertainty corresponds to a concentration in a single state,
leading to the minimum entropy, _H_ ( _p_ ) = 0. The maximum entropy is reached by a uniform
distribution, for which we get _H_ ( _p_ ) = log( _k_ ).


When working with data, we can calculate an empirical entropy, based on empirical probabilities.
We observe _X_ 1, ..., _Xn_, iid random variables, which may be either discrete or continuous. We
discretize these variables by defining _k_ possible states Ω1, ..., Ω _k_, which may for example be
intervals. We have P( _Xj ∈_ Ω _i_ ) = _pi_ for all _j ∈_ 1 _, n_ and _i ∈_ 1 _, k_ . We also define the empirical
probability � _pn_ = ( _p_ - _n,_ 1 _, ...,_ - _pn,k_ ) _[t]_ _∈_ [0 _,_ 1] _[k]_, such that� - - 


_p_ - _n,i_ = _n_ [1]



_n_

- 1 _Xj_ _∈_ Ω _i._ (1)


_j_ =1



The quantity _H_ ( _p_ - _n_ ) is then the empirical entropy. The asymptotic distribution of the empirical
entropy is either a chi-squared or a Gaussian distribution, depending on the nature of _p_ [7, 78].


One can also replace the probability _p_ by a conditional probability. It leads to the evaluation of the
complexity of the dependence structure between two variables, what has been shown to be useful
for time series, the presence of serial dependence being a useful asset for forecasting purposes [16].
It has been shown that the distribution of conditional Shannon entropy and of the close concept
of mutual information is similar to the one of the non-conditional entropy [53, 66, 15, 55].


Relative entropy, sometimes also called Kullback-Leibler divergence, uses the concept of entropy
to compare to each other two probability distributions, _p, q ∈_ (0 _,_ 1) _[k]_ :



_._
_qi_



_D_ KL ( _p∥q_ ) =



_k_





- _pi_ log _[p][i]_

_qi_

_i_ =1



Relative entropy is non-negative and not bounded. But, with a finite number of states, the infinity
of the relative entropy is equivalent to the existence of a state _i_ for which _pi ̸_ = 0 and _qi_ = 0.
Relative entropy is not symmetric in _p_ and _q_ . When _q_ is uniform, relative entropy more simply
writes _D_ KL ( _p∥q_ ) = log( _k_ ) _−_ _H_ ( _p_ ).


Again, replacing _p_ by its empirical counterpart � _pn_ leads to an empirical relative entropy. But one
can also use relative entropy to compare to each other two empirical probabilities. We deal with the
two-sample framework in this paper, considering that the two datasets are generated in the same
distribution _p_ . We are thus given iid observations _X_ 1 _, ..., Xn_ + _m_ with P( _Xj ∈_ Ω _i_ ) = _pi_ for all _j ∈_


3


1 _, n_ + _m_ and _i ∈_ 1 _, k_ . We define a first empirical probability based on _n_ observations, following
�equation (1), and a second empirical probability based on� - - _m_ other independent observations,



_q_ - _m,i_ = _m_ [1]



_n_ + _m_

- 1 _Xj_ _∈_ Ω _i._


_j_ = _n_ +1



In what follows, we study both _D_ KL ( _p_ - _n∥p_ ) and _D_ KL ( _p_ - _n∥q_ - _m_ ). It is worth mentioning a specific
challenge in the two-sample relative entropy: even though _pi >_ 0 whatever _i_, one cannot guarantee
that � _qm,i ̸_ = 0. Beyond this trivial situation which leads to an infinite estimate, � _qm,i_ can be lower
than the true value _pi_ and amplify much the empirical relative entropy.


**2.2** **Asymptotic and pre-asymptotic distributions of empirical relative entropy**


We first focus on the asymptotic distribution of empirical relative entropy, with one or two samples,
thanks to the central limit theorem. Then, an extension of Berry-Esseen bounds to a nonlinear
function of a sum statistic provides us with a non-asymptotic expression converging in distribution
toward the limit of the central limit theorem. We are assuming that the data are generated
according to the probability _p_, so that the theoretical relative entropy should be equal to zero.
Because of a well-known bias, the empirical relative entropy is positive.


The main challenge, when studying the statistical properties of the empirical relative entropy, is
that we have a nonlinear function of the observations. A Taylor expansion can however make the
problem feasible. Unlike what is done in the classical delta method, the first-order term of the
expansion is equal to zero, so we need a second-order expansion and thus a convergence toward a
chi-squared distribution [8, 54]. Another possibility consists in using Wilks’ theorem [70].


Regarding the speed of convergence toward the chi-squared distribution, we would like to use a
Berry-Esseen approach. However, the literature dedicated to Berry-Esseen pre-asymptotic bounds
in the case of a nonlinear statistic is very recent and still narrow. The purpose of Berry-Esseen
inequality is to provide an upper bound to the Kolmogorov-Smirnov statistic between the distribution of a finite-sample statistic and its limit according to the central limit theorem. When the
statistic is a nonlinear function of the observations, it may be possible to linearise it and thus
to express the divergence with respect to a Gaussian distribution [58, 64]. This solution is not
relevant in our case because relative entropy requires at least a quadratic approximation and has
a non-Gaussian limit. Divergence with respect to non-Gaussian distributions have been scarcely
explored in the literature. One can cite a first attempt with a statistic equal to the square of the
sum of the observations, the limit being _χ_ [2] 1 [[38]. It is a first step but it is not enough in our case]
for which the limit is _χ_ [2] _k−_ 1 [. Promising results have been obtained in multidimensional extensions,]
with a chi-square limit, but with a number of degrees of freedom higher than 9 [9] or 5 [40, 41], and
a constant in the bound not explicitly specified or obtained by an indirect numerical procedure,
requiring for example the number of integer vectors in a given ellipsoid [39, Theorem 1]. A very
recent article also puts forward a solution which is valid whatever the number of degrees of freedom
of the chi-squared distribution, but with unspecified constants and a limited domain of validity
that excludes the right tail of the distribution [25]. Unfortunately, the right tail is quite important
for our application to a statistical test for change-point detection.


Theorem 1 gives the central limit of the one-sample relative entropy along with pre-asymptotic
bounds in a Berry-Esseen approach. These bounds constitute a new result. We think it is one of the
very rare attempts to obtain Berry-Esseen bounds for a statistic defined by a non-trivial nonlinear
function of observations which does not converge to a Gaussian. It takes into account both the
error of the quadratic approximation, known as the relative Pearson divergence, and the speed


4


of convergence of this approximation. It exploits Raiˇc’s theorem, which is a recent multivariate
extension of Berry-Esseen inequality, with well-specified constants [60].


**Theorem 1.** _Let X_ 1 _, ..., Xn be iid variables such that_ P( _Xj ∈_ Ω _i_ ) = _pi with p_ = ( _p_ 1 _, ..., pk_ ) _[t]_ _∈_ (0 _,_ 1) _[k]_ _._
_Then, when n →∞, we have_


_d_
2 _nDKL_ ( _p_                 - _n∥p_ ) _−→_ _χ_ [2] _k−_ 1 _[,]_ (2)


_d_
_where_ _−→_ _stands for the convergence in distribution. Let x >_ 0 _. We have_



_Fχ_ 2 _k−_ 1 ~~�~~ _κ_ _[down]_ _n,k_ [(] _[x]_ [)] ~~�~~ _−En,k ≤_ P (2 _nDKL_ ( _p_     - _n∥p_ ) _≤_ _x_ ) _≤_ _Fχ_ 2 _k−_ 1


_where Fχ_ 2 _k−_ 1 _[is the cdf of the][ χ]_ _k_ [2] _−_ 1 _[distribution,]_




~~�~~ ~~�~~
_κ_ _[up]_ _n,k_ [(] _[x]_ [)] + _En,k,_ (3)



_k_
_En,k_ = �42( _k −_ 1) [1] _[/]_ [4] + 16�               

_i_ =1


_and where, for η ∈{up, down}, we have_



(1 _−_ _pi_ ) [3] _[/]_ [2]

( _npi_ ) [1] _[/]_ [2] _[,]_ (4)



(1 _−_ _pi_ ) [3] _[/]_ [2]







_κ_ _[η]_ _n,k_ [(] _[x]_ [) =] - min _{_ ( _−_ 1)1 _η_ = _upκn,k,r_ ( _x_ ) _|r ∈{_ 0 _,_ 1 _,_ 2 _},_ ( _−_ 1)1 _η_ = _upκn,k,r_ ( _x_ ) _>_ 0 _}_ _if_ 27 _x ≤_ 4 _µn_
min _{_ ( _−_ 1) [1] _[η]_ [=] _[up]_ _κn,k,>_ ( _x_ ) _|_ ( _−_ 1) [1] _[η]_ [=] _[up]_ _κn,k,>_ ( _x_ ) _>_ 0 _}_ _else,_



_with the convention_ min( _∅_ ) = + _∞, the notation µ_ = min _i∈_ 1 _,k_ _pi, as well as_

                                       -                                       


_√_
~~_µn_~~
_κn,k,r_ ( _x_ ) =



1 - 27 _x_

3 [arccos] 2 _µn_



27 _x_ 
_−_ [2] _[rπ]_
2 _µn_ _[−]_ [1] 3



3



3




- - 1
2 cos




- 
_−_ 1



_and_

_κn,k,>_ ( _x_ ) = �3 ~~_√µn_~~

3



�


3




9 [+ ][3] 2 _[x]_



9 [+ ][3] 2 _[x]_



+ [9]
3 4




_−_ _[µnx]_



+ [9]
3 4








_−_ _[µn]_



2 [+]




~~�~~




  
3

[9]

4 _[x]_ [2][ +]




_−_ _[µn]_



2 _[−]_




~~�~~



4 _[x]_ [2]



 _√_

~~_µn_~~
 _−_



_._
3




_−_ _[µnx]_



_Moreover, if n →∞, we have_

        _κ_ _[up]_ _n,k_ [(] _[x]_ [)] = _x_ + ~~_√_~~ _[x]_ [3] ~~_µn_~~ _[/]_ [2]



1 
_n_



~~_√_~~ _[x]_ ~~_µn_~~ + _O_ - _n_ 1

~~_√_~~ _[x]_ [3] ~~_µn_~~ _[/]_ [2] + _O_ - _n_ 1



_κ_ _[down]_ _n,k_ [(] _[x]_ [)] = _x −_ ~~_√_~~ _[x]_ [3] ~~_µn_~~ _[/]_ [2]



(5)
_n_ 1 - _._



The proof of Theorem 1 is postponed in Appendix B.


Formula (3) gives pre-asymptotic bounds for the cdf of the empirical relative entropy. The bounds
deal with two approximations. First, _En,k_ is the Berry-Esseen component, related to the speed
of convergence toward the asymptotic distribution. Second, the chi-squared cdf are not simple
functions of _x_ as it would be the case if relative entropy was a simple quadratic function. The
variable _x_ is to be replaced by _κ_ [up] _n,k_ [(] _[x]_ [) and] _[ κ]_ _n,k_ [down][(] _[x]_ [), which take into account the error of the]
quadratic approximation of relative entropy. These quantities are defined as the smallest positive
solutions of a cubic equation. When _n_ increases, _κ_ [up] _n,k_ [(] _[x]_ [) and] _[ κ]_ _n,k_ [down][(] _[x]_ [) tend toward] _[ x]_ [, as one can]
see in equation (5).


We think our approach can be reproduced for obtaining pre-asymptotic bounds for the cdf of
some nonlinear function of sum statistics, under some convexity condition: considering a Taylor
expansion of the nonlinear function, using Raiˇc’s theorem to get the speed of convergence, taking


5


into account a bound of the residual, which in our case is more subtle than the maximum third
derivative, which is infinite.


The Berry-Esseen part of the bounds is uniform in _x_ . There is a recent effort in the literature to
obtain non-uniform bounds in the linear framework [57]. Our problem could certainly benefit in
the future from potential extensions of these non-uniform bounds to nonlinear functions of sum
statistics.


For the two-sample problem, Theorem 2 proposes an asymptotic distribution. We haven’t found
such a result in the literature but it is worth mentioning a close contribution with the asymptotic distribution of the two-sample Jeffreys divergence, which is a symmetric version of relative
entropy [37].


**Theorem 2.** _Let X_ 1 _, ..., Xn_ + _m be iid variables such that_ P( _Xj ∈_ Ω _i_ ) = _pi with p_ = ( _p_ 1 _, ..., pk_ ) _[t]_ _∈_
(0 _,_ 1) _[k]_ _. Then, when n →∞, m →∞, and_ _n_ + _nm_ _[→]_ _[λ][ ∈]_ [(0] _[,]_ [ 1)] _[, we have]_


_d_

2 _[nm]_ _−→_ _χ_ [2] _k−_ 1 _[.]_

_n_ + _m_ _[D][KL]_ [(] _[p]_ [�] _[n][∥][q]_ [�] _[m]_ [)]


The proof of Theorem 2 is postponed in Appendix C.


When the two samples have the same size, that is _n_ = _m_, _DKL_ ( _p_ - _n∥q_ - _m_ ) is asymptotically distributed
like _χ_ [2] _k−_ 1 _[/n]_ [. It is to be compared to the more concentrated asymptotic distribution of] _[ D][KL]_ [(] _[p]_ [�] _[n][∥][p]_ [),]
which is _χ_ [2] _k−_ 1 _[/]_ [2] _[n]_ [.]


In the two-sample case, we do not propose Berry-Esseen bounds. Indeed, in the one-sample case, we
were able to find an upper bound of the rest expressed as a simple function of the two probabilities.
But in the two-sample case the rest of the quadratic approximation of relative entropy depends
on the divergence between each empirical probability and the true probability, that is � _pn,i −_ _pi_
and � _qm,i −_ _pi_, and not on the difference between the two empirical probabilities, � _pn,i −_ _q_ - _m,i_ . We
can however propose a Berry-Esseen bound for the quadratic approximation instead of the relative
entropy itself, as shown in Proposition 1. In Section 2.3, we will present finite-sample results in
the two-sample case, directly applied to relative entropy.



**Proposition 1.** _With the assumptions of Theorem 2 and x >_ 0 _, we have_


     - _k_     
P _nm_      - ( _p_      - _i −_ _q_      - _i_ ) [2] _≤_ _x_ _−_ _Fχ_ 2 _k−_ 1 [(] _[x]_ [)] _≤_ _n_ [2] + _m_
����� _n_ + _m_ _i_ =1 _pi_ ����� ( _nm_ ) [1] _[/]_ [2] ( _n_




_nm_

_n_ + _m_



_i_ =1



_n_ [2] + _m_ [2]
_≤_

( _nm_ ) [1] _[/]_ [2] ( _n_

�����



_k_




( _p_ - _i −_ _q_ - _i_ ) [2]








_−_ _Fχ_ 2 _k−_ 1 [(] _[x]_ [)]



( _nm_ ) [1] _[/]_ [2] ( _n_ + _m_ ) _[E][n]_ [+] _[m,k][,]_




 - _≤_ _x_

_pi_



_with E.,. defined in equation_ (4) _._


The proof of Proposition 1 is postponed in Appendix C.


When _n_ = _m_, the bound in Proposition 1 is _E_ 2 _n,k_, that is _En,k/_ 2 [1] _[/]_ [2] .


**2.3** **Concentration inequalities for empirical relative entropy**


Beside the Berry-Esseen bounds, one can obtain finite-sample bounds of the distribution of the
empirical relative entropy by the mean of concentration inequalities. They in general offer simpler
expressions than Berry-Esseen bounds, without referring to the limit distribution. Instead, they
exploit various methods such as the method of types for the famous Sanov inequality, and a
recursion technique or the moment-generating function in the two promising alternatives we present
below. In addition, as we will see in Section 4.1, concentration inequalities give tighter bounds
than the Berry-Esseen approach when _n_ is small.


6


In what follows, we expose three existing inequalities, with two among the most recent and promising ones, along with a small refinement for the last one. We propose as well a new concentration
inequalities in the two-sample case.


Sanov inequality is the most well-known concentration inequality for relative entropy. It writes

P ( _D_ KL ( _p_            - _n∥p_ ) _≥_ _x_ ) _≤_ [(] _[n]_ _n_ [ +] !( _k_ _[ k]_ _−_ _[ −]_ 1)! [1)!] _[e][−][nx][ ≤]_ [(] _[n]_ [ + 1)] _[k][e][−][nx][,]_ (6)


some works focusing on the first inequality [22, 54], while others prefer the second one [21, Theorem
11.2.1], which is a simpler bound, in particular when _k_ is large.


Based on a recursion technique, Mardia’s bounds improve Sanov inequality, in particular when one
increases _k_ . There are several Mardia’s bounds, each of which apply to a specific range of values
for _k_ . Among them, we focus on the one that demonstrated superior performance for the values
of _k_ considered in our tests:




- _k/_ 2
_e_ _[−][nx]_ _._ (7)



P ( _D_ KL ( _p_ - _n∥p_ ) _≥_ _x_ ) _≤_ _π_ [6][3] _[e][/]_ [2][2]




- _ne_ 3


2 _πk_



It holds when 3 _≤_ _k ≤_ 2 + ~~�~~ _ne_ [3] _/_ 2 _π_ [54].


Agrawal proposed another concentration inequality exploiting the moment-generating function of
the empirical relative entropy [2]. We expose it in the following proposition, along with a slightly
improved version.


**Proposition 2.** _If x >_ ( _k −_ 1) _/n, then_


P( _DKL_ ( _p_          - _n∥p_ ) _≥_ _x_ ) _≤M_ [1] _k,n_ [(] _[x]_ [)] _[ ≤M]_ _k,n_ [2] [(] _[x]_ [)] _[ ≤M]_ _k,n_ [3] [(] _[x]_ [)] _[,]_



_with_








_M_ [1] _k,n_ [(] _[x]_ [)] = inf _t∈_ [0 _,n_ ) _e_ _[−][tx]_ [ ��] _[n]_ _j_ =0 _n_ [2] _[j]_ ( _nn_ ! _−j_ )! _[t][j]_ [�] _[k][−]_ [1]

_M_ [2] _k,n_ [(] _[x]_ [)] = _e_ _[−][nx]_ [ ��] _[n]_ _j_ =0 _n_ _[j]_ ( _nn_ ! _−e_ _j_ )! �1 _−_ _[k]_ _nx_ _[−]_ [1] - _j_ [�] _[k][−]_ [1]

_M_ [3] _k,n_ [(] _[x]_ [)] = _e_ _[−][nx]_ [ �] _kenx−_ 1 - _k−_ 1 _._



The proof of Proposition 2 is postponed in Appendix D.


The third bound in Proposition 2 is the one put forward by Agrawal. As we will see in Section 4.1,
it is both simple and very performing, compared to the ones of equations (6) and (7), at least when
_k_ is small. The second bound, _M_ [2] _k,n_ [(] _[x]_ [), slightly improves] _[ M]_ _k,n_ [3] [(] _[x]_ [), but it is more appropriate]
when _n_ is small because it requires the calculation of a sum of _n_ terms. The algorithmic complexity
for obtaining the first bound, _M_ [1] _k,n_ [(] _[x]_ [), is even worse. Indeed, in addition to the sum of] _[ n]_ [ terms,]
it requires a numerical optimization.


We also remark, beyond the traditional _e_ _[−][nx]_ of the concentration inequalities, that _x_ appears
in other parts of the expression of the bounds of Proposition 2, whereas neither equation (6)
nor equation (7) exhibits this. The consequence is that Agrawal’s bounds are closer to the true
probability when _x_ is small, compared to other methods. When one looks for a quantile at a given
probability, _k_ small or _n_ not too small lead to a small quantile and thus Agrawal’s formula provides
us with a tighter upper bound of the quantile, compared to the alternatives of equations (6) and (7).
This will be confirmed in the study presented in Section 4.1.


For building a concentration inequality in the two-sample framework, we can split relative entropy
in two parts so that we can directly sum the upper bounds of the one-sample case. But such a


7


decomposition does not naturally arise because there is no triangle inequality for relative entropy.
Pinsker’s inequality shows however a correspondence between relative entropy and total variation,
which could be used to obtain the desired decomposition. For discrete probabilities, it writes


2 2

   - _k_   -   -   - [�] _k_   
  - 1 _pi_  


2




_≤_ 2 _D_ KL ( _p∥q_ ) _≤_ - 1 _−_ min _pi_
min _i∈_ �1 _,k_        - _qi_ _i∈_ �1 _,k_        - _qi_



2




_,_ (8)




- [�] _k_

 


_|pi −_ _qi|_

_i_ =1



_|pi −_ _qi|_

_i_ =1



the left bound being the traditional Pinsker’s inequality [68, 17] and the right bound one of the
reverse Pinsker’s inequalities [62], among several other versions [14, 63]. We note that (1 _/_ min _i qi_ ) _−_
(min _i pi/qi_ ) _≥_ _k −_ 1. We thus get, as an interesting side result, the following decomposition of
relative entropy.


**Proposition 3.** _Let p, q, and r be categorical distributions with k categories. If_ min _i∈_ 1 _,k_ _qi >_ 0 _,_

                                                         -                                                         _we have:_



_DKL_ ( _p∥q_ ) _≤_ - 2 _−_ 2 min _pi_
min _i∈_ �1 _,k_       - _qi_ _i∈_ �1 _,k_       - _qi_




( _DKL_ ( _p∥r_ ) + _DKL_ ( _q∥r_ )) _._



The proof of Proposition 3 is postponed in Appendix E.1.


In the particular case _r_ = _p_, it simplifies to


_D_ KL ( _p∥q_ ) _≤_       - 2 _−_ 2 min _pi_
min _i∈_ �1 _,k_               - _qi_ _i∈_ �1 _,k_               - _qi_




_D_ KL ( _q∥p_ ) _._



The scalar in front of the relative entropy on the right-hand side of the above equation can be very
large when one considers a probability _q_ that is far from being uniform and a probability _p_ that
largely diverges from _q_ . We also note that, in Proposition 3, the probabilities can be exchanged
in each relative entropy of the right-hand side of the inequality. Therefore, _D_ KL ( _p∥r_ ) + _D_ KL ( _q∥r_ )
can for example be replaced by _D_ KL ( _p∥r_ ) + _D_ KL ( _r∥q_ ).


Using the one-sample Agrawal’s concentration inequality introduced in Proposition 2 along with the
decomposition put forward in Proposition 3, we get a concentration inequality for the two-sample
relative entropy, when the two samples are assumed to follow the same theoretical distribution. It
is the purpose of Theorem 3.


**Theorem 3.** _Let m, n >_ 0 _and X_ 1 _, ..., Xn_ + _m be iid variables such that_ P( _Xj ∈_ Ω _i_ ) = _pi with_
_p_ = ( _p_ 1 _, ..., pk_ ) _[t]_ _∈_ (0 _,_ 1) _[k]_ _. We note_


_βm,n_ = 2 _−_ 2 min _p_              - _n,i_
min _i∈_ �1 _,k_                - _q_                - _m,i_ _i∈_ �1 _,k_                - _q_                - _m,i_

_and we assume that_ min _i∈_ �1 _,k_ - _q_ - _m,i >_ 0 _and that x ≥_ _βm,n_ ( _k −_ 1)( _m_ + _n_ ) _/mn. Then,_

P ( _DKL_ ( _p_          - _n∥q_          - _m_ ) _≥_ _x_ ) _≤_ _M_ [�][1] _k,n,m_ [(] _[x]_ [)] _[ ≤]_ _M_ [�][2] _k,n,m_ [(] _[x]_ [)] _[ ≤]_ _M_ [�][3] _k,n,m_ [(] _[x]_ [)] _[,]_



_with_








_and_



�� _m_ _m_ ! _n_ !
_M_ - [1] _k,n,m_ [(] _[x]_ [)] = inf _s∈_ [0 _,_ min( _m,n_ )) _e_ _[−][sx/β][m,n]_ _i_ =0 _m_ [2] _[i]_ ( _m−i_ )! _[s][i]_ [ �] _j_ _[n]_ =0 _n_ [2] _[j]_ ( _n−j_ )! _[s][j]_ [�] _[k][−]_ [1]

_M_ - [2] _k,n,m_ [(] _[x]_ [)] = _e_ _[−][σ][m,n,x][x/β][m,n]_ �� _mi_ =0 _m_ [2] _[i]_ ( _mm_ ! _−i_ )! _[σ]_ _m,n,x_ _[i]_ - _nj_ =0 _n_ [2] _[j]_ ( _nn_ ! _−j_ )! _[σ]_ _m,n,x_ _[j]_ - _k−_ 1



_M_ - [3] _k,n,m_ [(] _[x]_ [)] = _e_ _[−][σ][m,n,x][x/β][m,n]_ [ ��] 1 _−_ _[σ][m,n,x]_ _m_



_m_ ��1 _−_ _[σ][m,n,x]_ _n_



��1 _−k_
_n_



_βm,n_ [2] ( _k −_ 1) [2]



_k −_ 1) [2] + [(] _[m][ −]_ _[n]_ [)][2]

_x_ [2] 4



_._
4




_[ m]_

_−_ _[β][m,n]_ [(] _[k][ −]_ [1)]
2 _x_







_σm,n,x_ = _[n]_ [ +] _[ m]_




_−_
_x_



8


The proof of Theorem 3 is postponed in Appendix E.2.


When the two samples have the same size, that is _m_ = _n_, _σm,n,x_ becomes _n −_ 2 _βn,n_ ( _k −_ 1) _/x_, the
bounds simplify, and we get, for example,




       - _enx_ �2( _k−_ 1)
_M_ - [3] _k,n,n_ [(] _[x]_ [) =] _[ e][−][nx/β][n,n]_
2 _βn,n_ ( _k −_ 1)



or



_n_



_i_ =0




1 _−_ [2] _[β][n,n]_ [(] _[k][ −]_ [1)]

_nx_







2( _k−_ 1)




- _i_  _i−_ 1

 

_j_ =0




1 _−_ _[j]_

_n_




- []







_M_ - [2] _k,n,n_ [(] _[x]_ [) =] _[ e][−][nx/β][n,n]_






 _e_



_,_



expression in which we have replace the ratio of factorials by a product that is equal but easier to
compute for large values of _n_ . When _m →∞_, we have


_βm,n →_ _βn_ = 2 _−_ 2 min _p_            - _n,i_ _,_
min _i∈_ �1 _,k_                 - _qi_ _i∈_ �1 _,k_                 - _qi_

as well as _σm,n,x →_ _σn,x_ = _n −_ _βn_ ( _k −_ 1) _/x_ and




       - _enx_
_M_ - [3] _k,n,m_ [(] _[x]_ [)] _[ →]_ _[e][−][nx/β][n]_
_βn_ ( _k −_ 1)




- _k−_ 1
_._



We remark that this limit is different from the expression of _M_ [3] _k,n_ [(] _[x]_ [) provided for the one-sample]
case in Proposition 2. The reason is the presence of _βn_, which should be 1 in order for the limit to
match the one-sample case, but which in reality is higher than 2( _k −_ 1). It thus appears that the
reverse Pinsker’s inequality is quite pessimistic and that the _βm,n_ of Theorem 3 is too large. In
Section 4.1, we show that replacing _βm,n_ by 1 leads to upper bounds that are numerically satisfying.
We note _M_ [�][2] _k,n,m_ _[,⋆]_ [(] _[x]_ [) and][ �] _M_ [3] _k,n,m_ _[,⋆]_ [(] _[x]_ [) these new quantities. Even though we cannot prove it, we]
conjecture that Theorem 3 may apply for a large range of probabilities when one replaces _βm,n_ by
1.

### **3 Change-point detection**


Since relative entropy measures the divergence between two distributions, we can use it in the
framework of change-point detection. The various bounds for the distribution of its empirical
counterpart thus provide possible thresholds for a statistical test of change-point. We introduce
the test along with some other traditional offline alternatives. One of them, based on a difference
of AIC is close in some ways to our method. We will thus compare the two approaches.


**3.1** **Relative entropy and statistical tests for change-point detection**


We adopt the classical formalism of change-point detection [76]. We consider a sequence _X_ 1 _, ..., X_ 2 _n_ .
Under the null hypothesis, _X_ 1 _, ..., X_ 2 _n_ are identically distributed. Under the alternative hypothesis,
probabilitythere exists _q t_, whereas _[⋆]_ _∈_ �2 _,_ 2 _n X −_ _t_ 1 _⋆_ �+1such that _, ..., X_ 2 _n_ are also identically distributed but follow another discrete _X_ 1 _, ..., Xt⋆_ are identically distributed and follow a discrete
probability _p_ .


Two families of tests exist, based either on an online statistic or on an offline statistic. In the
online case, _t_ _[⋆]_ is close to 2 _n_ and the sequential update of the statistic is supposed to lead to a rapid


9


detection of the change-point [72]. CUSUM is a widespread statistic used in this framework [56]
and online change-point detection often consists in detecting a change in the cumulative mean or in
another moment. In the offline case, one generally focuses on _t_ _[⋆]_ = _n_ [47], so that people consider a
divergence statistic between the probabilities of the two sub-samples. The offline approach makes
it possible to compare probabilities, parametric or nonparametric ones, instead of only moments.


In this article, we are interested in offline change-point detection, specifically when _n_ is not very
large. Our test thus consists in _X_ 1 _, ..., Xn ∼_ _q_ and _Xn_ +1 _, ..., X_ 2 _n ∼_ _p_, with _H_ 0 corresponding to
_p_ = _q_ and _H_ 1 to _p ̸_ = _q_ . Translating the spirit of CUSUM in the offline approach, we propose
two baseline change-point tests based on the comparison of the mean (respectively the variance)
of the sub-samples _X_ 1 _, ..., Xn_ and _Xn_ +1 _, ..., X_ 2 _n_, using thus a t-test (resp. F-test), for which the
asymptotic distribution is well known.


The method we put forward here is based on the empirical relative entropy _D_ KL ( _p_ - _n∥q_ - _n_ ), with
the two distributions � _pn_ and � _qn_ estimated on the two sub-samples. Since the exact cdf _FH_ 0 _,n_ of
_D_ KL ( _p_ - _n∥q_ - _n_ ) under _H_ 0 is unknown, both in the one- and in the two-sample cases, simulations are
often used to select a threshold corresponding to a given significance level [59]. Instead, using the
two-sample asymptotic distribution of _D_ KL ( _p_ - _n∥q_ - _n_ ), as described in Theorem 2, we have a good
approximation of its true cdf under _H_ 0, provided that _n_ is not too small. For a more conservative
approximation of _FH_ 0 _,n_, specifically for small values of _n_, we can use the two-sample concentration
inequalities introduced in Theorem 3. Whatever the approximation _F_ [�] _H_ 0 _,n_ of _FH_ 0 _,n_, the quantile
_F_ - _H_ _[−]_ 0 [1] _,n_ [(1] _[ −]_ _[α]_ [) provides us with a threshold that can be used to define a test of nominal significance]

level _α_ . If _D_ KL ( _p_ - _n∥q_ - _n_ ) is above _F_ [�] _H_ _[−]_ 0 [1] _,n_ [(1] _[ −]_ _[α]_ [), we reject] _[ H]_ [0][.]

The simulation study of Section 4 evaluates the different bounds of the cdf _FH_ 0 _,n_ and provides as
well the actual significance level and the power of the statistical tests introduced above.


**3.2** **Another approach using AIC**


Another possible offline method to detect a change-point consists in determining whether a single
model for describing _X_ 1 _, ..., X_ 2 _n_ is more relevant than using a model for _X_ 1 _, ..., Xn_ and another one
for _Xn_ +1 _, ..., X_ 2 _n_ . This can be done by comparing information criteria in the two settings. This
approach has been explored both for parametric models [48, 29] and nonparametric ones [77, 44].


Like in Section 3.1, we consider the distributions � _qn_ and � _pn_, empirical counterparts of the _k_ categorical distributions _q_ and _p_ describing respectively the subsequence _X_ 1 _, ..., Xn_ and _Xn_ +1 _, ..., X_ 2 _n_ .
Thus, the likelihood of ( _X_ 1 _, ..., Xn_ ) is [�] _j_ _[n]_ =1 _[q][X]_ _j_ [=][ �] _i_ _[k]_ =1 _[q]_ _i_ _[n][q]_ [�] _[n,i]_ . Plugging the estimator � _qn_ of _q_ in
this expression and doing the same for ( _Xn_ +1 _, ..., X_ 2 _n_ ) leads to the following log-likelihood of
( _X_ 1 _, ..., X_ 2 _n_ ) in the two-model case, which corresponds to _H_ 1:



_k_

_ℓH_ 1 = _n_ - _{p_ - _n,i_ log ( _p_ - _n,i_ ) + � _qn,i_ log ( _q_ - _n,i_ ) _} ._

_i_ =1



If a single model describes the sequence _X_ 1 _, ..., X_ 2 _n_, that is under _H_ 0, the empirical probability of
the category _i_ is ( _p_ - _n,i_ + � _qn,i_ ) _/_ 2, so that the log-likelihood of ( _X_ 1 _, ..., X_ 2 _n_ ) is now



_k_





_._



_ℓH_ 0 = _n_




- ( _p_ - _n,i_ + � _qn,i_ ) log - _p_ - _n,i_ +2 - _qn,i_

_i_ =1



2



Noting that we have _k −_ 1 parameters in the one-model case and 2( _k −_ 1) otherwise, we obtain the
following criterion of difference of AICs between the approaches:


∆AIC( _p_          - _n,_          - _qn_ ) = _−_ 2( _k −_ 1) _−_ 2 ( _ℓH_ 0 _−_ _ℓH_ 1) _._


10


If ∆AIC( _p_ - _n,_ - _qn_ ) _>_ 0 then the two-model approach is informationally more relevant and we validate
the presence of a change-point.


**3.3** **Link between relative entropy and the AIC-based method**


Writing, for all _i ∈_ �1 _, k_ �, that � _pn,i_ = � _qn,i_ + _εi_, a second-order Taylor expansion provides us with



( _p_ - _n,i_ + � _qn,i_ ) log - _p_ - _n,i_ +2 - _qn,i_





_−_ _p_ - _n,i_ log ( _p_ - _n,i_ ) _−_ _q_ - _n,i_ log ( _q_ - _n,i_ ) = _−_ 4 _q_ - _[ε]_ _n,ii_ [2] + _o_ - _ε_ [2] _i_ 


and with



_D_ KL ( _p_ - _n∥q_ - _n_ ) =



_k_

- _i_ =1 _εi_ + 2 _q_ - _εn,i_ [2] _i_ + _o_ - _ε_ [2] _i_ - _._



Since � _pn,i_ and � _qn,i_ are probabilities, we have [�] _i_ _[k]_ =1 _[ε][i]_ [ = 0 and finally]

∆AIC( _q_       - _n_ + _ε,_       - _qn_ ) = _−_ 2( _k −_ 1) + _nD_ KL ( _q_       - _n_ + _ε∥q_       - _n_ ) + _o_       - _∥ε∥_ [2] 2� _._ (9)


Therefore, both approaches, based either on relative entropy or on a difference of AICs, are very
close to each other, up to an affine transformation, as soon as the two subsequences have close
probabilities. The method based on ∆AIC is not a statistical test but equation (9) gives a natural
threshold on the relative entropy for determining whether there is a change-point in the sequence.
This threshold is 2( _k −_ 1) _/n_ and is not based on any of the approximations presented in Section 2
of the distribution of the relative entropy.


The proximity between relative entropy and ∆AIC is however not a surprise. Indeed, the justification of AIC comes from the expectation of the relative entropy between the data-generating
distribution and an estimated parametric distribution, the number of parameters appearing because of this expectation [3, 19]. However, the fact that the first distribution is in practice replaced
by an empirical distribution is overlooked in the AIC method, which thus clearly states an asymptotic framework. Extensions of AIC, like the corrected AIC [45, 18], may be used instead in order
to better deal with small samples.


Change-point detection methods based either on the difference of AICs or on the relative entropy
both depend on the discretization parameter _k_ . As we will see in the simulations of Section 4.2 and
in the empirical study of Section 5, one detects a bigger number of change-points when _k_ is larger.
Indeed, the differences between two distributions are more apparent with finer discretizations.
We might therefore be inclined to increase _k_ but a limitation appears when we estimate a zero
probability in some categories, because of a too large _k_ with respect to _n_ . Using continuous
probabilities and the differential relative entropy would thus be an interesting extension of this
work.

### **4 A simulation study**


We want to compare all the bounds of the distribution of empirical relative entropy, provided
in Section 2, with the distribution obtained by simulation, for various sample sizes. Then, we
propose a comparison of the change-point detection methods introduced in Section 3, also based
on simulations.


11


**4.1** **A simulation-based evaluation of the bounds of the distribution of relative**
**entropy**


In this paragraph, we consider a uniform distribution for both _p_ and _q_ . Indeed, numerical experiments indicate that the distribution of empirical relative entropy obtained from simulations does
not seem to be significantly affected by changes in the probabilities, provided that the number of
categories remains unchanged and that we stay under _H_ 0: _p_ = _q_ .


We first evaluate the distributions coming from the central limit theorem and Berry-Esseen-like
bounds, introduced in Section 2.2. We display in Figure 1 the cdf of the empirical relative entropy
obtained by simulations, along with the asymptotic distribution and Berry-Esseen bounds, for _n_
equal to 500 _,_ 000 or 2 _,_ 000 _,_ 000 and _k ∈{_ 2 _,_ 4 _}_ .


Figure 1: For � _pn_ estimator of the uniform probability _p_, asymptotic
cdf of 2 _nD_ KL ( _p_        - _n∥p_ ) in _x_ (black dotted line) along with the BerryEsseen bounds (grey dotted lines), as in Theorem 1, and cdf obtained
by 1,000 simulations (black solid line). The size of the sample _n_ is
500,000 (left graphs) and 2,000,000 (right graphs), and the number
of categories _k_ is 2 (top graphs) and 4 (bottom graphs).


In this pre-asymptotic setting, we observe that the asymptotic cdf is very close to to simulated
one and that the Berry-Esseen bounds seem pessimistic when _k_ = 4. This is not surprising since
our bound _En,k_ is based on a uniform multivariate extension of Berry-Esseen inequality. Indeed,
there is currently no consensus on an optimal choice of the constant that appears there, unlike
the univariate case. In order to have an idea of the minimum _n_ making the bounds relevant, we
can stress that _En,k <_ 1 when _n >_ 1 _._ 37 _×_ 10 [5] (respectively _n >_ 3 _._ 36 _×_ 10 [3] ) in the case _k_ = 4


12


(resp. _k_ = 2). One may also wonder about the role of _κ_ [down] _n,k_ [(] _[x]_ [) and] _[ κ]_ _n,k_ [up] [(] _[x]_ [) in inequality 3.]
Their importance is very limited because they converge rapidly toward _x_ as _n_ goes to infinity.
Specifically, in Figure 1, the differences _x −_ _κ_ [down] _n,k_ [(] _[x]_ [) and] _[ κ]_ _n,k_ [up] [(] _[x]_ [)] _[ −]_ _[x]_ [ are increasing in] _[ x]_ [ and reach]
0 _._ 32% (respectively 0 _._ 45%, 0 _._ 64%, 0 _._ 91%) of _x_, for _x_ = 10, _n_ = 2 _,_ 000 _,_ 000, and _k_ = 2 (resp.
_n_ = 2 _,_ 000 _,_ 000 and _k_ = 4, _n_ = 500 _,_ 000 and _k_ = 2, _n_ = 500 _,_ 000 and _k_ = 4).


We now focus on finite-sample distributions and concentration inequalities. We will let _k_ and _n_ vary
around the baseline case _n_ = 100, _k_ = 4. We compare quantiles of the empirical relative entropy at
75% and 95%, in the one- and two-sample cases, following various methods: the empirical quantiles
obtained from 10,000 simulations, the quantile in the asymptotic distribution of Theorems 1 and 2,
and the quantiles obtained from concentration inequalities. Specifically, in the one-sample case,
the considered bounds are the two Sanov’s bounds of equation (6), Mardia’s bound provided in
equation (7), and Agrawal’s second and third bounds, _M_ [2] _k,n_ [and] _[ M]_ _k,n_ [3] [, defined in Proposition 2.]
With two samples, the only bounds considered are the extensions of the above Agrawal’s bounds,
_M_ - [2] _k,n,n_ _[,⋆]_ [and][ �] _M_ [3] _k,n,n_ _[,⋆]_ [, defined after Theorem 3, with the same size for the two subsamples.]


Some of the concentration inequalities are valid under some condition. Agrawal’s condition _x >_
( _k_ _−_ 1) _/n_ is always verified in our results and we have to discard the case _k_ = 2 for Mardia’s bound.
In the two-sample case, we also constrain _k_ to be less than or equal to 8; otherwise, simulations
sometimes lead to empty categories, and consequently to zero estimated probabilities � _qn_, when
_n_ = 100. This would cause the relative entropy to diverge to infinity.


The obtained quantiles, as functions of _n_, are displayed in Figure 2. We observe that the quantile
of the asymptotic distribution is very accurate: it only slightly underestimates the true quantile
when _n_ is small (less than 50) and this underestimation becomes more pronounced the further we
move into the tail of the distribution. Regarding concentration inequalities, Sanov’s bounds largely
overestimate the quantile. Mardia’s bound is only slightly better than the best Sanov’s bound,
whereas the two Agrawal’s bounds, which are very close to each other both in the one- and the
two-sample cases, are significantly better than the other bounds, though not as accurate as the
quantile of the asymptotic distribution.


Figure 3 shows the quantiles as functions of _k_ . The quantile at 95% of the asymptotic distribution
more clearly underestimates the true quantile when _k_ is larger. Once again, Sanov’s bounds are
the least accurate. Mardia’s bound comes next but it increases more slowly with _k_ than the
other concentration bounds do. This is consistent with its known reliability for large values of _k_ .
Nevertheless, for the range of values considered for _k_, Agrawal’s bounds are our best concentration
bounds. The two versions are again very close to each other, both for one and two samples. For
this reason, we will now only consider _M_ [3] _k,n_ [and][ �] _M_ [3] _k,n,m_ _[,⋆]_ [, whose expression is simpler than] _[ M]_ _k,n_ [2]
and _M_ [�][2] _k,n,m_ _[,⋆]_ [.]


**4.2** **Detection of change-point assessed by simulations**


We now evaluate the propensity of the methods introduced in Section 3 to detect an existing
change-point and not to trigger an alert when there is no change-point. The method put forward
in this work is a statistical test on the empirical relative entropy, with various approximations of its
distribution, but always in a two-sample context. The benchmark methods are a t-test, an F-test,
and the ∆AIC method.


Among all the possible models depicting a change-point, we use a simple parametric categorical
model, with _k_ categories _{_ 1 _,_ 2 _, ..., k}_ of ranked probability. For _i ∈_ 1 _, k_, it is defined by

                                     -                                     
_e_ _[−][ϕi]_
_πi_ _[ϕ,k]_ = ~~�~~ _k_ _[,]_
_j_ =1 _[e][−][ϕj]_


13


Figure 2: Quantile of the relative entropy as a function of _n_, at
75% (left graphs) and 95% (right graphs), for one (top graphs) or
two samples (bottom graphs), according to the 10,000 simulations
(red line), to the asymptotic distribution (black dashed line), to the
second and third Agrawal’s bounds (grey dashed lines), to the two
Sanov’s bounds (grey solid lines), and to Mardia’s bound (black solid
line). Other parameters are _k_ = 4 and _pi_ constant in _i_ .


14


Figure 3: Quantile of the relative entropy as a function of _k_, at 75%
(left graphs) and 95% (right graphs), for one (top graphs) or two
samples (bottom graphs), according to the 10,000 simulations (red
line), to the asymptotic distribution (black dashed line), to the second
and third Agrawal’s bounds (grey dashed lines), to the two Sanov’s
bounds (grey solid lines), and to Mardia’s bound (black solid line).
Other parameters are _n_ = 100 and _pi_ constant in _i_ .


15


which can be seen as either a finite version of a discrete exponential distribution [6] or a variation
of the finite geometric distribution [20]. It is a simple way of modelling with a single parameter
both the uniform distribution, when _ϕ_ = 0, and disparity between categories, when _ϕ ̸_ = 0. In our
change-point framework, we generate _n_ first i.i.d. variables in the distribution _π_ _[ϕ,k]_ then _n_ others
in the distribution _π_ _[ϕ]_ [+] _[ψ,k]_ . The presence of a change-point is thus characterized by _ψ ̸_ = 0. The
closer _ψ_ is to zero, the less significant is the change-point. We use the same baseline case as in
Section 4.1, that is _n_ = 100, _k_ = 4 and _ϕ_ = 0, and we consider the following standalone variations:
_n_ = 50, _k_ = 6, and _ϕ_ = 0 _._ 3. In all the experiments, we consider a large range of values for _ψ_ :

[ _−_ 0 _._ 8 _,_ 0 _._ 8].


For a simulated trajectory of length 2 _n_, with or without a change-point, depending on the value of
_ψ_, we carry out the statistical tests of Section 3. For the tests based on relative entropy, we estimate
an empirical probability for each category rather than the parameter of the distribution used to
generate the data. For each test, we record its outcome and average it over 10,000 trajectories
simulated with the same parameters. We then consider the proportion of times each test rejects
the null hypothesis of no change-point.


Figure 4 gathers the results. The nominal confidence level is set at 95%. Each curve intersects
the Y-axis ( _ψ_ = 0) at a probability equal to one minus the actual confidence level. The size of
each test can thus be read at this specific point of the corresponding graph. The ∆AIC approach
is liberal, producing more inappropriate rejections of _H_ 0 than expected. The other tests are at
or above the nominal confidence level, with two of them being even conservative: the F-test and
the relative entropy test based on the _M_ [�][3] _k,n,n_ _[,⋆]_ [approximation. The power of each test can be read]
directly from the graph when _ψ ̸_ = 0. It shows that the t-test is the most powerful, followed by the
∆AIC method, the relative entropy test based on the asymptotic distribution, the one based on
_M_ - [3] _k,n,n_ _[,⋆]_ [, and finally the F-test, which has some difficulties detecting change-points.]


Overall, the t-test seems the most appropriate for this example. However, this analysis, based on
the sole average of random variables, cannot be as rich as an approach based on the full distribution,
like tests on relative entropy. We thus consider another generating model, with states _{_ 1 _,_ 2 _,_ 3 _,_ 4 _}_
of probability _q_ = (0 _._ 25 _,_ 0 _._ 25 _,_ 0 _._ 25 _,_ 0 _._ 25) for the first sub-sample and _p_ = ( _p_ 1 _,_ 0 _._ 5 _−_ _p_ 1 _,_ 0 _._ 5 _−_ _p_ 1 _, p_ 1)
for the second one. The variables have the same expectation in the first and in the second subsamples, but not the same distribution. Figure 5 shows that the t-test is now unable to detect
change-points when they exist. The ∆AIC method is liberal again, whereas all the other methods
have an appropriate actual confidence level.


The conclusions of the two examples indicate that change-point tests based on relative entropy are
particularly relevant and do not seem to strongly depend on the way the data are generated.

### **5 Application to real data**


This section presents an application of the method based on relative entropy to detect changepoints. We study two datasets, one of temperature series and another one of volatility series.


**5.1** **Study of a climate dataset**


Change-point detection in temperature time series is a well-established topic in the climatology
literature, typically conducted on yearly or monthly averaged data. While CUSUM-type procedures
are widespread [28], many existing approaches are designed for offline analysis. Most methods aim
at detecting changes in the mean [61], or in linear or quadratic trends [27, 65]. Less frequently,
nonparametric trend changes are considered, using for example wavelet-based techniques [27].


16


Figure 4: Proportion of the 10,000 simulated time series for which a
change-point is detected, as a function of _ψ_ . The tests used are based
i/ on the relative entropy with a threshold defined by the 95% quantile
in either the asymptotic two-sample distribution (solid black line) or
the _M_ [�][3] _k,n,n_ _[,⋆]_ [bound (dashed black line); ii/ on the ∆AIC method (solid]
grey line); iii/ on the t-test with 95% confidence (solid red line); iv/
on the F-test with 95% confidence (dashed red line). The parameters
are ( _k, n, ϕ_ ) = (4 _,_ 100 _,_ 0), except in the top right graph ( _ϕ_ = 0 _._ 3),
in the bottom left graph ( _n_ = 50), and in the bottom right graph
( _k_ = 6).


17


Figure 5: Proportion of the 10,000 simulated time series for which
a change-point is detected, as a function of _p_ 1, where the generating probability of the _n_ = 100 first observations is _q_ =
(0 _._ 25 _,_ 0 _._ 25 _,_ 0 _._ 25 _,_ 0 _._ 25) and, for the next _n_ observations, _p_ = ( _p_ 1 _,_ 0 _._ 5 _−_
_p_ 1 _,_ 0 _._ 5 _−_ _p_ 1 _, p_ 1). The tests used are based i/ on the relative entropy
with a threshold defined by the 95% quantile in either the asymptotic two-sample distribution (solid black line) or the _M_ [�][3] _k,n,n_ _[,⋆]_ [bound]
(dashed black line); ii/ on the ∆AIC method (solid grey line); iii/ on
the t-test with 95% confidence (solid red line); iv/ on the F-test with
95% confidence (dashed red line).


18


Depending on the specification, the significance of the change-point is assessed using information
criteria such as ∆AIC [61, 65], classical statistical tests [28, 61], or statistical tests in a simulated
distribution under a specific noise assumption, for example a white Gaussian noise, a long-range
noise [13, 73], or even an alpha-stable noise [27]. Besides, standard normal homogeneity tests
(SNHT) are also widely used in climatology [4, 28]. The main purpose of SNHT is to detect
artificial changes in a station’s time series, such as those caused by instrument replacement, by a
comparison to the mean value of surrounding stations.


We use a public dataset of temperatures at Embrun (Hautes-Alpes, France), between January
1999 and December 2024, obtained from the website of M´et´eo-France. [1] The dataset is sampled
every 3 hours. We average it so that we get the daily and the weekly time series displayed in
Figure 6. Of course, the seasonality is an important feature of such time series. We thus consider
the distribution of daily or weekly temperatures in a one-year window. Then, we compare the
distribution of two distinct years using relative entropy, like in the method described in Section 3.
Constructed from high-resolution temporal data, this distribution-based approach therefore differs
from the more commonly used climatological methods, which rely on yearly averages.


Figure 6: Daily (left) and weekly (right) average temperature at Embrun between January 1999 and December 2024.


Figure 7 shows this relative entropy, along with the bounds corresponding to the quantile at 99%
of various distributions: the one- and two-sample asymptotic distribution and Agrawal’s type
bounds _M_ [3] _k,n_ [and][ �] _M_ [3] _k,n,n_ _[,⋆]_ [. The ∆AIC is very close to the affine transformation of the relative]
entropy, as expressed in equation (9), so that we also display the threshold 2( _k −_ 1) _/n_ as a relevant
approximation of the significance bound of the relative entropy. The two important parameters of
our change-point detection method are _n_ and _k_ . The first one simply corresponds to the number
of days or week in a year. Regarding _k_, we consider a small value, 4, and a bigger one, 10 for
daily and only 6 for weekly data. Indeed, when we simply consider 52 weekly observations, we get
several probabilities equal to zero if we work with a fine discretization of 10 categories. We plot
two curves showing the relative entropy between the year ending at the x-axis date and either the
previous year or a reference year taken as the first year in the dataset.


The results shown in Figure 7 reveal several change-points. The greater _k_, the finer the discretization of the probability and the higher the resolution of change-point detection. Increasing _n_, by
considering daily instead of weekly observations, also leads to the detection of more change-points.
Although the weekly graphs show an upward trend in relative entropy, a large peak in three of
the four graphs also indicates a strong change-point in 2007. It correspond to a particularly hot
winter in 2006-2007. This is confirmed by a kernel density representation of the temperatures this
year, compared to both the density the year before and the density during the first year of the


1 `[https://donneespubliques.meteofrance.fr/](https://donneespubliques.meteofrance.fr/)`, station 7591.


19


Figure 7: Relative entropy between empirical probabilities estimated
on a year of data finishing at the date indicated in abscissa and on
either the previous year (red curve) or the first year of data (black
curve). The categories are delimited by empirical quantiles on the
whole dataset, of probabilities _{_ 1 _/k,_ 2 _/k, ...,_ ( _k −_ 1) _/k}_, where _k_ is
either 4 (left graphs), 6 (bottom right graph), or 10 (top right graph).
Dashed horizontal lines are confidence bounds at 99%, obtained using
the one- and two-sample asymptotic distributions (thin and thick
red), as well as _M_ [3] _k,n_ [and][ �] _M_ [3] _k,n,n_ _[,⋆]_ [(thin and thick black). The solid]
horizontal line corresponds to the value 2( _k −_ 1) _/n_, where _n_ is the
number of observations each year, for the daily (top graphs) or weekly
sampling (bottom graphs).


20


dataset. See Figure 8. We see that the right tails coincide, whereas a large divergence appears in
the left tails, in addition to the fluctuations in the bulk of the density. The bandwidth used for
each density ranges between 1.3 and 1.7. It was selected so as to maximize a complexity criterion,
thereby ensuring a good balance between underfitting and overfitting [31].


Figure 8: Kernel density estimates of the daily (left graph) and weekly
(right graph) average temperature, with a complexity-based bandwidth. The data used correspond to a year of observations in 1999
(light grey), from July 2005 to June 2006 (dark grey), from July 2006
to June 2007 (black).


**5.2** **Study of a financial dataset**


Volatility is a key quantity in financial markets, which leads investment decisions. Starting from
a model in which volatility is a fixed parameter, a CUSUM method applied to squares of price
returns was first proposed to detect a change-point in volatility [46]. The heteroscedastic nature
of financial time series led to the introduction of stochastic processes of volatility, with serial
dependence. However, the traditional CUSUM method too frequently misses the detection of
change-points in this stochastic framework [23], requiring some adjustments [50]. Besides these
online approaches, offline methods have been proposed to detect multiple change-points in the
parameters of a stochastic volatility model like GARCH [51, 5]: they consist in finding the partition
of time optimizing a least-square-based accuracy criterion along with a penalization, in the spirit
of the methods based on information criteria described in Section 3.2.


While econometric models are usually defined in discrete time under regular sampling, quantitative
finance research often turns to continuous-time volatility models, which can be discretized under
irregular sampling schemes. Among these models, the rough volatility model, which is based
on a geometric fractional Brownian motion (fBm), is very popular [35]. It makes it possible to
depict serial dependence in volatility thanks to a single parameter, the Hurst exponent [16]. The
propensity of this kind of model to forecast future volatility is promising [30, 74, 12]. However,
some works suggest that the Hurst exponent is not enough to describe the serial dependence of
volatility series, and that multifractal extensions [32, 71] or jumps [1, 34] are to be considered. The
detection of change-points in volatility series is therefore crucial for identifying the time intervals
in which the fBm framework remains valid. The solutions proposed in the literature consist in
detecting jumps [11] or a change-point in the Hurst exponent using CUSUM [10] or using the local
absolute variation [75].


We propose here to apply the change-point tests described in Section 3. The data are five daily


21


time series of realized volatility computed with a five-minute discretization of prices of stock indices, imported from the formerly available Oxford-Man Institute of Quantitative Finance Realized
Library: the AEX index, the CAC 40 index, the Nikkei 225 index (N225), the Oslo Exchange Allshare index (OSEAX), and the S&P 500 index (SPX). The series starts on January 2000, except
N225, which starts in February 2000 and OSEAX in September 2001. The end date of our sample
is on the 12th April 2021. We import as well from Bloomberg the VIX, which is obtained from
option prices on the SPX, in the same time period 2000-2021. Figure 9 shows the six time series.


Figure 9: Daily time series of realized volatility for AEX (top left
graph), CAC 40 (top right), N225 (middle left), OSEAX (middle
right), SPX (bottom left), followed by the VIX (bottom right).


For forecasting applications, we’re interested in the dependence structure of a volatility series. We
therefore discretize each increment of the volatility process, depending on whether it is positive
or negative, resulting in an indicator equal to 1 or 0 [15, 16]. We then consider a vector of
three consecutive indicators, which has thus eight possible categories. We consider a window size
_n_, corresponding to one year of data, for estimating the categorical probabilities. We will also
consider a smaller _n_, with a three-month period. In the latter case, in order not to have zero
probabilities, we reduce the number of categories to six, merging the two categories corresponding


22


to a trend, that is (0 _,_ 0 _,_ 0) _[′]_ and (1 _,_ 1 _,_ 1) _[′]_, and merging those defined by a strict alternation of 0s
and 1s, that is (0 _,_ 1 _,_ 0) _[′]_ and (1 _,_ 0 _,_ 1) _[′]_ .


Depending on the series and the threshold selected, the relative entropy between two probabilities
built with one year of data indicates a few change-points, as one can see in Figure 10. A clear
change-point appears during the global financial crisis (around 2008-2009) for the VIX. Changepoints are less obvious for realized volatilities in this period, even though the relative entropy of
two consecutive years seems to be significant for SPX and N225, but not if the reference year is the
first year. Other important change-points include the interval 2014-2015 for OSEAX. This can be
explained by the Norwegian economy’s dependence on oil prices: the price of Brent crude fell by 50%
in less than a year starting in June 2014, which reshaped both the index’s sectoral composition and
the serial dependence pattern of its volatility. The peaks in relative entropy for the N225 starting
in March 2011 correspond to the major event of Fukushima accident. Interestingly, this appears as
a single isolated peak in the realized volatility series, whereas the change-point indicator remains
elevated for several months, more realistically reflecting the prolonged uncertainty following the
event.


Figure 11 displays the results for three-month windows for the realized volatility of SPX and VIX.
Relative entropy is much more fluctuating. For the two series, a change-point is detected during the
global financial crisis. The COVID-19 crisis (around 2020) also generates a change-point, whereas
it was not obvious with one-year windows.

### **6 Conclusion**


In this paper, we have been interested in the distribution of relative entropy, in the one- and
two-sample cases. We have thus presented the asymptotic distribution along with Berry-Esseen
bounds. For finite samples, we also have provided concentration bounds. All these approximations of the true distribution are useful for building change-point statistical tests. We have thus
described a method based on relative entropy and compared it to more classical approaches thanks
to extensive simulations. It highlights the limitations of moments-based tests in some situations,
where modifications in the probability distribution do not make the first moments deviate, thus
making the test based on relative entropy more general. Two applications to real data, namely climate and finance datasets, emphasize the practical relevance of this method. It makes the climate
change visible and unveils modifications in the serial dependence of volatility that we can relate
to macroeconomic events. Our theoretical findings rely on recent advances, including results on
the multivariate Berry-Esseen inequality and on reverse Pinsker’s inequality. These tools do not
yet exhibit the same level of theoretical maturity as classical results such as the univariate BerryEsseen inequality or Pinsker’s inequality. Further advances on these two inequalities, for example
regarding a non-uniform version of multivariate Berry-Esseen bounds, would directly strengthen
our theoretical bounds and enhance the accuracy of our change-point application.

### **References**


[1] E. Abi Jaber and N. De Carvalho. Reconciling rough volatility with jumps. _SIAM journal on_
_financial mathematics_, 15(3):785–823, 2024.


[2] R. Agrawal. Finite-sample concentration of the multinomial in relative entropy. _IEEE trans-_
_actions on information theory_, 66(10):6297–6302, 2020.


23


Figure 10: For each daily series of realized volatility for AEX (top
left graph), CAC 40 (top right), N225 (middle left), OSEAX (middle
right), SPX (bottom left), followed by the VIX (bottom right), relative entropy between empirical discrete probabilities estimated on a
year of data finishing at the date indicated in abscissa and on either
the previous year (red curve) or the first year of data (black curve).
Dashed horizontal lines are confidence bounds at 99%, obtained using the one- and two-sample asymptotic distributions (thin and thick
red), as well as _M_ [3] _k,n_ [and][ �] _M_ [3] _k,n,n_ _[,⋆]_ [(thin and thick black). The solid]
horizontal line corresponds to the value 2( _k −_ 1) _/n_, where _n_ is the
number of observations each year.


24


Figure 11: For the daily series of the realized volatility of SPX (left
graph) and the series of VIX (right graph) relative entropy between
empirical discrete probabilities estimated on three months of data
finishing at the date indicated in abscissa and on either the previous
year (red curve) or the first year of data (black curve). Dashed horizontal lines are confidence bounds at 99%, obtained using the oneand two-sample asymptotic distributions (thin and thick red), as well
as _M_ [3] _k,n_ [and][ �] _M_ [3] _k,n,n_ _[,⋆]_ [(thin and thick black). The solid horizontal line]
corresponds to the value 2( _k −_ 1) _/n_, where _n_ is the number of observations in the three-month window.


[3] H. Akaike. Information theory and an extension of the maximum likelihood principle. In
_Selected papers of Hirotugu Akaike_, pages 199–213. Springer, 1998.


[4] H. Alexandersson. A homogeneity test applied to precipitation data. _Journal of climatology_,
6(6):661–675, 1986.


[5] E. Andreou and E. Ghysels. Detecting multiple breaks in financial market volatility dynamics.
_Journal of applied econometrics_, 17(5):579–600, 2002.


[6] A. Barbiero and A. Hitaj. A new discrete exponential distribution: properties and applications.
_Journal of statistical theory and practice_, 19(3):39, 2025.


[7] G.P. Basharin. On a statistical estimate for the entropy of a sequence of independent random
variables. _Theory of probability & its applications_, 4(3):333–336, 1959.


[8] F. Bavaud. Information theory, relative entropy and statistics. In _Formal theories of infor-_
_mation: From Shannon to semantic information theory and general concepts of information_,
pages 54–78. Springer, 2009.


[9] V. Bentkus and F. G¨otze. Uniform rates of convergence in the CLT for quadratic forms in
multidimensional spaces. _Probability theory and related fields_, 109:367–416, 1997.


[10] M. Bibinger. Cusum tests for changes in the Hurst exponent and volatility of fractional
Brownian motion. _Statistics & probability letters_, 161:108725, 2020.


[11] M. Bibinger, M. Jirak, and M. Vetter. Nonparametric change-point analysis of volatility.
_Annals of statistics_, 45(4):1542–1578, 2017.


[12] M. Bibinger, J. Yu, and C. Zhang. Modeling and forecasting realized volatility with multivariate fractional Brownian motion. _Preprint_, 2025.


25


[13] R. Blender and K. Fraedrich. Long time memory in global warming simulations. _Geophysical_
_research letters_, 30(14):1769, 2003.


[14] J. Bretagnolle and C. Huber. Estimation des densit´es: risque minimax. _Zeitschrift f¨ur_
_Wahrscheinlichkeitstheorie und verwandte Gebiete_, 47(2):119–137, 1979.


[15] X. Brouty and M. Garcin. A statistical test of market efficiency based on information theory.
_Quantitative finance_, 23(6):1003–1018, 2023.


[16] X. Brouty and M. Garcin. Fractal properties, information theory, and market efficiency. _Chaos,_
_solitons & fractals_, 180:114543, 2024.


[17] C.L. Canonne. A short note on an inequality between KL and TV. _Preprint_, 2022.


[18] J.E. Cavanaugh. Unifying the derivations for the Akaike and corrected Akaike information
criteria. _Statistics & probability letters_, 33(2):201–208, 1997.


[19] J.E. Cavanaugh and A.A. Neath. The Akaike information criterion: Background, derivation, properties, application, interpretation, and refinements. _Wiley interdisciplinary reviews:_
_computational statistics_, 11(3):e1460, 2019.


[20] R. Chattamvelli and R. Shanmugam. Geometric distribution. In _Discrete distributions in_
_engineering and the applied sciences_, pages 65–82. Springer, 2020.


[21] T.M. Cover and J.A. Thomas. _Elements of information theory_ . John Wiley & sons, second
edition, 2006.


[22] I. Csisz´ar. The method of types. _IEEE transactions on information theory_, 44(6):2505–2523,
2002.


[23] M. De Pooter and D. Van Dijk. Testing for changes in volatility in heteroskedastic time series

   - a further examination. Technical report, 2004.


[24] S. Eguchi and J. Copas. Interpreting Kullback-Leibler divergence with the Neyman-Pearson
lemma. _Journal of multivariate analysis_, 97(9):2034–2040, 2006.


[25] X. Fang, S.-H. Liu, and Q.-M. Shao. Cram´er-type moderate deviation for quadratic forms
with a fast rate. _Bernoulli_, 29(3):2466–2491, 2023.


[26] T. Flynn and S. Yoo. Change detection with the kernel cumulative sum algorithm. In _2019_
_IEEE 58th conference on decision and control (CDC)_, pages 6092–6099. IEEE, 2019.


[27] C. Franzke. Nonlinear trends, long-range dependence, and climate noise properties of surface
temperature. _Journal of climate_, 25(12):4172–4183, 2012.


[28] C. Gallagher, R. Lund, and M. Robbins. Changepoint detection in climate time series with
long-term trends. _Journal of climate_, 26(14):4994–5006, 2013.


[29] Z. Gao, X. Xiao, Y.-P. Fang, J. Rao, and H. Mo. A selective review on information criteria in
multiple change point detection. _Entropy_, 26(1):50, 2024.


[30] M. Garcin. Forecasting with fractional Brownian motion: a financial perspective. _Quantitative_
_finance_, 22(8):1495–1512, 2022.


[31] M. Garcin. Complexity measure, kernel density estimation, bandwidth selection, and the
efficient market hypothesis. In A. Sinha, editor, _Select topics of econophysics_ . De Gruyter,
2024.


26


[32] M. Garcin and M. Grasselli. Long versus short time scales: the rough dilemma and beyond.
_Decisions in economics and finance_, 45(1):257–278, 2022.


[33] M. Garcin, J. Klein, and S. Laaribi. Estimation of time-varying kernel densities and chronology
of the impact of COVID-19 on financial markets. _Journal of applied statistics_, 51(11):2157–
2177, 2024.


[34] M. Garcin, K. Sawaya, and T. Valade. Prediction of linear fractional stable motions using
codifference, with application to non-Gaussian rough volatility. _Preprint_, 2025.


[35] J. Gatheral, T. Jaisson, and M. Rosenbaum. Volatility is rough. _Quantitative finance_,
18(6):933–949, 2018.


[36] A.L. Gibbs and F.E. Su. On choosing and bounding probability metrics. _International statis-_
_tical review_, 70(3):419–435, 2002.


[37] V. Glinskiy, A. Logachov, O. Logachova, H. Rojas, L. Serga, and A. Yambartsev. Asymptotic
properties of a statistical estimator of the Jeffreys divergence: the case of discrete distributions.
_Mathematics_, 12(21):3319, 2024.


[38] F. G¨otze and A.N. Tikhomirov. Asymptotic expansions in non-central limit theorems for
quadratic forms. _Journal of theoretical probability_, 18(4):757–811, 2005.


[39] F. G¨otze and V.V. Ulyanov. Asymptotic distribution of _χ_ [2] -type statistics. _Preprint_, 2003.


[40] F. G¨otze and A.Y. Zaitsev. Uniform rates of approximation by short asymptotic expansions
in the CLT for quadratic forms. _Journal of mathematical sciences_, 176(2):162–189, 2011.


[41] F. G¨otze and A.Y. Zaitsev. Explicit rates of approximation in the CLT for quadratic forms.
_Annals of probability_, 42(1):354–397, 2014.


[42] A. Hamadouche, A. Kouadri, and A. Bakdi. A modified Kullback divergence for direct fault
detection in large scale systems. _Journal of process control_, 59:28–36, 2017.


[43] A. Harvey and V. Oryshchenko. Kernel density estimation for time series data. _International_
_journal of forecasting_, 28(1):3–14, 2012.


[44] K. Haynes, P. Fearnhead, and I.A. Eckley. A computationally efficient nonparametric approach
for changepoint detection. _Statistics and computing_, 27(5):1293–1305, 2017.


[45] C.M. Hurvich and C.-L. Tsai. Regression and time series model selection in small samples.
_Biometrika_, 76(2):297–307, 1989.


[46] C. Inclan and G.C. Tiao. Use of cumulative sums of squares for retrospective detection of
changes of variance. _Journal of the American statistical association_, 89(427):913–923, 1994.


[47] B. James, K.L. James, and D. Siegmund. Tests for a change-point. _Biometrika_, 74(1):71–83,
1987.


[48] R.H. Jones and I. Dey. Determining one or more change points. _Chemistry and physics of_
_lipids_, 76(1):1–6, 1995.


[49] M. Kelbert. Survey of distances between the most popular distributions. _Analytics_, 2(1):225–
245, 2023.


[50] P. Kokoszka and R. Leipus. Change-point estimation in ARCH models. _Bernoulli_, 6(6):513–
539, 2000.


27


[51] M. Lavielle and E. Moulines. Least-squares estimation of an unknown number of shifts in a
time series. _Journal of time series analysis_, 21(1):33–59, 2000.


[52] S. Liu, M. Yamada, N. Collier, and M. Sugiyama. Change-point detection in time-series data
by relative density-ratio estimation. _Neural networks_, 43:72–83, 2013.


[53] Z.A. Lomnicki and S.K. Zaremba. The asymptotic distributions of estimators of the amount
of transmitted information. _Information and control_, 2(3):260–284, 1959.


[54] J. Mardia, J. Jiao, E. T´anczos, R.D. Nowak, and T. Weissman. Concentration inequalities for
the empirical distribution of discrete distributions: beyond the method of types. _Information_
_and inference: a journal of the IMA_, 9(4):813–850, 2020.


[55] M. Marinescu and C. Balcau. On the use of mutual information for testing independence.
_Preprint_, 2025.


[56] E.S. Page. Continuous inspection schemes. _Biometrika_, 41(1/2):100–115, 1954.


[57] I. Pinelis. On the nonuniform Berry–Esseen bound. In _Inequalities and extremal problems in_
_probability and statistics_, pages 103–138. Elsevier, 2017.


[58] I. Pinelis and R. Molzon. Optimal-order bounds on the rate of convergence to normality in
the multivariate delta method. _Electronic journal of statistics_, 10:1001–1063, 2016.


[59] J. Plasse and N.M. Adams. Multiple changepoint detection in categorical data streams. _Statis-_
_tics and computing_, 29(5):1109–1125, 2019.


[60] M. Raiˇc. A multivariate Berry–Esseen theorem with explicit constants. _Bernoulli_,
25(4A):2824–2853, 2019.


[61] J. Reeves, J. Chen, X.L. Wang, R. Lund, and Q.Q. Lu. A review and comparison of changepoint detection techniques for climate data. _Journal of applied meteorology and climatology_,
46(6):900–915, 2007.


[62] I. Sason and S. Verd´u. Upper bounds on the relative entropy and R´enyi divergence as a
function of total variation distance for finite alphabets. In _2015 IEEE information theory_
_workshop-fall_, pages 214–218. IEEE, 2015.


[63] I. Sason and S. Verd´u. _f_ -divergence inequalities. _IEEE transactions on information theory_,
62(11):5973–6006, 2016.


[64] Q.-M. Shao and Z.-S. Zhang. Berry–Esseen bounds for multivariate nonlinear statistics
with applications to M-estimators and stochastic gradient descent algorithms. _Bernoulli_,
28(3):1548–1576, 2022.


[65] X. Shi, C. Beaulieu, R. Killick, and R. Lund. Changepoint detection: An analysis of the
Central England temperature series. _Journal of climate_, 35(19):6329–6342, 2022.


[66] A. Shternshis, P. Mazzarisi, and S. Marmi. Measuring market efficiency: The Shannon entropy
of high-frequency financial time series. _Chaos, solitons & fractals_, 162:112403, 2022.


[67] C. Truong, L. Oudre, and N. Vayatis. Selective review of offline change point detection
methods. _Signal processing_, 167:107299, 2020.


[68] A.B. Tsybakov. _Introduction to nonparametric estimation_ . Springer science & business media,
2008.


28


[69] P. Wang and W. Ning. Nonparametric CUSUM change-point detection procedures based on
modified empirical likelihood. _Computational statistics_, 40:4991–5021, 2025.


[70] S.S. Wilks. The large-sample distribution of the likelihood ratio for testing composite hypotheses. _Annals of mathematical statistics_, 9(1):60–62, 1938.


[71] P. Wu, J.-F. Muzy, and E. Bacry. From rough to multifractal volatility: The log S-fBm model.
_Physica A: statistical mechanics and its applications_, 604:127919, 2022.


[72] Y. Yu, O.H. Madrid Padilla, D. Wang, and A. Rinaldo. A note on online change point
detection. _Sequential analysis_, 42(4):438–471, 2023.


[73] N. Yuan, M. Ding, Y. Huang, Z. Fu, E. Xoplaki, and J. Luterbacher. On the long-term climate
memory in the surface air temperature records over Antarctica: A nonnegligible factor for
trend evaluation. _Journal of climate_, 28(15):5922–5934, 2015.


[74] Q. Zhu, X. Diao, and C. Wu. Volatility forecast with the regularity modifications. _Finance_
_research letters_, 58:104008, 2023.


[75] Q. Zhu, X. Diao, and C Wu. A test for change points under the roughness of stochastic
volatility: the case of the VIX index. _Applied economics letters_, 32(7):951–959, 2025.


[76] S. Zhu, B. Chen, Z. Chen, and P. Yang. Asymptotically optimal one-and two-sample testing
with kernels. _IEEE transactions on information theory_, 67(4):2074–2092, 2021.


[77] C. Zou, G. Yin, L. Feng, and Z. Wang. Nonparametric maximum likelihood approach to
multiple change-point problems. _Annals of statistics_, 42(3):970–1002, 2014.


[78] A.M. Zubkov. Limit distributions for a statistical estimate of the entropy. _Theory of probability_
_& its applications_, 18(3):611–618, 1974.

### **A A useful lemma for proving Theorem 1**


**Lemma 1.** _The real solutions of the cubic equation ax_ [3] + _x_ [2] _−_ _d_ = 0 _are_





_−_ [2] _[rπ]_

3



2




- 
_−_ 1 _,_ (10)



1

3 _a_




- - 1
2 cos



1 - 27 _a_ 2 _d −_ 2

3 [arccos] 2



_with r ∈{_ 0 _,_ 1 _,_ 2 _}, when d ≤_ 4 _/_ 27 _a_ [2] _, and_





3



1

_−_ _[d]_
27 _a_ [3] [+] 2 _a_ [+]




    
_−_ 4 _d_ + 27 _d_ [2] _a_ [2] 3

+
108 _a_ [4]



1

_−_ _[d]_
27 _a_ [3] [+] 2 _a_ _[−]_



1

_−_ _[d]_
27 _a_ [3] [+] 2




_−_ 4 _d_ + 27 _d_ [2] _a_ [2]



+ 27 _d_ [2] _a_ [2]

_−_ [1]
108 _a_ [4] 3 _a_




~~�~~




~~�~~



3 _a_



_when d >_ 4 _/_ 27 _a_ [2] _. Moreover, when d >_ 0 _and a →_ 0 _, we get three real roots. One diverges to_ + _∞_
_if a <_ 0 _and to −∞_ _if a >_ 0 _. The other two behave like:_
_√_

          - _x_ 0 = _d_ _−_ _[d]_ _[a]_ [ +] _[ O]_ [(] _[a]_ [2][)]



_d_ _−_ _[d]_



_x_ 0 = _√d_ _−_ _[d]_ 2 _[a]_ [ +] _[ O]_ [(] _[a]_ [2][)]

_x_ 1 = _−_ _d −_ _[d]_ _[a]_ [ +] _[ O]_ [(] _[a]_



_d −_ _[d]_




_[d]_ 2 _[a]_ [ +] _[ O]_ [(] _[a]_ [2][)] _[.]_



_Proof._ Defining _y_ by _x_ +1 _/_ 3 _a_, we get the following equation in _y_ : _y_ [3] + _py_ + _q_ = 0, where _p_ = _−_ 1 _/_ 3 _a_ [2]

and _q_ = (2 _/_ 27 _a_ [3] ) _−_ ( _d/a_ ). The discriminant _−_ ( _p/_ 3) [3] _−_ ( _q/_ 2) [2] is equal to (4 _d −_ 27 _d_ [2] _a_ [2] ) _/_ 108 _a_ [4] .


29


There are two cases, depending on the value of _d_ . First, if _d ≤_ 4 _/_ 27 _a_ [2], the discriminant is nonnegative and there are three real roots (given below for the equation in _x_ ), among which one is
double when the discriminant is zero, after Cardano’s formula,



2




- 
_−_ 1 _,_



1

3 _a_




- - 1
2 cos



1 - 27 _a_ 2 _d −_ 2

3 [arccos] 2





_−_ [2] _[rπ]_

3





_−_ [2] _[rπ]_



where _r ∈{_ 0 _,_ 1 _,_ 2 _}_ . If _d >_ 4 _/_ 27 _a_ [2], the discriminant is non-negative and there is only one real root
for the equation in _x_,





3




_−_ _[q]_

2 [+]




_−_ _[q]_

2 _[−]_




~~�~~ 2 ~~�~~ _p_
+

3




 
~~�~~ 3 3
+




~~�~~ 3

_−_ [1]

3 _a_ _[,]_



~~��~~ _q_


2



~~��~~ _q_


2




~~�~~ 2 ~~�~~ _p_
+

3




~~�~~ 2 ~~�~~ _p_
+



that is, when replacing _p_ and _q_ by their expression in _a_ and _d_,





3



1

_−_ _[d]_
27 _a_ [3] [+] 2 _a_ [+]



1

_−_ _[d]_
27 _a_ [3] [+] 2 _a_ _[−]_




    
_−_ 4 _d_ + 27 _d_ [2] _a_ [2] 3

+
108 _a_ [4]




_−_ 4 _d_ + 27 _d_ [2] _a_ [2]



+ 27 _d_ [2] _a_ [2]

_−_ [1]
108 _a_ [4] 3



3 _a_ _[.]_




~~�~~




~~�~~



When _a →_ 0, the leading term in the discriminant is _d/_ 27 _a_ [4], which is positive if _d >_ 0. Therefore,
the solutions are like in equation (10). We easily get the limit



1 - 27 _a_ 2 _d −_ 2

3 [arccos] 2





_−_ [2] _[rπ]_

3




   - 1
lim
_a→_ 0 [2 cos]



2




- - 0 if _r ∈{_ 0 _,_ 1 _}_

_−_ 1 =

_−_ 3 if _r_ = 2 _._



In this asymptotic case, the root with _r_ = 2 will diverge to + _∞_ if _a <_ 0 and to _−∞_ if _a >_ 0.
For the two other roots, we apply a perturbative expansion. We st _√_ art with t _√_ he simplified problem
corresponding to _a_ = 0, that is _x_ [2] _−_ _d_ = 0. The two solutions are _d_ and _−_ _d_ . Next, we consider

the perturbation of the two roots:
_√_

          - _x_ 0 = _d_ + _β_ 0 _a_ + _O_ ( _a_ [2] )



_√_
_d_ and _−_



_x_ 0 = _√d_ + _β_ 0 _a_ + _O_ ( _a_ [2] )

_x_ 1 = _−_ _d_ + _β_ 1 _a_ + _O_ ( _a_ [2]



_d_ + _β_ 1 _a_ + _O_ ( _a_ [2] ) _._



Plugging _x_ 0 in the cubic equation, we get


_ad_ [3] _[/]_ [2] + 2 _d_ [1] _[/]_ [2] _β_ 0 _a_ = _O_ ( _a_ [2] ) _,_


so that _β_ 0 = _−d/_ 2. Using the same approach for _x_ 1, we find as well that _β_ 1 = _−d/_ 2. We therefore
get the result displayed in the lemma.

### **B Proof of Theorem 1**


The proof of Theorem 1 uses Lemma 1, which is provided in Appendix A.


_Proof._ The observations _Xj_ being independent of each other, the random vector _np_ - _n_ is a multinomial variable of parameter _p_ . The multivariate central limit theorem thus gives
_√n_ ( _p_          - _n −_ _p_ ) _−→N_ d �0 _,_ diag( _p_ ) _−_ _pp_ _[t]_ [�] _,_

where _N_ ( _µ, σ_ [2] ) is the Gaussian distribution of mean _µ_ and variance _σ_ [2] . As a consequence,






 _−→N_ d (0 _,_ Γ _k_ ) _,_ (11)



30



_p_ - _n,_ ~~_√_~~ 1 _−p_ 1
~~_p_~~ 1
...
_p_ - _n,k_ ~~_√_~~ _−pk_
~~_p_~~ _k_



_√_
_n_











where Γ _k_ = _Ik −_ _uu_ _[t]_, with _Ik_ the identity matrix and _u_ = ( _[√]_ ~~_p_~~ 1 ~~_,_~~ _...,_ _[√]_ ~~_p_~~ _k_ ~~)~~ _[t]_ . We now need the
eigenspaces of Γ _k_ . Since Γ _k_ is real and symmetric, its eigenvectors form an orthogonal basis. First,
_u_ is an eigenvector associated with the eigenvalue 0:



_k_

- _pi_


_i_ =1





_u_ = 0 _._



Γ _ku_ = _u −_ _uu_ _[t]_ _u_ = (1 _−_ _u_ _[t]_ _u_ ) _u_ =





1 _−_



Let’s consider any vector _v_ orthogonal to _u_, that is _u_ _[t]_ _v_ = 0. The space of all possible vectors
_v_ is thus of dimension _k −_ 1. Then, from the orthogonality condition, we easily get Γ _kv_ = _v_ .
Therefore, the eigenspace of Γ _k_ associated with the eigenvalue 1 is of dimension _k −_ 1, which is
also the rank of the matrix Γ _k_ . The nonzero eigenvalues of Γ _k_ being all equal to 1, we have the
reduced spectral decomposition Γ _k_ = _V Ik−_ 1 _V_ _[t]_ = _V V_ _[t]_, where _V ∈_ R _[k][×]_ [(] _[k][−]_ [1)] is a matrix whose
columns are orthonormal unit-eigenvalue eigenvectors, so that _V_ _[t]_ _V_ = _Ik−_ 1.


Let us now focus on the empirical relative entropy, _f_ ( _p_ - _n_ ), written as a function of � _pn_, where
_f_ ( _q_ ) = _D_ KL ( _q∥p_ ). The function _f_ can be decomposed in a sum of _k_ univariate functions: _f_ ( _q_ ) =

- _ki_ =1 _[f][i]_ [(] _[q][i]_ [), where] _[ f][i]_ [(] _[q][i]_ [) =] _[ q][i]_ [ log(] _[q][i][/p][i]_ [),] _[ f]_ _i_ _[ ′]_ [(] _[q][i]_ [) = 1 + log(] _[q][i][/p][i]_ [), and] _[ f]_ _i_ [ (] _[j]_ [)] ( _qi_ ) = ( _−_ 1) _[j]_ ( _j −_ 2)! _qi_ [1] _[−][j]_
for _j ≥_ 2. This decomposition eases the second-order Taylor expansion of _f_, which can be seen as
the sum of _k_ distinct expansions, and where we also note that _fi_ ( _pi_ ) = 0 and _fi_ _[′]_ [(] _[p][i]_ [) = 1:]



_k_

- _Ri_ ( _p_ - _n,i_ ) _,_ (12)

_i_ =1



_f_ ( _p_ - _n_ ) = [1] 2



_k_



_i_ =1



( _p_ - _n,i −_ _pi_ ) [2] +

_pi_



with _Ri_ ( _p_ - _n,i_ ) = _fi_ [(3)] ( _ξn,i_ )( _p_ - _n,i −_ _pi_ ) [3] _/_ 6 = ( _pi −_ _p_ - _n,i_ ) [3] _/_ 6 _ξn,i_ [2] [, where] _[ ξ][n,i]_ [ is in the interval delimited]
by � _pn,i_ and _pi_ .

From the decomposition Γ _k_ = _V V_ _[t]_, we can write the limit appearing in formula (11) as _V G_, where
_G ∈_ R _[k][−]_ [1] is a standard Gaussian vector. Then,



_k_



_i_ =1



_n_ ( _p_ - _n,i −_ _pi_ ) [2]

_pi_



_−→_ d _G_ _[t]_ _V_ _[t]_ _V G_ = _G_ _[t]_ _G,_



using the orthonormal property of _V_ . This limit follows a chi-square distribution _χ_ [2] _k−_ 1 [. Moreover,]

since � _pn,i_ _−→_ P _pi_, we have _ξn,i_ _−→_ P _pi_ and, by the continuous mapping theorem, ( _pi_ _−p_ - _n,i_ ) _/_ 6 _ξn,i_ [2] _−→_ P

0, because _pi ̸_ = 0. Therefore, _nRi_ ( _p_ - _n,i_ ) = _n_ ( _pi −_ _p_ - _n,i_ ) [2] ( _pi −_ _p_ - _n,i_ ) _/_ 6 _ξn,i_ [2] _−→_ P 0, since it is a product
of a sequence converging in distribution with a sequence converging in probability to zero. Finally,
starting from equation (12), we see that 2 _nf_ ( _p_ - _n_ ) is a sum of a sequence converging in distribution
to _χ_ [2] _k−_ 1 [with] _[ k]_ [ sequences converging in probability to zero.] Slutsky’s theorem thus leads to
equation (2).


We now have to prove the second part of the theorem. We’re going to use a multivariate extension
of Berry-Esseen theorem [60] and apply it first to the quadratic part of _f_ : the rest, namely the _Ri_
functions, will be considered in a second stage. We define _Y_ 1, ..., _Yn_, iid vectors of R _[k]_, by



_Yj_ = - 1 _Xj_ ~~_√_~~ _∈_ Ω1 _−_ _p_ 1 _, ...,_ [1] _[X][j]_ ~~_√_~~ _[∈]_ [Ω] _[k]_ _[−]_ _[p][k]_
~~_np_~~ 1 ~~_np_~~ _k_




- _t_
_._



It is easy to see that E( _Yj_ ) = 0. We will need the expression of the third moment of the Euclidean


31


norm of _Yj_ :



�3 _/_ 2 [�]



E �� _Yj_ _[t][Y][j]_ �3 _/_ 2 [�] = E



��� _k_ (1 _Xj_ _∈_ Ω _i_ _−pi_ ) [2]
_i_ =1 _npi_



�3 _/_ 2



= - _ku_ =1 [P][(] _[X][j][ ∈]_ [Ω] _[u]_ [)] - (1 _−nppu_ ) [2]



_p_ [2] _i_
_i_ = _u_ _npi_



(13)



_npuu_ + [�]



= _n_ [3] 1 _[/]_ [2] - _ku_ =1 _[p][u]_ - (1 _−ppuu_ ) [2] + 1 _−_ _pu_ �3 _/_ 2



= _n_ [3] 1 _[/]_ [2] - _ku_ =1 (1 _−pp_ [1] _uu_ _[/]_ [2] ) [3] _[/]_ [2] _._



We also define _W_ = [�] _j_ _[n]_ =1 _[Y][j]_ [, whose] _[ i]_ [-th component, for] _[ i][ ∈]_ [�][1] _[, k]_ [�][, is (] _[p]_ [�] _[n,i][ −]_ _[p][i]_ [)(] _[n/p][i]_ [)][1] _[/]_ [2][, and]



Θ = _W_ _[t]_ _W_ =



_k_



_i_ =1



_n_ ( _p_ - _n,i −_ _pi_ ) [2] _._

_pi_



The properties of multinomial variables indicate that the covariance matrix of the vector _W_ is
Γ _k_ = _V V_ _[t]_, whose rank is _k −_ 1 as explained above. Therefore, we can decompose _W_ in an
orthonormal basis of dimension _k −_ 1: we define a vector _U ∈_ R _[k][−]_ [1] as _V_ _[t]_ _W_, so that _W_ = _V U_ and
Θ = _U_ _[t]_ _U_ . Then, it appears that _U_ = [�] _j_ _[n]_ =1 _[T][j]_ [, where] _[ T][j]_ [ =] _[ V][ t][Y][j]_ [. By linearity,][ E][(] _[T][j]_ [) = 0 and,]
using the independence of _Yj_ and _Yℓ_ for _ℓ_ = _j_, we have as well

      - _nj_ =1 [E][(] _[T][j][T][ t]_ _j_ [)] =       - _nj_ =1 _[V][ t]_ [E][(] _[Y][j][Y]_ _j_ _[ t]_ [)] _[V]_
= _V_ _[t]_ E( _WW_ _[t]_ ) _V_
= _V_ _[t]_ Γ _kV_
= _V_ _[t]_ _V V_ _[t]_ _V_ = _Ik−_ 1 _._


These conditions on _U_ and _Tj_ are the ones that are required for Raiˇc’s multivariate Berry-Esseen
theorem [60, Theorem 1.1]. For a proper application of this theorem, two other assumptions
are still to be verified. First, because _Yj_ = _V Tj_, _Yj_ and _Tj_ have the same Euclidean norm,

_Yj_ _[t][Y][j]_ [ =] _[ T][ t]_ _j_ _[V][ t][V T][j]_ [ =] _[ T][ t]_ _j_ _[T][j]_ [, so that][ E] �� _Tj_ _[t][T][j]_ �3 _/_ 2 [�] = [�] _u_ _[k]_ =1 [(1] _[ −]_ _[p][u]_ [)][3] _[/]_ [2] _[/n]_ [3] _[/]_ [2] _[p]_ _u_ [1] _[/]_ [2] after equation (13).

Second, the set _Ak−_ 1 _,x_ = _{Z ∈_ R _[k][−]_ [1] _|Z_ _[t]_ _Z ≤_ _x}_ is convex because it is the sublevel set of a convex
function. We now have all the conditions for applying Raiˇc’s theorem [60, Theorem 1.1]:
���P (Θ _≤_ _x_ ) _−_ _Fχ_ 2 _k−_ 1 [(] _[x]_ [)] ��� = _|_ P ( _U ∈Ak−_ 1 _,x_ ) _−_ P( _G ∈Ak−_ 1 _,x_ ) _|_



_≤_ �42( _k −_ 1) [1] _[/]_ [4] + 16�� _nj_ =1 [E] �� _Tj_ _[t][T][j]_ �3 _/_ 2 [�]



(14)



_≤_ �42( _k −_ 1) [1] _[/]_ [4] + 16�� _ku_ =1 (1( _np−puu_ )) [1][3] _[/][/]_ [2][2] _[,]_



where _G ∈_ R _[k][−]_ [1] is a standard Gaussian vector.

We now adapt the above Berry-Esseen inequality to include the residual _R_ = 2 _n_ [�] _i_ _[k]_ =1 _[R][i]_ [(] _[p]_ [�] _[n,i]_ [) of]
2 _nf_ ( _p_ - _n_ ), given by the Taylor expansion displayed in equation 12. If � _pn,i ≥_ _pi_, then _ξn,i ≥_ _pi_ and
we simply have _|Ri_ ( _p_ - _n,i_ ) _| ≤|pi −_ _p_ - _n,i|_ [3] _/_ 6 _p_ [2] _i_ [. If][ �] _[p][n,i][ < p][i]_ [, we cannot properly bound the residual]
of the second-order Taylor expansion with the same method and we need a more precise analysis.
The non-truncated Taylor series provides, for _x ∈_ [0 _, p_ ],



_∞_
_Ri_ ( _x_ ) = ( _x −_ _pi_ ) [3]             

_j_ =3


Noting _hi_ ( _x_ ) = _Ri_ ( _x_ ) _/_ ( _x −_ _pi_ ) [3], we have



( _−_ 1) _[j]_ ( _x −_ _pi_ ) _[j][−]_ [3]

_._
_j_ ( _j −_ 1) _p_ _[j]_ _i_ _[−]_ [1]



( _pi −_ _x_ ) _[j][−]_ [2] ( _j −_ 3)

_,_
_j_ ( _j −_ 1) _p_ _[j]_ _i_ _[−]_ [1]


32



_h_ _[′]_ _i_ [(] _[x]_ [) =]



_∞_



_j_ =3


which is non-negative whatever _x ≤_ _pi_ . Since we also have _hi_ ( _pi_ ) = _−_ 1 _/_ 6 _p_ [2] _i_ [and] _[ h][i]_ [(0) =] _[ −]_ [1] _[/]_ [2] _[p]_ _i_ [2][,]
we finally get max _x∈_ [0 _,pi_ ] _|hi_ ( _x_ ) _|_ = 1 _/_ 2 _p_ [2] _i_ [. Combining this result with our analysis for] _[ x][ ≥]_ _[p][i]_ [, we]
find that 1 _/_ 2 _p_ [2] _i_ [is an upper bound for] _[ |][h]_ [(] _[x]_ [)] _[|]_ [ whatever] _[ x][ ∈]_ [[0] _[,]_ [ 1]. Then, noting that] _[ L][q]_ [ norms are]
non-increasing functions of _q_, and writing _µ_ = min _i∈_ 1 _,k_ _pi_, we get

                                   -                                   


= ~~_√_~~ [Θ][3] _[/]_ [2] _._

~~_µn_~~




- _k_

 

_i_ =1



2 [�][3] _[/]_ [2]



_|R| ≤_ 2 _n_



_k_



_i_ =1



_|pi −_ _p_ - _n,i|_ [3] _≤_ ~~_√_~~ _n_

2 _p_ [2] _i_ ~~_µ_~~



_pi −_ ~~_√_~~ _p_ - _n,i_
���� ~~_p_~~ _i_ ����



Since 2 _nf_ ( _p_ - _n_ ) = Θ + _R_ and (Θ _≤_ _x −_ _ρ_ ) _⇒_ (Θ + _R ≤_ _x_ ) _⇒_ (Θ _≤_ _x_ + _ρ_ ) for _ρ ≥|R|_, we have

P        - _Kn,k_ [down][(Θ)] _[ ≤]_ _[x]_        - _≤_ P (2 _nf_ ( _p_        - _n_ ) _≤_ _x_ ) _≤_ P        - _Kn,k_ [up] [(Θ)] _[ ≤]_ _[x]_        - _,_ (15)


where _Kn,k_ [up] [:] _[ z][ ≥]_ [0] _[ �→]_ _[z][ −]_ [(] _[µn]_ [)] _[−]_ [1] _[/]_ [2] _[z]_ [3] _[/]_ [2][ and] _[ K]_ _n,k_ [down] : _z ≥_ 0 _�→_ _z_ + ( _µn_ ) _[−]_ [1] _[/]_ [2] _z_ [3] _[/]_ [2] . Also, because
of the convexity of _f_ [21, Theorem 2.7.2], we have the convexity of the set _{q|_ 2 _nf_ ( _q_ ) _≤_ _x}_ . So
we can refine formula (15) by taking into account a constraint of convexity, that is the three
sets in this formula have to be convex. Noting that _Kn,k_ [up] [(0) =] _[ K]_ _n,k_ [down][(0) = 0] _[ < x]_ [ and that]
(Θ = 0) _⇒_ ( _∀i,_ - _pn,i_ = _pi_ ) _⇒_ (2 _nf_ ( _p_ - _n_ ) = 0 _< x_ ), 0 belongs to the three intervals of admissible
values of Θ and the convexity constraint modifies inequalities (15) in


P �Θ _∈_ [0 _, κ_ [down] _n,k_ [(] _[x]_ [)]]       - _≤_ P (2 _nf_ ( _p_       - _n_ ) _≤_ _x_ ) _≤_ P �Θ _∈_ [0 _, κ_ [up] _n,k_ [(] _[x]_ [)]]       - _,_



where _κ_ [up] _n,k_ [(] _[x]_ [) (respectively] _[ κ]_ _n,k_ [down][(] _[x]_ [)) is the smallest positive root of] _[ z][ �→]_ _[K]_ _n,k_ [up] [(] _[z]_ [)] _[ −]_ _[x]_ [ (resp.]
_z �→_ _Kn,k_ [down][(] _[z]_ [)] _[ −]_ _[x]_ [) if it exists, otherwise the probabilities are trivially equal to 1. An explicit]
expression for _κ_ [up] _n,k_ [(] _[x]_ [) and] _[ κ]_ _n,k_ [down][(] _[x]_ [) is obtained as the solution of a cubic equation, solved with]
Cardano’s formula. Indeed, using Lemma 1 applied to _[√]_ ~~_z_~~ ~~,~~ with _d_ = _x_ and _a_ = _−_ ( _µn_ ) _[−]_ [1] _[/]_ [2] for
_κ_ [up] _n,k_ [(] _[x]_ [) or] _[ a]_ [ = (] _[µn]_ [)] _[−]_ [1] _[/]_ [2][ for] _[ κ]_ _n,k_ [down][(] _[x]_ [), we directly obtain the expression displayed in Theorem 1.]
We also have ( _µn_ ) _[−]_ [1] _[/]_ [2] _→_ 0 and Lemma 1 provides us with the asymptotic behaviour of _κ_ [up] _n,k_ [(] _[x]_ [),]
namely

       - _x_       - 1       


_x_        - 1
_κ_ [up] _n,k_ [(] _[x]_ [) =] _[ √][x]_ [ +]
2 ~~_[√]_~~ ~~_µn_~~ [+] _[ O]_ _n_




_,_



_n_



which gives



_κ_ [up] _n,k_ [(] _[x]_ [) =] _[ x]_ [ +] ~~_√_~~ _[x]_ [3] _[/]_ [2]



_n_




   - 1
~~_√_~~ _[x]_ [3] _[/]_ [2] _O_
~~_µn_~~ + _n_




_._



A similar idea makes it possible to lead to the result displayed in Theorem 1 for _κ_ [down] _n,k_ [(] _[x]_ [).]


Finally, we can use Raiˇc’s theorem again, using directly formula (14) with a new value for _x_ :


             -              P (2 _nf_ ( _p_       - _n_ ) _≤_ _x_ ) _≤_ P Θ _≤_ _κ_ [up] _n,k_ [(] _[x]_ [)]

                    -                    _≤_ _Fχ_ 2 _k−_ 1 [(] _[κ]_ _n,k_ [up] [(] _[x]_ [)) +] ���P Θ _≤_ _κ_ [up] _n,k_ [(] _[x]_ [)] _−_ _Fχ_ 2 _k−_ 1 [(] _[κ]_ _n,k_ [up] [(] _[x]_ [))] ���

_≤_ _Fχ_ 2 _k−_ 1 [(] _[κ]_ _n,k_ [up] [(] _[x]_ [)) +] �42( _k −_ 1) [1] _[/]_ [4] + 16�� _ku_ =1 (1( _np−puu_ )) [1][3] _[/][/]_ [2][2] _[.]_


We similarly get


             -             P (2 _nf_ ( _p_      - _n_ ) _≤_ _x_ ) _≥_ P Θ _≤_ _κ_ [down] _n,k_ [(] _[x]_ [)]

                    -                    _≥_ _Fχ_ 2 _k−_ 1 [(] _[κ]_ _n,k_ [down][(] _[x]_ [))] _[ −]_ ���P Θ _≤_ _κ_ [down] _n,k_ [(] _[x]_ [)] _−_ _Fχ_ 2 _k−_ 1 [(] _[κ]_ _n,k_ [up] [(] _[x]_ [))] ���


and we can now conclude with the statement of the theorem.


33


### **C Proof of Theorem 2 and Proposition 1**

We start by the proof of Theorem 2.


_Proof._ As seen in the proof of Theorem 1, the vector whose _i_ -th component, for _i ∈_ 1 _, k_, is
_√_ ~~_n_~~ ~~(~~ _p_ - _n,i −_ _pi_ ) _/√_ ~~_p_~~ _i_ converges toward a Gaussian vector _G_ 1 of covariance matrix Γ _k_ . We have the� same result when considering _[√]_ ~~_m_~~ ( _q_ - _m,i_ _−pi_ ) _/_ _[√]_ ~~_p_~~ _i_ ~~,~~ with the convergence toward the Gaussian vector
_G_ 2 of covariance matrix Γ _k_ and independent of _G_ 1. Therefore, we get, for _n, m →∞_,



d _√_
_−→_



_√_
1 _−_ _λG_ 1 _−_



_p_ - _n,_ 1 ~~_√_~~ _−q_ - _m,_ 1
~~_p_~~ 1
...
_p_ - _n,k_ ~~_√_~~ _−q_ - _m,k_
~~_p_~~ _k_





 = _√_ ~~_n_~~ ~~�~~




_p_ - _n,_ ~~_√_~~ 1 _−p_ 1
~~_p_~~ 1
...
_p_ - _n,k_ ~~_√_~~ _−pk_
~~_p_~~ _k_






 
 _[−√]_ ~~_[m]_~~



_q_ - _m,_ ~~_√_~~ 1 _−p_ 1
~~_p_~~ 1
...
_q_ - _m,k_ ~~_√_~~ _−pk_
~~_p_~~ _k_
















_nm_
_n_ + _m_












_m_
_n_ + _m_












_n_
_n_ + _m_












_λG_ 2 _,_



which, by independence, is a Gaussian vector of covariance matrix Γ _k_ .


The successive derivatives of the relative entropy are _∂piDKL_ ( _p∥q_ ) = 1+log( _pi/qi_ ), _∂qiDKL_ ( _p∥q_ ) =
_−pi/qi_, _∂p_ [2] _ipi_ _[D][KL]_ [(] _[p][∥][q]_ [) = 1] _[/p][i]_ [,] _[ ∂]_ _q_ [2] _iqi_ _[D][KL]_ [(] _[p][∥][q]_ [) =] _[ p][i][/q]_ _i_ [2][, and] _[ ∂]_ _p_ [2] _iqi_ _[D][KL]_ [(] _[p][∥][q]_ [) =] _[ −]_ [1] _[/q][i]_ [. Therefore,]
noting _δn,i_ = � _pn,i −_ _pi_ and _γm,i_ = � _qm,i −_ _pi_, a second-order Taylor expansion gives



_DKL_ ( _p_ - _n∥q_ - _m_ ) = - _ki_ =1 - _δn,i −_ _γm,i_ + _δ_ 2 _n,i_ [2] _pi_ [+] _γ_ 2 _m,i_ [2] _pi_ _[−]_ _[δ][n,i]_ _p_ _[γ]_ _i_ _[m,i]_




_[γ][m,i]_ - - [�]

_pi_ + _o_ _δn,i_ [2] [+] _[ γ]_ _m,i_ [2]



= - _ki_ =1 ( _δn,i−_ 2 _pγim,i_ ) [2]

= - _ki_ =1 ( _p_ - _n,i−_ 2 _pq_ - _m,i_ ) [2]



2 _pim,i_ + _o_ - _δn,i_ [2] [+] _[ γ]_ _m,i_ [2] 


2 _p_ - _im,i_ + _o_ - _δn,i_ [2] [+] _[ γ]_ _m,i_ [2] - _._



The leading term of this expansion is proportional to the Euclidean norm of the vector whose limit
it the above Gaussian of covariance Γ _k_ . The framework is so exactly the same as in the proof of
Theorem 1 and we can conclude that


d

2 _[nm]_ _−→_ _χ_ [2] _k−_ 1 _[.]_

_n_ + _m_ _[D][KL]_ [(] _[p]_ [�] _[n][∥][q]_ [�] _[m]_ [)]



We now prove Proposition 1.


_Proof._ We define, for _j ∈_ 1 _, n_ + _m_,

              -               


( _n_ + _m_ ) _pk_




- _t_



_Yj_ =




1 _Xj_ _∈_ Ω1 _−_ _p_ 1

~~�~~



_j_ _∈_ Ω1 _−_ _p_ 1 _, ...,_ [1] _[X][j]_ _[∈]_ [Ω] _[k]_ _[−]_ _[p][k]_

( _n_ + _m_ ) _p_ 1 ~~�~~ ( _n_ + _m_ ) _pk_



and
�� ~~_m_~~
_Y_           - _j_ = ~~[�]~~ _n_




_−_ ~~[�]~~ _n_ _[Y]_ ~~_[n]_~~ _[j][Y][j]_ ifif _j j ∈ ∈_ �1 _n, n_ + 1�



_m_ _[Y][j]_ if _j ∈_ - _n_ + 1 _, n_ + _m_ - _._



Then, _Y_ [�] 1 _, ...,_ _Y_ [�] _n_ + _m_ are independent of each other and such that, noting _W_ [�] = [�] _j_ _[n]_ =1 [+] _[m]_ _[Y]_ [�] _[j]_ [, we have]

E( _Y_ [�] _j_ ) = 0 and, after Theorem 2, [�] _j_ _[n]_ =1 [+] _[m]_ [E][(] _[Y]_ [�] _[j]_ [ �] _[Y]_ _j_ _[ t]_ [) =][ E][(] _W_ [�] _W_ [�] _[t]_ ) = Γ _k_, with the same Γ _k_ = _V V_ _[t]_ as in
the proof of Theorem 1. Therefore, we can use Raiˇc’s theorem again [60, Theorem 1.1], applied to
_U_ - = _V_ _[t]_ _W_ - and _T_ [�] _j_ = _V_ _[t]_ [ �] _Yj_, _T_ [�] _j_ having the same Euclidean norm as _Y_ [�] _j_ . Noting



_nm_
Θ =� - _W_ _[t]_ _W_ [�] = _n_ + _m_


34



_k_



_i_ =1



( _p_ - _i −_ _q_ - _i_ ) [2] _,_

_pi_


we can thus write, like in the proof of Theorem 1,

       -       -       -       - _[n]_       - [+] _[m]_ �� �3 _/_ 2 [�]
���P Θ� _≤_ _x_ _−_ _Fχ_ 2 _k−_ 1 [(] _[x]_ [)] ��� _≤_ 42( _k −_ 1) [1] _[/]_ [4] + 16 E _Y_               - _j_ _[t][Y]_ [�] _[j]_ _._ (16)

_j_ =1

We know, from equation (13) the expression of E �� _Yj_ _[t][Y][j]_ �3 _/_ 2 [�], from which we deduce that



_m_ [3] _[/]_ [2]  - _k_ (1 _−pu_ ) [3] _[/]_ [2]

_n_ [3] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 _p_ [1] _u_ _[/]_ [2]

_n_ [3] _[/]_ [2]   - _k_ (1 _−pu_ ) [3] _[/]_ [2]

_m_ [3] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 [1] _[/]_ [2]



�� �3 _/_ 2 [�]
E _Y_ - _j_ _[t][Y]_ [�] _[j]_ =











_m_ [3] _[/]_ [2]



_n_ [3] _[/]_ [2]



_u_ if _j ∈_ 1 _, n_

_p_ [1] _u_ _[/]_ [2]

        -         
[3] _[/]_ [2]



_u_ if _j ∈_ _n_ + 1 _, n_ + _m_ _._

_p_ [1] _u_ _[/]_ [2]

        -         


Therefore

   - _nj_ =1+ _m_ [E] �� _Y_   - _j_ _[t][Y]_ [�] _[j]_ �3 _/_ 2 [�] =   - _n_ [1] _[/]_ [2] ( _mn_ [3] + _[/]_ [2]



_n_ [3] _[/]_ [2] �� _k_ (1 _−pu_ ) [3] _[/]_ [2]

_m_ [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 [1] _[/]_ [2]



_m_ [3] _[/]_ [2] _n_ [3] _[/]_ [2]

_n_ [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] [+] _m_ [1] _[/]_ [2] ( _n_ +



_n_ [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _m_ [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 _p_ [1] _u_ _[/]_ [2]

= _m_ [2] + _n_ [2] - _k_ (1 _−pu_ ) [3] _[/]_ [2]

[1] _[/]_ [2] [3] _[/]_ [2] [1] _[/]_ [2]



_m_ [2] + _n_ [2]  - _k_ (1 _−pu_ ) [3] _[/]_ [2]

( _nm_ ) [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 [1] _[/]_ [2]



( _nm_ ) [1] _[/]_ [2] ( _n_ + _m_ ) [3] _[/]_ [2] _u_ =1 _p_ [1] _u_ _[/]_ [2]


and, combining it with equation (16), we finally obtain the result displayed in Proposition 1.


### **D Proof of Proposition 2**

_Proof._ We know that the moment-generating function of _D_ KL ( _p_ - _n∥p_ ) is upper bounded by



_k−_ 1










_M_ ( _t_ ) =



 _n_

 

_j_ =0



_n_ !
_n_ [2] _[j]_ ( _n −_ _j_ )! _[t][j]_



_,_



for _t ∈_ [0 _, n_ ) [2, Proposition II.2, Lemmas II.4 and II.5]. So, using Chernoff inequality, we get
the first bound displayed in Proposition 2, _M_ [1] _k,n_ [(] _[x]_ [).] The last bound, _M_ [3] _k,n_ [(] _[x]_ [), is Agrawal’s]

bound, which relies on the fact that _M_ ( _t_ ) _≤_ _M_ Agrawal( _t_ ) = (1 _−_ _t/n_ ) _[−][k]_ [+1] and on the optimisation
in _t_ of the Chernoff bound inf _t∈_ [0 _,n_ ) _e_ _[−][tx]_ _M_ Agrawal( _t_ ), the minimum being reached for _t_ equal to
_t_ _[⋆]_ = _n −_ ( _k −_ 1) _/x ∈_ [0 _, n_ ) [2, Theorem I.2]. Using this same value _t_ _[⋆]_ in _M_ ( _t_ ), we get the middle
bound, _M_ [2] _k,n_ [(] _[x]_ [), which is higher than] _[ M]_ [1] _k,n_ [(] _[x]_ [) because] _[ t][⋆]_ _[∈]_ [[0] _[, n]_ [), and lower than] _[ M]_ [3] _k,n_ [(] _[x]_ [)]
because _M_ ( _t_ ) _≤_ _M_ Agrawal( _t_ ) for all _t ∈_ [0 _, n_ ), including _t_ _[⋆]_ .

### **E Proofs for two-sample concentration inequalities**


**E.1** **Proof of Proposition 3**


_Proof._ Noting _∥x∥_ 1 = [�] _i_ _[k]_ =1 _[|][x][i][|]_ [, we have, by triangular inequality] _[ ∥][x][ −]_ _[y][∥]_ [1] _[ ≤∥][x][∥]_ [1][ +] _[ ∥][y][∥]_ [1][, so,]
combining this with Young’s inequality, we get

_∥x −_ _y∥_ [2] 1 _[≤]_ [2] _[∥][x][∥]_ 1 [2] [+ 2] _[∥][y][∥]_ 1 [2] _[.]_ (17)


Using successively the right part of formula (8), formula (17), and the left part of formula (8), we
get
_D_ KL ( _p∥q_ ) _≤_     - 2 min1 _i qi_ _[−]_ [min] _[i]_ 2 _pqii_     - _∥p −_ _q∥_ [2] 1



_≤_ - min1 _i qi_ _[−]_ [min] _[i]_ _pqii_



�� _∥p −_ _r∥_ [2] 1 [+] _[ ∥][q][ −]_ _[r][∥]_ 1 [2] 


_≤_ - min2 _i qi_ _[−]_ [2 min] _[i]_ _pqii_


35




( _D_ KL ( _p∥r_ ) + _D_ KL ( _q∥r_ )) _._


**E.2** **Proof of Theorem 3**


_Proof._ Using Proposition 3 along with the independence of � _pn_ and � _qm_, the moment-generating
function of the two-sample relative entropy follows, for _t ≥_ 0:


E [exp ( _tD_ KL ( _p_       - _n∥q_       - _m_ ))] _≤_ E [exp ( _tβm,nD_ KL ( _p_       - _n∥p_ )] E [ _tβm,nD_ KL ( _q_       - _m∥p_ ))] _._


According to Agrawal’s inequality, we thus have, for _t ∈_ [0 _,_ min( _m, n_ ) _/βm,n_ ) [2, Theorem 1.3]:




        - 1
E [exp ( _tD_ KL ( _p_ - _n∥q_ - _m_ ))] _≤_ (1 _−_ _tβm,n/m_ )(1 _−_ _tβm,n/n_ )




- _k−_ 1
_._ (18)



Therefore, by a substitution _s_ = _tβm,n_ and Chernoff inequality, we obtain, for _x >_ 0, P ( _D_ KL ( _p_ - _n∥q_ - _n_ ) _≥_ _x_ ) _≤_
inf _s∈_ [0 _,_ min( _m,n_ )) exp( _f_ ( _s_ )), with




       
_[sx]_ _−_ ( _k −_ 1) log 1 _−_ _[s]_

_βm,n_ _m_




- 
_−_ ( _k −_ 1) log 1 _−_ _[s]_

_n_




- 
_−_ ( _k −_ 1) log 1 _−_ _[s]_




_._



_f_ ( _s_ ) = _−_ _[sx]_



_m_



We have _f_ (0) = 0 and lim _s→_ min( _m,n_ ) _f_ ( _s_ ) = + _∞_ . The derivative of _f_ is



_x_               - 1 1
_f_ _[′]_ ( _s_ ) = _−_ _−_ ( _k −_ 1)
_βm,n_ _s −_ _m_ [+] _s −_ _n_


Noting _λ_ = _x/βm,n_ ( _k −_ 1), _s_ is a zero of _f_ _[′]_ iff




_._



_λs_ [2] + (2 _−_ _λ_ ( _n_ + _m_ )) _s_ + _λmn −_ _m −_ _n_ = 0 _,_



equation whose discriminant is 4 + _λ_ [2] ( _m −_ _n_ ) [2] _>_ 0, leading to the two possible roots



_λ_ ( _n_ + _m_ ) _−_ 2 _−_ _[√]_ 4+ _λ_ [2] ( _m−n_ ) [2]

 _s_ _[⋆]_ =





_λ_ ( _n_ + _m_ ) _−_ 2 _−_ _[√]_

 _s_ _[⋆]_ = 2 _λ_

_λ_ ( _n_ + _m_ ) _−_ 2+ ~~_[√]_~~

 _s_ _[•]_ =



4+ _λ_ [2] ( _m−n_ ) [2]



2 _λ_
_λ_ ( _n_ + _m_ ) _−_ 2+ ~~_[√]_~~
_s_ _[•]_ =



4+ _λ_ [2] ( _m−n_ ) [2]



2 _λ_ _._



By symmetry, one can assume _m ≤_ _n_ . The condition _s_ _[⋆]_ _∈_ [0 _,_ min( _m, n_ )) gives, for the upper
bound,
_λ_ ( _n_ + _m_ ) _−_ 2 _−_ �4 + _λ_ [2] ( _m −_ _n_ ) [2] _<_ 2 _λm,_



By symmetry, one can assume _m ≤_ _n_ . The condition _s_ _[⋆]_ _∈_ [0 _,_ min( _m, n_ )) gives, for the upper
bound,
_λ_ ( _n_ + _m_ ) _−_ 2 _−_ �4 + _λ_ [2] ( _m −_ _n_ ) [2] _<_ 2 _λm,_


that is
_λ_ ( _n −_ _m_ ) _−_ 2 _<_ ~~�~~ 4 + _λ_ [2] ( _m −_ _n_ ) [2] _._ (19)



that is
_λ_ ( _n −_ _m_ ) _−_ 2 _<_ ~~�~~



When _λ_ ( _n−m_ ) _<_ 2, formula (19) always holds. If _λ_ ( _n−m_ ) _≥_ 2, then, after considering the square,
formula (19) simplifies to _−_ 4 _λ_ ( _n_ _−_ _m_ ) _<_ 0, which always holds since _λ_ ( _n_ _−_ _m_ ) _≥_ 2. Now, the lower
constraint _s_ _[⋆]_ _≥_ 0 leads to
_λ_ ( _m_ + _n_ ) _−_ 2 _≥_ ~~�~~ 4 + _λ_ [2] ( _m −_ _n_ ) [2] _,_


that is _λ_ ( _m_ + _n_ ) _≥_ 2 and _λ ≥_ ( _m_ + _n_ ) _/mn_ . A direct calculation also shows that ( _m_ + _n_ ) _/mn >_
2 _/_ ( _m_ + _n_ ). On the other hand, the root _s_ _[•]_ never falls in the interval [0 _,_ min( _m, n_ )). Indeed, still
assuming _m ≤_ _n_, the condition _s_ _[•]_ _< m_ requires



_λ_ ( _n −_ _m_ ) _−_ 2 _< −_ 


4 + _λ_ [2] ( _m −_ _n_ ) [2] _,_ (20)



which is never verified because, by the elementary inequality, 2(4 + _λ_ [2] ( _m −_ _n_ ) [2] ) _≥_ (2 + _λ|m −_ _n|_ ) [2],
so that condition (20) writes


_λ_ ( _n −_ _m_ ) _−_ 2 _< −_ ~~_√_~~ [1]

2 [(2 +] _[ λ][|][m][ −]_ _[n][|]_ [)]


36


_√_
and _λ_ ( _n −_ _m_ ) _<_



_√_
2(2 _−_



_√_
2) _/_ (1 +



2) _<_ 0, which contradicts the assumptions.



Therefore, when _λ ≥_ max(2 _/_ ( _m_ + _n_ ) _,_ ( _m_ + _n_ ) _/mn_ ) = ( _m_ + _n_ ) _/mn_, then _f_ _[′]_ (0) _≤_ 0 and the minimum
of _f_ in the interval [0 _,_ min( _m, n_ )) is reached in _s_ _[⋆]_ . If _λ <_ ( _m_ + _n_ ) _/mn_, then _f_ _[′]_ (0) _>_ 0, with no
root of _f_ _[′]_ in [0 _,_ min( _m, n_ )), so that the minimum of _f_ in this interval is reached in 0. This provides
us with the bound _M_ [�][3] _k,n,m_ [(] _[x]_ [), in which] _[ σ][m,n,x]_ [ is simply equal to] _[ s][⋆]_ [.]


Like in Proposition 2, one can also modify formula (18) to use the true moment-generating function,
so we get _M_ [�][1] _k,n,m_ [(] _[x]_ [) and][ �] _M_ [2] _k,n,m_ [(] _[x]_ [).]


37


